id,updated,published,title,summary,database,query_name,query_value
http://arxiv.org/abs/2407.08710v1,2024-07-11T17:47:31Z,2024-07-11T17:47:31Z,"end-to-end orchestration of nextg media services over the distributed
  compute continuum","NextG (5G and beyond) networks, through the increasing integration of
cloud/edge computing technologies, are becoming highly distributed compute
platforms ideally suited to host emerging resource-intensive and
latency-sensitive applications (e.g., industrial automation, extended reality,
distributed AI). The end-to-end orchestration of such demanding applications,
which involves function/data placement, flow routing, and joint
communication/computation/storage resource allocation, requires new models and
algorithms able to capture: (i) their disaggregated microservice-based
architecture, (ii) their complex processing graph structures, including
multiple-input multiple-output processing stages, and (iii) the opportunities
for efficiently sharing and replicating data streams that may be useful for
multiple functions and/or end users. To this end, we first identify the
technical gaps in existing literature that prevent efficiently addressing the
optimal orchestration of emerging applications described by information-aware
directed acyclic graphs (DAGs). We then leverage the recently proposed Cloud
Network Flow optimization framework and a novel functionally-equivalent
DAG-to-Forest graph transformation procedure to design IDAGO (Information-Aware
DAG Orchestration), a polynomial-time multi-criteria approximation algorithm
for the optimal orchestration of NextG media services over NextG
compute-integrated networks.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2405.05017v1,2024-05-08T12:34:33Z,2024-05-08T12:34:33Z,6g software engineering: a systematic mapping study,"6G will revolutionize the software world allowing faster cellular
communications and a massive number of connected devices. 6G will enable a
shift towards a continuous edge-to-cloud architecture. Current cloud solutions,
where all the data is transferred and computed in the cloud, are not
sustainable in such a large network of devices. Current technologies, including
development methods, software architectures, and orchestration and offloading
systems, still need to be prepared to cope with such requirements. In this
paper, we conduct a Systematic Mapping Study to investigate the current
research status of 6G Software Engineering. Results show that 18 research
papers have been proposed in software process, software architecture,
orchestration and offloading methods. Of these, software architecture and
software-defined networks are respectively areas and topics that have received
the most attention in 6G Software Engineering. In addition, the main types of
results of these papers are methods, architectures, platforms, frameworks and
algorithms. For the five tools/frameworks proposed, they are new and not
currently studied by other researchers. The authors of these findings are
mainly from China, India and Saudi Arabia. The results will enable researchers
and practitioners to further research and extend for 6G Software Engineering.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2405.00030v1,2024-02-29T15:00:07Z,2024-02-29T15:00:07Z,deepops & slurm: your gpu cluster guide,"In the ever evolving landscape of deep learning, unlocking the potential of
cutting-edge models demands computational resources that surpass the
capabilities of individual machines. Enter the NVIDIA DeepOps Slurm cluster, a
meticulously orchestrated symphony of high-performance nodes, each equipped
with powerful GPUs and meticulously managed by the efficient Slurm resource
allocation system. This guide serves as your comprehensive roadmap, empowering
you to harness the immense parallel processing capabilities of this cluster and
propel your deep learning endeavors to new heights. Whether you are a seasoned
deep learning practitioner seeking to optimize performance or a newcomer eager
to unlock the power of parallel processing, this guide caters to your needs. We
wll delve into the intricacies of the cluster hardware architecture, exploring
the capabilities of its GPUs and the underlying network fabric. You will master
the art of leveraging DeepOps containers for efficient and reproducible
workflows, fine-tune resource configurations for optimal performance, and
confidently submit jobs to unleash the full potential of parallel processing.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2402.01664v1,2024-01-15T06:41:13Z,2024-01-15T06:41:13Z,edge offloading in smart grid,"The energy transition supports the shift towards more sustainable energy
alternatives, paving towards decentralized smart grids, where the energy is
generated closer to the point of use. The decentralized smart grids foresee
novel data-driven low latency applications for improving resilience and
responsiveness, such as peer-to-peer energy trading, microgrid control, fault
detection, or demand response. However, the traditional cloud-based smart grid
architectures are unable to meet the requirements of the new emerging
applications such as low latency and high-reliability thus alternative
architectures such as edge, fog, or hybrid need to be adopted. Moreover, edge
offloading can play a pivotal role for the next-generation smart grid AI
applications because it enables the efficient utilization of computing
resources and addresses the challenges of increasing data generated by IoT
devices, optimizing the response time, energy consumption, and network
performance. However, a comprehensive overview of the current state of research
is needed to support sound decisions regarding energy-related applications
offloading from cloud to fog or edge, focusing on smart grid open challenges
and potential impacts. In this paper, we delve into smart grid and
computational distribution architec-tures, including edge-fog-cloud models,
orchestration architecture, and serverless computing, and analyze the
decision-making variables and optimization algorithms to assess the efficiency
of edge offloading. Finally, the work contributes to a comprehensive
understanding of the edge offloading in smart grid, providing a SWOT analysis
to support decision making.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2311.01224v1,2023-11-02T13:24:04Z,2023-11-02T13:24:04Z,"eisim: a platform for simulating intelligent edge orchestration
  solutions","To support the stringent requirements of the future intelligent and
interactive applications, intelligence needs to become an essential part of the
resource management in the edge environment. Developing intelligent
orchestration solutions is a challenging and arduous task, where the evaluation
and comparison of the proposed solution is a focal point. Simulation is
commonly used to evaluate and compare proposed solutions. However, the
currently existing, openly available simulators are lacking in terms of
supporting the research on intelligent edge orchestration methods. To address
this need, this article presents a simulation platform called Edge Intelligence
Simulator (EISim), the purpose of which is to facilitate the research on
intelligent edge orchestration solutions. EISim is extended from an existing
fog simulator called PureEdgeSim. In its current form, EISim supports
simulating deep reinforcement learning based solutions and different
orchestration control topologies in scenarios related to task offloading and
resource pricing on edge. The platform also includes additional tools for
creating simulation environments, running simulations for agent training and
evaluation, and plotting results.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2307.12903v2,2024-09-19T08:46:30Z,2023-07-24T15:51:06Z,"towards bridging the fl performance-explainability trade-off: a
  trustworthy 6g ran slicing use-case","In the context of sixth-generation (6G) networks, where diverse network
slices coexist, the adoption of AI-driven zero-touch management and
orchestration (MANO) becomes crucial. However, ensuring the trustworthiness of
AI black-boxes in real deployments is challenging. Explainable AI (XAI) tools
can play a vital role in establishing transparency among the stakeholders in
the slicing ecosystem. But there is a trade-off between AI performance and
explainability, posing a dilemma for trustworthy 6G network slicing because the
stakeholders require both highly performing AI models for efficient resource
allocation and explainable decision-making to ensure fairness, accountability,
and compliance. To balance this trade off and inspired by the closed loop
automation and XAI methodologies, this paper presents a novel
explanation-guided in-hoc federated learning (FL) approach where a constrained
resource allocation model and an explainer exchange -- in a closed loop (CL)
fashion -- soft attributions of the features as well as inference predictions
to achieve a transparent 6G network slicing resource management in a RAN-Edge
setup under non-independent identically distributed (non-IID) datasets. In
particular, we quantitatively validate the faithfulness of the explanations via
the so-called attribution-based confidence metric that is included as a
constraint to guide the overall training process in the run-time FL
optimization task. In this respect, Integrated-Gradient (IG) as well as Input
$\times$ Gradient and SHAP are used to generate the attributions for our
proposed in-hoc scheme, wherefore simulation results under different methods
confirm its success in tackling the performance-explainability trade-off and
its superiority over the unconstrained Integrated-Gradient post-hoc FL
baseline.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2307.11181v1,2023-07-20T18:49:45Z,2023-07-20T18:49:45Z,"smotec: an edge computing testbed for adaptive smart mobility
  experimentation","Smart mobility becomes paramount for meeting net-zero targets. However,
autonomous, self-driving and electric vehicles require more than ever before an
efficient, resilient and trustworthy computational offloading backbone that
expands throughout the edge-to-cloud continuum. Utilizing on-demand
heterogeneous computational resources for smart mobility is challenging and
often cost-ineffective. This paper introduces SMOTEC, a novel open-source
testbed for adaptive smart mobility experimentation with edge computing. SMOTEC
provides for the first time a modular end-to-end instrumentation for
prototyping and optimizing placement of intelligence services on edge devices
such as augmented reality and real-time traffic monitoring. SMOTEC supports a
plug-and-play Docker container integration of the SUMO simulator for urban
mobility, Raspberry Pi edge devices communicating via ZeroMQ and EPOS for an
AI-based decentralized load balancing across edge-to-cloud. All components are
orchestrated by the K3s lightweight Kubernetes. A proof-of-concept of
self-optimized service placements for traffic monitoring from Munich
demonstrates in practice the applicability and cost-effectiveness of SMOTEC.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2305.13732v1,2023-05-23T06:38:07Z,2023-05-23T06:38:07Z,"task containerization and container placement optimization for mec: a
  joint communication and computing perspective","Containers are used by an increasing number of Internet service providers to
deploy their applications in multi-access edge computing (MEC) systems.
Although container-based virtualization technologies significantly increase
application availability, they may suffer expensive communication overhead and
resource use imbalances. However, so far there has been a scarcity of studies
to conquer these difficulties. In this paper, we design a workflow-based
mathematical model for applications built upon interdependent multitasking
composition, formulate a multi-objective combinatorial optimization problem
composed of two subproblems -- graph partitioning and multi-choice vector bin
packing, and propose several joint
task-containerization-and-container-placement methods to reduce communication
overhead and balance multi-type computing resource utilization. The performance
superiority of the proposed algorithms is demonstrated by comparison with the
state-of-the-art task and container scheduling schemes.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2304.11941v1,2023-04-24T09:21:56Z,2023-04-24T09:21:56Z,partitioning and deployment of deep neural networks on edge clusters,"Edge inference has become more widespread, as its diverse applications range
from retail to wearable technology. Clusters of networked resource-constrained
edge devices are becoming common, yet no system exists to split a DNN across
these clusters while maximizing the inference throughput of the system.
Additionally, no production-ready orchestration system exists for deploying
said models over such edge networks which adopts the robustness and scalability
of the cloud. We present an algorithm which partitions DNNs and distributes
them across a set of edge devices with the goal of minimizing the bottleneck
latency and therefore maximizing inference throughput. The system scales well
to systems of different node memory capacities and numbers of nodes, while
being node fault-tolerant. We find that we can reduce the bottleneck latency by
10x over a random algorithm and 35% over a greedy joint partitioning-placement
algorithm, although the joint-partitioning algorithm outperforms our algorithm
in most practical use-cases. Furthermore we find empirically that for the set
of representative models we tested, the algorithm produces results within 9.2%
of the optimal bottleneck latency. We then developed a standalone cluster
network emulator on which we tested configurations of up to 20 nodes and found
a steady increase in throughput and decrease in end-to-end latency as the
cluster size scales. In these tests, we observed that our system has multi-node
fault-tolerance as well as network and system IO fault-tolerance. We have
implemented our framework in open-source software that is publicly available to
the research community at https://github.com/ANRGUSC/SEIFER.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2301.13624v1,2023-01-31T13:32:36Z,2023-01-31T13:32:36Z,"a kubernetes-based edge architecture for controlling the trajectory of a
  resource-constrained aerial robot by enabling model predictive control","In recent years, cloud and edge architectures have gained tremendous focus
for offloading computationally heavy applications. From machine learning and
Internet of Thing (IOT) to industrial procedures and robotics, cloud computing
have been used extensively for data processing and storage purposes, thanks to
its ""infinite"" resources. On the other hand, cloud computing is characterized
by long time delays due to the long distance between the cloud servers and the
machine requesting the resources. In contrast, edge computing provides almost
real-time services since edge servers are located significantly closer to the
source of data. This capability sets edge computing as an ideal option for
real-time applications, like high level control, for resource-constrained
platforms. In order to utilize the edge resources, several technologies, with
basic ones as containers and orchestrators like Kubernetes, have been developed
to provide an environment with many features, based on each application's
requirements. In this context, this works presents the implementation and
evaluation of a novel edge architecture based on Kubernetes orchestration for
controlling the trajectory of a resource-constrained Unmanned Aerial Vehicle
(UAV) by enabling Model Predictive Control (MPC).",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2301.07803v2,2023-01-23T22:56:42Z,2023-01-18T22:22:23Z,"relating edge computing and microservices by means of architecture
  approaches and features, orchestration, choreography, and offloading: a
  systematic literature review","Context: Microservices running and being powered by Edge Computing have been
gaining much attention in the industry and academia. Since 2014, when Martin
Fowler popularized the Microservice term, many studies have been published
relating these subjects to explore how the Edge's low-latency feature could be
combined with the high throughput of the distributed paradigm from
Microservices. Objective: Identifying how Microservices work together with Edge
Computing whereas they take advantage when running on Edge. Method: In order to
better understand this relationship, we first identified its key concepts,
which are: architecture approaches and features, microservice composition
(orchestration/choreography), and offloading. Afterward, we conducted a
Systematic Literature Review (SLR) as the survey method. Results: We reviewed
111 selected studies and built a taxonomy of Microservices on Edge Computing
demonstrating their current architecture approaches and features, composition,
and offloading modes. Moreover, we identify the research gaps and trends.
Conclusion: This paper is a step forward to help researchers and professionals
get a general overview of how Microservices and Edge have been related in the
last years. It also discusses gaps and research trends. This SLR will also be a
good introduction for new researchers in Edge and Microservices.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2301.03358v1,2022-12-31T06:03:14Z,2022-12-31T06:03:14Z,"cost-effective two-stage network slicing for edge-cloud orchestrated
  vehicular networks","In this paper, we study a network slicing problem for edge-cloud orchestrated
vehicular networks, in which the edge and cloud servers are orchestrated to
process computation tasks for reducing network slicing cost while satisfying
the quality of service requirements. We propose a two-stage network slicing
framework, which consists of 1) network planning stage in a large timescale to
perform slice deployment, edge resource provisioning, and cloud resource
provisioning, and 2) network operation stage in a small timescale to perform
resource allocation and task dispatching. Particularly, we formulate the
network slicing problem as a two-timescale stochastic optimization problem to
minimize the network slicing cost. Since the problem is NP-hard due to coupled
network planning and network operation stages, we develop a Two timescAle
netWork Slicing (TAWS) algorithm by collaboratively integrating reinforcement
learning (RL) and optimization methods, which can jointly make network planning
and operation decisions. Specifically, by leveraging the timescale separation
property of decisions, we decouple the problem into a large-timescale network
planning subproblem and a small-timescale network operation subproblem. The
former is solved by an RL method, and the latter is solved by an optimization
method. Simulation results based on real-world vehicle traffic traces show that
the TAWS can effectively reduce the network slicing cost as compared to the
benchmark scheme.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2210.12218v2,2022-11-17T23:13:10Z,2022-10-21T19:56:54Z,seifer: scalable edge inference for deep neural networks,"Edge inference is becoming ever prevalent through its applications from
retail to wearable technology. Clusters of networked resource-constrained edge
devices are becoming common, yet there is no production-ready orchestration
system for deploying deep learning models over such edge networks which adopts
the robustness and scalability of the cloud. We present SEIFER, a framework
utilizing a standalone Kubernetes cluster to partition a given DNN and place
these partitions in a distributed manner across an edge network, with the goal
of maximizing inference throughput. The system is node fault-tolerant and
automatically updates deployments based on updates to the model's version. We
provide a preliminary evaluation of a partitioning and placement algorithm that
works within this framework, and show that we can improve the inference
pipeline throughput by 200% by utilizing sufficient numbers of
resource-constrained nodes. We have implemented SEIFER in open-source software
that is publicly available to the research community.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2210.10147v2,2023-07-25T12:57:15Z,2022-10-18T20:26:56Z,"tefl: turbo explainable federated learning for 6g trustworthy zero-touch
  network slicing","Sixth-generation (6G) networks anticipate intelligently supporting a massive
number of coexisting and heterogeneous slices associated with various vertical
use cases. Such a context urges the adoption of artificial intelligence
(AI)-driven zero-touch management and orchestration (MANO) of the end-to-end
(E2E) slices under stringent service level agreements (SLAs). Specifically, the
trustworthiness of the AI black-boxes in real deployment can be achieved by
explainable AI (XAI) tools to build transparency between the interacting actors
in the slicing ecosystem, such as tenants, infrastructure providers and
operators. Inspired by the turbo principle, this paper presents a novel
iterative explainable federated learning (FL) approach where a constrained
resource allocation model and an \emph{explainer} exchange -- in a closed loop
(CL) fashion -- soft attributions of the features as well as inference
predictions to achieve a transparent and SLA-aware zero-touch service
management (ZSM) of 6G network slices at RAN-Edge setup under non-independent
identically distributed (non-IID) datasets. In particular, we quantitatively
validate the faithfulness of the explanations via the so-called
attribution-based \emph{confidence metric} that is included as a constraint in
the run-time FL optimization task. In this respect, Integrated-Gradient (IG) as
well as Input $\times$ Gradient and SHAP are used to generate the attributions
for the turbo explainable FL (TEFL), wherefore simulation results under
different methods confirm its superiority over an unconstrained
Integrated-Gradient \emph{post-hoc} FL baseline.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2210.06080v2,2022-11-16T03:31:36Z,2022-10-12T10:52:15Z,computing power network: a survey,"With the rapid development of cloud computing, edge computing, and smart
devices, computing power resources indicate a trend of ubiquitous deployment.
The traditional network architecture cannot efficiently leverage these
distributed computing power resources due to computing power island effect. To
overcome these problems and improve network efficiency, a new network computing
paradigm is proposed, i.e., Computing Power Network (CPN). Computing power
network can connect ubiquitous and heterogenous computing power resources
through networking to realize computing power scheduling flexibly. In this
survey, we make an exhaustive review on the state-of-the-art research efforts
on computing power network. We first give an overview of computing power
network, including definition, architecture, and advantages. Next, a
comprehensive elaboration of issues on computing power modeling, information
awareness and announcement, resource allocation, network forwarding, computing
power transaction platform and resource orchestration platform is presented.
The computing power network testbed is built and evaluated. The applications
and use cases in computing power network are discussed. Then, the key enabling
technologies for computing power network are introduced. Finally, open
challenges and future research directions are presented as well.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2209.01353v2,2023-05-20T18:55:08Z,2022-09-03T07:55:48Z,"a repeated unknown game: decentralized task offloading in vehicular fog
  computing","Offloading computation to nearby edge/fog computing nodes, including the ones
carried by moving vehicles, e.g., vehicular fog nodes (VFN), has proved to be a
promising approach for enabling low-latency and compute-intensive mobility
applications, such as cooperative and autonomous driving. This work considers
vehicular fog computing scenarios where the clients of computation offloading
services try to minimize their own costs while deciding which VFNs to offload
their tasks. We focus on decentralized multi-agent decision-making in a
repeated unknown game where each agent, e.g., service client, can observe only
its own action and realized cost. In other words, each agent is unaware of the
game composition or even the existence of opponents. We apply a completely
uncoupled learning rule to generalize the decentralized decision-making
algorithm presented in \cite{Cho2021} for the multi-agent case. The multi-agent
solution proposed in this work can capture the unknown offloading cost
variations susceptive to resource congestion under an adversarial framework
where each agent may take implicit cost estimation and suitable resource choice
adapting to the dynamics associated with volatile supply and demand. According
to the evaluation via simulation, this work reveals that such individual
perturbations for robustness to uncertainty and adaptation to dynamicity ensure
a certain level of optimality in terms of social welfare, e.g., converging the
actual sequence of play with unknown and asymmetric attributes and lowering the
correspondent cost in social welfare due to the self-interested behaviors of
agents.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2207.05399v2,2023-02-14T10:03:40Z,2022-07-12T09:03:08Z,"placement of microservices-based iot applications in fog computing: a
  taxonomy and future directions","The Fog computing paradigm utilises distributed, heterogeneous and
resource-constrained devices at the edge of the network for efficient
deployment of latency-critical and bandwidth-hungry IoT application services.
Moreover, MicroService Architecture (MSA) is increasingly adopted to keep up
with the rapid development and deployment needs of fast-evolving IoT
applications. Due to the fine-grained modularity of the microservices and their
independently deployable and scalable nature, MSA exhibits great potential in
harnessing Fog and Cloud resources, thus giving rise to novel paradigms like
Osmotic computing. The loosely coupled nature of the microservices, aided by
the container orchestrators and service mesh technologies, enables the dynamic
composition of distributed and scalable microservices to achieve diverse
performance requirements of the IoT applications using distributed Fog
resources. To this end, efficient placement of microservice plays a vital role,
and scalable placement algorithms are required to utilise the said
characteristics of the MSA while overcoming novel challenges introduced by the
architecture. Thus, we present a comprehensive taxonomy of recent literature on
microservices-based IoT applications placement within Fog computing
environments. Furthermore, we organise multiple taxonomies to capture the main
aspects of the placement problem, analyse and classify related works, identify
research gaps within each category, and discuss future research directions.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2205.14188v2,2023-01-04T15:13:31Z,2022-05-27T18:34:36Z,"introducing k4.0s: a model for mixed-criticality container orchestration
  in industry 4.0 (extended)","Time predictable edge cloud is seen as the answer for many arising needs in
Industry 4.0 environments, since it is able to provide flexible, modular, and
reconfigurable services with low latency and reduced costs. Orchestration
systems are becoming the core component of clouds since they take decisions on
the placement and lifecycle of software components. Current solutions start
introducing real-time containers support for time predictability; however,
these approaches lack of determinism as well as support for workloads requiring
multiple levels of assurance/criticality.
  In this paper, we present k4.0s, an orchestration model for real-time and
mixed-criticality environments, which includes timeliness, criticality and
network requirements. The model leverages new abstractions for both node and
jobs, e.g., node assurance, and requires novel monitoring strategies. We sketch
an implementation of the proposal based on Kubernetes, and present an
experimentation motivating the need for node assurance levels and adequate
monitoring.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2205.13770v1,2022-05-27T06:11:50Z,2022-05-27T06:11:50Z,"leaf + aio: edge-assisted energy-aware object detection for mobile
  augmented reality","Today very few deep learning-based mobile augmented reality (MAR)
applications are applied in mobile devices because they are significantly
energy-guzzling. In this paper, we design an edge-based energy-aware MAR system
that enables MAR devices to dynamically change their configurations, such as
CPU frequency, computation model size, and image offloading frequency based on
user preferences, camera sampling rates, and available radio resources. Our
proposed dynamic MAR configuration adaptations can minimize the per frame
energy consumption of multiple MAR clients without degrading their preferred
MAR performance metrics, such as latency and detection accuracy. To thoroughly
analyze the interactions among MAR configurations, user preferences, camera
sampling rate, and energy consumption, we propose, to the best of our
knowledge, the first comprehensive analytical energy model for MAR devices.
Based on the proposed analytical model, we design a LEAF optimization algorithm
to guide the MAR configuration adaptation and server radio resource allocation.
An image offloading frequency orchestrator, coordinating with the LEAF, is
developed to adaptively regulate the edge-based object detection invocations
and to further improve the energy efficiency of MAR devices. Extensive
evaluations are conducted to validate the performance of the proposed
analytical model and algorithms.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2205.01944v1,2022-05-04T08:32:24Z,2022-05-04T08:32:24Z,"joint compute-caching-communication control for online data-intensive
  service delivery","Emerging Metaverse applications, designed to deliver highly interactive and
immersive experiences that seamlessly blend physical reality and digital
virtuality, are accelerating the need for distributed compute platforms with
unprecedented storage, computation, and communication requirements. To this
end, the integrated evolution of next-generation networks (e.g., 5G and beyond)
and distributed cloud technologies (e.g., fog and mobile edge computing), have
emerged as a promising paradigm to address the interaction- and
resource-intensive nature of Metaverse applications. In this paper, we focus on
the design of control policies for the joint orchestration of compute, caching,
and communication (3C) resources in next-generation distributed cloud networks
for the efficient delivery of Metaverse applications that require the real-time
aggregation, processing, and distribution of multiple live media streams and
pre-stored digital assets. We describe Metaverse applications via directed
acyclic graphs able to model the combination of real-time stream-processing and
content distribution pipelines. We design the first throughput-optimal control
policy that coordinates joint decisions around (i) routing paths and processing
locations for live data streams, together with (ii) cache selection and
distribution paths for associated data objects. We then extend the proposed
solution to include a max-throughput database placement policy and two
efficient replacement policies. In addition, we characterize the network
stability regions for all studied scenarios. Numerical results demonstrate the
superior performance obtained via the novel multi-pipeline flow control and 3C
resource orchestration mechanisms of the proposed policy, compared with
state-of-the-art algorithms that lack full 3C integrated control.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2201.11067v3,2022-02-25T15:19:23Z,2022-01-26T17:26:59Z,roma: resource orchestration for microservices-based 5g applications,"With the growth of 5G, Internet of Things (IoT), edge computing and cloud
computing technologies, the infrastructure (compute and network) available to
emerging applications (AR/VR, autonomous driving, industry 4.0, etc.) has
become quite complex. There are multiple tiers of computing (IoT devices, near
edge, far edge, cloud, etc.) that are connected with different types of
networking technologies (LAN, LTE, 5G, MAN, WAN, etc.). Deployment and
management of applications in such an environment is quite challenging. In this
paper, we propose ROMA, which performs resource orchestration for
microservices-based 5G applications in a dynamic, heterogeneous, multi-tiered
compute and network fabric. We assume that only application-level requirements
are known, and the detailed requirements of the individual microservices in the
application are not specified. As part of our solution, ROMA identifies and
leverages the coupling relationship between compute and network usage for
various microservices and solves an optimization problem in order to
appropriately identify how each microservice should be deployed in the complex,
multi-tiered compute and network fabric, so that the end-to-end application
requirements are optimally met. We implemented two real-world 5G applications
in video surveillance and intelligent transportation system (ITS) domains.
Through extensive experiments, we show that ROMA is able to save up to 90%, 55%
and 44% compute and up to 80%, 95% and 75% network bandwidth for the
surveillance (watchlist) and transportation application (person and car
detection), respectively. This improvement is achieved while honoring the
application performance requirements, and it is over an alternative scheme that
employs a static and overprovisioned resource allocation strategy by ignoring
the resource coupling relationships.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2201.00994v1,2022-01-04T05:55:39Z,2022-01-04T05:55:39Z,toward a utm-based service orchestration for uavs in mec-nfv environment,"The increased use of Unmanned Aerial Vehicles (UAVs) in numerous domains will
result in high traffic densities in the low-altitude airspace. Consequently,
UAVs Traffic Management (UTM) systems that allow the integration of UAVs in the
low-altitude airspace are gaining a lot of momentum. Furthermore, the 5th
generation of mobile networks (5G) will most likely provide the underlying
support for UTM systems by providing connectivity to UAVs, enabling the
control, tracking and communication with remote applications and services.
However, UAVs may need to communicate with services with different
communication Quality of Service (QoS) requirements, ranging form best-effort
services to Ultra-Reliable Low-Latency Communications (URLLC) services. Indeed,
5G can ensure efficient Quality of Service (QoS) enhancements using new
technologies, such as network slicing and Multi-access Edge Computing (MEC). In
this context, Network Functions Virtualization (NFV) is considered as one of
the pillars of 5G systems, by providing a QoS-aware Management and
Orchestration (MANO) of softwarized services across cloud and MEC platforms.
The MANO process of UAV's services can be enhanced further using the
information provided by the UTM system, such as the UAVs'flight plans. In this
paper,we propose an extended framework for the management and orchestration of
UAVs'services in MECNFV environment by combining the functionalities provided
by the MEC-NFV management and orchestration framework with the functionalities
of a UTM system. Moreover, we propose an Integer Linear Programming (ILP) model
of the placement scheme of our framework and we evaluate its performances.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2111.08663v1,2021-11-16T17:55:25Z,2021-11-16T17:55:25Z,"engineering edge-cloud offloading of big data for channel modelling in
  thz-range communications","Channel estimation in mmWave and THz-range wireless communications (producing
Gb/Tb-range of data) is critical to configuring system parameters related to
transmission signal quality, and yet it remains a daunting challenge both in
software and hardware. Current methods of channel estimations, be it modeling-
or data-based (machine learning (ML)), - use and create big data. This in turn
requires a large amount of computational resources, read operations to prove if
there is some predefined channel configurations, e.g., QoS requirements, in the
database, as well as write operations to store the new combinations of QoS
parameters in the database. Especially the ML-based approach requires high
computational and storage resources, low latency and a higher hardware
flexibility. In this paper, we engineer and study the offloading of the above
operations to edge and cloud computing systems to understand the suitability of
edge and cloud computing to provide rapid response with channel and link
configuration parameters on the example of THz channel modeling. We evaluate
the performance of the engineered system when the computational and storage
resources are orchestrated based on: 1) monolithic architecture, 2)
microservices architectures, both in edge-cloud based approach. For
microservices approach, we engineer both Docker Swarm and Kubernetes systems.
The measurements show a great promise of edge computing and microservices that
can quickly respond to properly configure parameters and improve transmission
distance and signal quality with ultra-high speed wireless communications.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2110.01863v2,2022-03-31T17:46:28Z,2021-10-05T07:55:19Z,"deepedge: a deep reinforcement learning based task orchestrator for edge
  computing","The improvements in the edge computing technology pave the road for
diversified applications that demand real-time interaction. However, due to the
mobility of the end-users and the dynamic edge environment, it becomes
challenging to handle the task offloading with high performance. Moreover,
since each application in mobile devices has different characteristics, a task
orchestrator must be adaptive and have the ability to learn the dynamics of the
environment. For this purpose, we develop a deep reinforcement learning based
task orchestrator, DeepEdge, which learns to meet different task requirements
without needing human interaction even under the heavily-loaded stochastic
network conditions in terms of mobile users and applications. Given the dynamic
offloading requests and time-varying communication conditions, we successfully
model the problem as a Markov process and then apply the Double Deep Q-Network
(DDQN) algorithm to implement DeepEdge. To evaluate the robustness of DeepEdge,
we experiment with four different applications including image rendering,
infotainment, pervasive health, and augmented reality in the network under
various loads. Furthermore, we compare the performance of our agent with the
four different task offloading approaches in the literature. Our results show
that DeepEdge outperforms its competitors in terms of the percentage of
satisfactorily completed tasks.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2109.07999v4,2022-09-01T03:39:32Z,2021-09-16T14:11:31Z,"learning from peers: deep transfer reinforcement learning for joint
  radio and cache resource allocation in 5g ran slicing","Network slicing is a critical technique for 5G communications that covers
radio access network (RAN), edge, transport and core slicing.The evolving
network architecture requires the orchestration of multiple network resources
such as radio and cache resources. In recent years, machine learning (ML)
techniques have been widely applied for network management. However, most
existing works do not take advantage of the knowledge transfer capability in
ML. In this paper, we propose a deep transfer reinforcement learning (DTRL)
scheme for joint radio and cache resource allocation to serve 5G RAN slicing.
We first define a hierarchical architecture for joint resource allocation. Then
we propose two DTRL algorithms: Q-value-based deep transfer reinforcement
learning (QDTRL) and action selection-based deep transfer reinforcement
learning (ADTRL). In the proposed schemes, learner agents utilize expert
agents' knowledge to improve their performance on current tasks. The proposed
algorithms are compared with both the model-free exploration bonus deep
Q-learning (EB-DQN) and the model-based priority proportional fairness and
time-to-live (PPF-TTL) algorithms. Compared with EB-DQN, our proposed
DTRL-based method presents 21.4% lower delay for Ultra Reliable Low Latency
Communications (URLLC) slice and 22.4% higher throughput for enhanced Mobile
Broad Band (eMBB) slice, while achieving significantly faster convergence than
EB-DQN. Moreover, 40.8% lower URLLC delay and 59.8% higher eMBB throughput are
observed with respect to PPF-TTL.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2104.14392v3,2021-07-09T13:08:48Z,2021-04-29T15:09:44Z,"cosco: container orchestration using co-simulation and gradient based
  optimization for fog computing environments","Intelligent task placement and management of tasks in large-scale fog
platforms is challenging due to the highly volatile nature of modern workload
applications and sensitive user requirements of low energy consumption and
response time. Container orchestration platforms have emerged to alleviate this
problem with prior art either using heuristics to quickly reach scheduling
decisions or AI driven methods like reinforcement learning and evolutionary
approaches to adapt to dynamic scenarios. The former often fail to quickly
adapt in highly dynamic environments, whereas the latter have run-times that
are slow enough to negatively impact response time. Therefore, there is a need
for scheduling policies that are both reactive to work efficiently in volatile
environments and have low scheduling overheads. To achieve this, we propose a
Gradient Based Optimization Strategy using Back-propagation of gradients with
respect to Input (GOBI). Further, we leverage the accuracy of predictive
digital-twin models and simulation capabilities by developing a Coupled
Simulation and Container Orchestration Framework (COSCO). Using this, we create
a hybrid simulation driven decision approach, GOBI*, to optimize Quality of
Service (QoS) parameters. Co-simulation and the back-propagation approaches
allow these methods to adapt quickly in volatile environments. Experiments
conducted using real-world data on fog applications using the GOBI and GOBI*
methods, show a significant improvement in terms of energy consumption,
response time, Service Level Objective and scheduling time by up to 15, 40, 4,
and 82 percent respectively when compared to the state-of-the-art algorithms.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2102.02318v1,2021-02-03T22:45:09Z,2021-02-03T22:45:09Z,"system intelligence for uav-based mission critical with challenging
  5g/b5g connectivity","Unmanned aerial vehicles (UAVs) and communication systems are fundamental
elements in Mission Critical services, such as search and rescue. In this
article, we introduce an architecture for managing and orchestrating 5G and
beyond networks that operate over a heterogeneous infrastructure with UAVs'
aid. UAVs are used for collecting and processing data, as well as improving
communications. The proposed System Intelligence (SI) architecture was designed
to comply with recent standardization works, especially the ETSI Experiential
Networked Intelligence specifications. Another contribution of this article is
an evaluation using a testbed based on a virtualized non-standalone 5G core and
a 4G Radio Access Network (RAN) implemented with open-source software. The
experimental results indicate, for instance, that SI can substantially improve
the latency of UAV-based services by splitting deep neural networks between UAV
and edge or cloud equipment. Other experiments explore the slicing of RAN
resources and efficient placement of virtual network functions to assess the
benefits of incorporating intelligence in UAV-based mission-critical services.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2012.03257v1,2020-12-06T13:15:52Z,2020-12-06T13:15:52Z,"coedge: cooperative dnn inference with adaptive workload partitioning
  over heterogeneous edge devices","Recent advances in artificial intelligence have driven increasing intelligent
applications at the network edge, such as smart home, smart factory, and smart
city. To deploy computationally intensive Deep Neural Networks (DNNs) on
resource-constrained edge devices, traditional approaches have relied on either
offloading workload to the remote cloud or optimizing computation at the end
device locally. However, the cloud-assisted approaches suffer from the
unreliable and delay-significant wide-area network, and the local computing
approaches are limited by the constrained computing capability. Towards
high-performance edge intelligence, the cooperative execution mechanism offers
a new paradigm, which has attracted growing research interest recently. In this
paper, we propose CoEdge, a distributed DNN computing system that orchestrates
cooperative DNN inference over heterogeneous edge devices. CoEdge utilizes
available computation and communication resources at the edge and dynamically
partitions the DNN inference workload adaptive to devices' computing
capabilities and network conditions. Experimental evaluations based on a
realistic prototype show that CoEdge outperforms status-quo approaches in
saving energy with close inference latency, achieving up to 25.5%~66.9% energy
reduction for four widely-adopted CNN models.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2006.10978v1,2020-06-19T06:58:35Z,2020-06-19T06:58:35Z,"cooling-aware resource allocation and load management for mobile edge
  computing systems","Driven by explosive computation demands of Internet of Things (IoT), mobile
edge computing (MEC) provides a promising technique to enhance the computation
capability for mobile users. In this paper, we propose a joint resource
allocation and load management mechanism in an MEC system with wireless power
transfer (WPT), by jointly optimizing the transmit power for WPT, the
local/edge computing load, the offloading time, and the frequencies of the
central processing units (CPUs) at the access point (AP) and the users. To
achieve an energy-efficient and sustainable WPT-MEC system, we minimize the
total energy consumption of the AP, while meeting computation latency
requirements. Cooling energy which is non-negligible, is taken into account in
minimizing the energy consumption of the MEC system. By rigorously
orchestrating the state-of-the-art optimization techniques, we design an
iterative algorithm and obtain the optimal solution in a semi-closed form.
Based on the solution, interesting properties and insights are summarized.
Extensive numerical tests show that the proposed algorithm can save up to 90.4%
the energy of existing benchmarks.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2002.05531v1,2020-02-12T09:27:00Z,2020-02-12T09:27:00Z,modelling fog offloading performance,"Fog computing has emerged as a computing paradigm aimed at addressing the
issues of latency, bandwidth and privacy when mobile devices are communicating
with remote cloud services. The concept is to offload compute services closer
to the data. However many challenges exist in the realisation of this approach.
During offloading, (part of) the application underpinned by the services may be
unavailable, which the user will experience as down time. This paper describes
work aimed at building models to allow prediction of such down time based on
metrics (operational data) of the underlying and surrounding infrastructure.
Such prediction would be invaluable in the context of automated Fog offloading
and adaptive decision making in Fog orchestration. Models that cater for four
container-based stateless and stateful offload techniques, namely Save and
Load, Export and Import, Push and Pull and Live Migration, are built using four
(linear and non-linear) regression techniques. Experimental results comprising
over 42 million data points from multiple lab-based Fog infrastructure are
presented. The results highlight that reasonably accurate predictions (measured
by the coefficient of determination for regression models, mean absolute
percentage error, and mean absolute error) may be obtained when considering 25
metrics relevant to the infrastructure.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2001.10300v1,2020-01-28T12:59:52Z,2020-01-28T12:59:52Z,"dynamic network slicing for scalable fog computing systems with energy
  harvesting","This paper studies fog computing systems, in which cloud data centers can be
supplemented by a large number of fog nodes deployed in a wide geographical
area. Each node relies on harvested energy from the surrounding environment to
provide computational services to local users. We propose the concept of
dynamic network slicing in which a regional orchestrator coordinates workload
distribution among local fog nodes, providing partitions/slices of energy and
computational resources to support a specific type of service with certain
quality-of-service (QoS) guarantees. The resources allocated to each slice can
be dynamically adjusted according to service demands and energy availability. A
stochastic overlapping coalition-formation game is developed to investigate
distributed cooperation and joint network slicing between fog nodes under
randomly fluctuating energy harvesting and workload arrival processes. We
observe that the overall processing capacity of the fog computing network can
be improved by allowing fog nodes to maintain a belief function about the
unknown state and the private information of other nodes. An algorithm based on
a belief-state partially observable Markov decision process (B-POMDP) is
proposed to achieve the optimal resource slicing structure among all fog nodes.
We describe how to implement our proposed dynamic network slicing within the
3GPP network sharing architecture, and evaluate the performance of our proposed
framework using the real BS location data of a real cellular system with over
200 BSs deployed in the city of Dublin. Our numerical results show that our
framework can significantly improve the workload processing capability of fog
computing networks. In particular, even when each fog node can coordinate only
with its closest neighbor, the total amount of workload processed by fog nodes
can be almost doubled under certain scenarios.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/2001.06328v1,2020-01-16T00:54:08Z,2020-01-16T00:54:08Z,energy efficient cloud-fog architecture,"The advancements of cloud computing came as a radical transformation in the
way Information and Communication Technology (ICT) services are deployed and
maintained. Cloud computing provides ubiquitous on-demand access to an
Internet-based pool of processing, storage, and communication resources offered
to a large set of geographically distributed users. As the cloud computing
infrastructure grows and demand increases, the need for a new breed of
on-demand computing that can efficiently maintain Quality of Service (QoS)
requirements has increased. Fog computing was proposed to address the
limitations of cloud computing, in terms of delay and high bandwidth
requirements, by extending the on-demand resources of clouds to the edge of the
network bringing them closer to the users. The massive growth and wide use of
cloud-fog services have created serious power consumption concerns. This
article delves into the energy consumption of cloud-fog services by raising
headline questions related to; how significant the problem itself is, how
different conditions/scenarios affect the energy consumption of the
architecture, and how to orchestrate the use of the architecture in an
energy-efficient manner. We start by summarizing the cloud-fog architecture
including different communication and computing layers. Additionally, we give a
brief overview of the role of Virtual Machine (VM) placement in optimally using
cloud-fog resources in a dynamic manner. Then, we present the problem of energy
efficient VMs placement and provide numerical results.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/1911.03600v2,2020-05-19T11:51:47Z,2019-11-09T03:06:48Z,"distributed redundant placement for microservice-based applications at
  the edge","Multi-access Edge Computing (MEC) is booming as a promising paradigm to push
the computation and communication resources from cloud to the network edge to
provide services and to perform computations. With container technologies,
mobile devices with small memory footprint can run composite microservice-based
applications without time-consuming backbone. Service placement at the edge is
of importance to put MEC from theory into practice. However, current
state-of-the-art research does not sufficiently take the composite property of
services into consideration. Besides, although Kubernetes has certain abilities
to heal container failures, high availability cannot be ensured due to
heterogeneity and variability of edge sites. To deal with these problems, we
propose a distributed redundant placement framework SAA-RP and a GA-based
Server Selection (GASS) algorithm for microservice-based applications with
sequential combinatorial structure. We formulate a stochastic optimization
problem with the uncertainty of microservice request considered, and then
decide for each microservice, how it should be deployed and with how many
instances as well as on which edge sites to place them. Benchmark policies are
implemented in two scenarios, where redundancy is allowed and not,
respectively. Numerical results based on a real-world dataset verify that GASS
significantly outperforms all the benchmark policies.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/1909.05530v2,2019-10-30T13:14:33Z,2019-09-12T09:28:53Z,"reinforcing edge computing with multipath tcp enabled mobile device
  clouds","In recent years, enormous growth has been witnessed in the computational and
storage capabilities of mobile devices. However, much of this computational and
storage capabilities are not always fully used. On the other hand, popularity
of mobile edge computing which aims to replace the traditional centralized
powerful cloud with multiple edge servers is rapidly growing. In particular,
applications having strict latency requirements can be best served by the
mobile edge clouds due to a reduced round-trip delay. In this paper we propose
a Multi-Path TCP (MPTCP) enabled mobile device cloud (MDC) as a replacement to
the existing TCP based or D2D device cloud techniques, as it effectively makes
use of the available bandwidth by providing much higher throughput as well as
ensures robust wireless connectivity. We investigate the congestion in
mobile-device cloud formation resulting mainly due to the message passing for
service providing nodes at the time of discovery, service continuity and
formation of cloud composition. We propose a user space agent called congestion
handler that enable offloading of packets from one sub-flow to the other under
link quality constraints. Further, we discuss the benefits of this design and
perform preliminary analysis of the system.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/1901.04280v1,2019-01-14T13:12:39Z,2019-01-14T13:12:39Z,"sdn-enabled mimo heterogeneous cooperative networks with flexible cell
  association","Small-cell densification is a strategy enabling the offloading of users from
macro base stations (MBSs), in order to alleviate their load and increase the
coverage, especially, for cell-edge users. In parallel, as the network
increases in density, the BS cooperation emerges as an efficient design method
towards the demands for drastic improvement of the system performance against
the detrimental overall interference. In addition, the tiers are enhanced with
cell association policies by introducing the concept of the association
probability. Above this and motivated by the advantages of cooperation among
BSs, the small base stations (SBSs) are enriched with this property in their
design. SBS cooperation allows shedding light into its impact on the cell
selection rules in multi-antenna HetNets. Under these settings,
software-defined networking (SDN) is introduced smoothly to play the leading
role in the orchestration of the network. {In particular, heavy operations such
as the coordination and the cell association are undertaken by virtue of an SDN
controller performing and managing efficiently the corresponding computations
due to its centralized adaptability and dynamicity towards the enhancement and
potential scalability of the network}. In this context, we derive the coverage
probability and the mean achievable rate. Not only we show the outperformance
of BS cooperation over uncoordinated BSs, but we also demonstrate that the SBS
cooperation enables the admittance of more users from the macro-cell BSs
(MBSs). Moreover, we investigate the performance of different transmission
techniques, and we identify the optimal bias in each case when SBSs cooperate.
Finally, we depict that the SBS densification is beneficial until a specific
density value since a further increase does not increase the coverage
probability.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/1812.00300v1,2018-12-02T01:58:23Z,2018-12-02T01:58:23Z,"containers orchestration with cost-efficient autoscaling in cloud
  computing environments","Containers are standalone, self-contained units that package software and its
dependencies together. They offer lightweight performance isolation, fast and
flexible deployment, and fine-grained resource sharing. They have gained
popularity in better application management and deployment in recent years and
are being widely used by organizations to deploy their increasingly diverse
workloads such as web services, big data, and IoT in either proprietary
clusters or cloud data centres. This has led to the emergence of container
orchestration platforms, which are designed to manage the deployment of
containerized applications in large-scale clusters. The majority of these
platforms are tailored to optimize the scheduling of containers on a
fixed-sized private cluster but are not enabled to autoscale the size of the
cluster nor to consider features specific to public cloud environments. In this
work, we propose a comprehensive container resource management approach that
has three different objectives. The first one is to optimize the initial
placement of containers by efficiently scheduling them on existing resources.
The second one is to autoscale the number of resources at runtime based on the
current cluster's workload. The third one is a rescheduling mechanism to
further support the efficient use of resources by consolidating applications
into fewer VMs when possible. Our algorithms are implemented as a
plugin-scheduler for Kubernetes platform. We evaluated our framework and the
effectiveness of the proposed algorithms on an Australian national cloud
infrastructure. Our experiments demonstrate that considerable cost savings can
be achieved by dynamically managing the cluster size and placement of
applications. We find that our proposed approaches are capable of reducing the
cost by 58% when compared to the default Kubernetes scheduler.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/1806.03157v1,2018-06-08T13:53:33Z,2018-06-08T13:53:33Z,"a publish/subscribe qos-aware framework for massive iot traffic
  orchestration","Internet of Things (IoT) application deployment requires the allocation of
resources such as virtual machines, storage, and network elements that must be
deployed over distinct infrastructures such as cloud computing, Cloud of Things
(CoT), datacenters and backbone networks. For massive IoT data acquisition, a
gateway-based data aggregation approach is commonly used featuring sensor/
actuator seamless access and providing cache/ buffering and preprocessing
functionality. In this perspective , gateways acting as producers need to
allocate network resources to send IoT data to consumers. In this paper, it is
proposed a Publish/-Subscribe (PubSub) quality of service (QoS) aware framework
(PSIoT-Orch) that orchestrates IoT traffic and allocates network resources
between aggregates and consumers for massive IoT traffic. PSIoT-Orch schedules
IoT data flows based on its configured QoS requirements. Additionally , the
framework allocates network resources (LSP/ bandwidth) over a controlled
backbone network with limited and constrained resources between IoT data users
and consumers. Network resources are allocated using a Bandwidth Allocation
Model (BAM) to achieve efficient network resource allocation for scheduled IoT
data streams. PSIoT-Orch adopts an ICN (Information-Centric Network) PubSub
architecture approach to handle IoT data transfers requests among framework
components. The proposed framework aims at gathering the inherent advantages of
an ICN-centric approach using a PubSub message scheme while allocating
resources efficiently keeping QoS awareness and handling restricted network
resources (bandwidth) for massive IoT traffic.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
http://arxiv.org/abs/1004.3099v1,2010-04-19T05:57:47Z,2010-04-19T05:57:47Z,"controlled growth, patterning and placement of carbon nanotube thin
  films","Controlled growth, patterning and placement of carbon nanotube (CNT) thin
films for electronic applications are demonstrated. The density of CNT films is
controlled by optimizing the feed gas composition as well as the concentration
of growth catalyst in a chemical vapor deposition process. Densities of CNTs
ranging from 0.02 CNTs/{\mu}m^2 to 1.29 CNTs/{\mu}m^2 are obtained. The
resulting pristine CNT thin films are then successfully patterned using either
pre-growth or post-growth techniques. By developing a layered photoresist
process that is compatible with ferric nitrate catalyst, significant
improvements over popular pre-growth patterning methods are obtained.
Limitations of traditional post-growth patterning methods are circumvented by
selective transfer printing of CNTs with either thermoplastic or metallic
stamps. Resulting as-grown patterns of CNT thin films have edge roughness (< 1
{\mu}m) and resolution (< 5 {\mu}m) comparable to standard photolithography.
Bottom gate CNT thin film devices are fabricated with field-effect mobilities
up to 20 cm^2/Vs and on/off ratios of the order of 10^3. The patterning and
transfer printing methods discussed here have a potential to be generalized to
include other nanomaterials in new device configurations.",arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement'
