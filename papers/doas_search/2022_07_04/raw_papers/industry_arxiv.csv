id,updated,published,title,summary,database,query_name,query_value
http://arxiv.org/abs/2207.05164v1,2022-07-11T19:58:56Z,2022-07-11T19:58:56Z,"""Why do so?"" -- A Practical Perspective on Machine Learning Security","Despite the large body of academic work on machine learning security, little
is known about the occurrence of attacks on machine learning systems in the
wild. In this paper, we report on a quantitative study with 139 industrial
practitioners. We analyze attack occurrence and concern and evaluate
statistical hypotheses on factors influencing threat perception and exposure.
Our results shed light on real-world attacks on deployed machine learning. On
the organizational level, while we find no predictors for threat exposure in
our sample, the amount of implement defenses depends on exposure to threats or
expected likelihood to become a target. We also provide a detailed analysis of
practitioners' replies on the relevance of individual machine learning attacks,
unveiling complex concerns like unreliable decision making, business
information leakage, and bias introduction into models. Finally, we find that
on the individual level, prior knowledge about machine learning security
influences threat perception. Our work paves the way for more research about
adversarial machine learning in practice, but yields also insights for
regulation and auditing.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2207.03066v1,2022-07-07T03:23:04Z,2022-07-07T03:23:04Z,Device-Cloud Collaborative Recommendation via Meta Controller,"On-device machine learning enables the lightweight deployment of
recommendation models in local clients, which reduces the burden of the
cloud-based recommenders and simultaneously incorporates more real-time user
features. Nevertheless, the cloud-based recommendation in the industry is still
very important considering its powerful model capacity and the efficient
candidate generation from the billion-scale item pool. Previous attempts to
integrate the merits of both paradigms mainly resort to a sequential mechanism,
which builds the on-device recommender on top of the cloud-based
recommendation. However, such a design is inflexible when user interests
dramatically change:
  the on-device model is stuck by the limited item cache while the cloud-based
recommendation based on the large item pool do not respond without the new
re-fresh feedback.
  To overcome this issue, we propose a meta controller to dynamically manage
the collaboration between the on-device recommender and the cloud-based
recommender, and introduce a novel efficient sample construction from the
causal perspective to solve the dataset absence issue of meta controller. On
the basis of the counterfactual samples and the extended training, extensive
experiments in the industrial recommendation scenarios show the promise of meta
controller in the device-cloud collaboration.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2207.01595v1,2022-07-04T17:27:03Z,2022-07-04T17:27:03Z,"Deep Learning for Short-term Instant Energy Consumption Forecasting in
  the Manufacturing Sector","Electricity is a volatile power source that requires great planning and
resource management for both short and long term. More specifically, in the
short-term, accurate instant energy consumption forecasting contributes greatly
to improve the efficiency of buildings, opening new avenues for the adoption of
renewable energy. In that regard, data-driven approaches, namely the ones based
on machine learning, are begin to be preferred over more traditional ones since
they provide not only more simplified ways of deployment but also state of the
art results. In that sense, this work applies and compares the performance of
several deep learning algorithms, LSTM, CNN, mixed CNN-LSTM and TCN, in a real
testbed within the manufacturing sector. The experimental results suggest that
the TCN is the most reliable method for predicting instant energy consumption
in the short-term.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2207.01219v1,2022-07-04T06:08:01Z,2022-07-04T06:08:01Z,"Masked Self-Supervision for Remaining Useful Lifetime Prediction in
  Machine Tools","Prediction of Remaining Useful Lifetime(RUL) in the modern manufacturing and
automation workplace for machines and tools is essential in Industry 4.0. This
is clearly evident as continuous tool wear, or worse, sudden machine breakdown
will lead to various manufacturing failures which would clearly cause economic
loss. With the availability of deep learning approaches, the great potential
and prospect of utilizing these for RUL prediction have resulted in several
models which are designed driven by operation data of manufacturing machines.
Current efforts in these which are based on fully-supervised models heavily
rely on the data labeled with their RULs. However, the required RUL prediction
data (i.e. the annotated and labeled data from faulty and/or degraded machines)
can only be obtained after the machine breakdown occurs. The scarcity of broken
machines in the modern manufacturing and automation workplace in real-world
situations increases the difficulty of getting sufficient annotated and labeled
data. In contrast, the data from healthy machines is much easier to be
collected. Noting this challenge and the potential for improved effectiveness
and applicability, we thus propose (and also fully develop) a method based on
the idea of masked autoencoders which will utilize unlabeled data to do
self-supervision. In thus the work here, a noteworthy masked self-supervised
learning approach is developed and utilized. This is designed to seek to build
a deep learning model for RUL prediction by utilizing unlabeled data. The
experiments to verify the effectiveness of this development are implemented on
the C-MAPSS datasets (which are collected from the data from the NASA turbofan
engine). The results rather clearly show that our development and approach here
perform better, in both accuracy and effectiveness, for RUL prediction when
compared with approaches utilizing a fully-supervised model.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.15285v1,2022-06-30T13:44:48Z,2022-06-30T13:44:48Z,"Machine learning for automated quality control in injection moulding
  manufacturing","Machine learning (ML) may improve and automate quality control (QC) in
injection moulding manufacturing. As the labelling of extensive, real-world
process data is costly, however, the use of simulated process data may offer a
first step towards a successful implementation. In this study, simulated data
was used to develop a predictive model for the product quality of an injection
moulded sorting container. The achieved accuracy, specificity and sensitivity
on the test set was $99.4\%$, $99.7\%$ and $94.7\%$, respectively. This study
thus shows the potential of ML towards automated QC in injection moulding and
encourages the extension to ML models trained on real-world data.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.11618v1,2022-06-23T11:02:10Z,2022-06-23T11:02:10Z,Learning to Use Local Cuts,"An essential component in modern solvers for mixed-integer (linear) programs
(MIPs) is the separation of additional inequalities (cutting planes) to tighten
the linear programming relaxation. Various algorithmic decisions are necessary
when integrating cutting plane methods into a branch-and-bound (B&B) solver as
there is always the trade-off between the efficiency of the cuts and their
costs, given that they tend to slow down the solution time of the relaxation.
One of the most crucial questions is: Should cuts only be generated globally at
the root or also locally at nodes of the tree? We address this question by a
machine learning approach for which we train a regression forest to predict the
speed-up (or slow-down) provided by using local cuts. We demonstrate with an
open implementation that this helps to improve the performance of the FICO
Xpress MIP solver on a public test set of general MIP instances. We further
report on the impact of a practical implementation inside Xpress on a large,
diverse set of real-world industry MIPs.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.11338v1,2022-06-22T19:33:21Z,2022-06-22T19:33:21Z,Synthetic Data-Based Simulators for Recommender Systems: A Survey,"This survey aims at providing a comprehensive overview of the recent trends
in the field of modeling and simulation (M&S) of interactions between users and
recommender systems and applications of the M&S to the performance improvement
of industrial recommender engines. We start with the motivation behind the
development of frameworks implementing the simulations -- simulators -- and the
usage of them for training and testing recommender systems of different types
(including Reinforcement Learning ones). Furthermore, we provide a new
consistent classification of existing simulators based on their functionality,
approbation, and industrial effectiveness and moreover make a summary of the
simulators found in the research literature. Besides other things, we discuss
the building blocks of simulators: methods for synthetic data (user, item,
user-item responses) generation, methods for what-if experimental analysis,
methods and datasets used for simulation quality evaluation (including the
methods that monitor and/or close possible simulation-to-reality gaps), and
methods for summarization of experimental simulation results. Finally, this
survey considers emerging topics and open problems in the field.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.09707v1,2022-06-20T10:56:08Z,2022-06-20T10:56:08Z,The Role of Machine Learning in Cybersecurity,"Machine Learning (ML) represents a pivotal technology for current and future
information systems, and many domains already leverage the capabilities of ML.
However, deployment of ML in cybersecurity is still at an early stage,
revealing a significant discrepancy between research and practice. Such
discrepancy has its root cause in the current state-of-the-art, which does not
allow to identify the role of ML in cybersecurity. The full potential of ML
will never be unleashed unless its pros and cons are understood by a broad
audience.
  This paper is the first attempt to provide a holistic understanding of the
role of ML in the entire cybersecurity domain -- to any potential reader with
an interest in this topic. We highlight the advantages of ML with respect to
human-driven detection methods, as well as the additional tasks that can be
addressed by ML in cybersecurity. Moreover, we elucidate various intrinsic
problems affecting real ML deployments in cybersecurity. Finally, we present
how various stakeholders can contribute to future developments of ML in
cybersecurity, which is essential for further progress in this field. Our
contributions are complemented with two real case studies describing industrial
applications of ML as defense against cyber-threats.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.09590v1,2022-06-20T06:36:45Z,2022-06-20T06:36:45Z,"From Multi-agent to Multi-robot: A Scalable Training and Evaluation
  Platform for Multi-robot Reinforcement Learning","Multi-agent reinforcement learning (MARL) has been gaining extensive
attention from academia and industries in the past few decades. One of the
fundamental problems in MARL is how to evaluate different approaches
comprehensively. Most existing MARL methods are evaluated in either video games
or simplistic simulated scenarios. It remains unknown how these methods perform
in real-world scenarios, especially multi-robot systems. This paper introduces
a scalable emulation platform for multi-robot reinforcement learning (MRRL)
called SMART to meet this need. Precisely, SMART consists of two components: 1)
a simulation environment that provides a variety of complex interaction
scenarios for training and 2) a real-world multi-robot system for realistic
performance evaluation. Besides, SMART offers agent-environment APIs that are
plug-and-play for algorithm implementation. To illustrate the practicality of
our platform, we conduct a case study on the cooperative driving lane change
scenario. Building off the case study, we summarize several unique challenges
of MRRL, which are rarely considered previously. Finally, we open-source the
simulation environments, associated benchmark tasks, and state-of-the-art
baselines to encourage and empower MRRL research.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.06817v1,2022-06-14T13:11:22Z,2022-06-14T13:11:22Z,"Physics-Informed Transfer Learning Strategy to Accelerate Unsteady Fluid
  Flow Simulations","Since the derivation of the Navier Stokes equations, it has become possible
to numerically solve real world viscous flow problems (computational fluid
dynamics (CFD)). However, despite the rapid advancements in the performance of
central processing units (CPUs), the computational cost of simulating transient
flows with extremely small time/grid scale physics is still unrealistic. In
recent years, machine learning (ML) technology has received significant
attention across industries, and this big wave has propagated various interests
in the fluid dynamics community. Recent ML CFD studies have revealed that
completely suppressing the increase in error with the increase in interval
between the training and prediction times in data driven methods is
unrealistic. The development of a practical CFD acceleration methodology that
applies ML is a remaining issue. Therefore, the objectives of this study were
developing a realistic ML strategy based on a physics-informed transfer
learning and validating the accuracy and acceleration performance of this
strategy using an unsteady CFD dataset. This strategy can determine the timing
of transfer learning while monitoring the residuals of the governing equations
in a cross coupling computation framework. Consequently, our hypothesis that
continuous fluid flow time series prediction is feasible was validated, as the
intermediate CFD simulations periodically not only reduce the increased
residuals but also update the network parameters. Notably, the cross coupling
strategy with a grid based network model does not compromise the simulation
accuracy for computational acceleration. The simulation was accelerated by 1.8
times in the laminar counterflow CFD dataset condition including the parameter
updating time. Open source CFD software OpenFOAM and open-source ML software
TensorFlow were used in this feasibility study.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.05096v1,2022-06-10T13:43:03Z,2022-06-10T13:43:03Z,Skill Transfer for Temporally-Extended Task Specifications,"Deploying robots in real-world domains, such as households and flexible
manufacturing lines, requires the robots to be taskable on demand. Linear
temporal logic (LTL) is a widely-used specification language with a
compositional grammar that naturally induces commonalities across tasks.
However, the majority of prior research on reinforcement learning with LTL
specifications treats every new formula independently. We propose LTL-Transfer,
a novel algorithm that enables subpolicy reuse across tasks by segmenting
policies for training tasks into portable transition-centric skills capable of
satisfying a wide array of unseen LTL specifications while respecting
safety-critical constraints. Our experiments in a Minecraft-inspired domain
demonstrate the capability of LTL-Transfer to satisfy over 90% of 500 unseen
tasks while training on only 50 task specifications and never violating a
safety constraint. We also deployed LTL-Transfer on a quadruped mobile
manipulator in a household environment to show its ability to transfer to many
fetch and delivery tasks in a zero-shot fashion.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.04967v2,2022-06-15T02:01:25Z,2022-06-10T09:45:25Z,Deep Learning-based Massive MIMO CSI Acquisition for 5G Evolution and 6G,"Recently, inspired by successful applications in many fields, deep learning
(DL) technologies for CSI acquisition have received considerable research
interest from both academia and industry. Considering the practical feedback
mechanism of 5th generation (5G) New radio (NR) networks, we propose two
implementation schemes for artificial intelligence for CSI (AI4CSI), the
DL-based receiver and end-to-end design, respectively. The proposed AI4CSI
schemes were evaluated in 5G NR networks in terms of spectrum efficiency (SE),
feedback overhead, and computational complexity, and compared with legacy
schemes. To demonstrate whether these schemes can be used in real-life
scenarios, both the modeled-based channel data and practically measured
channels were used in our investigations. When DL-based CSI acquisition is
applied to the receiver only, which has little air interface impact, it
provides approximately 25\% SE gain at a moderate feedback overhead level. It
is feasible to deploy it in current 5G networks during 5G evolutions. For the
end-to-end DL-based CSI enhancements, the evaluations also demonstrated their
additional performance gain on SE, which is 6% -- 26% compared with DL-based
receivers and 33% -- 58% compared with legacy CSI schemes. Considering its
large impact on air-interface design, it will be a candidate technology for 6th
generation (6G) networks, in which an air interface designed by artificial
intelligence can be used.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.04355v1,2022-06-09T08:56:11Z,2022-06-09T08:56:11Z,Graph Attention Multi-Layer Perceptron,"Graph neural networks (GNNs) have achieved great success in many graph-based
applications. However, the enormous size and high sparsity level of graphs
hinder their applications under industrial scenarios. Although some scalable
GNNs are proposed for large-scale graphs, they adopt a fixed $K$-hop
neighborhood for each node, thus facing the over-smoothing issue when adopting
large propagation depths for nodes within sparse regions. To tackle the above
issue, we propose a new GNN architecture -- Graph Attention Multi-Layer
Perceptron (GAMLP), which can capture the underlying correlations between
different scales of graph knowledge. We have deployed GAMLP in Tencent with the
Angel platform, and we further evaluate GAMLP on both real-world datasets and
large-scale industrial datasets. Extensive experiments on these 14 graph
datasets demonstrate that GAMLP achieves state-of-the-art performance while
enjoying high scalability and efficiency. Specifically, it outperforms GAT by
1.3\% regarding predictive accuracy on our large-scale Tencent Video dataset
while achieving up to $50\times$ training speedup. Besides, it ranks top-1 on
both the leaderboards of the largest homogeneous and heterogeneous graph (i.e.,
ogbn-papers100M and ogbn-mag) of Open Graph Benchmark.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.03960v1,2022-06-08T15:25:58Z,2022-06-08T15:25:58Z,Predict better with less training data using a QNN,"Over the past decade, machine learning revolutionized vision-based quality
assessment for which convolutional neural networks (CNNs) have now become the
standard. In this paper, we consider a potential next step in this development
and describe a quanvolutional neural network (QNN) algorithm that efficiently
maps classical image data to quantum states and allows for reliable image
analysis. We practically demonstrate how to leverage quantum devices in
computer vision and how to introduce quantum convolutions into classical CNNs.
Dealing with a real world use case in industrial quality control, we implement
our hybrid QNN model within the PennyLane framework and empirically observe it
to achieve better predictions using much fewer training data than classical
CNNs. In other words, we empirically observe a genuine quantum advantage for an
industrial application where the advantage is due to superior data encoding.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.03288v1,2022-06-07T13:28:43Z,2022-06-07T13:28:43Z,"Collaborative Intelligence Orchestration: Inconsistency-Based Fusion of
  Semi-Supervised Learning and Active Learning","While annotating decent amounts of data to satisfy sophisticated learning
models can be cost-prohibitive for many real-world applications. Active
learning (AL) and semi-supervised learning (SSL) are two effective, but often
isolated, means to alleviate the data-hungry problem. Some recent studies
explored the potential of combining AL and SSL to better probe the unlabeled
data. However, almost all these contemporary SSL-AL works use a simple
combination strategy, ignoring SSL and AL's inherent relation. Further, other
methods suffer from high computational costs when dealing with large-scale,
high-dimensional datasets. Motivated by the industry practice of labeling data,
we propose an innovative Inconsistency-based virtual aDvErsarial Active
Learning (IDEAL) algorithm to further investigate SSL-AL's potential
superiority and achieve mutual enhancement of AL and SSL, i.e., SSL propagates
label information to unlabeled samples and provides smoothed embeddings for AL,
while AL excludes samples with inconsistent predictions and considerable
uncertainty for SSL. We estimate unlabeled samples' inconsistency by
augmentation strategies of different granularities, including fine-grained
continuous perturbation exploration and coarse-grained data transformations.
Extensive experiments, in both text and image domains, validate the
effectiveness of the proposed algorithm, comparing it against state-of-the-art
baselines. Two real-world case studies visualize the practical industrial value
of applying and deploying the proposed data sampling algorithm.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.00868v1,2022-06-02T04:34:57Z,2022-06-02T04:34:57Z,"6G Survey on Challenges, Requirements, Applications, Key Enabling
  Technologies, Use Cases, AI integration issues and Security aspects","The fifth-generation (5G) network is likely to bring in high data rates, more
reliability, and low delays for mobile, personal and local area networks.
Alongside the rapid growth of smart wireless sensing and communication
technologies, data traffic has significantly risen, and existing 5G networks
are not fully capable of supporting future massive data traffic in terms of
services, storage, and processing. To meet the forthcoming challenges, the
research community is investigating the Terahertz-based sixth-generation (6G)
wireless network which is supposed to be offered for industrial usage in around
10 years. This is the right time to explore and learn about various 6G aspects
that will play a key role in the successful execution and implementation of 6G
networks in the future. This survey provides a review of specifications,
requirements, applications, enabling technologies including disruptive and
innovative, integration of 6G with advanced architectures and networks like
software-defined networks (SDN), network functions virtualization (NFV),
cloud/fog computing, etc, artificial intelligence (AI) oriented technologies,
privacy and security issues and solutions, and potential futuristic use cases:
virtual reality, smart healthcare and Industry 5.0. Furthermore, based on the
conducted review, challenges and future research directions are highlighted to
aid the deployment of 6G networks.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.02628v1,2022-06-01T09:57:34Z,2022-06-01T09:57:34Z,HYCEDIS: HYbrid Confidence Engine for Deep Document Intelligence System,"Measuring the confidence of AI models is critical for safely deploying AI in
real-world industrial systems. One important application of confidence
measurement is information extraction from scanned documents. However, there
exists no solution to provide reliable confidence score for current
state-of-the-art deep-learning-based information extractors. In this paper, we
propose a complete and novel architecture to measure confidence of current deep
learning models in document information extraction task. Our architecture
consists of a Multi-modal Conformal Predictor and a Variational
Cluster-oriented Anomaly Detector, trained to faithfully estimate its
confidence on its outputs without the need of host models modification. We
evaluate our architecture on real-wold datasets, not only outperforming
competing confidence estimators by a huge margin but also demonstrating
generalization ability to out-of-distribution data.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.11267v1,2022-05-23T12:32:38Z,2022-05-23T12:32:38Z,"Fed-DART and FACT: A solution for Federated Learning in a production
  environment","Federated Learning as a decentralized artificial intelligence (AI) solution
solves a variety of problems in industrial applications. It enables a
continuously self-improving AI, which can be deployed everywhere at the edge.
However, bringing AI to production for generating a real business impact is a
challenging task. Especially in the case of Federated Learning, expertise and
resources from multiple domains are required to realize its full potential.
Having this in mind we have developed an innovative Federated Learning
framework FACT based on Fed-DART, enabling an easy and scalable deployment,
helping the user to fully leverage the potential of their private and
decentralized data.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.10635v1,2022-05-21T16:24:47Z,2022-05-21T16:24:47Z,"SplitPlace: AI Augmented Splitting and Placement of Large-Scale Neural
  Networks in Mobile Edge Environments","In recent years, deep learning models have become ubiquitous in industry and
academia alike. Deep neural networks can solve some of the most complex
pattern-recognition problems today, but come with the price of massive compute
and memory requirements. This makes the problem of deploying such large-scale
neural networks challenging in resource-constrained mobile edge computing
platforms, specifically in mission-critical domains like surveillance and
healthcare. To solve this, a promising solution is to split resource-hungry
neural networks into lightweight disjoint smaller components for pipelined
distributed processing. At present, there are two main approaches to do this:
semantic and layer-wise splitting. The former partitions a neural network into
parallel disjoint models that produce a part of the result, whereas the latter
partitions into sequential models that produce intermediate results. However,
there is no intelligent algorithm that decides which splitting strategy to use
and places such modular splits to edge nodes for optimal performance. To combat
this, this work proposes a novel AI-driven online policy, SplitPlace, that uses
Multi-Armed-Bandits to intelligently decide between layer and semantic
splitting strategies based on the input task's service deadline demands.
SplitPlace places such neural network split fragments on mobile edge devices
using decision-aware reinforcement learning for efficient and scalable
computing. Moreover, SplitPlace fine-tunes its placement engine to adapt to
volatile environments. Our experiments on physical mobile-edge environments
with real-world workloads show that SplitPlace can significantly improve the
state-of-the-art in terms of average response time, deadline violation rate,
inference accuracy, and total reward by up to 46, 69, 3 and 12 percent
respectively.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.10538v1,2022-05-21T08:35:02Z,2022-05-21T08:35:02Z,"Automated machine learning: AI-driven decision making in business
  analytics","The realization that AI-driven decision-making is indispensable in todays
fast-paced and ultra-competitive marketplace has raised interest in industrial
machine learning (ML) applications significantly. The current demand for
analytics experts vastly exceeds the supply. One solution to this problem is to
increase the user-friendliness of ML frameworks to make them more accessible
for the non-expert. Automated machine learning (AutoML) is an attempt to solve
the problem of expertise by providing fully automated off-the-shelf solutions
for model choice and hyperparameter tuning. This paper analyzed the potential
of AutoML for applications within business analytics, which could help to
increase the adoption rate of ML across all industries. The H2O AutoML
framework was benchmarked against a manually tuned stacked ML model on three
real-world datasets to test its performance, robustness, and reliability. The
manually tuned ML model could reach a performance advantage in all three case
studies used in the experiment. Nevertheless, the H2O AutoML package proved to
be quite potent. It is fast, easy to use, and delivers reliable results, which
come close to a professionally tuned ML model. The H2O AutoML framework in its
current capacity is a valuable tool to support fast prototyping with the
potential to shorten development and deployment cycles. It can also bridge the
existing gap between supply and demand for ML experts and is a big step towards
fully automated decisions in business analytics.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.09084v1,2022-05-18T17:19:34Z,2022-05-18T17:19:34Z,"Industry 5.0 is Coming: A Survey on Intelligent NextG Wireless Networks
  as Technological Enablers","Industry 5.0 vision, a step toward the next industrial revolution and
enhancement to Industry 4.0, envisioned the new goals of resilient,
sustainable, and human-centric approaches in diverse emerging applications,
e.g., factories-of-the-future, digital society. The vision seeks to leverage
human intelligence and creativity in nexus with intelligent, efficient, and
reliable cognitive collaborating robots (cobots) to achieve zero waste,
zerodefect, and mass customization-based manufacturing solutions. However, the
vision requires the merging of cyber-physical worlds through utilizing Industry
5.0 technological enablers, e.g., cognitive cobots, person-centric artificial
intelligence (AI), cyberphysical systems, digital twins, hyperconverged data
storage and computing, communication infrastructure, and others. In this
regard, the convergence of the emerging computational intelligence (CI)
paradigm and next-generation wireless networks (NGWNs) can fulfill the
stringent communication and computation requirements of the technological
enablers in the Industry 5.0 vision, which is the aim of this survey-based
tutorial. In this article, we address this issue by reviewing and analyzing
current emerging concepts and technologies, e.g., CI tools and frameworks,
network-in-box architecture, open radio access networks, softwarized service
architectures, potential enabling services, and others, essential for designing
the objectives of CINGWNs to fulfill the Industry 5.0 vision requirements.
Finally, we provide a list of lessons learned from our detailed review,
research challenges, and open issues that should be addressed in CI-NGWNs to
realize Industry 5.0.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.11248v1,2022-05-18T11:49:25Z,2022-05-18T11:49:25Z,Efficient Mixed Dimension Embeddings for Matrix Factorization,"Despite the prominence of neural network approaches in the field of
recommender systems, simple methods such as matrix factorization with quadratic
loss are still used in industry for several reasons. These models can be
trained with alternating least squares, which makes them easy to implement in a
massively parallel manner, thus making it possible to utilize billions of
events from real-world datasets. Large-scale recommender systems need to
account for severe popularity skew in the distributions of users and items, so
a lot of research is focused on implementing sparse, mixed dimension or shared
embeddings to reduce both the number of parameters and overfitting on rare
users and items. In this paper we propose two matrix factorization models with
mixed dimension embeddings, which can be optimized in a massively parallel
fashion using the alternating least squares approach.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.08292v2,2022-07-12T03:09:15Z,2022-05-17T12:44:54Z,"Providing Location Information at Edge Networks: A Federated
  Learning-Based Approach","Recently, the development of mobile edge computing has enabled exhilarating
edge artificial intelligence (AI) with fast response and low communication
cost. The location information of edge devices is essential to support the edge
AI in many scenarios, like smart home, intelligent transportation systems and
integrated health care. Taking advantages of deep learning intelligence, the
centralized machine learning (ML)-based positioning technique has received
heated attention from both academia and industry. However, some potential
issues, such as location information leakage and huge data traffic, limit its
application. Fortunately, a newly emerging privacy-preserving distributed ML
mechanism, named federated learning (FL), is expected to alleviate these
concerns. In this article, we illustrate a framework of FL-based localization
system as well as the involved entities at edge networks. Moreover, the
advantages of such system are elaborated. On practical implementation of it, we
investigate the field-specific issues associated with system-level solutions,
which are further demonstrated over a real-word database. Moreover, future
challenging open problems in this field are outlined.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.08084v2,2022-05-19T06:50:31Z,2022-05-17T04:13:42Z,"M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender
  Systems","Industrial recommender systems have been growing increasingly complex, may
involve \emph{diverse domains} such as e-commerce products and user-generated
contents, and can comprise \emph{a myriad of tasks} such as retrieval, ranking,
explanation generation, and even AI-assisted content production. The mainstream
approach so far is to develop individual algorithms for each domain and each
task. In this paper, we explore the possibility of developing a unified
foundation model to support \emph{open-ended domains and tasks} in an
industrial recommender system, which may reduce the demand on downstream
settings' data and can minimize the carbon footprint by avoiding training a
separate model from scratch for every task. Deriving a unified foundation is
challenging due to (i) the potentially unlimited set of downstream domains and
tasks, and (ii) the real-world systems' emphasis on computational efficiency.
We thus build our foundation upon M6, an existing large-scale industrial
pretrained language model similar to GPT-3 and T5, and leverage M6's pretrained
ability for sample-efficient downstream adaptation, by representing user
behavior data as plain texts and converting the tasks to either language
understanding or generation. To deal with a tight hardware budget, we propose
an improved version of prompt tuning that outperforms fine-tuning with
negligible 1\% task-specific parameters, and employ techniques such as late
interaction, early exiting, parameter sharing, and pruning to further reduce
the inference time and the model size. We demonstrate the foundation model's
versatility on a wide range of tasks such as retrieval, ranking, zero-shot
recommendation, explanation generation, personalized content creation, and
conversational recommendation, and manage to deploy it on both cloud servers
and mobile devices.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.04189v1,2022-05-09T11:08:45Z,2022-05-09T11:08:45Z,"FoReCo: a forecast-based recovery mechanism for real-time remote control
  of robotic manipulators","Wireless communications represent a game changer for future manufacturing
plants, enabling flexible production chains as machinery and other components
are not restricted to a location by the rigid wired connections on the factory
floor. However, the presence of electromagnetic interference in the wireless
spectrum may result in packet loss and delay, making it a challenging
environment to meet the extreme reliability requirements of industrial
applications. In such conditions, achieving real-time remote control, either
from the Edge or Cloud, becomes complex. In this paper, we investigate a
forecast-based recovery mechanism for real-time remote control of robotic
manipulators (FoReCo) that uses Machine Learning (ML) to infer lost commands
caused by interference in the wireless channel. FoReCo is evaluated through
both simulation and experimentation in interference prone IEEE 802.11 wireless
links, and using a commercial research robot that performs pick-and-place
tasks. Results show that in case of interference, FoReCo trajectory error is
decreased by x18 and x2 times in simulation and experimentation, and that
FoReCo is sufficiently lightweight to be deployed in the hardware of already
used in existing solutions.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.02446v1,2022-05-05T05:33:01Z,2022-05-05T05:33:01Z,"Multi-Graph based Multi-Scenario Recommendation in Large-scale Online
  Video Services","Recently, industrial recommendation services have been boosted by the
continual upgrade of deep learning methods. However, they still face de-biasing
challenges such as exposure bias and cold-start problem, where circulations of
machine learning training on human interaction history leads algorithms to
repeatedly suggest exposed items while ignoring less-active ones. Additional
problems exist in multi-scenario platforms, e.g. appropriate data fusion from
subsidiary scenarios, which we observe could be alleviated through graph
structured data integration via message passing.
  In this paper, we present a multi-graph structured multi-scenario
recommendation solution, which encapsulates interaction data across scenarios
with multi-graph and obtains representation via graph learning. Extensive
offline and online experiments on real-world datasets are conducted where the
proposed method demonstrates an increase of 0.63% and 0.71% in CTR and Video
Views per capita on new users over deployed set of baselines and outperforms
regular method in increasing the number of outer-scenario videos by 25% and
video watches by 116%, validating its superiority in activating cold videos and
enriching target recommendation.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.00258v1,2022-04-30T13:03:53Z,2022-04-30T13:03:53Z,"EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language
  Processing","The success of Pre-Trained Models (PTMs) has reshaped the development of
Natural Language Processing (NLP). Yet, it is not easy to obtain
high-performing models and deploy them online for industrial practitioners. To
bridge this gap, EasyNLP is designed to make it easy to build NLP applications,
which supports a comprehensive suite of NLP algorithms. It further features
knowledge-enhanced pre-training, knowledge distillation and few-shot learning
functionalities for large-scale PTMs, and provides a unified framework of model
training, inference and deployment for real-world applications. Currently,
EasyNLP has powered over ten business units within Alibaba Group and is
seamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.
The source code of our EasyNLP toolkit is released at GitHub
(https://github.com/alibaba/EasyNLP).",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.14069v1,2022-04-29T13:05:41Z,2022-04-29T13:05:41Z,"Gating-adapted Wavelet Multiresolution Analysis for Exposure Sequence
  Modeling in CTR prediction","The exposure sequence is being actively studied for user interest modeling in
Click-Through Rate (CTR) prediction. However, the existing methods for exposure
sequence modeling bring extensive computational burden and neglect noise
problems, resulting in an excessively latency and the limited performance in
online recommenders. In this paper, we propose to address the high latency and
noise problems via Gating-adapted wavelet multiresolution analysis (Gama),
which can effectively denoise the extremely long exposure sequence and
adaptively capture the implied multi-dimension user interest with linear
computational complexity. This is the first attempt to integrate non-parametric
multiresolution analysis technique into deep neural networks to model user
exposure sequence. Extensive experiments on large scale benchmark dataset and
real production dataset confirm the effectiveness of Gama for exposure sequence
modeling, especially in cold-start scenarios. Benefited from its low latency
and high effecitveness, Gama has been deployed in our real large-scale
industrial recommender, successfully serving over hundreds of millions users.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.10899v1,2022-04-22T19:20:49Z,2022-04-22T19:20:49Z,"Comparative Study of Machine Learning Test Case Prioritization for
  Continuous Integration Testing","There is a growing body of research indicating the potential of machine
learning to tackle complex software testing challenges. One such challenge
pertains to continuous integration testing, which is highly time-constrained,
and generates a large amount of data coming from iterative code commits and
test runs. In such a setting, we can use plentiful test data for training
machine learning predictors to identify test cases able to speed up the
detection of regression bugs introduced during code integration. However,
different machine learning models can have different fault prediction
performance depending on the context and the parameters of continuous
integration testing, for example variable time budget available for continuous
integration cycles, or the size of test execution history used for learning to
prioritize failing test cases. Existing studies on test case prioritization
rarely study both of these factors, which are essential for the continuous
integration practice. In this study we perform a comprehensive comparison of
the fault prediction performance of machine learning approaches that have shown
the best performance on test case prioritization tasks in the literature. We
evaluate the accuracy of the classifiers in predicting fault-detecting tests
for different values of the continuous integration time budget and with
different length of test history used for training the classifiers. In
evaluation, we use real-world industrial datasets from a continuous integration
practice. The results show that different machine learning models have
different performance for different size of test history used for model
training and for different time budget available for test case execution. Our
results imply that machine learning approaches for test prioritization in
continuous integration testing should be carefully configured to achieve
optimal performance.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.10183v1,2022-04-20T13:30:04Z,2022-04-20T13:30:04Z,"Multi-Component Optimization and Efficient Deployment of Neural-Networks
  on Resource-Constrained IoT Hardware","The majority of IoT devices like smartwatches, smart plugs, HVAC controllers,
etc., are powered by hardware with a constrained specification (low memory,
clock speed and processor) which is insufficient to accommodate and execute
large, high-quality models. On such resource-constrained devices, manufacturers
still manage to provide attractive functionalities (to boost sales) by
following the traditional approach of programming IoT devices/products to
collect and transmit data (image, audio, sensor readings, etc.) to their
cloud-based ML analytics platforms. For decades, this online approach has been
facing issues such as compromised data streams, non-real-time analytics due to
latency, bandwidth constraints, costly subscriptions, recent privacy issues
raised by users and the GDPR guidelines, etc. In this paper, to enable
ultra-fast and accurate AI-based offline analytics on resource-constrained IoT
devices, we present an end-to-end multi-component model optimization sequence
and open-source its implementation. Researchers and developers can use our
optimization sequence to optimize high memory, computation demanding models in
multiple aspects in order to produce small size, low latency, low-power
consuming models that can comfortably fit and execute on resource-constrained
hardware. The experimental results show that our optimization components can
produce models that are; (i) 12.06 x times compressed; (ii) 0.13% to 0.27% more
accurate; (iii) Orders of magnitude faster unit inference at 0.06 ms. Our
optimization sequence is generic and can be applied to any state-of-the-art
models trained for anomaly detection, predictive maintenance, robotics, voice
recognition, and machine vision.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.03998v1,2022-04-08T11:08:03Z,2022-04-08T11:08:03Z,"SnapMode: An Intelligent and Distributed Large-Scale Fashion Image
  Retrieval Platform Based On Big Data and Deep Generative Adversarial Network
  Technologies","Fashion is now among the largest industries worldwide, for it represents
human history and helps tell the worlds story. As a result of the Fourth
Industrial Revolution, the Internet has become an increasingly important source
of fashion information. However, with a growing number of web pages and social
data, it is nearly impossible for humans to manually catch up with the ongoing
evolution and the continuously variable content in this domain. The proper
management and exploitation of big data can pave the way for the substantial
growth of the global economy as well as citizen satisfaction. Therefore,
computer scientists have found it challenging to handle e-commerce fashion
websites by using big data and machine learning technologies. This paper first
proposes a scalable focused Web Crawler engine based on the distributed
computing platforms to extract and process fashion data on e-commerce websites.
The role of the proposed platform is then described in developing a
disentangled feature extraction method by employing deep convolutional
generative adversarial networks (DCGANs) for content-based image indexing and
retrieval. Finally, the state-of-the-art solutions are compared, and the
results of the proposed approach are analyzed on a standard dataset. For the
real-life implementation of the proposed solution, a Web-based application is
developed on Apache Storm, Kafka, Solr, and Milvus platforms to create a
fashion search engine called SnapMode.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.03332v1,2022-04-07T10:02:52Z,2022-04-07T10:02:52Z,"Predicting Performance of Heterogeneous AI Systems with Discrete-Event
  Simulations","In recent years, artificial intelligence (AI) technologies have found
industrial applications in various fields. AI systems typically possess complex
software and heterogeneous CPU/GPU hardware architecture, making it difficult
to answer basic questions considering performance evaluation and software
optimization. Where is the bottleneck impeding the system? How does the
performance scale with the workload? How the speed-up of a specific module
would contribute to the whole system? Finding the answers to these questions
through experiments on the real system could require a lot of computational,
human, financial, and time resources. A solution to cut these costs is to use a
fast and accurate simulation model preparatory to implementing anything in the
real system. In this paper, we propose a discrete-event simulation model of a
high-load heterogeneous AI system in the context of video analytics. Using the
proposed model, we estimate: 1) the performance scalability with the increasing
number of cameras; 2) the performance impact of integrating a new module; 3)
the performance gain from optimizing a single module. We show that the
performance estimation accuracy of the proposed model is higher than 90%. We
also demonstrate, that the considered system possesses a counter-intuitive
relationship between workload and performance, which nevertheless is correctly
inferred by the proposed simulation model.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.01379v3,2022-04-11T12:46:57Z,2022-04-04T10:52:20Z,"Taking ROCKET on an Efficiency Mission: Multivariate Time Series
  Classification with LightWaveS","Nowadays, with the rising number of sensors in sectors such as healthcare and
industry, the problem of multivariate time series classification (MTSC) is
getting increasingly relevant and is a prime target for machine and deep
learning approaches. Their expanding adoption in real-world environments is
causing a shift in focus from the pursuit of ever-higher prediction accuracy
with complex models towards practical, deployable solutions that balance
accuracy and parameters such as prediction speed. An MTSC model that has
attracted attention recently is ROCKET, based on random convolutional kernels,
both because of its very fast training process and its state-of-the-art
accuracy. However, the large number of features it utilizes may be detrimental
to inference time. Examining its theoretical background and limitations enables
us to address potential drawbacks and present LightWaveS: a framework for
accurate MTSC, which is fast both during training and inference. Specifically,
utilizing wavelet scattering transformation and distributed feature selection,
we manage to create a solution that employs just 2.5% of the ROCKET features,
while achieving accuracy comparable to recent MTSC models. LightWaveS also
scales well across multiple compute nodes and with the number of input channels
during training. In addition, it can significantly reduce the input size and
provide insight to an MTSC problem by keeping only the most useful channels. We
present three versions of our algorithm and their results on distributed
training time and scalability, accuracy, and inference speedup. We show that we
achieve speedup ranging from 9x to 53x compared to ROCKET during inference on
an edge device, on datasets with comparable accuracy.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.01075v1,2022-04-03T13:49:36Z,2022-04-03T13:49:36Z,"Data Cards: Purposeful and Transparent Dataset Documentation for
  Responsible AI","As research and industry moves towards large-scale models capable of numerous
downstream tasks, the complexity of understanding multi-modal datasets that
give nuance to models rapidly increases. A clear and thorough understanding of
a dataset's origins, development, intent, ethical considerations and evolution
becomes a necessary step for the responsible and informed deployment of models,
especially those in people-facing contexts and high-risk domains. However, the
burden of this understanding often falls on the intelligibility, conciseness,
and comprehensiveness of the documentation. It requires consistency and
comparability across the documentation of all datasets involved, and as such
documentation must be treated as a user-centric product in and of itself. In
this paper, we propose Data Cards for fostering transparent, purposeful and
human-centered documentation of datasets within the practical contexts of
industry and research. Data Cards are structured summaries of essential facts
about various aspects of ML datasets needed by stakeholders across a dataset's
lifecycle for responsible AI development. These summaries provide explanations
of processes and rationales that shape the data and consequently the models,
such as upstream sources, data collection and annotation methods; training and
evaluation methods, intended use; or decisions affecting model performance. We
also present frameworks that ground Data Cards in real-world utility and
human-centricity. Using two case studies, we report on desirable
characteristics that support adoption across domains, organizational
structures, and audience groups. Finally, we present lessons learned from
deploying over 20 Data Cards.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.01042v1,2022-04-01T07:05:13Z,2022-04-01T07:05:13Z,"Machine Learning and Artificial Intelligence in Circular Economy: A
  Bibliometric Analysis and Systematic Literature Review","With unorganized, unplanned and improper use of limited raw materials, an
abundant amount of waste is being produced, which is harmful to our environment
and ecosystem. While traditional linear production lines fail to address
far-reaching issues like waste production and a shorter product life cycle, a
prospective concept, namely circular economy (CE), has shown promising
prospects to be adopted at industrial and governmental levels. CE aims to
complete the product life cycle loop by bringing out the highest values from
raw materials in the design phase and later on by reusing, recycling, and
remanufacturing. Innovative technologies like artificial intelligence (AI) and
machine learning(ML) provide vital assistance in effectively adopting and
implementing CE in real-world practices. This study explores the adoption and
integration of applied AI techniques in CE. First, we conducted bibliometric
analysis on a collection of 104 SCOPUS indexed documents exploring the critical
research criteria in AI and CE. Forty papers were picked to conduct a
systematic literature review from these documents. The selected documents were
further divided into six categories: sustainable development, reverse
logistics, waste management, supply chain management, recycle & reuse, and
manufacturing development. Comprehensive research insights and trends have been
extracted and delineated. Finally, the research gap needing further attention
has been identified and the future research directions have also been
discussed.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.02360v1,2022-03-30T21:42:21Z,2022-03-30T21:42:21Z,"Scientometric Review of Artificial Intelligence for Operations &
  Maintenance of Wind Turbines: The Past, Present and Future","Wind energy has emerged as a highly promising source of renewable energy in
recent times. However, wind turbines regularly suffer from operational
inconsistencies, leading to significant costs and challenges in operations and
maintenance (O&M). Condition-based monitoring (CBM) and performance
assessment/analysis of turbines are vital aspects for ensuring efficient O&M
planning and cost minimisation. Data-driven decision making techniques have
witnessed rapid evolution in the wind industry for such O&M tasks during the
last decade, from applying signal processing methods in early 2010 to
artificial intelligence (AI) techniques, especially deep learning in 2020. In
this article, we utilise statistical computing to present a scientometric
review of the conceptual and thematic evolution of AI in the wind energy
sector, providing evidence-based insights into present strengths and
limitations of data-driven decision making in the wind industry. We provide a
perspective into the future and on current key challenges in data availability
and quality, lack of transparency in black box-natured AI models, and
prevailing issues in deploying models for real-time decision support, along
with possible strategies to overcome these problems. We hope that a systematic
analysis of the past, present and future of CBM and performance assessment can
encourage more organisations to adopt data-driven decision making techniques in
O&M towards making wind energy sources more reliable, contributing to the
global efforts of tackling climate change.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.16407v1,2022-03-30T15:40:43Z,2022-03-30T15:40:43Z,"Hybrid Diffractive Optics Design via Hardware-in-the-Loop Methodology
  for Achromatic Extended-Depth-of-Field Imaging","End-to-end optimization of diffractive optical elements (DOEs) profile
through a digital differentiable model combined with computational imaging have
gained an increasing attention in emerging applications due to the compactness
of resultant physical setups. Despite recent works have shown the potential of
this methodology to design optics, its performance in physical setups is still
limited and affected by manufacturing artifacts of DOE, mismatch between
simulated and resultant experimental point spread functions, and calibration
errors. Additionally, the computational burden of the digital differentiable
model to effectively design the DOE is increasing, thus limiting the size of
the DOE that can be designed. To overcome the above mentioned limitations, the
broadband imaging system with phase-only spatial light modulator (SLM) as an
encoded DOE is proposed and developed in this paper. A co-design of the SLM
phase pattern and image reconstruction algorithm is produced following the
end-to-end strategy, using for optimization a convolutional neural network
equipped with quantitative and qualitative loss functions. The optics of the
imaging system is hybrid consisting of SLM as DOE and refractive lens. SLM
phase-pattern is optimized by applying the Hardware-in-the-loop technique,
which helps to eliminate the mismatch between numerical modeling and physical
reality of image formation as light propagation is not numerically modeled but
is physically done. In our experiments, the hybrid optics is implemented by the
optical projection of the SLM phase-pattern on a lens plane for a depth range
0.4-1.9m. Comparison with compound multi-lens optics such as Sony A7 III and
iPhone Xs Max cameras show that the proposed system is advanced in all-in-focus
sharp imaging.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.16393v1,2022-03-30T15:23:37Z,2022-03-30T15:23:37Z,Online Motion Style Transfer for Interactive Character Control,"Motion style transfer is highly desired for motion generation systems for
gaming. Compared to its offline counterpart, the research on online motion
style transfer under interactive control is limited. In this work, we propose
an end-to-end neural network that can generate motions with different styles
and transfer motion styles in real-time under user control. Our approach
eliminates the use of handcrafted phase features, and could be easily trained
and directly deployed in game systems. In the experiment part, we evaluate our
approach from three aspects that are essential for industrial game design:
accuracy, flexibility, and variety, and our model performs a satisfying result.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.10562v2,2022-03-22T09:34:04Z,2022-03-20T14:28:38Z,CRISPnet: Color Rendition ISP Net,"Image signal processors (ISPs) are historically grown legacy software systems
for reconstructing color images from noisy raw sensor measurements. They are
usually composited of many heuristic blocks for denoising, demosaicking, and
color restoration. Color reproduction in this context is of particular
importance, since the raw colors are often severely distorted, and each smart
phone manufacturer has developed their own characteristic heuristics for
improving the color rendition, for example of skin tones and other visually
important colors.
  In recent years there has been strong interest in replacing the historically
grown ISP systems with deep learned pipelines. Much progress has been made in
approximating legacy ISPs with such learned models. However, so far the focus
of these efforts has been on reproducing the structural features of the images,
with less attention paid to color rendition.
  Here we present CRISPnet, the first learned ISP model to specifically target
color rendition accuracy relative to a complex, legacy smart phone ISP. We
achieve this by utilizing both image metadata (like a legacy ISP would), as
well as by learning simple global semantics based on image classification --
similar to what a legacy ISP does to determine the scene type. We also
contribute a new ISP image dataset consisting of both high dynamic range
monitor data, as well as real-world data, both captured with an actual cell
phone ISP pipeline under a variety of lighting conditions, exposure times, and
gain settings.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.07521v1,2022-03-14T22:06:48Z,2022-03-14T22:06:48Z,"Automatic lane change scenario extraction and generation of scenarios in
  OpenX format from real-world data","Autonomous Vehicles (AV)'s wide-scale deployment appears imminent despite
many safety challenges yet to be resolved. The modern autonomous vehicles will
undoubtedly include machine learning and probabilistic techniques that add
significant complexity to the traditional verification and validation methods.
Road testing is essential before the deployment, but scenarios are repeatable,
and it's hard to collect challenging events. Exploring numerous, diverse and
crucial scenarios is a time-consuming and expensive approach. The research
community and industry have widely accepted scenario-based testing in the last
few years. As it is focused directly on the relevant critical road situations,
it can reduce the effort required in testing. The scenario-based testing in
simulation requires the realistic behaviour of the traffic participants to
assess the System Under Test (SUT). It is essential to capture the scenarios
from the real world to encode the behaviour of actual traffic participants.
This paper proposes a novel scenario extraction method to capture the lane
change scenarios using point-cloud data and object tracking information. This
method enables fully automatic scenario extraction compared to similar
approaches in this area. The generated scenarios are represented in OpenX
format to reuse them in the SUT evaluation easily. The motivation of this
framework is to build a validation dataset to generate many critical concrete
scenarios. The code is available online at
https://github.com/dkarunakaran/scenario_extraction_framework.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.05784v1,2022-03-11T07:50:15Z,2022-03-11T07:50:15Z,"AI-enabled Automatic Multimodal Fusion of Cone-Beam CT and Intraoral
  Scans for Intelligent 3D Tooth-Bone Reconstruction and Clinical Applications","A critical step in virtual dental treatment planning is to accurately
delineate all tooth-bone structures from CBCT with high fidelity and accurate
anatomical information. Previous studies have established several methods for
CBCT segmentation using deep learning. However, the inherent resolution
discrepancy of CBCT and the loss of occlusal and dentition information largely
limited its clinical applicability. Here, we present a Deep Dental Multimodal
Analysis (DDMA) framework consisting of a CBCT segmentation model, an intraoral
scan (IOS) segmentation model (the most accurate digital dental model), and a
fusion model to generate 3D fused crown-root-bone structures with high fidelity
and accurate occlusal and dentition information. Our model was trained with a
large-scale dataset with 503 CBCT and 28,559 IOS meshes manually annotated by
experienced human experts. For CBCT segmentation, we use a five-fold cross
validation test, each with 50 CBCT, and our model achieves an average Dice
coefficient and IoU of 93.99% and 88.68%, respectively, significantly
outperforming the baselines. For IOS segmentations, our model achieves an mIoU
of 93.07% and 95.70% on the maxillary and mandible on a test set of 200 IOS
meshes, which are 1.77% and 3.52% higher than the state-of-art method. Our DDMA
framework takes about 20 to 25 minutes to generate the fused 3D mesh model
following the sequential processing order, compared to over 5 hours by human
experts. Notably, our framework has been incorporated into a software by a
clear aligner manufacturer, and real-world clinical cases demonstrate that our
model can visualize crown-root-bone structures during the entire orthodontic
treatment and can predict risks like dehiscence and fenestration. These
findings demonstrate the potential of multi-modal deep learning to improve the
quality of digital dental models and help dentists make better clinical
decisions.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.00112v2,2022-07-07T21:37:00Z,2022-02-28T22:00:02Z,GraphWorld: Fake Graphs Bring Real Insights for GNNs,"Despite advances in the field of Graph Neural Networks (GNNs), only a small
number (~5) of datasets are currently used to evaluate new models. This
continued reliance on a handful of datasets provides minimal insight into the
performance differences between models, and is especially challenging for
industrial practitioners who are likely to have datasets which look very
different from those used as academic benchmarks. In the course of our work on
GNN infrastructure and open-source software at Google, we have sought to
develop improved benchmarks that are robust, tunable, scalable,and
generalizable. In this work we introduce GraphWorld, a novel methodology and
system for benchmarking GNN models on an arbitrarily-large population of
synthetic graphs for any conceivable GNN task. GraphWorld allows a user to
efficiently generate a world with millions of statistically diverse datasets.
It is accessible, scalable, and easy to use. GraphWorld can be run on a single
machine without specialized hardware, or it can be easily scaled up to run on
arbitrary clusters or cloud frameworks. Using GraphWorld, a user has
fine-grained control over graph generator parameters, and can benchmark
arbitrary GNN models with built-in hyperparameter tuning. We present insights
from GraphWorld experiments regarding the performance characteristics of tens
of thousands of GNN models over millions of benchmark datasets. We further show
that GraphWorld efficiently explores regions of benchmark dataset space
uncovered by standard benchmarks, revealing comparisons between models that
have not been historically obtainable. Using GraphWorld, we also are able to
study in-detail the relationship between graph properties and task performance
metrics, which is nearly impossible with the classic collection of real-world
benchmarks.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.10075v2,2022-04-14T11:09:37Z,2022-02-21T09:37:28Z,"ICSML: Industrial Control Systems Machine Learning Inference Framework
  natively executing on IEC 61131-3 compliant devices","Industrial Control Systems (ICS) have played a catalytic role in enabling the
4th Industrial Revolution. ICS devices like Programmable Logic Controllers
(PLCs), automate, monitor, and control critical processes in industrial,
energy, and commercial environments. The convergence of traditional Operational
Technology (OT) with Information Technology (IT) has opened a new and unique
threat landscape. This has inspired defense research that focuses heavily on
Machine Learning (ML) based anomaly detection methods that run on external IT
hardware, which means an increase in costs and the further expansion of the
threat landscape. To remove this requirement, we introduce the ICS machine
learning inference framework (ICSML) which enables the execution of ML model
inference natively on the PLC. ICSML is implemented in IEC 61131-3 code and
provides several optimizations to bypass the limitations imposed by the
domain-specific languages. Therefore, it works \emph{on every PLC without the
need for vendor support}. ICSML provides a complete set of components for the
creation of full ML models similarly to established ML frameworks. We run a
series of benchmarks studying memory and performance and compare our solution
to the TFLite inference framework. At the same time, we develop domain-specific
model optimizations to improve the efficiency of ICSML. To demonstrate the
abilities of ICSML, we evaluate a case study of a real defense for
process-aware attacks targeting a desalination plant.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.09549v1,2022-02-19T08:21:56Z,2022-02-19T08:21:56Z,"Learning to Detect Slip with Barometric Tactile Sensors and a Temporal
  Convolutional Neural Network","The ability to perceive object slip via tactile feedback enables humans to
accomplish complex manipulation tasks including maintaining a stable grasp.
Despite the utility of tactile information for many applications, tactile
sensors have yet to be widely deployed in industrial robotics settings; part of
the challenge lies in identifying slip and other events from the tactile data
stream. In this paper, we present a learning-based method to detect slip using
barometric tactile sensors. These sensors have many desirable properties
including high durability and reliability, and are built from inexpensive,
off-the-shelf components. We train a temporal convolution neural network to
detect slip, achieving high detection accuracies while displaying robustness to
the speed and direction of the slip motion. Further, we test our detector on
two manipulation tasks involving a variety of common objects and demonstrate
successful generalization to real-world scenarios not seen during training. We
argue that barometric tactile sensing technology, combined with data-driven
learning, is suitable for many manipulation tasks such as slip compensation.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.09113v1,2022-02-18T10:36:11Z,2022-02-18T10:36:11Z,How to Manage Tiny Machine Learning at Scale: An Industrial Perspective,"Tiny machine learning (TinyML) has gained widespread popularity where machine
learning (ML) is democratized on ubiquitous microcontrollers, processing sensor
data everywhere in real-time. To manage TinyML in the industry, where mass
deployment happens, we consider the hardware and software constraints, ranging
from available onboard sensors and memory size to ML-model architectures and
runtime platforms. However, Internet of Things (IoT) devices are typically
tailored to specific tasks and are subject to heterogeneity and limited
resources. Moreover, TinyML models have been developed with different
structures and are often distributed without a clear understanding of their
working principles, leading to a fragmented ecosystem. Considering these
challenges, we propose a framework using Semantic Web technologies to enable
the joint management of TinyML models and IoT devices at scale, from modeling
information to discovering possible combinations and benchmarking, and
eventually facilitate TinyML component exchange and reuse. We present an
ontology (semantic schema) for neural network models aligned with the World
Wide Web Consortium (W3C) Thing Description, which semantically describes IoT
devices. Furthermore, a Knowledge Graph of 23 publicly available ML models and
six IoT devices were used to demonstrate our concept in three case studies, and
we shared the code and examples to enhance reproducibility:
https://github.com/Haoyu-R/How-to-Manage-TinyML-at-Scale",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.08897v1,2022-02-17T21:00:59Z,2022-02-17T21:00:59Z,"Implementing Spiking Neural Networks on Neuromorphic Architectures: A
  Review","Recently, both industry and academia have proposed several different
neuromorphic systems to execute machine learning applications that are designed
using Spiking Neural Networks (SNNs). With the growing complexity on design and
technology fronts, programming such systems to admit and execute a machine
learning application is becoming increasingly challenging. Additionally,
neuromorphic systems are required to guarantee real-time performance, consume
lower energy, and provide tolerance to logic and memory failures. Consequently,
there is a clear need for system software frameworks that can implement machine
learning applications on current and emerging neuromorphic systems, and
simultaneously address performance, energy, and reliability. Here, we provide a
comprehensive overview of such frameworks proposed for both, platform-based
design and hardware-software co-design. We highlight challenges and
opportunities that the future holds in the area of system software technology
for neuromorphic computing.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.10336v1,2022-02-15T03:34:56Z,2022-02-15T03:34:56Z,Artificial Intelligence for the Metaverse: A Survey,"Along with the massive growth of the Internet from the 1990s until now,
various innovative technologies have been created to bring users breathtaking
experiences with more virtual interactions in cyberspace. Many virtual
environments with thousands of services and applications, from social networks
to virtual gaming worlds, have been developed with immersive experience and
digital transformation, but most are incoherent instead of being integrated
into a platform. In this context, metaverse, a term formed by combining meta
and universe, has been introduced as a shared virtual world that is fueled by
many emerging technologies, such as fifth-generation networks and beyond,
virtual reality, and artificial intelligence (AI). Among such technologies, AI
has shown the great importance of processing big data to enhance immersive
experience and enable human-like intelligence of virtual agents. In this
survey, we make a beneficial effort to explore the role of AI in the foundation
and development of the metaverse. We first deliver a preliminary of AI,
including machine learning algorithms and deep learning architectures, and its
role in the metaverse. We then convey a comprehensive investigation of AI-based
methods concerning six technical aspects that have potentials for the
metaverse: natural language processing, machine vision, blockchain, networking,
digital twin, and neural interface, and being potential for the metaverse.
Subsequently, several AI-aided applications, such as healthcare, manufacturing,
smart cities, and gaming, are studied to be deployed in the virtual worlds.
Finally, we conclude the key contribution of this survey and open some future
research directions in AI for the metaverse.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.06622v3,2022-07-06T09:34:32Z,2022-02-14T11:17:10Z,"An Edge-Cloud based Reference Architecture to support cognitive
  solutions in the Process Industry","Process Industry (PI e.g. Steel, Metals, Chemicals, Cement, Asphalt,
Ceramics) is one of the leading sectors of the world economy, characterized
however by intense environmental impact, and very high energy consumption. In
spite of a traditional low innovation pace in PI, in the recent years a strong
push at worldwide level towards the dual objective of improving the efficiency
of plants and the quality of products, significantly reducing the consumption
of electricity and CO2 emissions has taken momentum. Digital Technologies
(namely Smart Embedded Systems, IoT, Data, AI and Edge-to-Cloud Technologies)
are enabling drivers for a Twin Digital-Green Transition, as well as
foundations for human centric, safe, comfortable and inclusive work places.
Currently, digital sensors in plants produce a large amount of data which in
most cases constitutes just a potential and not a real value for Process
Industry, often locked-in in close proprietary systems and seldomly exploited.
Digital technologies, with process modelling-simulation via digital twins, can
build a bridge between the physical and the virtual worlds, bringing innovation
with great efficiency and drastic reduction of waste. In accordance with the
guidelines of Industrie 4.0, the H2020 funded CAPRI project aims to innovate
the process industry, with a modular and scalable Reference Architecture, based
on open source software, which can be implemented both in brownfield and
greenfield scenarios. The ability to distribute processing between the edge,
where the data is created, and the cloud, where the greatest computational
resources are available, facilitates the development of integrated digital
solutions with cognitive capabilities. The reference architecture is being
validated in the asphalt, steel & pharma pilot plants, allowing the development
of integrated planning solutions, with scheduling and control of the plants.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.06149v1,2022-02-12T21:43:08Z,2022-02-12T21:43:08Z,"Automatic Issue Classifier: A Transfer Learning Framework for
  Classifying Issue Reports","Issue tracking systems are used in the software industry for the facilitation
of maintenance activities that keep the software robust and up to date with
ever-changing industry requirements. Usually, users report issues that can be
categorized into different labels such as bug reports, enhancement requests,
and questions related to the software. Most of the issue tracking systems make
the labelling of these issue reports optional for the issue submitter, which
leads to a large number of unlabeled issue reports. In this paper, we present a
state-of-the-art method to classify the issue reports into their respective
categories i.e. bug, enhancement, and question. This is a challenging task
because of the common use of informal language in the issue reports. Existing
studies use traditional natural language processing approaches adopting
key-word based features, which fail to incorporate the contextual relationship
between words and therefore result in a high rate of false positives and false
negatives. Moreover, previous works utilize a uni-label approach to classify
the issue reports however, in reality, an issue-submitter can tag one issue
report with more than one label at a time. This paper presents our approach to
classify the issue reports in a multi-label setting. We use an off-the-shelf
neural network called RoBERTa and fine-tune it to classify the issue reports.
We validate our approach on issue reports belonging to numerous industrial
projects from GitHub. We were able to achieve promising F-1 scores of 81%, 74%,
and 80% for bug reports, enhancements, and questions, respectively. We also
develop an industry tool called Automatic Issue Classifier (AIC), which
automatically assigns labels to newly reported issues on GitHub repositories
with high accuracy.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.04834v1,2022-02-10T04:47:47Z,2022-02-10T04:47:47Z,"Geometric Digital Twinning of Industrial Facilities: Retrieval of
  Industrial Shapes","This paper devises, implements and benchmarks a novel shape retrieval method
that can accurately match individual labelled point clusters (instances) of
existing industrial facilities with their respective CAD models. It employs a
combination of image and point cloud deep learning networks to classify and
match instances to their geometrically similar CAD model. It extends our
previous research on geometric digital twin generation from point cloud data,
which currently is a tedious, manual process. Experiments with our joint
network reveal that it can reliably retrieve CAD models at 85.2\% accuracy. The
proposed research is a fundamental framework to enable the geometric Digital
Twin (gDT) pipeline and incorporate the real geometric configuration into the
Digital Twin.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.03028v2,2022-03-01T14:15:45Z,2022-02-07T09:41:24Z,QUARK: A Framework for Quantum Computing Application Benchmarking,"Quantum computing (QC) is anticipated to provide a speedup over classical HPC
approaches for specific problems in optimization, simulation, and machine
learning. With the advances in quantum computing toward practical applications,
the need to analyze and compare different quantum solutions increases. While
different low-level benchmarks for QC exist, these benchmarks do not provide
sufficient insights into real-world application-level performance. We propose
an application-centric benchmark method and the QUantum computing Application
benchmaRK (QUARK) framework to foster the investigation and creation of
application benchmarks for QC. This paper establishes three significant
contributions: (1) it makes a case for application-level benchmarks and
provides an in-depth ""pen and paper"" benchmark formulation of two reference
problems: robot path and vehicle option optimization from the industrial
domain; (2) it proposes the open-source QUARK framework for designing,
implementing, executing, and analyzing benchmarks; (3) it provides multiple
reference implementations for these two reference problems based on different
known, and where needed, extended, classical and quantum algorithmic approaches
and analyzes their performance on different types of infrastructures.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.02813v2,2022-02-19T12:11:48Z,2022-02-06T16:29:15Z,A Coding Framework and Benchmark towards Compressed Video Understanding,"Most video understanding methods are learned on high-quality videos. However,
in real-world scenarios, the videos are first compressed before the
transportation and then decompressed for understanding. The decompressed videos
may have lost the critical information to the downstream tasks. To address this
issue, we propose the first coding framework for compressed video
understanding, where another learnable analytic bitstream is simultaneously
transported with the original video bitstream. With the dedicatedly designed
self-supervised optimization target and dynamic network architectures, this new
stream largely boosts the downstream tasks yet with a small bit cost. By only
one-time training, our framework can be deployed for multiple downstream tasks.
Our framework also enjoys the best of both two worlds, (1) high efficiency of
industrial video codec and (2) flexible coding capability of neural networks
(NNs). Finally, we build a rigorous benchmark for compressed video
understanding on three popular tasks over seven large-scale datasets and four
different compression levels. The proposed Understanding oriented Video Coding
framework UVC consistently demonstrates significantly stronger performances
than the baseline industrial codec.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.12170v4,2022-06-08T14:35:01Z,2022-01-28T15:11:34Z,"Unsupervised Single-shot Depth Estimation using Perceptual
  Reconstruction","Real-time estimation of actual object depth is an essential module for
various autonomous system tasks such as 3D reconstruction, scene understanding
and condition assessment. During the last decade of machine learning, extensive
deployment of deep learning methods to computer vision tasks has yielded
approaches that succeed in achieving realistic depth synthesis out of a simple
RGB modality. Most of these models are based on paired RGB-depth data and/or
the availability of video sequences and stereo images. The lack of sequences,
stereo data and RGB-depth pairs makes depth estimation a fully unsupervised
single-image transfer problem that has barely been explored so far. This study
builds on recent advances in the field of generative neural networks in order
to establish fully unsupervised single-shot depth estimation. Two generators
for RGB-to-depth and depth-to-RGB transfer are implemented and simultaneously
optimized using the Wasserstein-1 distance, a novel perceptual reconstruction
term and hand-crafted image filters. We comprehensively evaluate the models
using industrial surface depth data as well as the Texas 3D Face Recognition
Database, the CelebAMask-HQ database of human portraits and the SURREAL dataset
that records body depth. For each evaluation dataset the proposed method shows
a significant increase in depth accuracy compared to state-of-the-art
single-image transfer methods.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.06735v1,2022-01-18T04:20:37Z,2022-01-18T04:20:37Z,AI Augmented Digital Metal Component,"The aim of this work is to propose a new paradigm that imparts intelligence
to metal parts with the fusion of metal additive manufacturing and artificial
intelligence (AI). Our digital metal part classifies the status with real time
data processing with convolutional neural network (CNN). The training data for
the CNN is collected from a strain gauge embedded in metal parts by laser
powder bed fusion process. We implement this approach using additive
manufacturing, demonstrate a self-cognitive metal part recognizing partial
screw loosening, malfunctioning, and external impacting object. The results
indicate that metal part can recognize subtle change of multiple fixation state
under repetitive compression with 89.1% accuracy with test sets. The proposed
strategy showed promising potential in contributing to the hyper-connectivity
for next generation of digital metal based mechanical systems",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.06616v2,2022-01-20T15:00:50Z,2022-01-17T20:15:37Z,Improving the quality control of seismic data through active learning,"In image denoising problems, the increasing density of available images makes
an exhaustive visual inspection impossible and therefore automated methods
based on machine-learning must be deployed for this purpose. This is
particulary the case in seismic signal processing. Engineers/geophysicists have
to deal with millions of seismic time series. Finding the sub-surface
properties useful for the oil industry may take up to a year and is very costly
in terms of computing/human resources. In particular, the data must go through
different steps of noise attenuation. Each denoise step is then ideally
followed by a quality control (QC) stage performed by means of human expertise.
To learn a quality control classifier in a supervised manner, labeled training
data must be available, but collecting the labels from human experts is
extremely time-consuming. We therefore propose a novel active learning
methodology to sequentially select the most relevant data, which are then given
back to a human expert for labeling. Beyond the application in geophysics, the
technique we promote in this paper, based on estimates of the local error and
its uncertainty, is generic. Its performance is supported by strong empirical
evidence, as illustrated by the numerical experiments presented in this
article, where it is compared to alternative active learning strategies both on
synthetic and real seismic datasets.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.06599v1,2022-01-17T19:25:33Z,2022-01-17T19:25:33Z,"Who supervises the supervisor? Model monitoring in production using deep
  feature embeddings with applications to workpiece inspection","The automation of condition monitoring and workpiece inspection plays an
essential role in maintaining high quality as well as high throughput of the
manufacturing process. To this end, the recent rise of developments in machine
learning has lead to vast improvements in the area of autonomous process
supervision. However, the more complex and powerful these models become, the
less transparent and explainable they generally are as well. One of the main
challenges is the monitoring of live deployments of these machine learning
systems and raising alerts when encountering events that might impact model
performance. In particular, supervised classifiers are typically build under
the assumption of stationarity in the underlying data distribution. For
example, a visual inspection system trained on a set of material surface
defects generally does not adapt or even recognize gradual changes in the data
distribution - an issue known as ""data drift"" - such as the emergence of new
types of surface defects. This, in turn, may lead to detrimental
mispredictions, e.g. samples from new defect classes being classified as
non-defective. To this end, it is desirable to provide real-time tracking of a
classifier's performance to inform about the putative onset of additional error
classes and the necessity for manual intervention with respect to classifier
re-training. Here, we propose an unsupervised framework that acts on top of a
supervised classification system, thereby harnessing its internal deep feature
representations as a proxy to track changes in the data distribution during
deployment and, hence, to anticipate classifier performance degradation.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.05115v1,2022-01-13T18:20:32Z,2022-01-13T18:20:32Z,Functional Anomaly Detection: a Benchmark Study,"The increasing automation in many areas of the Industry expressly demands to
design efficient machine-learning solutions for the detection of abnormal
events. With the ubiquitous deployment of sensors monitoring nearly
continuously the health of complex infrastructures, anomaly detection can now
rely on measurements sampled at a very high frequency, providing a very rich
representation of the phenomenon under surveillance. In order to exploit fully
the information thus collected, the observations cannot be treated as
multivariate data anymore and a functional analysis approach is required. It is
the purpose of this paper to investigate the performance of recent techniques
for anomaly detection in the functional setup on real datasets. After an
overview of the state-of-the-art and a visual-descriptive study, a variety of
anomaly detection methods are compared. While taxonomies of abnormalities (e.g.
shape, location) in the functional setup are documented in the literature,
assigning a specific type to the identified anomalies appears to be a
challenging task. Thus, strengths and weaknesses of the existing approaches are
benchmarked in view of these highlighted types in a simulation study. Anomaly
detection methods are next evaluated on two datasets, related to the monitoring
of helicopters in flight and to the spectrometry of construction materials
namely. The benchmark analysis is concluded by recommendation guidance for
practitioners.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.04263v1,2022-01-12T01:22:26Z,2022-01-12T01:22:26Z,The Human Factor in AI Safety,"AI-based systems have been used widely across various industries for
different decisions ranging from operational decisions to tactical and
strategic ones in low- and high-stakes contexts. Gradually the weaknesses and
issues of these systems have been publicly reported including, ethical issues,
biased decisions, unsafe outcomes, and unfair decisions, to name a few.
Research has tended to optimize AI less has focused on its risk and unexpected
negative consequences. Acknowledging this serious potential risks and scarcity
of re-search I focus on unsafe outcomes of AI. Specifically, I explore this
issue from a Human-AI interaction lens during AI deployment. It will be
discussed how the interaction of individuals and AI during its deployment
brings new concerns, which need a solid and holistic mitigation plan. It will
be dis-cussed that only AI algorithms' safety is not enough to make its
operation safe. The AI-based systems' end-users and their decision-making
archetypes during collaboration with these systems should be considered during
the AI risk management. Using some real-world scenarios, it will be highlighted
that decision-making archetypes of users should be considered a design
principle in AI-based systems.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.02028v1,2022-01-06T12:36:35Z,2022-01-06T12:36:35Z,"A Light in the Dark: Deep Learning Practices for Industrial Computer
  Vision","In recent years, large pre-trained deep neural networks (DNNs) have
revolutionized the field of computer vision (CV). Although these DNNs have been
shown to be very well suited for general image recognition tasks, application
in industry is often precluded for three reasons: 1) large pre-trained DNNs are
built on hundreds of millions of parameters, making deployment on many devices
impossible, 2) the underlying dataset for pre-training consists of general
objects, while industrial cases often consist of very specific objects, such as
structures on solar wafers, 3) potentially biased pre-trained DNNs raise legal
issues for companies. As a remedy, we study neural networks for CV that we
train from scratch. For this purpose, we use a real-world case from a solar
wafer manufacturer. We find that our neural networks achieve similar
performances as pre-trained DNNs, even though they consist of far fewer
parameters and do not rely on third-party datasets.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.15277v1,2021-12-31T03:38:37Z,2021-12-31T03:38:37Z,Machine Learning Application Development: Practitioners' Insights,"Nowadays, intelligent systems and services are getting increasingly popular
as they provide data-driven solutions to diverse real-world problems, thanks to
recent breakthroughs in Artificial Intelligence (AI) and Machine Learning (ML).
However, machine learning meets software engineering not only with promising
potentials but also with some inherent challenges. Despite some recent research
efforts, we still do not have a clear understanding of the challenges of
developing ML-based applications and the current industry practices. Moreover,
it is unclear where software engineering researchers should focus their efforts
to better support ML application developers. In this paper, we report about a
survey that aimed to understand the challenges and best practices of ML
application development. We synthesize the results obtained from 80
practitioners (with diverse skills, experience, and application domains) into
17 findings; outlining challenges and best practices for ML application
development. Practitioners involved in the development of ML-based software
systems can leverage the summarized best practices to improve the quality of
their system. We hope that the reported challenges will inform the research
community about topics that need to be investigated to improve the engineering
process and the quality of ML-based applications.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.14070v1,2021-12-28T09:50:39Z,2021-12-28T09:50:39Z,Intelligent Document Processing -- Methods and Tools in the real world,"The originality of this publication is to look at the subject of IDP
(Intelligent Document Processing) from the perspective of an end-user and
industrialist and not that of a Computer Science researcher. This domain is one
part of the challenge of information digitalisation that constitutes the
Industrial Revolution of the twenty first century (Industry 4.0) and this paper
looks specifically at the difficult areas of classifying, extracting
information and subsequent integration into business processes with respect to
forms and invoices. Since the focus is on practical implementation a brief
review is carried out of the market in commercial tools for OCR, document
classification and data extraction in so far as this is publicly available
together with pricing (if known). Brief definitions of the main terms
encountered in Computer Science publications and commercial prospectuses are
provided in order to de-mystify the language for the layman. A small number of
practical tests are carried out on a few real documents in order to illustrate
the capabilities of tools that are commonly available at a reasonable price.
The unsolved (so far) issue of tables contained in invoices is raised. The case
of a typical large industrial company is evoked where the requirement is to
extract 100 per cent of the information with 100 per cent reliability in order
to integrate into the back-end Enterprise Resource Planning system. Finally a
brief description is given of the state-of-the-art research by the huge
corporations who are pushing the boundaries of deep learning techniques further
and further with massive computing and financial power - progress that will
undoubtedly trickle down into the real world at some later date. The paper
finishes by asking the question whether the objectives and timing of the
commercial world and the progress of Computer Science are fully aligned.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.13389v1,2021-12-26T14:38:17Z,2021-12-26T14:38:17Z,"Attributed Graph Neural Networks for Recommendation Systems on
  Large-Scale and Sparse Graph","Link prediction in structured-data is an important problem for many
applications, especially for recommendation systems. Existing methods focus on
how to learn the node representation based on graph-based structure.
High-dimensional sparse edge features are not fully exploited. Because
balancing precision and computation efficiency is significant for
recommendation systems in real world, multiple-level feature representation in
large-scale sparse graph still lacks effective and efficient solution. In this
paper, we propose a practical solution about graph neural networks called
Attributed Graph Convolutional Networks(AGCN) to incorporate edge attributes
when apply graph neural networks in large-scale sparse networks. We formulate
the link prediction problem as a subgraph classification problem. We firstly
propose an efficient two-level projection to decompose topological structures
to node-edge pairs and project them into the same interaction feature space.
Then we apply multi-layer GCN to combine the projected node-edge pairs to
capture the topological structures. Finally, the pooling representation of two
units is treated as the input of classifier to predict the probability. We
conduct offline experiments on two industrial datasets and one public dataset
and demonstrate that AGCN outperforms other excellent baselines. Moreover, we
also deploy AGCN method to important scenarios on Xianyu and AliExpress. In
online systems, AGCN achieves over 5% improvement on online metrics.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.13267v1,2021-12-25T18:39:21Z,2021-12-25T18:39:21Z,Task and Model Agnostic Adversarial Attack on Graph Neural Networks,"Graph neural networks (GNNs) have witnessed significant adoption in the
industry owing to impressive performance on various predictive tasks.
Performance alone, however, is not enough. Any widely deployed machine learning
algorithm must be robust to adversarial attacks. In this work, we investigate
this aspect for GNNs, identify vulnerabilities, and link them to graph
properties that may potentially lead to the development of more secure and
robust GNNs. Specifically, we formulate the problem of task and model agnostic
evasion attacks where adversaries modify the test graph to affect the
performance of any unknown downstream task. The proposed algorithm, GRAND
($Gr$aph $A$ttack via $N$eighborhood $D$istortion) shows that distortion of
node neighborhoods is effective in drastically compromising prediction
performance. Although neighborhood distortion is an NP-hard problem, GRAND
designs an effective heuristic through a novel combination of Graph Isomorphism
Network with deep $Q$-learning. Extensive experiments on real datasets show
that, on average, GRAND is up to $50\%$ more effective than state of the art
techniques, while being more than $100$ times faster.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.13845v1,2021-12-24T10:16:28Z,2021-12-24T10:16:28Z,Raw Produce Quality Detection with Shifted Window Self-Attention,"Global food insecurity is expected to worsen in the coming decades with the
accelerated rate of climate change and the rapidly increasing population. In
this vein, it is important to remove inefficiencies at every level of food
production. The recent advances in deep learning can help reduce such
inefficiencies, yet their application has not yet become mainstream throughout
the industry, inducing economic costs at a massive scale. To this point, modern
techniques such as CNNs (Convolutional Neural Networks) have been applied to
RPQD (Raw Produce Quality Detection) tasks. On the other hand, Transformer's
successful debut in the vision among other modalities led us to expect a better
performance with these Transformer-based models in RPQD. In this work, we
exclusively investigate the recent state-of-the-art Swin (Shifted Windows)
Transformer which computes self-attention in both intra- and inter-window
fashion. We compare Swin Transformer against CNN models on four RPQD image
datasets, each containing different kinds of raw produce: fruits and
vegetables, fish, pork, and beef. We observe that Swin Transformer not only
achieves better or competitive performance but also is data- and
compute-efficient, making it ideal for actual deployment in real-world setting.
To the best of our knowledge, this is the first large-scale empirical study on
RPQD task, which we hope will gain more attention in future works.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.11561v2,2022-04-27T07:25:54Z,2021-12-21T22:51:37Z,"Explainable artificial intelligence for autonomous driving: An overview
  and guide for future research directions","Autonomous driving has achieved a significant milestone in research and
development over the last decade. There is increasing interest in the field as
the deployment of self-operating vehicles promises safer and more ecologically
friendly transportation systems. With the rise of computationally powerful
artificial intelligence (AI) techniques, autonomous vehicles can sense their
environment with high precision, make safe real-time decisions, and operate
reliably without human intervention. However, intelligent decision-making in
autonomous cars is not generally understandable by humans in the current state
of the art, and such deficiency hinders this technology from being socially
acceptable. Hence, aside from making safe real-time decisions, the AI systems
of autonomous vehicles also need to explain how their decisions are constructed
in order to be regulatory compliant across many jurisdictions. Our study sheds
a comprehensive light on the development of explainable artificial intelligence
(XAI) approaches for autonomous vehicles. In particular, we make the following
contributions. First, we provide a thorough overview of the state-of-the-art
studies on XAI for autonomous driving. We then propose an XAI framework that
considers all the societal and legal requirements for explainability of
autonomous driving systems. Finally, as future research directions, we provide
a guide to XAI approaches that can improve operational safety and transparency
to support public approval of autonomous driving technology by regulators,
manufacturers, and all engaged stakeholders.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.10229v1,2021-12-19T19:01:01Z,2021-12-19T19:01:01Z,On Causal Inference for Data-free Structured Pruning,"Neural networks (NNs) are making a large impact both on research and
industry. Nevertheless, as NNs' accuracy increases, it is followed by an
expansion in their size, required number of compute operations and energy
consumption. Increase in resource consumption results in NNs' reduced adoption
rate and real-world deployment impracticality. Therefore, NNs need to be
compressed to make them available to a wider audience and at the same time
decrease their runtime costs. In this work, we approach this challenge from a
causal inference perspective, and we propose a scoring mechanism to facilitate
structured pruning of NNs. The approach is based on measuring mutual
information under a maximum entropy perturbation, sequentially propagated
through the NN. We demonstrate the method's performance on two datasets and
various NNs' sizes, and we show that our approach achieves competitive
performance under challenging conditions.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.09559v2,2022-01-12T08:03:19Z,2021-12-17T15:14:22Z,"ColO-RAN: Developing Machine Learning-based xApps for Open RAN
  Closed-loop Control on Programmable Experimental Platforms","In spite of the new opportunities brought about by the Open RAN, advances in
ML-based network automation have been slow, mainly because of the
unavailability of large-scale datasets and experimental testing infrastructure.
This slows down the development and widespread adoption of Deep Reinforcement
Learning (DRL) agents on real networks, delaying progress in intelligent and
autonomous RAN control. In this paper, we address these challenges by proposing
practical solutions and software pipelines for the design, training, testing,
and experimental evaluation of DRL-based closed-loop control in the Open RAN.
We introduce ColO-RAN, the first publicly-available large-scale O-RAN testing
framework with software-defined radios-in-the-loop. Building on the scale and
computational capabilities of the Colosseum wireless network emulator, ColO-RAN
enables ML research at scale using O-RAN components, programmable base
stations, and a ""wireless data factory"". Specifically, we design and develop
three exemplary xApps for DRL-based control of RAN slicing, scheduling and
online model training, and evaluate their performance on a cellular network
with 7 softwarized base stations and 42 users. Finally, we showcase the
portability of ColO-RAN to different platforms by deploying it on Arena, an
indoor programmable testbed. Extensive results from our first-of-its-kind
large-scale evaluation highlight the benefits and challenges of DRL-based
adaptive control. They also provide insights on the development of wireless DRL
pipelines, from data analysis to the design of DRL agents, and on the tradeoffs
associated to training on a live RAN. ColO-RAN and the collected large-scale
dataset will be made publicly available to the research community.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.07263v1,2021-12-14T09:52:18Z,2021-12-14T09:52:18Z,Quantifying Multimodality in World Models,"Model-based Deep Reinforcement Learning (RL) assumes the availability of a
model of an environment's underlying transition dynamics. This model can be
used to predict future effects of an agent's possible actions. When no such
model is available, it is possible to learn an approximation of the real
environment, e.g. by using generative neural networks, sometimes also called
World Models. As most real-world environments are stochastic in nature and the
transition dynamics are oftentimes multimodal, it is important to use a
modelling technique that is able to reflect this multimodal uncertainty. In
order to safely deploy such learning systems in the real world, especially in
an industrial context, it is paramount to consider these uncertainties. In this
work, we analyze existing and propose new metrics for the detection and
quantification of multimodal uncertainty in RL based World Models. The correct
modelling & detection of uncertain future states lays the foundation for
handling critical situations in a safe way, which is a prerequisite for
deploying RL systems in real-world settings.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.07019v2,2021-12-25T23:05:11Z,2021-12-13T21:14:35Z,"Synapse Compression for Event-Based Convolutional-Neural-Network
  Accelerators","Manufacturing-viable neuromorphic chips require novel computer architectures
to achieve the massively parallel and efficient information processing the
brain supports so effortlessly. Emerging event-based architectures are making
this dream a reality. However, the large memory requirements for synaptic
connectivity are a showstopper for the execution of modern convolutional neural
networks (CNNs) on massively parallel, event-based (spiking) architectures.
This work overcomes this roadblock by contributing a lightweight hardware
scheme to compress the synaptic memory requirements by several thousand times,
enabling the execution of complex CNNs on a single chip of small form factor. A
silicon implementation in a 12-nm technology shows that the technique increases
the system's implementation cost by only 2%, despite achieving a total
memory-footprint reduction of up to 374x compared to the best previously
published technique.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.06986v2,2021-12-19T11:17:49Z,2021-12-13T19:41:26Z,"On The Reliability Of Machine Learning Applications In Manufacturing
  Environments","The increasing deployment of advanced digital technologies such as Internet
of Things (IoT) devices and Cyber-Physical Systems (CPS) in industrial
environments is enabling the productive use of machine learning (ML) algorithms
in the manufacturing domain. As ML applications transcend from research to
productive use in real-world industrial environments, the question of
reliability arises. Since the majority of ML models are trained and evaluated
on static datasets, continuous online monitoring of their performance is
required to build reliable systems. Furthermore, concept and sensor drift can
lead to degrading accuracy of the algorithm over time, thus compromising
safety, acceptance and economics if undetected and not properly addressed. In
this work, we exemplarily highlight the severity of the issue on a publicly
available industrial dataset which was recorded over the course of 36 months
and explain possible sources of drift. We assess the robustness of ML
algorithms commonly used in manufacturing and show, that the accuracy strongly
declines with increasing drift for all tested algorithms. We further
investigate how uncertainty estimation may be leveraged for online performance
estimation as well as drift detection as a first step towards continually
learning applications. The results indicate, that ensemble algorithms like
random forests show the least decay of confidence calibration under drift.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.06060v1,2021-12-11T19:55:18Z,2021-12-11T19:55:18Z,"GenMotion: Data-driven Motion Generators for Real-time Animation
  Synthesis","With the recent success of deep learning algorithms, many researchers have
focused on generative models for human motion animation. However, the research
community lacks a platform for training and benchmarking various algorithms,
and the animation industry needs a toolkit for implementing advanced motion
synthesizing techniques. To facilitate the study of deep motion synthesis
methods for skeleton-based human animation and their potential applications in
practical animation making, we introduce \genmotion: a library that provides
unified pipelines for data loading, model training, and animation sampling with
various deep learning algorithms. Besides, by combining Python coding in the
animation software \genmotion\ can assist animators in creating real-time 3D
character animation. Source code is available at
https://github.com/realvcla/GenMotion/.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.05792v1,2021-12-08T15:55:57Z,2021-12-08T15:55:57Z,"Digital Twin of Electrical Tomography for Quantitative Multiphase Flow
  Imaging","We report a digital twin (DT) framework of electrical tomography (ET) to
address the challenge of real-time quantitative multiphase flow imaging based
on non-invasive and non-radioactive technologies. Multiphase flow is ubiquitous
in nature, industry, and research. Accurate flow imaging is the key to
understanding this complex phenomenon. Existing non-radioactive multiphase flow
imaging methods based on electrical tomography are limited to providing
qualitative images. The proposed DT framework, building upon a synergistic
integration of 3D field coupling simulation, model-based deep learning, and
edge computing, allows ET to dynamically learn the flow features in the virtual
space and implement the model in the physical system, thus providing
unprecedented resolution and accuracy. The DT framework is demonstrated on
gas-liquid two-phase flow and electrical capacitance tomography (ECT). It can
be readily extended to various tomography modalities, scenarios, and scales in
biomedical, energy, and aerospace applications as an effective alternative to
radioactive solutions for precise flow visualization and characterization.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.04492v1,2021-12-08T13:14:38Z,2021-12-08T13:14:38Z,Daily peak electrical load forecasting with a multi-resolution approach,"In the context of smart grids and load balancing, daily peak load forecasting
has become a critical activity for stakeholders of the energy industry. An
understanding of peak magnitude and timing is paramount for the implementation
of smart grid strategies such as peak shaving. The modelling approach proposed
in this paper leverages high-resolution and low-resolution information to
forecast daily peak demand size and timing. The resulting multi-resolution
modelling framework can be adapted to different model classes. The key
contributions of this paper are a) a general and formal introduction to the
multi-resolution modelling approach, b) a discussion on modelling approaches at
different resolutions implemented via Generalised Additive Models and Neural
Networks and c) experimental results on real data from the UK electricity
market. The results confirm that the predictive performance of the proposed
modelling approach is competitive with that of low- and high-resolution
alternatives.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.03912v1,2021-12-07T06:32:27Z,2021-12-07T06:32:27Z,RID-Noise: Towards Robust Inverse Design under Noisy Environments,"From an engineering perspective, a design should not only perform well in an
ideal condition, but should also resist noises. Such a design methodology,
namely robust design, has been widely implemented in the industry for product
quality control. However, classic robust design requires a lot of evaluations
for a single design target, while the results of these evaluations could not be
reused for a new target. To achieve a data-efficient robust design, we propose
Robust Inverse Design under Noise (RID-Noise), which can utilize existing noisy
data to train a conditional invertible neural network (cINN). Specifically, we
estimate the robustness of a design parameter by its predictability, measured
by the prediction error of a forward neural network. We also define a
sample-wise weight, which can be used in the maximum weighted likelihood
estimation of an inverse model based on a cINN. With the visual results from
experiments, we clearly justify how RID-Noise works by learning the
distribution and robustness from data. Further experiments on several
real-world benchmark tasks with noises confirm that our method is more
effective than other state-of-the-art inverse design methods. Code and
supplementary is publicly available at
https://github.com/ThyrixYang/rid-noise-aaai22",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.01016v1,2021-12-02T07:02:27Z,2021-12-02T07:02:27Z,"On Two XAI Cultures: A Case Study of Non-technical Explanations in
  Deployed AI System","Explainable AI (XAI) research has been booming, but the question ""$\textbf{To
whom}$ are we making AI explainable?"" is yet to gain sufficient attention. Not
much of XAI is comprehensible to non-AI experts, who nonetheless, are the
primary audience and major stakeholders of deployed AI systems in practice. The
gap is glaring: what is considered ""explained"" to AI-experts versus non-experts
are very different in practical scenarios. Hence, this gap produced two
distinct cultures of expectations, goals, and forms of XAI in real-life AI
deployments.
  We advocate that it is critical to develop XAI methods for non-technical
audiences. We then present a real-life case study, where AI experts provided
non-technical explanations of AI decisions to non-technical stakeholders, and
completed a successful deployment in a highly regulated industry. We then
synthesize lessons learned from the case, and share a list of suggestions for
AI experts to consider when explaining AI decisions to non-technical
stakeholders.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.14125v1,2021-11-28T12:13:30Z,2021-11-28T12:13:30Z,"AirSPEC: An IoT-empowered Air Quality Monitoring System integrated with
  a Machine Learning Framework to Detect and Predict defined Air Quality
  parameters","The air that surrounds us is the cardinal source of respiration of all
life-forms. Therefore, it is undoubtedly vital to highlight that balanced air
quality is utmost important to the respiratory health of all living beings,
environmental homeostasis, and even economical equilibrium. Nevertheless, a
gradual deterioration of air quality has been observed in the last few decades,
due to the continuous increment of polluted emissions from automobiles and
industries into the atmosphere. Even though many people have scarcely
acknowledged the depth of the problem, the persistent efforts of determined
parties, including the World Health Organization, have consistently pushed the
boundaries for a qualitatively better global air homeostasis, by facilitating
technology-driven initiatives to timely detect and predict air quality in
regional and global scales. However, the existing frameworks for air quality
monitoring lack the capability of real-time responsiveness and flexible
semantic distribution. In this paper, a novel Internet of Things framework is
proposed which is easily implementable, semantically distributive, and
empowered by a machine learning model. The proposed system is equipped with a
NodeRED dashboard which processes, visualizes, and stores the primary sensor
data that are acquired through a public air quality sensor network, and
further, the dashboard is integrated with a machine-learning model to obtain
temporal and geo-spatial air quality predictions. ESP8266 NodeMCU is
incorporated as a subscriber to the NodeRED dashboard via a message queuing
telemetry transport broker to communicate quantitative air quality data or
alarming emails to the end-users through the developed web and mobile
applications. Therefore, the proposed system could become highly beneficial in
empowering public engagement in air quality through an unoppressive,
data-driven, and semantic framework.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.13657v2,2021-12-13T13:02:15Z,2021-11-26T18:35:38Z,"Amazon SageMaker Model Monitor: A System for Real-Time Insights into
  Deployed Machine Learning Models","With the increasing adoption of machine learning (ML) models and systems in
high-stakes settings across different industries, guaranteeing a model's
performance after deployment has become crucial. Monitoring models in
production is a critical aspect of ensuring their continued performance and
reliability. We present Amazon SageMaker Model Monitor, a fully managed service
that continuously monitors the quality of machine learning models hosted on
Amazon SageMaker. Our system automatically detects data, concept, bias, and
feature attribution drift in models in real-time and provides alerts so that
model owners can take corrective actions and thereby maintain high quality
models. We describe the key requirements obtained from customers, system design
and architecture, and methodology for detecting different types of drift.
Further, we provide quantitative evaluations followed by use cases, insights,
and lessons learned from more than 1.5 years of production deployment.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.13330v2,2021-12-11T19:25:47Z,2021-11-26T06:35:15Z,"ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural
  Networks","Over the past few years, deep neural networks (DNNs) have achieved tremendous
success and have been continuously applied in many application domains.
However, during the practical deployment in the industrial tasks, DNNs are
found to be erroneous-prone due to various reasons such as overfitting, lacking
robustness to real-world corruptions during practical usage. To address these
challenges, many recent attempts have been made to repair DNNs for version
updates under practical operational contexts by updating weights (i.e., network
parameters) through retraining, fine-tuning, or direct weight fixing at a
neural level. In this work, as the first attempt, we initiate to repair DNNs by
jointly optimizing the architecture and weights at a higher (i.e., block)
level.
  We first perform empirical studies to investigate the limitation of whole
network-level and layer-level repairing, which motivates us to explore a novel
repairing direction for DNN repair at the block level. To this end, we first
propose adversarial-aware spectrum analysis for vulnerable block localization
that considers the neurons' status and weights' gradients in blocks during the
forward and backward processes, which enables more accurate candidate block
localization for repairing even under a few examples. Then, we further propose
the architecture-oriented search-based repairing that relaxes the targeted
block to a continuous repairing search space at higher deep feature levels. By
jointly optimizing the architecture and weights in that space, we can identify
a much better block architecture. We implement our proposed repairing
techniques as a tool, named ArchRepair, and conduct extensive experiments to
validate the proposed method. The results show that our method can not only
repair but also enhance accuracy & robustness, outperforming the
state-of-the-art DNN repair techniques.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.13760v1,2021-11-23T11:16:35Z,2021-11-23T11:16:35Z,"Interpreting Machine Learning Models for Room Temperature Prediction in
  Non-domestic Buildings","An ensuing challenge in Artificial Intelligence (AI) is the perceived
difficulty in interpreting sophisticated machine learning models, whose
ever-increasing complexity makes it hard for such models to be understood,
trusted and thus accepted by human beings. The lack, if not complete absence,
of interpretability for these so-called black-box models can lead to serious
economic and ethical consequences, thereby hindering the development and
deployment of AI in wider fields, particularly in those involving critical and
regulatory applications. Yet, the building services industry is a
highly-regulated domain requiring transparency and decision-making processes
that can be understood and trusted by humans. To this end, the design and
implementation of autonomous Heating, Ventilation and Air Conditioning systems
for the automatic but concurrently interpretable optimisation of energy
efficiency and room thermal comfort is of topical interest. This work therefore
presents an interpretable machine learning model aimed at predicting room
temperature in non-domestic buildings, for the purpose of optimising the use of
the installed HVAC system. We demonstrate experimentally that the proposed
model can accurately forecast room temperatures eight hours ahead in real-time
by taking into account historical RT information, as well as additional
environmental and time-series features. In this paper, an enhanced feature
engineering process is conducted based on the Exploratory Data Analysis
results. Furthermore, beyond the commonly used Interpretable Machine Learning
techniques, we propose a Permutation Feature-based Frequency Response Analysis
(PF-FRA) method for quantifying the contributions of the different predictors
in the frequency domain. Based on the generated reason codes, we find that the
historical RT feature is the dominant factor that has most impact on the model
prediction.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.11108v1,2021-11-22T10:58:53Z,2021-11-22T10:58:53Z,"Unsupervised Time Series Outlier Detection with Diversity-Driven
  Convolutional Ensembles -- Extended Version","With the sweeping digitalization of societal, medical, industrial, and
scientific processes, sensing technologies are being deployed that produce
increasing volumes of time series data, thus fueling a plethora of new or
improved applications. In this setting, outlier detection is frequently
important, and while solutions based on neural networks exist, they leave room
for improvement in terms of both accuracy and efficiency. With the objective of
achieving such improvements, we propose a diversity-driven, convolutional
ensemble. To improve accuracy, the ensemble employs multiple basic outlier
detection models built on convolutional sequence-to-sequence autoencoders that
can capture temporal dependencies in time series. Further, a novel
diversity-driven training method maintains diversity among the basic models,
with the aim of improving the ensemble's accuracy. To improve efficiency, the
approach enables a high degree of parallelism during training. In addition, it
is able to transfer some model parameters from one basic model to another,
which reduces training time. We report on extensive experiments using
real-world multivariate time series that offer insight into the design choices
underlying the new approach and offer evidence that it is capable of improved
accuracy and efficiency. This is an extended version of ""Unsupervised Time
Series Outlier Detection with Diversity-Driven Convolutional Ensembles"", to
appear in PVLDB 2022.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.11251v1,2021-11-19T15:30:43Z,2021-11-19T15:30:43Z,Machine Learning-Based Soft Sensors for Vacuum Distillation Unit,"Product quality assessment in the petroleum processing industry can be
difficult and time-consuming, e.g. due to a manual collection of liquid samples
from the plant and subsequent chemical laboratory analysis of the samples. The
product quality is an important property that informs whether the products of
the process are within the specifications. In particular, the delays caused by
sample processing (collection, laboratory measurements, results analysis,
reporting) can lead to detrimental economic effects. One of the strategies to
deal with this problem is soft sensors. Soft sensors are a collection of models
that can be used to predict and forecast some infrequently measured properties
(such as laboratory measurements of petroleum products) based on more frequent
measurements of quantities like temperature, pressure and flow rate provided by
physical sensors. Soft sensors short-cut the pathway to obtain relevant
information about the product quality, often providing measurements as
frequently as every minute. One of the applications of soft sensors is for the
real-time optimization of a chemical process by a targeted adaptation of
operating parameters. Models used for soft sensors can have various forms,
however, among the most common are those based on artificial neural networks
(ANNs). While soft sensors can deal with some of the issues in the refinery
processes, their development and deployment can pose other challenges that are
addressed in this paper. Firstly, it is important to enhance the quality of
both sets of data (laboratory measurements and physical sensors) in a data
pre-processing stage (as described in Methodology section). Secondly, once the
data sets are pre-processed, different models need to be tested against
prediction error and the model's interpretability. In this work, we present a
framework for soft sensor development from raw data to ready-to-use models.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.09478v1,2021-11-18T02:18:27Z,2021-11-18T02:18:27Z,"Software Engineering for Responsible AI: An Empirical Study and
  Operationalised Patterns","Although artificial intelligence (AI) is solving real-world challenges and
transforming industries, there are serious concerns about its ability to behave
and make decisions in a responsible way. Many AI ethics principles and
guidelines for responsible AI have been recently issued by governments,
organisations, and enterprises. However, these AI ethics principles and
guidelines are typically high-level and do not provide concrete guidance on how
to design and develop responsible AI systems. To address this shortcoming, we
first present an empirical study where we interviewed 21 scientists and
engineers to understand the practitioners' perceptions on AI ethics principles
and their implementation. We then propose a template that enables AI ethics
principles to be operationalised in the form of concrete patterns and suggest a
list of patterns using the newly created template. These patterns provide
concrete, operationalised guidance that facilitate the development of
responsible AI systems.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.06123v1,2021-11-11T10:01:01Z,2021-11-11T10:01:01Z,"Spatio-Temporal Scene-Graph Embedding for Autonomous Vehicle Collision
  Prediction","In autonomous vehicles (AVs), early warning systems rely on collision
prediction to ensure occupant safety. However, state-of-the-art methods using
deep convolutional networks either fail at modeling collisions or are too
expensive/slow, making them less suitable for deployment on AV edge hardware.
To address these limitations, we propose sg2vec, a spatio-temporal scene-graph
embedding methodology that uses Graph Neural Network (GNN) and Long Short-Term
Memory (LSTM) layers to predict future collisions via visual scene perception.
We demonstrate that sg2vec predicts collisions 8.11% more accurately and 39.07%
earlier than the state-of-the-art method on synthesized datasets, and 29.47%
more accurately on a challenging real-world collision dataset. We also show
that sg2vec is better than the state-of-the-art at transferring knowledge from
synthetic datasets to real-world driving datasets. Finally, we demonstrate that
sg2vec performs inference 9.3x faster with an 88.0% smaller model, 32.4% less
power, and 92.8% less energy than the state-of-the-art method on the
industry-standard Nvidia DRIVE PX 2 platform, making it more suitable for
implementation on the edge.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.13720v2,2022-01-06T20:23:24Z,2021-10-26T14:13:57Z,"Deep DIC: Deep Learning-Based Digital Image Correlation for End-to-End
  Displacement and Strain Measurement","Digital image correlation (DIC) has become an industry standard to retrieve
accurate displacement and strain measurement in tensile testing and other
material characterization. Though traditional DIC offers a high precision
estimation of deformation for general tensile testing cases, the prediction
becomes unstable at large deformation or when the speckle patterns start to
tear. In addition, traditional DIC requires a long computation time and often
produces a low spatial resolution output affected by filtering and speckle
pattern quality. To address these challenges, we propose a new deep
learning-based DIC approach--Deep DIC, in which two convolutional neural
networks, DisplacementNet and StrainNet, are designed to work together for
end-to-end prediction of displacements and strains. DisplacementNet predicts
the displacement field and adaptively tracks a region of interest. StrainNet
predicts the strain field directly from the image input without relying on the
displacement prediction, which significantly improves the strain prediction
accuracy. A new dataset generation method is developed to synthesize a
realistic and comprehensive dataset, including the generation of speckle
patterns and the deformation of the speckle image with synthetic displacement
fields. Though trained on synthetic datasets only, Deep DIC gives highly
consistent and comparable predictions of displacement and strain with those
obtained from commercial DIC software for real experiments, while it
outperforms commercial software with very robust strain prediction even at
large and localized deformation and varied pattern qualities. In addition, Deep
DIC is capable of real-time prediction of deformation with a calculation time
down to milliseconds.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.13465v2,2022-04-04T02:20:53Z,2021-10-26T08:00:03Z,"CS-Rep: Making Speaker Verification Networks Embracing
  Re-parameterization","Automatic speaker verification (ASV) systems, which determine whether two
speeches are from the same speaker, mainly focus on verification accuracy while
ignoring inference speed. However, in real applications, both inference speed
and verification accuracy are essential. This study proposes cross-sequential
re-parameterization (CS-Rep), a novel topology re-parameterization strategy for
multi-type networks, to increase the inference speed and verification accuracy
of models. CS-Rep solves the problem that existing re-parameterization methods
are unsuitable for typical ASV backbones. When a model applies CS-Rep, the
training-period network utilizes a multi-branch topology to capture speaker
information, whereas the inference-period model converts to a time-delay neural
network (TDNN)-like plain backbone with stacked TDNN layers to achieve the fast
inference speed. Based on CS-Rep, an improved TDNN with friendly test and
deployment called Rep-TDNN is proposed. Compared with the state-of-the-art
model ECAPA-TDNN, which is highly recognized in the industry, Rep-TDNN
increases the actual inference speed by about 50% and reduces the EER by 10%.
The code will be released.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.11290v1,2021-10-21T17:18:52Z,2021-10-21T17:18:52Z,Physical Side-Channel Attacks on Embedded Neural Networks: A Survey,"During the last decade, Deep Neural Networks (DNN) have progressively been
integrated on all types of platforms, from data centers to embedded systems
including low-power processors and, recently, FPGAs. Neural Networks (NN) are
expected to become ubiquitous in IoT systems by transforming all sorts of
real-world applications, including applications in the safety-critical and
security-sensitive domains. However, the underlying hardware security
vulnerabilities of embedded NN implementations remain unaddressed. In
particular, embedded DNN implementations are vulnerable to Side-Channel
Analysis (SCA) attacks, which are especially important in the IoT and edge
computing contexts where an attacker can usually gain physical access to the
targeted device. A research field has therefore emerged and is rapidly growing
in terms of the use of SCA including timing, electromagnetic attacks and power
attacks to target NN embedded implementations. Since 2018, research papers have
shown that SCA enables an attacker to recover inference models architectures
and parameters, to expose industrial IP and endangers data confidentiality and
privacy. Without a complete review of this emerging field in the literature so
far, this paper surveys state-of-the-art physical SCA attacks relative to the
implementation of embedded DNNs on micro-controllers and FPGAs in order to
provide a thorough analysis on the current landscape. It provides a taxonomy
and a detailed classification of current attacks. It first discusses mitigation
techniques and then provides insights for future research leads.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.11073v4,2022-02-20T13:08:08Z,2021-10-18T12:48:02Z,"RL4RS: A Real-World Benchmark for Reinforcement Learning based
  Recommender System","Reinforcement learning based recommender systems (RL-based RS) aim at
learning a good policy from a batch of collected data, by casting sequential
recommendations to multi-step decision-making tasks. However, current RL-based
RS benchmarks commonly have a large reality gap, because they involve
artificial RL datasets or semi-simulated RS datasets, and the trained policy is
directly evaluated in the simulation environment. In real-world situations, not
all recommendation problems are suitable to be transformed into reinforcement
learning problems. Unlike previous academic RL research, RL-based RS suffers
from extrapolation error and the difficulties of being well-validated before
deployment. In this paper, we introduce the RL4RS (Reinforcement Learning for
Recommender Systems) benchmark - a new resource fully collected from industrial
applications to train and evaluate RL algorithms with special concerns on the
above issues. It contains two datasets, tuned simulation environments, related
advanced RL baselines, data understanding tools, and counterfactual policy
evaluation algorithms. The RL4RS suit can be found at
https://github.com/fuxiAIlab/RL4RS. In addition to the RL-based recommender
systems, we expect the resource to contribute to research in reinforcement
learning and neural combinatorial optimization.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.04003v2,2021-12-06T15:10:15Z,2021-10-08T09:59:12Z,Learning to Centralize Dual-Arm Assembly,"Robotic manipulators are widely used in modern manufacturing processes.
However, their deployment in unstructured environments remains an open problem.
To deal with the variety, complexity, and uncertainty of real-world
manipulation tasks, it is essential to develop a flexible framework with
reduced assumptions on the environment characteristics. In recent years,
reinforcement learning (RL) has shown great results for single-arm robotic
manipulation. However, research focusing on dual-arm manipulation is still
rare. From a classical control perspective, solving such tasks often involves
complex modeling of interactions between two manipulators and the objects
encountered in the tasks, as well as the two robots coupling at a control
level. Instead, in this work, we explore the applicability of model-free RL to
dual-arm assembly. As we aim to contribute towards an approach that is not
limited to dual-arm assembly, but dual-arm manipulation in general, we keep
modeling efforts at a minimum. Hence, to avoid modeling the interaction between
the two robots and the used assembly tools, we present a modular approach with
two decentralized single-arm controllers which are coupled using a single
centralized learned policy. We reduce modeling effort to a minimum by using
sparse rewards only. Our architecture enables successful assembly and simple
transfer from simulation to the real world. We demonstrate the effectiveness of
the framework on dual-arm peg-in-hole and analyze sample efficiency and success
rates for different action spaces. Moreover, we compare results on different
clearances and showcase disturbance recovery and robustness, when dealing with
position uncertainties. Finally we zero-shot transfer policies trained in
simulation to the real world and evaluate their performance.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.03979v2,2021-12-23T14:39:28Z,2021-10-08T08:58:36Z,"MilliTRACE-IR: Contact Tracing and Temperature Screening via mm-Wave and
  Infrared Sensing","Social distancing and temperature screening have been widely employed to
counteract the COVID-19 pandemic, sparking great interest from academia,
industry and public administrations worldwide. While most solutions have dealt
with these aspects separately, their combination would greatly benefit the
continuous monitoring of public spaces and help trigger effective
countermeasures. This work presents milliTRACE-IR, a joint mmWave radar and
infrared imaging sensing system performing unobtrusive and privacy preserving
human body temperature screening and contact tracing in indoor spaces.
milliTRACE-IR combines, via a robust sensor fusion approach, mmWave radars and
infrared thermal cameras. It achieves fully automated measurement of distancing
and body temperature, by jointly tracking the subjects's faces in the thermal
camera image plane and the human motion in the radar reference system.
Moreover, milliTRACE-IR performs contact tracing: a person with high body
temperature is reliably detected by the thermal camera sensor and subsequently
traced across a large indoor area in a non-invasive way by the radars. When
entering a new room, a subject is re-identified among several other individuals
by computing gait-related features from the radar reflections through a deep
neural network and using a weighted extreme learning machine as the final
re-identification tool. Experimental results, obtained from a real
implementation of milliTRACE-IR, demonstrate decimeter-level accuracy in
distance/trajectory estimation, inter-personal distance estimation (effective
for subjects getting as close as 0.2 m), and accurate temperature monitoring
(max. errors of 0.5{\deg}C). Furthermore, milliTRACE-IR provides contact
tracing through highly accurate (95%) person re-identification, in less than 20
seconds.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.03785v1,2021-10-07T20:38:14Z,2021-10-07T20:38:14Z,"Addressing practical challenges in Active Learning via a hybrid query
  strategy","Active Learning (AL) is a powerful tool to address modern machine learning
problems with significantly fewer labeled training instances. However,
implementation of traditional AL methodologies in practical scenarios is
accompanied by multiple challenges due to the inherent assumptions. There are
several hindrances, such as unavailability of labels for the AL algorithm at
the beginning; unreliable external source of labels during the querying
process; or incompatible mechanisms to evaluate the performance of Active
Learner. Inspired by these practical challenges, we present a hybrid query
strategy-based AL framework that addresses three practical challenges
simultaneously: cold-start, oracle uncertainty and performance evaluation of
Active Learner in the absence of ground truth. While a pre-clustering approach
is employed to address the cold-start problem, the uncertainty surrounding the
expertise of labeler and confidence in the given labels is incorporated to
handle oracle uncertainty. The heuristics obtained during the querying process
serve as the fundamental premise for accessing the performance of Active
Learner. The robustness of the proposed AL framework is evaluated across three
different environments and industrial settings. The results demonstrate the
capability of the proposed framework to tackle practical challenges during AL
implementation in real-world scenarios.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.01864v1,2021-10-05T08:00:46Z,2021-10-05T08:00:46Z,"Mobile authentication of copy detection patterns: how critical is to
  know fakes?","Protection of physical objects against counterfeiting is an important task
for the modern economies. In recent years, the high-quality counterfeits appear
to be closer to originals thanks to the rapid advancement of digital
technologies. To combat these counterfeits, an anti-counterfeiting technology
based on hand-crafted randomness implemented in a form of copy detection
patterns (CDP) is proposed enabling a link between the physical and digital
worlds and being used in various brand protection applications. The modern
mobile phone technologies make the verification process of CDP easier and
available to the end customers. Besides a big interest and attractiveness, the
CDP authentication based on the mobile phone imaging remains insufficiently
studied. In this respect, in this paper we aim at investigating the CDP
authentication under the real-life conditions with the codes printed on an
industrial printer and enrolled via a modern mobile phone under the regular
light conditions. The authentication aspects of the obtained CDP are
investigated with respect to the four types of copy fakes. The impact of fakes'
type used for training of authentication classifier is studied in two
scenarios: (i) supervised binary classification under various assumptions about
the fakes and (ii) one-class classification under unknown fakes. The obtained
results show that the modern machine-learning approaches and the technical
capacity of modern mobile phones allow to make the CDP authentication under
unknown fakes feasible with respect to the considered types of fakes and code
design.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.01709v1,2021-10-04T20:44:57Z,2021-10-04T20:44:57Z,"Benchmarking Memory-Centric Computing Systems: Analysis of Real
  Processing-in-Memory Hardware","Many modern workloads such as neural network inference and graph processing
are fundamentally memory-bound. For such workloads, data movement between
memory and CPU cores imposes a significant overhead in terms of both latency
and energy. A major reason is that this communication happens through a narrow
bus with high latency and limited bandwidth, and the low data reuse in
memory-bound workloads is insufficient to amortize the cost of memory access.
Fundamentally addressing this data movement bottleneck requires a paradigm
where the memory system assumes an active role in computing by integrating
processing capabilities. This paradigm is known as processing-in-memory (PIM).
Recent research explores different forms of PIM architectures, motivated by the
emergence of new technologies that integrate memory with a logic layer, where
processing elements can be easily placed. Past works evaluate these
architectures in simulation or, at best, with simplified hardware prototypes.
In contrast, the UPMEM company has designed and manufactured the first
publicly-available real-world PIM architecture. The UPMEM PIM architecture
combines traditional DRAM memory arrays with general-purpose in-order cores,
called DRAM Processing Units (DPUs), integrated in the same chip. This paper
presents key takeaways from the first comprehensive analysis of the first
publicly-available real-world PIM architecture. We provide four key takeaways
about the UPMEM PIM architecture, which stem from our study. More insights
about suitability of different workloads to the PIM system, programming
recommendations for software designers, and suggestions and hints for hardware
and architecture designers of future PIM systems are available in
arXiv:2105.03814",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.01659v2,2021-10-06T17:21:46Z,2021-10-04T18:49:51Z,Cross-Modal Virtual Sensing for Combustion Instability Monitoring,"In many cyber-physical systems, imaging can be an important but expensive or
'difficult to deploy' sensing modality. One such example is detecting
combustion instability using flame images, where deep learning frameworks have
demonstrated state-of-the-art performance. The proposed frameworks are also
shown to be quite trustworthy such that domain experts can have sufficient
confidence to use these models in real systems to prevent unwanted incidents.
However, flame imaging is not a common sensing modality in engine combustors
today. Therefore, the current roadblock exists on the hardware side regarding
the acquisition and processing of high-volume flame images. On the other hand,
the acoustic pressure time series is a more feasible modality for data
collection in real combustors. To utilize acoustic time series as a sensing
modality, we propose a novel cross-modal encoder-decoder architecture that can
reconstruct cross-modal visual features from acoustic pressure time series in
combustion systems. With the ""distillation"" of cross-modal features, the
results demonstrate that the detection accuracy can be enhanced using the
virtual visual sensing modality. By providing the benefit of cross-modal
reconstruction, our framework can prove to be useful in different domains well
beyond the power generation and transportation industries.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.00468v1,2021-10-01T15:03:03Z,2021-10-01T15:03:03Z,"New Evolutionary Computation Models and their Applications to Machine
  Learning","Automatic Programming is one of the most important areas of computer science
research today. Hardware speed and capability have increased exponentially, but
the software is years behind. The demand for software has also increased
significantly, but it is still written in old fashion: by using humans.
  There are multiple problems when the work is done by humans: cost, time,
quality. It is costly to pay humans, it is hard to keep them satisfied for a
long time, it takes a lot of time to teach and train them and the quality of
their output is in most cases low (in software, mostly due to bugs).
  The real advances in human civilization appeared during the industrial
revolutions. Before the first revolution, most people worked in agriculture.
Today, very few percent of people work in this field.
  A similar revolution must appear in the computer programming field.
Otherwise, we will have so many people working in this field as we had in the
past working in agriculture.
  How do people know how to write computer programs? Very simple: by learning.
Can we do the same for software? Can we put the software to learn how to write
software?
  It seems that is possible (to some degree) and the term is called Machine
Learning. It was first coined in 1959 by the first person who made a computer
perform a serious learning task, namely, Arthur Samuel.
  However, things are not so easy as in humans (well, truth to be said - for
some humans it is impossible to learn how to write software). So far we do not
have software that can learn perfectly to write software. We have some
particular cases where some programs do better than humans, but the examples
are sporadic at best. Learning from experience is difficult for computer
programs. Instead of trying to simulate how humans teach humans how to write
computer programs, we can simulate nature.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.00086v1,2021-09-30T20:56:37Z,2021-09-30T20:56:37Z,On the Trustworthiness of Tree Ensemble Explainability Methods,"The recent increase in the deployment of machine learning models in critical
domains such as healthcare, criminal justice, and finance has highlighted the
need for trustworthy methods that can explain these models to stakeholders.
Feature importance methods (e.g. gain and SHAP) are among the most popular
explainability methods used to address this need. For any explainability
technique to be trustworthy and meaningful, it has to provide an explanation
that is accurate and stable. Although the stability of local feature importance
methods (explaining individual predictions) has been studied before, there is
yet a knowledge gap about the stability of global features importance methods
(explanations for the whole model). Additionally, there is no study that
evaluates and compares the accuracy of global feature importance methods with
respect to feature ordering. In this paper, we evaluate the accuracy and
stability of global feature importance methods through comprehensive
experiments done on simulations as well as four real-world datasets. We focus
on tree-based ensemble methods as they are used widely in industry and measure
the accuracy and stability of explanations under two scenarios: 1) when inputs
are perturbed 2) when models are perturbed. Our findings provide a comparison
of these methods under a variety of settings and shed light on the limitations
of global feature importance methods by indicating their lack of accuracy with
and without noisy inputs, as well as their lack of stability with respect to:
1) increase in input dimension or noise in the data; 2) perturbations in models
initialized by different random seeds or hyperparameter settings.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.13602v1,2021-09-28T10:23:46Z,2021-09-28T10:23:46Z,"SafetyNet: Safe planning for real-world self-driving vehicles using
  machine-learned policies","In this paper we present the first safe system for full control of
self-driving vehicles trained from human demonstrations and deployed in
challenging, real-world, urban environments. Current industry-standard
solutions use rule-based systems for planning. Although they perform reasonably
well in common scenarios, the engineering complexity renders this approach
incompatible with human-level performance. On the other hand, the performance
of machine-learned (ML) planning solutions can be improved by simply adding
more exemplar data. However, ML methods cannot offer safety guarantees and
sometimes behave unpredictably. To combat this, our approach uses a simple yet
effective rule-based fallback layer that performs sanity checks on an ML
planner's decisions (e.g. avoiding collision, assuring physical feasibility).
This allows us to leverage ML to handle complex situations while still assuring
the safety, reducing ML planner-only collisions by 95%. We train our ML planner
on 300 hours of expert driving demonstrations using imitation learning and
deploy it along with the fallback layer in downtown San Francisco, where it
takes complete control of a real vehicle and navigates a wide variety of
challenging urban driving scenarios.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.13521v2,2021-11-23T05:34:54Z,2021-09-28T06:49:40Z,"A multi-stage semi-supervised improved deep embedded clustering method
  for bearing fault diagnosis under the situation of insufficient labeled
  samples","Although data-driven fault diagnosis methods have been widely applied,
massive labeled data are required for model training. However, a difficulty of
implementing this in real industries hinders the application of these methods.
Hence, an effective diagnostic approach that can work well in such situation is
urgently needed.In this study, a multi-stage semi-supervised improved deep
embedded clustering (MS-SSIDEC) method, which combines semi-supervised learning
with improved deep embedded clustering (IDEC), is proposed to jointly explore
scarce labeled data and massive unlabeled data. In the first stage, a
skip-connection-based convolutional auto-encoder (SCCAE) that can automatically
map the unlabeled data into a low-dimensional feature space is proposed and
pre-trained to be a fault feature extractor. In the second stage, a
semi-supervised improved deep embedded clustering (SSIDEC) network is proposed
for clustering. It is first initialized with available labeled data and then
used to simultaneously optimize the clustering label assignment and make the
feature space to be more clustering-friendly. To tackle the phenomenon of
overfitting, virtual adversarial training (VAT) is introduced as a
regularization term in this stage. In the third stage, pseudo labels are
obtained by the high-quality results of SSIDEC. The labeled dataset can be
augmented by these pseudo-labeled data and then leveraged to train a bearing
fault diagnosis model. Two public datasets of vibration data from rolling
bearings are used to evaluate the performance of the proposed method.
Experimental results indicate that the proposed method achieves a promising
performance in both semi-supervised and unsupervised fault diagnosis tasks.
This method provides a new approach for fault diagnosis under the situation of
limited labeled samples by effectively exploring unsupervised data.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.13375v1,2021-09-27T22:37:55Z,2021-09-27T22:37:55Z,"Automated Estimation of Construction Equipment Emission using Inertial
  Sensors and Machine Learning Models","The construction industry is one of the main producers of greenhouse gasses
(GHG). Quantifying the amount of air pollutants including GHG emissions during
a construction project has become an additional project objective to
traditional metrics such as time, cost, and safety in many parts of the world.
A major contributor to air pollution during construction is the use of heavy
equipment and thus their efficient operation and management can substantially
reduce the harm to the environment. Although the on-road vehicle emission
prediction is a widely researched topic, construction equipment emission
measurement and reduction have received very little attention. This paper
describes the development and deployment of a novel framework that uses machine
learning (ML) methods to predict the level of emissions from heavy construction
equipment monitored via an Internet of Things (IoT) system comprised of
accelerometer and gyroscope sensors. The developed framework was validated
using an excavator performing real-world construction work. A portable emission
measurement system (PEMS) was employed along with the inertial sensors to
record data including the amount of CO, NOX, CO2, SO2, and CH4 pollutions
emitted by the equipment. Different ML algorithms were developed and compared
to identify the best model to predict emission levels from inertial sensors
data. The results showed that Random Forest with the coefficient of
determination (R2) of 0.94, 0.91 and 0.94 for CO, NOX, CO2, respectively was
the best algorithm among different models evaluated in this study.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.09530v1,2021-09-20T13:30:11Z,2021-09-20T13:30:11Z,A Novel Online Incremental Learning Intrusion Prevention System,"Attack vectors are continuously evolving in order to evade Intrusion
Detection systems. Internet of Things (IoT) environments, while beneficial for
the IT ecosystem, suffer from inherent hardware limitations, which restrict
their ability to implement comprehensive security measures and increase their
exposure to vulnerability attacks. This paper proposes a novel Network
Intrusion Prevention System that utilises a SelfOrganizing Incremental Neural
Network along with a Support Vector Machine. Due to its structure, the proposed
system provides a security solution that does not rely on signatures or rules
and is capable to mitigate known and unknown attacks in real-time with high
accuracy. Based on our experimental results with the NSL KDD dataset, the
proposed framework can achieve on-line updated incremental learning, making it
suitable for efficient and scalable industrial applications.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.07827v2,2022-04-27T16:38:20Z,2021-09-16T09:36:53Z,"Enabling risk-aware Reinforcement Learning for medical interventions
  through uncertainty decomposition","Reinforcement Learning (RL) is emerging as tool for tackling complex control
and decision-making problems. However, in high-risk environments such as
healthcare, manufacturing, automotive or aerospace, it is often challenging to
bridge the gap between an apparently optimal policy learnt by an agent and its
real-world deployment, due to the uncertainties and risk associated with it.
Broadly speaking RL agents face two kinds of uncertainty, 1. aleatoric
uncertainty, which reflects randomness or noise in the dynamics of the world,
and 2. epistemic uncertainty, which reflects the bounded knowledge of the agent
due to model limitations and finite amount of information/data the agent has
acquired about the world. These two types of uncertainty carry fundamentally
different implications for the evaluation of performance and the level of risk
or trust. Yet these aleatoric and epistemic uncertainties are generally
confounded as standard and even distributional RL is agnostic to this
difference. Here we propose how a distributional approach (UA-DQN) can be
recast to render uncertainties by decomposing the net effects of each
uncertainty. We demonstrate the operation of this method in grid world examples
to build intuition and then show a proof of concept application for an RL agent
operating as a clinical decision support system in critical care",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.06126v3,2022-05-31T03:50:21Z,2021-09-13T17:05:43Z,"Neural Network Guided Evolutionary Fuzzing for Finding Traffic
  Violations of Autonomous Vehicles","Self-driving cars and trucks, autonomous vehicles (AVs), should not be
accepted by regulatory bodies and the public until they have much higher
confidence in their safety and reliability -- which can most practically and
convincingly be achieved by testing. But existing testing methods are
inadequate for checking the end-to-end behaviors of AV controllers against
complex, real-world corner cases involving interactions with multiple
independent agents such as pedestrians and human-driven vehicles. While
test-driving AVs on streets and highways fails to capture many rare events,
existing simulation-based testing methods mainly focus on simple scenarios and
do not scale well for complex driving situations that require sophisticated
awareness of the surroundings. To address these limitations, we propose a new
fuzz testing technique, called AutoFuzz, which can leverage widely-used AV
simulators' API grammars to generate semantically and temporally valid complex
driving scenarios (sequences of scenes). To efficiently search for traffic
violations-inducing scenarios in a large search space, we propose a constrained
neural network (NN) evolutionary search method to optimize AutoFuzz. Evaluation
of our prototype on one state-of-the-art learning-based controller, two
rule-based controllers, and one industrial-grade controller in five scenarios
shows that AutoFuzz efficiently finds hundreds of traffic violations in
high-fidelity simulation environments. For each scenario, AutoFuzz can find on
average 10-39% more unique traffic violations than the best-performing baseline
method. Further, fine-tuning the learning-based controller with the traffic
violations found by AutoFuzz successfully reduced the traffic violations found
in the new version of the AV controller software.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.05821v1,2021-09-13T09:46:50Z,2021-09-13T09:46:50Z,Cyber-Security in the Emerging World of Smart Everything,"The fourth industrial revolution (4IR) is a revolution many authors believe
have come to stay. It is a revolution that has been fast blurring the line
between physical, digital and biological technologies. These disruptive
technologies largely rely on high-speed internet connectivity, Cloud
technologies, Augmented Reality, Additive Manufacturing, Data science and
Artificial Intelligence. Most developed economies have embraced the it while
the developing economies are struggling to adopt 4IR because they lack the
requisite skills, knowledge and technology. Thus, this study investigates
Nigeria as one of the developing economies to understand her readiness for 4IR
and the level of preparedness to mitigate the sophisticated cyber-attacks that
comes with it. The investigation adopted quantitative research approach and
developed an online questionnaire that was shared amongst the population of
interest that includes academic, industry experts and relevant stakeholders.
The questionnaire returned 116 valid responses which were analysed with
descriptive statistical tools in SPSS. Results suggest that 60 of the
respondents opined that Nigerian government at are not showing enough evidence
to demonstrate her preparedness to leverage these promised potentials by
developing 4IR relevant laws, strong institutional frameworks and policies.
They lack significant development capacity to mitigate risks associated with
digital ecosystem and cyber ecosystem that are ushered in by the 4IR. In the
universities, 52 of the courses offered at the undergraduate and 42 at the
post-graduate levels are relevant in the development of skills required in the
revolution. The study recommends that the government at all levels make
adequate efforts in developing the countrys intangible assets. In all, this
paper posits that successful implementation of these could equip Nigeria to
embrace the 4IR in all its aspects.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.02629v1,2021-09-01T08:06:23Z,2021-09-01T08:06:23Z,"An Efficient Deep Learning Approach Using Improved Generative
  Adversarial Networks for Incomplete Information Completion of Self-driving","Autonomous driving is the key technology of intelligent logistics in
Industrial Internet of Things (IIoT). In autonomous driving, the appearance of
incomplete point clouds losing geometric and semantic information is inevitable
owing to limitations of occlusion, sensor resolution, and viewing angle when
the Light Detection And Ranging (LiDAR) is applied. The emergence of incomplete
point clouds, especially incomplete vehicle point clouds, would lead to the
reduction of the accuracy of autonomous driving vehicles in object detection,
traffic alert, and collision avoidance. Existing point cloud completion
networks, such as Point Fractal Network (PF-Net), focus on the accuracy of
point cloud completion, without considering the efficiency of inference
process, which makes it difficult for them to be deployed for vehicle point
cloud repair in autonomous driving. To address the above problem, in this
paper, we propose an efficient deep learning approach to repair incomplete
vehicle point cloud accurately and efficiently in autonomous driving. In the
proposed method, an efficient downsampling algorithm combining incremental
sampling and one-time sampling is presented to improves the inference speed of
the PF-Net based on Generative Adversarial Network (GAN). To evaluate the
performance of the proposed method, a real dataset is used, and an autonomous
driving scene is created, where three incomplete vehicle point clouds with 5
different sizes are set for three autonomous driving situations. The improved
PF-Net can achieve the speedups of over 19x with almost the same accuracy when
compared to the original PF-Net. Experimental results demonstrate that the
improved PF-Net can be applied to efficiently complete vehicle point clouds in
autonomous driving.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.13381v1,2021-08-30T17:04:04Z,2021-08-30T17:04:04Z,"Trustworthy AI for Process Automation on a Chylla-Haase Polymerization
  Reactor","In this paper, genetic programming reinforcement learning (GPRL) is utilized
to generate human-interpretable control policies for a Chylla-Haase
polymerization reactor. Such continuously stirred tank reactors (CSTRs) with
jacket cooling are widely used in the chemical industry, in the production of
fine chemicals, pigments, polymers, and medical products. Despite appearing
rather simple, controlling CSTRs in real-world applications is quite a
challenging problem to tackle. GPRL utilizes already existing data from the
reactor and generates fully automatically a set of optimized simplistic control
strategies, so-called policies, the domain expert can choose from. Note that
these policies are white-box models of low complexity, which makes them easy to
validate and implement in the target control system, e.g., SIMATIC PCS 7.
However, despite its low complexity the automatically-generated policy yields a
high performance in terms of reactor temperature control deviation, which we
empirically evaluate on the original reactor template.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.13475v1,2021-08-18T13:39:50Z,2021-08-18T13:39:50Z,"An Analysis Of Entire Space Multi-Task Models For Post-Click Conversion
  Prediction","Industrial recommender systems are frequently tasked with approximating
probabilities for multiple, often closely related, user actions. For example,
predicting if a user will click on an advertisement and if they will then
purchase the advertised product. The conceptual similarity between these tasks
has promoted the use of multi-task learning: a class of algorithms that aim to
bring positive inductive transfer from related tasks. Here, we empirically
evaluate multi-task learning approaches with neural networks for an online
advertising task. Specifically, we consider approximating the probability of
post-click conversion events (installs) (CVR) for mobile app advertising on a
large-scale advertising platform, using the related click events (CTR) as an
auxiliary task. We use an ablation approach to systematically study recent
approaches that incorporate both multitask learning and ""entire space modeling""
which train the CVR on all logged examples rather than learning a conditional
likelihood of conversion given clicked. Based on these results we show that
several different approaches result in similar levels of positive transfer from
the data-abundant CTR task to the CVR task and offer some insight into how the
multi-task design choices address the two primary problems affecting the CVR
task: data sparsity and data bias. Our findings add to the growing body of
evidence suggesting that standard multi-task learning is a sensible approach to
modelling related events in real-world large-scale applications and suggest the
specific multitask approach can be guided by ease of implementation in an
existing system.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.04465v1,2021-08-10T06:20:18Z,2021-08-10T06:20:18Z,"Industrial Digital Twins at the Nexus of NextG Wireless Networks and
  Computational Intelligence: A Survey","By amalgamating recent communication and control technologies, computing and
data analytics techniques, and modular manufacturing, Industry~4.0 promotes
integrating cyber-physical worlds through cyber-physical systems (CPS) and
digital twin (DT) for monitoring, optimization, and prognostics of industrial
processes. A DT is an emerging but conceptually different construct than CPS.
Like CPS, DT relies on communication to create a highly-consistent,
synchronized digital mirror image of the objects or physical processes. DT, in
addition, uses built-in models on this precise image to simulate, analyze,
predict, and optimize their real-time operation using feedback. DT is rapidly
diffusing in the industries with recent advances in the industrial Internet of
things (IIoT), edge and cloud computing, machine learning, artificial
intelligence, and advanced data analytics. However, the existing literature
lacks in identifying and discussing the role and requirements of these
technologies in DT-enabled industries from the communication and computing
perspective. In this article, we first present the functional aspects, appeal,
and innovative use of DT in smart industries. Then, we elaborate on this
perspective by systematically reviewing and reflecting on recent research in
next-generation (NextG) wireless technologies (e.g., 5G and beyond networks),
various tools (e.g., age of information, federated learning, data analytics),
and other promising trends in networked computing (e.g., edge and cloud
computing). Moreover, we discuss the DT deployment strategies at different
industrial communication layers to meet the monitoring and control requirements
of industrial applications. We also outline several key reflections and future
research challenges and directions to facilitate industrial DT's adoption.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.04058v1,2021-08-05T12:59:38Z,2021-08-05T12:59:38Z,"An Interpretable Probabilistic Model for Short-Term Solar Power
  Forecasting Using Natural Gradient Boosting","The stochastic nature of photovoltaic (PV) power has led both academia and
industry to a large amount of research work aiming at the development of
accurate PV power forecasting models. However, most of those models are based
on machine learning algorithms and are considered as black boxes which do not
provide any insight or explanation about their predictions. Therefore, their
direct implementation in environments, where transparency is required, and the
trust associated with their predictions may be questioned. To this end, we
propose a two stage probabilistic forecasting framework able to generate highly
accurate, reliable, and sharp forecasts yet offering full transparency on both
the point forecasts and the prediction intervals (PIs). In the first stage, we
exploit natural gradient boosting (NGBoost) for yielding probabilistic
forecasts while in the second stage, we calculate the Shapley additive
explanation (SHAP) values in order to fully understand why a prediction was
made. To highlight the performance and the applicability of the proposed
framework, real data from two PV parks located in Southern Germany are
employed. Initially, the natural gradient boosting is thoroughly compared with
two state-of-the-art algorithms, namely Gaussian process and lower upper bound
estimation, in a wide range of forecasting metrics. Secondly, a detailed
analysis of the model's complex nonlinear relationships and interaction effects
between the various features is presented. The latter allows us to interpret
the model, identify some learned physical properties, explain individual
predictions, reduce the computational requirements for the training without
jeopardizing the model accuracy, detect possible bugs, and gain trust in the
model. Finally, we conclude that the model was able to develop nonlinear
relationships following human logic and intuition based on learned physical
properties.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.10205v1,2021-08-02T13:09:53Z,2021-08-02T13:09:53Z,"Power transformer faults diagnosis using undestructive methods (Roger
  and IEC) and artificial neural network for dissolved gas analysis applied on
  the functional transformer in the Algerian north-eastern: a comparative study","Nowadays, power transformer aging and failures are viewed with great
attention in power transmission industry. Dissolved gas analysis (DGA) is
classified among the biggest widely used methods used within the context of
asset management policy to detect the incipient faults in their earlier stage
in power transformers. Up to now, several procedures have been employed for the
lecture of DGA results. Among these useful means, we find Key Gases, Rogers
Ratios, IEC Ratios, the historical technique less used today Doernenburg
Ratios, the two types of Duval Pentagons methods, several versions of the Duval
Triangles method and Logarithmic Nomograph. Problem. DGA data extracted from
different units in service served to verify the ability and reliability of
these methods in assessing the state of health of the power transformer. Aim.
An improving the quality of diagnostics of electrical power transformer by
artificial neural network tools based on two conventional methods in the case
of a functional power transformer at S\'etif province in East North of Algeria.
Methodology. Design an inelegant tool for power transformer diagnosis using
neural networks based on traditional methods IEC and Rogers, which allows to
early detection faults, to increase the reliability, of the entire electrical
energy system from transport to consumers and improve a continuity and quality
of service. Results. The solution of the problem was carried out by using
feed-forward back-propagation neural networks implemented in MATLAB-Simulink
environment. Four real power transformers working under different environment
and climate conditions such as: desert, humid, cold were taken into account.
The practical results of the diagnosis of these power transformers by the DGA
are presented. Practical value.....",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.02565v1,2021-07-30T12:17:36Z,2021-07-30T12:17:36Z,"Dependable Neural Networks Through Redundancy, A Comparison of Redundant
  Architectures","With edge-AI finding an increasing number of real-world applications,
especially in industry, the question of functionally safe applications using AI
has begun to be asked. In this body of work, we explore the issue of achieving
dependable operation of neural networks. We discuss the issue of dependability
in general implementation terms before examining lockstep solutions. We intuit
that it is not necessarily a given that two similar neural networks generate
results at precisely the same time and that synchronization between the
platforms will be required. We perform some preliminary measurements that may
support this intuition and introduce some work in implementing lockstep neural
network engines.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.13252v1,2021-07-28T10:28:05Z,2021-07-28T10:28:05Z,"Multi Agent System for Machine Learning Under Uncertainty in Cyber
  Physical Manufacturing System","Recent advancements in predictive machine learning has led to its application
in various use cases in manufacturing. Most research focused on maximising
predictive accuracy without addressing the uncertainty associated with it.
While accuracy is important, focusing primarily on it poses an overfitting
danger, exposing manufacturers to risk, ultimately hindering the adoption of
these techniques. In this paper, we determine the sources of uncertainty in
machine learning and establish the success criteria of a machine learning
system to function well under uncertainty in a cyber-physical manufacturing
system (CPMS) scenario. Then, we propose a multi-agent system architecture
which leverages probabilistic machine learning as a means of achieving such
criteria. We propose possible scenarios for which our proposed architecture is
useful and discuss future work. Experimentally, we implement Bayesian Neural
Networks for multi-tasks classification on a public dataset for the real-time
condition monitoring of a hydraulic system and demonstrate the usefulness of
the system by evaluating the probability of a prediction being accurate given
its uncertainty. We deploy these models using our proposed agent-based
framework and integrate web visualisation to demonstrate its real-time
feasibility.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.12433v1,2021-07-26T18:52:00Z,2021-07-26T18:52:00Z,"The Graph Neural Networking Challenge: A Worldwide Competition for
  Education in AI/ML for Networks","During the last decade, Machine Learning (ML) has increasingly become a hot
topic in the field of Computer Networks and is expected to be gradually adopted
for a plethora of control, monitoring and management tasks in real-world
deployments. This poses the need to count on new generations of students,
researchers and practitioners with a solid background in ML applied to
networks. During 2020, the International Telecommunication Union (ITU) has
organized the ""ITU AI/ML in 5G challenge'', an open global competition that has
introduced to a broad audience some of the current main challenges in ML for
networks. This large-scale initiative has gathered 23 different challenges
proposed by network operators, equipment manufacturers and academia, and has
attracted a total of 1300+ participants from 60+ countries. This paper narrates
our experience organizing one of the proposed challenges: the ""Graph Neural
Networking Challenge 2020''. We describe the problem presented to participants,
the tools and resources provided, some organization aspects and participation
statistics, an outline of the top-3 awarded solutions, and a summary with some
lessons learned during all this journey. As a result, this challenge leaves a
curated set of educational resources openly available to anyone interested in
the topic.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.00401v1,2021-07-01T12:20:48Z,2021-07-01T12:20:48Z,"CarSNN: An Efficient Spiking Neural Network for Event-Based Autonomous
  Cars on the Loihi Neuromorphic Research Processor","Autonomous Driving (AD) related features provide new forms of mobility that
are also beneficial for other kind of intelligent and autonomous systems like
robots, smart transportation, and smart industries. For these applications, the
decisions need to be made fast and in real-time. Moreover, in the quest for
electric mobility, this task must follow low power policy, without affecting
much the autonomy of the mean of transport or the robot. These two challenges
can be tackled using the emerging Spiking Neural Networks (SNNs). When deployed
on a specialized neuromorphic hardware, SNNs can achieve high performance with
low latency and low power consumption. In this paper, we use an SNN connected
to an event-based camera for facing one of the key problems for AD, i.e., the
classification between cars and other objects. To consume less power than
traditional frame-based cameras, we use a Dynamic Vision Sensor (DVS). The
experiments are made following an offline supervised learning rule, followed by
mapping the learnt SNN model on the Intel Loihi Neuromorphic Research Chip. Our
best experiment achieves an accuracy on offline implementation of 86%, that
drops to 83% when it is ported onto the Loihi Chip. The Neuromorphic Hardware
implementation has maximum 0.72 ms of latency for every sample, and consumes
only 310 mW. To the best of our knowledge, this work is the first
implementation of an event-based car classifier on a Neuromorphic Chip.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.00127v1,2021-06-30T22:18:49Z,2021-06-30T22:18:49Z,"SQRP: Sensing Quality-aware Robot Programming System for Non-expert
  Programmers","Robot programming typically makes use of a set of mechanical skills that is
acquired by machine learning. Because there is in general no guarantee that
machine learning produces robot programs that are free of surprising behavior,
the safe execution of a robot program must utilize monitoring modules that take
sensor data as inputs in real time to ensure the correctness of the skill
execution. Owing to the fact that sensors and monitoring algorithms are usually
subject to physical restrictions and that effective robot programming is
sensitive to the selection of skill parameters, these considerations may lead
to different sensor input qualities such as the view coverage of a vision
system that determines whether a skill can be successfully deployed in
performing a task. Choosing improper skill parameters may cause the monitoring
modules to delay or miss the detection of important events such as a mechanical
failure. These failures may reduce the throughput in robotic manufacturing and
could even cause a destructive system crash. To address above issues, we
propose a sensing quality-aware robot programming system that automatically
computes the sensing qualities as a function of the robot's environment and
uses the information to guide non-expert users to select proper skill
parameters in the programming phase. We demonstrate our system framework on a
6DOF robot arm for an object pick-up task.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.13802v1,2021-06-25T17:57:04Z,2021-06-25T17:57:04Z,"Efficient Document Image Classification Using Region-Based Graph Neural
  Network","Document image classification remains a popular research area because it can
be commercialized in many enterprise applications across different industries.
Recent advancements in large pre-trained computer vision and language models
and graph neural networks has lent document image classification many tools.
However using large pre-trained models usually requires substantial computing
resources which could defeat the cost-saving advantages of automatic document
image classification. In the paper we propose an efficient document image
classification framework that uses graph convolution neural networks and
incorporates textual, visual and layout information of the document. We have
rigorously benchmarked our proposed algorithm against several state-of-art
vision and language models on both publicly available dataset and a real-life
insurance document classification dataset. Empirical results on both publicly
available and real-world data show that our methods achieve near SOTA
performance yet require much less computing resources and time for model
training and inference. This results in solutions than offer better cost
advantages, especially in scalable deployment for enterprise applications. The
results showed that our algorithm can achieve classification performance quite
close to SOTA. We also provide comprehensive comparisons of computing
resources, model sizes, train and inference time between our proposed methods
and baselines. In addition we delineate the cost per image using our method and
other baselines.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.10251v4,2022-05-06T11:03:26Z,2021-06-18T17:33:13Z,Active Offline Policy Selection,"This paper addresses the problem of policy selection in domains with abundant
logged data, but with a restricted interaction budget. Solving this problem
would enable safe evaluation and deployment of offline reinforcement learning
policies in industry, robotics, and recommendation domains among others.
Several off-policy evaluation (OPE) techniques have been proposed to assess the
value of policies using only logged data. However, there is still a big gap
between the evaluation by OPE and the full online evaluation. Yet, large
amounts of online interactions are often not possible in practice. To overcome
this problem, we introduce active offline policy selection - a novel sequential
decision approach that combines logged data with online interaction to identify
the best policy. We use OPE estimates to warm start the online evaluation.
Then, in order to utilize the limited environment interactions wisely we decide
which policy to evaluate next based on a Bayesian optimization method with a
kernel that represents policy similarity. We use multiple benchmarks, including
real-world robotics, with a large number of candidate policies to show that the
proposed approach improves upon state-of-the-art OPE estimates and pure online
policy evaluation.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.08209v4,2022-06-22T13:16:46Z,2021-06-15T15:16:54Z,"A methodology to identify identical single-board computers based on
  hardware behavior fingerprinting","The connectivity and resource-constrained nature of single-board devices open
the door to cybersecurity concerns affecting Internet of Things (IoT)
scenarios. One of the most important issues is the presence of unauthorized IoT
devices that want to impersonate legitimate ones by using identical hardware
and software specifications. This situation can provoke sensitive information
leakages, data poisoning, or privilege escalation in IoT scenarios. Combining
behavioral fingerprinting and Machine/Deep Learning (ML/DL) techniques is a
promising approach to identify these malicious spoofing devices by detecting
minor performance differences generated by imperfections in manufacturing.
However, existing solutions are not suitable for single-board devices since
they do not consider their hardware and software limitations, underestimate
critical aspects such as fingerprint stability or context changes, and do not
explore the potential of ML/DL techniques. To improve it, this work first
identifies the essential properties for single-board device identification:
uniqueness, stability, diversity, scalability, efficiency, robustness, and
security. Then, a novel methodology relies on behavioral fingerprinting to
identify identical single-board devices and meet the previous properties. The
methodology leverages the different built-in components of the system and ML/DL
techniques, comparing the device internal behavior with each other to detect
manufacturing variations. The methodology validation has been performed in a
real environment composed of 15 identical Raspberry Pi 4 B and 10 Raspberry Pi
3 B+ devices, obtaining a 91.9% average TPR and identifying all devices by
setting a 50% threshold in the evaluation process. Finally, a discussion
compares the proposed solution with related work, highlighting the fingerprint
properties not met, and provides important lessons learned and limitations.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.06180v1,2021-06-11T05:48:00Z,2021-06-11T05:48:00Z,"Gas Detection and Identification Using Multimodal Artificial
  Intelligence Based Sensor Fusion","With the rapid industrialization and technological advancements, innovative
engineering technologies which are cost effective, faster and easier to
implement are essential. One such area of concern is the rising number of
accidents happening due to gas leaks at coal mines, chemical industries, home
appliances etc. In this paper we propose a novel approach to detect and
identify the gaseous emissions using the multimodal AI fusion techniques. Most
of the gases and their fumes are colorless, odorless, and tasteless, thereby
challenging our normal human senses. Sensing based on a single sensor may not
be accurate, and sensor fusion is essential for robust and reliable detection
in several real-world applications. We manually collected 6400 gas samples
(1600 samples per class for four classes) using two specific sensors: the
7-semiconductor gas sensors array, and a thermal camera. The early fusion
method of multimodal AI, is applied The network architecture consists of a
feature extraction module for individual modality, which is then fused using a
merged layer followed by a dense layer, which provides a single output for
identifying the gas. We obtained the testing accuracy of 96% (for fused model)
as opposed to individual model accuracies of 82% (based on Gas Sensor data
using LSTM) and 93% (based on thermal images data using CNN model). Results
demonstrate that the fusion of multiple sensors and modalities outperforms the
outcome of a single sensor.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.06150v1,2021-06-11T03:30:25Z,2021-06-11T03:30:25Z,Global Neighbor Sampling for Mixed CPU-GPU Training on Giant Graphs,"Graph neural networks (GNNs) are powerful tools for learning from graph data
and are widely used in various applications such as social network
recommendation, fraud detection, and graph search. The graphs in these
applications are typically large, usually containing hundreds of millions of
nodes. Training GNN models on such large graphs efficiently remains a big
challenge. Despite a number of sampling-based methods have been proposed to
enable mini-batch training on large graphs, these methods have not been proved
to work on truly industry-scale graphs, which require GPUs or mixed-CPU-GPU
training. The state-of-the-art sampling-based methods are usually not optimized
for these real-world hardware setups, in which data movement between CPUs and
GPUs is a bottleneck. To address this issue, we propose Global Neighborhood
Sampling that aims at training GNNs on giant graphs specifically for
mixed-CPU-GPU training. The algorithm samples a global cache of nodes
periodically for all mini-batches and stores them in GPUs. This global cache
allows in-GPU importance sampling of mini-batches, which drastically reduces
the number of nodes in a mini-batch, especially in the input layer, to reduce
data copy between CPU and GPU and mini-batch computation without compromising
the training convergence rate or model accuracy. We provide a highly efficient
implementation of this method and show that our implementation outperforms an
efficient node-wise neighbor sampling baseline by a factor of 2X-4X on giant
graphs. It outperforms an efficient implementation of LADIES with small layers
by a factor of 2X-14X while achieving much higher accuracy than LADIES.We also
theoretically analyze the proposed algorithm and show that with cached node
data of a proper size, it enjoys a comparable convergence rate as the
underlying node-wise sampling method.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.05688v2,2021-10-05T16:33:20Z,2021-06-10T12:10:51Z,AI-enabled Automation for Completeness Checking of Privacy Policies,"Technological advances in information sharing have raised concerns about data
protection. Privacy policies contain privacy-related requirements about how the
personal data of individuals will be handled by an organization or a software
system (e.g., a web service or an app). In Europe, privacy policies are subject
to compliance with the General Data Protection Regulation (GDPR). A
prerequisite for GDPR compliance checking is to verify whether the content of a
privacy policy is complete according to the provisions of GDPR. Incomplete
privacy policies might result in large fines on violating organization as well
as incomplete privacy-related software specifications. Manual completeness
checking is both time-consuming and error-prone. In this paper, we propose
AI-based automation for the completeness checking of privacy policies. Through
systematic qualitative methods, we first build two artifacts to characterize
the privacy-related provisions of GDPR, namely a conceptual model and a set of
completeness criteria. Then, we develop an automated solution on top of these
artifacts by leveraging a combination of natural language processing and
supervised machine learning. Specifically, we identify the GDPR-relevant
information content in privacy policies and subsequently check them against the
completeness criteria. To evaluate our approach, we collected 234 real privacy
policies from the fund industry. Over a set of 48 unseen privacy policies, our
approach detected 300 of the total of 334 violations of some completeness
criteria correctly, while producing 23 false positives. The approach thus has a
precision of 92.9% and recall of 89.8%. Compared to a baseline that applies
keyword search only, our approach results in an improvement of 24.5% in
precision and 38% in recall.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.04008v2,2021-06-09T16:58:52Z,2021-06-07T23:31:47Z,Widening Access to Applied Machine Learning with TinyML,"Broadening access to both computational and educational resources is critical
to diffusing machine-learning (ML) innovation. However, today, most ML
resources and experts are siloed in a few countries and organizations. In this
paper, we describe our pedagogical approach to increasing access to applied ML
through a massive open online course (MOOC) on Tiny Machine Learning (TinyML).
We suggest that TinyML, ML on resource-constrained embedded devices, is an
attractive means to widen access because TinyML both leverages low-cost and
globally accessible hardware, and encourages the development of complete,
self-contained applications, from data collection to deployment. To this end, a
collaboration between academia (Harvard University) and industry (Google)
produced a four-part MOOC that provides application-oriented instruction on how
to develop solutions using TinyML. The series is openly available on the edX
MOOC platform, has no prerequisites beyond basic programming, and is designed
for learners from a global variety of backgrounds. It introduces pupils to
real-world applications, ML algorithms, data-set engineering, and the ethical
considerations of these technologies via hands-on programming and deployment of
TinyML applications in both the cloud and their own microcontrollers. To
facilitate continued learning, community building, and collaboration beyond the
courses, we launched a standalone website, a forum, a chat, and an optional
course-project competition. We also released the course materials publicly,
hoping they will inspire the next generation of ML practitioners and educators
and further broaden access to cutting-edge ML technologies.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.01674v1,2021-06-03T08:23:24Z,2021-06-03T08:23:24Z,"JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale
  Online Inference at Baidu","In modern internet industries, deep learning based recommender systems have
became an indispensable building block for a wide spectrum of applications,
such as search engine, news feed, and short video clips. However, it remains
challenging to carry the well-trained deep models for online real-time
inference serving, with respect to the time-varying web-scale traffics from
billions of users, in a cost-effective manner. In this work, we present JIZHI -
a Model-as-a-Service system - that per second handles hundreds of millions of
online inference requests to huge deep models with more than trillions of
sparse parameters, for over twenty real-time recommendation services at Baidu,
Inc. In JIZHI, the inference workflow of every recommendation request is
transformed to a Staged Event-Driven Pipeline (SEDP), where each node in the
pipeline refers to a staged computation or I/O intensive task processor. With
traffics of real-time inference requests arrived, each modularized processor
can be run in a fully asynchronized way and managed separately. Besides, JIZHI
introduces heterogeneous and hierarchical storage to further accelerate the
online inference process by reducing unnecessary computations and potential
data access latency induced by ultra-sparse model parameters. Moreover, an
intelligent resource manager has been deployed to maximize the throughput of
JIZHI over the shared infrastructure by searching the optimal resource
allocation plan from historical logs and fine-tuning the load shedding policies
over intermediate system feedback. Extensive experiments have been done to
demonstrate the advantages of JIZHI from the perspectives of end-to-end service
latency, system-wide throughput, and resource consumption. JIZHI has helped
Baidu saved more than ten million US dollars in hardware and utility costs
while handling 200% more traffics without sacrificing inference efficiency.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.00241v1,2021-06-01T05:46:22Z,2021-06-01T05:46:22Z,"Reinforced Iterative Knowledge Distillation for Cross-Lingual Named
  Entity Recognition","Named entity recognition (NER) is a fundamental component in many
applications, such as Web Search and Voice Assistants. Although deep neural
networks greatly improve the performance of NER, due to the requirement of
large amounts of training data, deep neural networks can hardly scale out to
many languages in an industry setting. To tackle this challenge, cross-lingual
NER transfers knowledge from a rich-resource language to languages with low
resources through pre-trained multilingual language models. Instead of using
training data in target languages, cross-lingual NER has to rely on only
training data in source languages, and optionally adds the translated training
data derived from source languages. However, the existing cross-lingual NER
methods do not make good use of rich unlabeled data in target languages, which
is relatively easy to collect in industry applications. To address the
opportunities and challenges, in this paper we describe our novel practice in
Microsoft to leverage such large amounts of unlabeled data in target languages
in real production settings. To effectively extract weak supervision signals
from the unlabeled data, we develop a novel approach based on the ideas of
semi-supervised learning and reinforcement learning. The empirical study on
three benchmark data sets verifies that our approach establishes the new
state-of-the-art performance with clear edges. Now, the NER techniques reported
in this paper are on their way to become a fundamental component for Web
ranking, Entity Pane, Answers Triggering, and Question Answering in the
Microsoft Bing search engine. Moreover, our techniques will also serve as part
of the Spoken Language Understanding module for a commercial voice assistant.
We plan to open source the code of the prototype framework after deployment.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.13420v1,2021-05-27T19:48:23Z,2021-05-27T19:48:23Z,Model Selection for Production System via Automated Online Experiments,"A challenge that machine learning practitioners in the industry face is the
task of selecting the best model to deploy in production. As a model is often
an intermediate component of a production system, online controlled experiments
such as A/B tests yield the most reliable estimation of the effectiveness of
the whole system, but can only compare two or a few models due to budget
constraints. We propose an automated online experimentation mechanism that can
efficiently perform model selection from a large pool of models with a small
number of online experiments. We derive the probability distribution of the
metric of interest that contains the model uncertainty from our Bayesian
surrogate model trained using historical logs. Our method efficiently
identifies the best model by sequentially selecting and deploying a list of
models from the candidate set that balance exploration-exploitation. Using
simulations based on real data, we demonstrate the effectiveness of our method
on two different tasks.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.12899v1,2021-05-27T01:16:00Z,2021-05-27T01:16:00Z,Learning to Optimize Industry-Scale Dynamic Pickup and Delivery Problems,"The Dynamic Pickup and Delivery Problem (DPDP) is aimed at dynamically
scheduling vehicles among multiple sites in order to minimize the cost when
delivery orders are not known a priori. Although DPDP plays an important role
in modern logistics and supply chain management, state-of-the-art DPDP
algorithms are still limited on their solution quality and efficiency. In
practice, they fail to provide a scalable solution as the numbers of vehicles
and sites become large. In this paper, we propose a data-driven approach,
Spatial-Temporal Aided Double Deep Graph Network (ST-DDGN), to solve
industry-scale DPDP. In our method, the delivery demands are first forecast
using spatial-temporal prediction method, which guides the neural network to
perceive spatial-temporal distribution of delivery demand when dispatching
vehicles. Besides, the relationships of individuals such as vehicles are
modelled by establishing a graph-based value function. ST-DDGN incorporates
attention-based graph embedding with Double DQN (DDQN). As such, it can make
the inference across vehicles more efficiently compared with traditional
methods. Our method is entirely data driven and thus adaptive, i.e., the
relational representation of adjacent vehicles can be learned and corrected by
ST-DDGN from data periodically. We have conducted extensive experiments over
real-world data to evaluate our solution. The results show that ST-DDGN reduces
11.27% number of the used vehicles and decreases 13.12% total transportation
cost on average over the strong baselines, including the heuristic algorithm
deployed in our UAT (User Acceptance Test) environment and a variety of vanilla
DRL methods. We are due to fully deploy our solution into our online logistics
system and it is estimated that millions of USD logistics cost can be saved per
year.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.11216v1,2021-05-24T11:51:46Z,2021-05-24T11:51:46Z,"CONECT4: Desarrollo de componentes basados en Realidad Mixta, Realidad
  Virtual Y Conocimiento Experto para generacin de entornos de aprendizaje
  Hombre-Mquina","This work presents the results of project CONECT4, which addresses the
research and development of new non-intrusive communication methods for the
generation of a human-machine learning ecosystem oriented to predictive
maintenance in the automotive industry. Through the use of innovative
technologies such as Augmented Reality, Virtual Reality, Digital Twin and
expert knowledge, CONECT4 implements methodologies that allow improving the
efficiency of training techniques and knowledge management in industrial
companies. The research has been supported by the development of content and
systems with a low level of technological maturity that address solutions for
the industrial sector applied in training and assistance to the operator. The
results have been analyzed in companies in the automotive sector, however, they
are exportable to any other type of industrial sector. -- --
  En esta publicaci\'on se presentan los resultados del proyecto CONECT4, que
aborda la investigaci\'on y desarrollo de nuevos m\'etodos de comunicaci\'on no
intrusivos para la generaci\'on de un ecosistema de aprendizaje
hombre-m\'aquina orientado al mantenimiento predictivo en la industria de
automoci\'on. A trav\'es del uso de tecnolog\'ias innovadoras como la Realidad
Aumentada, la Realidad Virtual, el Gemelo Digital y conocimiento experto,
CONECT4 implementa metodolog\'ias que permiten mejorar la eficiencia de las
t\'ecnicas de formaci\'on y gesti\'on de conocimiento en las empresas
industriales. La investigaci\'on se ha apoyado en el desarrollo de contenidos y
sistemas con un nivel de madurez tecnol\'ogico bajo que abordan soluciones para
el sector industrial aplicadas en la formaci\'on y asistencia al operario. Los
resultados han sido analizados en empresas del sector de automoci\'on, no
obstante, son exportables a cualquier otro tipo de sector industrial.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.08886v3,2022-05-02T13:38:28Z,2021-05-19T02:11:43Z,"Towards Trusted and Intelligent Cyber-Physical Systems: A
  Security-by-Design Approach","The complexity of cyberattacks in Cyber-Physical Systems (CPSs) calls for a
mechanism that can evaluate the operational behaviour and security without
negatively affecting the operation of live systems. In this regard, Digital
Twins (DTs) are revolutionizing the CPSs. DTs strengthen the security of CPSs
throughout the product lifecycle, while assuming that the DT data is trusted,
providing agility to predict and respond to real-time changes. However,
existing DTs solutions in CPS are constrained with untrustworthy data
dissemination among multiple stakeholders and timely course correction. Such
limitations reinforce the significance of designing trustworthy distributed
solutions with the ability to create actionable insights in real-time. To do
so, we propose a framework that focuses on trusted and intelligent DT by
integrating blockchain and Artificial Intelligence (AI). Following a hybrid
approach, the proposed framework not only acquires process knowledge from the
specifications of the CPS, but also relies on AI to learn security threats
based on sensor data. Furthermore, we integrate blockchain to safeguard product
lifecycle data. We discuss the applicability of the proposed framework for the
automotive industry as a CPS use case. Finally, we identify the open challenges
that impede the implementation of intelligence-driven architectures in CPSs.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.08649v3,2021-08-22T05:41:15Z,2021-05-18T16:27:20Z,"DCAP: Deep Cross Attentional Product Network for User Response
  Prediction","User response prediction, which aims to predict the probability that a user
will provide a predefined positive response in a given context such as clicking
on an ad or purchasing an item, is crucial to many industrial applications such
as online advertising, recommender systems, and search ranking. However, due to
the high dimensionality and super sparsity of the data collected in these
tasks, handcrafting cross features is inevitably time expensive. Prior studies
in predicting user response leveraged the feature interactions by enhancing
feature vectors with products of features to model second-order or high-order
cross features, either explicitly or implicitly. Nevertheless, these existing
methods can be hindered by not learning sufficient cross features due to model
architecture limitations or modeling all high-order feature interactions with
equal weights. This work aims to fill this gap by proposing a novel
architecture Deep Cross Attentional Product Network (DCAP), which keeps cross
network's benefits in modeling high-order feature interactions explicitly at
the vector-wise level. Beyond that, it can differentiate the importance of
different cross features in each network layer inspired by the multi-head
attention mechanism and Product Neural Network (PNN), allowing practitioners to
perform a more in-depth analysis of user behaviors. Additionally, our proposed
model can be easily implemented and train in parallel. We conduct comprehensive
experiments on three real-world datasets. The results have robustly
demonstrated that our proposed model DCAP achieves superior prediction
performance compared with the state-of-the-art models. Public codes are
available at https://github.com/zachstarkk/DCAP.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.14870v1,2021-04-30T09:53:28Z,2021-04-30T09:53:28Z,"Action in Mind: A Neural Network Approach to Action Recognition and
  Segmentation","Recognizing and categorizing human actions is an important task with
applications in various fields such as human-robot interaction, video analysis,
surveillance, video retrieval, health care system and entertainment industry.
This thesis presents a novel computational approach for human action
recognition through different implementations of multi-layer architectures
based on artificial neural networks. Each system level development is designed
to solve different aspects of the action recognition problem including online
real-time processing, action segmentation and the involvement of objects. The
analysis of the experimental results are illustrated and described in six
articles. The proposed action recognition architecture of this thesis is
composed of several processing layers including a preprocessing layer, an
ordered vector representation layer and three layers of neural networks. It
utilizes self-organizing neural networks such as Kohonen feature maps and
growing grids as the main neural network layers. Thus the architecture presents
a biological plausible approach with certain features such as topographic
organization of the neurons, lateral interactions, semi-supervised learning and
the ability to represent high dimensional input space in lower dimensional
maps. For each level of development the system is trained with the input data
consisting of consecutive 3D body postures and tested with generalized input
data that the system has never met before. The experimental results of
different system level developments show that the system performs well with
quite high accuracy for recognizing human actions.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.13190v1,2021-04-27T16:21:48Z,2021-04-27T16:21:48Z,Extending Isolation Forest for Anomaly Detection in Big Data via K-Means,"Industrial Information Technology (IT) infrastructures are often vulnerable
to cyberattacks. To ensure security to the computer systems in an industrial
environment, it is required to build effective intrusion detection systems to
monitor the cyber-physical systems (e.g., computer networks) in the industry
for malicious activities. This paper aims to build such intrusion detection
systems to protect the computer networks from cyberattacks. More specifically,
we propose a novel unsupervised machine learning approach that combines the
K-Means algorithm with the Isolation Forest for anomaly detection in industrial
big data scenarios. Since our objective is to build the intrusion detection
system for the big data scenario in the industrial domain, we utilize the
Apache Spark framework to implement our proposed model which was trained in
large network traffic data (about 123 million instances of network traffic)
stored in Elasticsearch. Moreover, we evaluate our proposed model on the live
streaming data and find that our proposed system can be used for real-time
anomaly detection in the industrial setup. In addition, we address different
challenges that we face while training our model on large datasets and
explicitly describe how these issues were resolved. Based on our empirical
evaluation in different use-cases for anomaly detection in real-world network
traffic data, we observe that our proposed system is effective to detect
anomalies in big data scenarios. Finally, we evaluate our proposed model on
several academic datasets to compare with other models and find that it
provides comparable performance with other state-of-the-art approaches.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.13114v1,2021-04-27T11:29:02Z,2021-04-27T11:29:02Z,"One Backward from Ten Forward, Subsampling for Large-Scale Deep Learning","Deep learning models in large-scale machine learning systems are often
continuously trained with enormous data from production environments. The sheer
volume of streaming training data poses a significant challenge to real-time
training subsystems and ad-hoc sampling is the standard practice. Our key
insight is that these deployed ML systems continuously perform forward passes
on data instances during inference, but ad-hoc sampling does not take advantage
of this substantial computational effort. Therefore, we propose to record a
constant amount of information per instance from these forward passes. The
extra information measurably improves the selection of which data instances
should participate in forward and backward passes. A novel optimization
framework is proposed to analyze this problem and we provide an efficient
approximation algorithm under the framework of Mini-batch gradient descent as a
practical solution. We also demonstrate the effectiveness of our framework and
algorithm on several large-scale classification and regression tasks, when
compared with competitive baselines widely used in industry.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.09876v1,2021-04-20T10:16:04Z,2021-04-20T10:16:04Z,"IIoT-Enabled Health Monitoring for Integrated Heat Pump System Using
  Mixture Slow Feature Analysis","The sustaining evolution of sensing and advancement in communications
technologies have revolutionized prognostics and health management for various
electrical equipment towards data-driven ways. This revolution delivers a
promising solution for the health monitoring problem of heat pump (HP) system,
a vital device widely deployed in modern buildings for heating use, to timely
evaluate its operation status to avoid unexpected downtime. Many HPs were
practically manufactured and installed many years ago, resulting in fewer
sensors available due to technology limitations and cost control at that time.
It raises a dilemma to safeguard HPs at an affordable cost. We propose a hybrid
scheme by integrating industrial Internet-of-Things (IIoT) and intelligent
health monitoring algorithms to handle this challenge. To start with, an IIoT
network is constructed to sense and store measurements. Specifically,
temperature sensors are properly chosen and deployed at the inlet and outlet of
the water tank to measure water temperature. Second, with temperature
information, we propose an unsupervised learning algorithm named mixture slow
feature analysis (MSFA) to timely evaluate the health status of the integrated
HP. Characterized by frequent operation switches of different HPs due to the
variable demand for hot water, various heating patterns with different heating
speeds are observed. Slowness, a kind of dynamics to measure the varying speed
of steady distribution, is properly considered in MSFA for both heating pattern
division and health evaluation. Finally, the efficacy of the proposed method is
verified through a real integrated HP with five connected HPs installed ten
years ago. The experimental results show that MSFA is capable of accurately
identifying health status of the system, especially failure at a preliminary
stage compared to its competing algorithms.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.08542v2,2021-05-11T14:11:46Z,2021-04-17T13:36:19Z,"ScaleFreeCTR: MixCache-based Distributed Training System for CTR Models
  with Huge Embedding Table","Because of the superior feature representation ability of deep learning,
various deep Click-Through Rate (CTR) models are deployed in the commercial
systems by industrial companies. To achieve better performance, it is necessary
to train the deep CTR models on huge volume of training data efficiently, which
makes speeding up the training process an essential problem. Different from the
models with dense training data, the training data for CTR models is usually
high-dimensional and sparse. To transform the high-dimensional sparse input
into low-dimensional dense real-value vectors, almost all deep CTR models adopt
the embedding layer, which easily reaches hundreds of GB or even TB. Since a
single GPU cannot afford to accommodate all the embedding parameters, when
performing distributed training, it is not reasonable to conduct the
data-parallelism only. Therefore, existing distributed training platforms for
recommendation adopt model-parallelism. Specifically, they use CPU (Host)
memory of servers to maintain and update the embedding parameters and utilize
GPU worker to conduct forward and backward computations. Unfortunately, these
platforms suffer from two bottlenecks: (1) the latency of pull \& push
operations between Host and GPU; (2) parameters update and synchronization in
the CPU servers. To address such bottlenecks, in this paper, we propose the
ScaleFreeCTR: a MixCache-based distributed training system for CTR models.
Specifically, in SFCTR, we also store huge embedding table in CPU but utilize
GPU instead of CPU to conduct embedding synchronization efficiently. To reduce
the latency of data transfer between both GPU-Host and GPU-GPU, the MixCache
mechanism and Virtual Sparse Id operation are proposed. Comprehensive
experiments and ablation studies are conducted to demonstrate the effectiveness
and efficiency of SFCTR.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.08178v1,2021-04-14T20:32:39Z,2021-04-14T20:32:39Z,"Design of an Efficient, Ease-of-use and Affordable Artificial
  Intelligence based Nucleic Acid Amplification Diagnosis Technology for
  Tuberculosis and Multi-drug Resistant Tuberculosis","Current technologies that facilitate diagnosis for simultaneous detection of
Mycobacterium tuberculosis and its resistance to first-line anti-tuberculosis
drugs (Isoniazid and Rifampicim) are designed for lab-based settings and are
unaffordable for large scale testing implementations. The suitability of a TB
diagnosis instrument, generally required in low-resource settings, to be
implementable in point-of-care last mile public health centres depends on
manufacturing cost, ease-of-use, automation and portability. This paper
discusses a portable, low-cost, machine learning automated Nucleic acid
amplification testing (NAAT) device that employs the use of a smartphone-based
fluorescence detection using novel image processing and chromaticity detection
algorithms. To test the instrument, real time polymerase chain reaction (qPCR)
experiment on cDNA dilution spanning over two concentrations (40 ng/uL and 200
ng/uL) was performed and sensitive detection of multiplexed positive control
assay was verified.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.03613v1,2021-04-08T08:50:44Z,2021-04-08T08:50:44Z,Uncertainty-aware Remaining Useful Life predictor,"Remaining Useful Life (RUL) estimation is the problem of inferring how long a
certain industrial asset can be expected to operate within its defined
specifications. Deploying successful RUL prediction methods in real-life
applications is a prerequisite for the design of intelligent maintenance
strategies with the potential of drastically reducing maintenance costs and
machine downtimes. In light of their superior performance in a wide range of
engineering fields, Machine Learning (ML) algorithms are natural candidates to
tackle the challenges involved in the design of intelligent maintenance
systems. In particular, given the potentially catastrophic consequences or
substantial costs associated with maintenance decisions that are either too
late or too early, it is desirable that ML algorithms provide uncertainty
estimates alongside their predictions. However, standard data-driven methods
used for uncertainty estimation in RUL problems do not scale well to large
datasets or are not sufficiently expressive to model the high-dimensional
mapping from raw sensor data to RUL estimates. In this work, we consider Deep
Gaussian Processes (DGPs) as possible solutions to the aforementioned
limitations. We perform a thorough evaluation and comparison of several
variants of DGPs applied to RUL predictions. The performance of the algorithms
is evaluated on the N-CMAPSS (New Commercial Modular Aero-Propulsion System
Simulation) dataset from NASA for aircraft engines. The results show that the
proposed methods are able to provide very accurate RUL predictions along with
sensible uncertainty estimates, providing more reliable solutions for
(safety-critical) real-life industrial applications.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.02980v1,2021-04-07T08:07:57Z,2021-04-07T08:07:57Z,"Synthetic training data generation for deep learning based quality
  inspection","Deep learning is now the gold standard in computer vision-based quality
inspection systems. In order to detect defects, supervised learning is often
utilized, but necessitates a large amount of annotated images, which can be
costly: collecting, cleaning, and annotating the data is tedious and limits the
speed at which a system can be deployed as everything the system must detect
needs to be observed first. This can impede the inspection of rare defects,
since very few samples can be collected by the manufacturer. In this work, we
focus on simulations to solve this issue. We first present a generic simulation
pipeline to render images of defective or healthy (non defective) parts. As
metallic parts can be highly textured with small defects like holes, we design
a texture scanning and generation method. We assess the quality of the
generated images by training deep learning networks and by testing them on real
data from a manufacturer. We demonstrate that we can achieve encouraging
results on real defect detection using purely simulated data. Additionally, we
are able to improve global performances by concatenating simulated and real
data, showing that simulations can complement real images to boost
performances. Lastly, using domain adaptation techniques helps improving
slightly our final results.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.01036v1,2021-04-02T13:17:11Z,2021-04-02T13:17:11Z,"Hybrid Policy Learning for Energy-Latency Tradeoff in MEC-Assisted VR
  Video Service","Virtual reality (VR) is promising to fundamentally transform a broad spectrum
of industry sectors and the way humans interact with virtual content. However,
despite unprecedented progress, current networking and computing
infrastructures are incompetent to unlock VR's full potential. In this paper,
we consider delivering the wireless multi-tile VR video service over a mobile
edge computing (MEC) network. The primary goal is to minimize the system
latency/energy consumption and to arrive at a tradeoff thereof. To this end, we
first cast the time-varying view popularity as a model-free Markov chain to
effectively capture its dynamic characteristics. After jointly assessing the
caching and computing capacities on both the MEC server and the VR playback
device, a hybrid policy is then implemented to coordinate the dynamic caching
replacement and the deterministic offloading, so as to fully utilize the system
resources. The underlying multi-objective problem is reformulated as a
partially observable Markov decision process, and a deep deterministic policy
gradient algorithm is proposed to iteratively learn its solution, where a long
short-term memory neural network is embedded to continuously predict the
dynamics of the unobservable popularity. Simulation results demonstrate the
superiority of the proposed scheme in achieving a trade-off between the energy
efficiency and the latency reduction over the baseline methods.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.16938v3,2022-07-08T08:38:15Z,2021-03-31T09:43:38Z,"Unpaired Single-Image Depth Synthesis with cycle-consistent Wasserstein
  GANs","Real-time estimation of actual environment depth is an essential module for
various autonomous system tasks such as localization, obstacle detection and
pose estimation. During the last decade of machine learning, extensive
deployment of deep learning methods to computer vision tasks yielded successful
approaches for realistic depth synthesis out of a simple RGB modality. While
most of these models rest on paired depth data or availability of video
sequences and stereo images, there is a lack of methods facing single-image
depth synthesis in an unsupervised manner. Therefore, in this study, latest
advancements in the field of generative neural networks are leveraged to fully
unsupervised single-image depth synthesis. To be more exact, two
cycle-consistent generators for RGB-to-depth and depth-to-RGB transfer are
implemented and simultaneously optimized using the Wasserstein-1 distance. To
ensure plausibility of the proposed method, we apply the models to a self
acquised industrial data set as well as to the renown NYU Depth v2 data set,
which allows comparison with existing approaches. The observed success in this
study suggests high potential for unpaired single-image depth estimation in
real world applications.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.15245v3,2021-04-03T14:55:20Z,2021-03-28T23:36:56Z,"Game Theory Based Privacy Preserving Approach for Collaborative Deep
  Learning in IoT","The exponential growth of Internet of Things (IoT) has become a transcending
force in creating innovative smart devices and connected domains including
smart homes, healthcare, transportation and manufacturing. With billions of IoT
devices, there is a huge amount of data continuously being generated,
transmitted, and stored at various points in the IoT architecture. Deep
learning is widely being used in IoT applications to extract useful insights
from IoT data. However, IoT users have security and privacy concerns and prefer
not to share their personal data with third party applications or stakeholders.
In order to address user privacy concerns, Collaborative Deep Learning (CDL)
has been largely employed in data-driven applications which enables multiple
IoT devices to train their models locally on edge gateways. In this chapter, we
first discuss different types of deep learning approaches and how these
approaches can be employed in the IoT domain. We present a privacy-preserving
collaborative deep learning approach for IoT devices which can achieve benefits
from other devices in the system. This learning approach is analyzed from the
behavioral perspective of mobile edge devices using a game-theoretic model. We
analyze the Nash Equilibrium in N-player static game model. We further present
a novel fair collaboration strategy among edge IoT devices using cluster based
approach to solve the CDL game, which enforces mobile edge devices for
cooperation. We also present implementation details and evaluation analysis in
a real-world smart home deployment.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.14407v2,2021-04-13T12:48:31Z,2021-03-26T11:32:27Z,Bellman: A Toolbox for Model-Based Reinforcement Learning in TensorFlow,"In the past decade, model-free reinforcement learning (RL) has provided
solutions to challenging domains such as robotics. Model-based RL shows the
prospect of being more sample-efficient than model-free methods in terms of
agent-environment interactions, because the model enables to extrapolate to
unseen situations. In the more recent past, model-based methods have shown
superior results compared to model-free methods in some challenging domains
with non-linear state transitions. At the same time, it has become apparent
that RL is not market-ready yet and that many real-world applications are going
to require model-based approaches, because model-free methods are too
sample-inefficient and show poor performance in early stages of training. The
latter is particularly important in industry, e.g. in production systems that
directly impact a company's revenue. This demonstrates the necessity for a
toolbox to push the boundaries for model-based RL. While there is a plethora of
toolboxes for model-free RL, model-based RL has received little attention in
terms of toolbox development. Bellman aims to fill this gap and introduces the
first thoroughly designed and tested model-based RL toolbox using
state-of-the-art software engineering practices. Our modular approach enables
to combine a wide range of environment models with generic model-based agent
classes that recover state-of-the-art algorithms. We also provide an experiment
harness to compare both model-free and model-based agents in a systematic
fashion w.r.t. user-defined evaluation metrics (e.g. cumulative reward). This
paves the way for new research directions, e.g. investigating uncertainty-aware
environment models that are not necessarily neural-network-based, or developing
algorithms to solve industrially-motivated benchmarks that share
characteristics with real-world problems.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.11879v2,2021-09-13T07:07:50Z,2021-03-22T14:16:16Z,Real-time End-to-End Federated Learning: An Automotive Case Study,"With the development and the increasing interests in ML/DL fields, companies
are eager to apply Machine Learning/Deep Learning approaches to increase
service quality and customer experience. Federated Learning was implemented as
an effective model training method for distributing and accelerating
time-consuming model training while protecting user data privacy. However,
common Federated Learning approaches, on the other hand, use a synchronous
protocol to conduct model aggregation, which is inflexible and unable to adapt
to rapidly changing environments and heterogeneous hardware settings in
real-world scenarios. In this paper, we present an approach to real-time
end-to-end Federated Learning combined with a novel asynchronous model
aggregation protocol. Our method is validated in an industrial use case in the
automotive domain, focusing on steering wheel angle prediction for autonomous
driving. Our findings show that asynchronous Federated Learning can
significantly improve the prediction performance of local edge models while
maintaining the same level of accuracy as centralized machine learning.
Furthermore, by using a sliding training window, the approach can minimize
communication overhead, accelerate model training speed and consume real-time
streaming data, proving high efficiency when deploying ML/DL components to
heterogeneous real-world embedded systems.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.09430v3,2021-10-20T22:40:40Z,2021-03-17T04:08:03Z,OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs,"Enabling effective and efficient machine learning (ML) over large-scale graph
data (e.g., graphs with billions of edges) can have a great impact on both
industrial and scientific applications. However, existing efforts to advance
large-scale graph ML have been largely limited by the lack of a suitable public
benchmark. Here we present OGB Large-Scale Challenge (OGB-LSC), a collection of
three real-world datasets for facilitating the advancements in large-scale
graph ML. The OGB-LSC datasets are orders of magnitude larger than existing
ones, covering three core graph learning tasks -- link prediction, graph
regression, and node classification. Furthermore, we provide dedicated baseline
experiments, scaling up expressive graph ML models to the massive datasets. We
show that expressive models significantly outperform simple scalable baselines,
indicating an opportunity for dedicated efforts to further improve graph ML at
scale. Moreover, OGB-LSC datasets were deployed at ACM KDD Cup 2021 and
attracted more than 500 team registrations globally, during which significant
performance improvements were made by a variety of innovative techniques. We
summarize the common techniques used by the winning solutions and highlight the
current best practices in large-scale graph ML. Finally, we describe how we
have updated the datasets after the KDD Cup to further facilitate research
advances. The OGB-LSC datasets, baseline code, and all the information about
the KDD Cup are available at https://ogb.stanford.edu/docs/lsc/ .",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.06326v2,2021-07-05T03:46:03Z,2021-03-10T20:13:21Z,"S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement
  Learning","Offline reinforcement learning proposes to learn policies from large
collected datasets without interacting with the physical environment. These
algorithms have made it possible to learn useful skills from data that can then
be deployed in the environment in real-world settings where interactions may be
costly or dangerous, such as autonomous driving or factories. However, current
algorithms overfit to the dataset they are trained on and exhibit poor
out-of-distribution generalization to the environment when deployed. In this
paper, we study the effectiveness of performing data augmentations on the state
space, and study 7 different augmentation schemes and how they behave with
existing offline RL algorithms. We then combine the best data performing
augmentation scheme with a state-of-the-art Q-learning technique, and improve
the function approximation of the Q-networks by smoothening out the learned
state-action space. We experimentally show that using this Surprisingly Simple
Self-Supervision technique in RL (S4RL), we significantly improve over the
current state-of-the-art algorithms on offline robot learning environments such
as MetaWorld [1] and RoboSuite [2,3], and benchmark datasets such as D4RL [4].",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.04263v2,2021-03-11T01:08:38Z,2021-03-07T04:40:15Z,Deepfake Videos in the Wild: Analysis and Detection,"AI-manipulated videos, commonly known as deepfakes, are an emerging problem.
Recently, researchers in academia and industry have contributed several
(self-created) benchmark deepfake datasets, and deepfake detection algorithms.
However, little effort has gone towards understanding deepfake videos in the
wild, leading to a limited understanding of the real-world applicability of
research contributions in this space. Even if detection schemes are shown to
perform well on existing datasets, it is unclear how well the methods
generalize to real-world deepfakes. To bridge this gap in knowledge, we make
the following contributions: First, we collect and present the largest dataset
of deepfake videos in the wild, containing 1,869 videos from YouTube and
Bilibili, and extract over 4.8M frames of content. Second, we present a
comprehensive analysis of the growth patterns, popularity, creators,
manipulation strategies, and production methods of deepfake content in the
real-world. Third, we systematically evaluate existing defenses using our new
dataset, and observe that they are not ready for deployment in the real-world.
Fourth, we explore the potential for transfer learning schemes and
competition-winning techniques to improve defenses.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.03544v2,2021-03-10T21:27:51Z,2021-03-05T08:52:31Z,Challenges of engineering safe and secure highly automated vehicles,"After more than a decade of intense focus on automated vehicles, we are still
facing huge challenges for the vision of fully autonomous driving to become a
reality. The same ""disillusionment"" is true in many other domains, in which
autonomous Cyber-Physical Systems (CPS) could considerably help to overcome
societal challenges and be highly beneficial to society and individuals. Taking
the automotive domain, i.e. highly automated vehicles (HAV), as an example,
this paper sets out to summarize the major challenges that are still to
overcome for achieving safe, secure, reliable and trustworthy highly automated
resp. autonomous CPS. We constrain ourselves to technical challenges,
acknowledging the importance of (legal) regulations, certification,
standardization, ethics, and societal acceptance, to name but a few, without
delving deeper into them as this is beyond the scope of this paper. Four
challenges have been identified as being the main obstacles to realizing HAV:
Realization of continuous, post-deployment systems improvement, handling of
uncertainties and incomplete information, verification of HAV with machine
learning components, and prediction. Each of these challenges is described in
detail, including sub-challenges and, where appropriate, possible approaches to
overcome them. By working together in a common effort between industry and
academy and focusing on these challenges, the authors hope to contribute to
overcome the ""disillusionment"" for realizing HAV.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.03512v1,2021-03-05T07:47:54Z,2021-03-05T07:47:54Z,"Amino acid frequency and domain features serve well for random forest
  based classification of thermophilic and mesophilic protein; a case study on
  serine proteases","Thermostability is an important prerequisite for enzymes employed for
industrial applications. Several machine learning based models have thus been
formulated for protein classification based on this particular trait. These
models have employed features derived from sequences, structures or both
resulting in a >93% accuracy based on a 10-fold cross-validation. Besides using
various proteins from a wide range of organisms, such studies also rely on
hundreds of features. In the present study, an enzyme specific classification
model was created using significantly less number of features that provides a
similar accuracy of classification for thermophilic and non-thermophilic enzyme
serine proteases. For building the classifier, 219 thermophilic and 200
mesophilic bacterial genomes were mined for their respective serine protease
sequences. Features were extracted for 800 sequences followed by feature
selection. We deployed a random forest based classifier that identified
thermophilic and non-thermophilic serine proteases with an accuracy of 95.71%.
Knowledge of thermostability along with amino acid positional shifts can be
vital for downstream protein engineering techniques. Thus, to emphasize the
real time application of the enzyme specific classification model, a web
platform has been designed. Combining the sequence data and the classification
model, this prototype can allow users to align their query serine protease
sequence against the custom database and identify its thermophilic nature.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.02938v1,2021-03-04T10:38:46Z,2021-03-04T10:38:46Z,FootApp: an AI-Powered System for Football Match Annotation,"In the last years, scientific and industrial research has experienced a
growing interest in acquiring large annotated data sets to train artificial
intelligence algorithms for tackling problems in different domains. In this
context, we have observed that even the market for football data has
substantially grown. The analysis of football matches relies on the annotation
of both individual players' and team actions, as well as the athletic
performance of players. Consequently, annotating football events at a
fine-grained level is a very expensive and error-prone task. Most existing
semi-automatic tools for football match annotation rely on cameras and computer
vision. However, those tools fall short in capturing team dynamics, and in
extracting data of players who are not visible in the camera frame. To address
these issues, in this manuscript we present FootApp, an AI-based system for
football match annotation. First, our system relies on an advanced and mixed
user interface that exploits both vocal and touch interaction. Second, the
motor performance of players is captured and processed by applying machine
learning algorithms to data collected from inertial sensors worn by players.
Artificial intelligence techniques are then used to check the consistency of
generated labels, including those regarding the physical activity of players,
to automatically recognize annotation errors. Notably, we implemented a full
prototype of the proposed system, performing experiments to show its
effectiveness in a real-world adoption scenario.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.03227v1,2021-03-02T21:14:44Z,2021-03-02T21:14:44Z,"Graph Computing for Financial Crime and Fraud Detection: Trends,
  Challenges and Outlook","The rise of digital payments has caused consequential changes in the
financial crime landscape. As a result, traditional fraud detection approaches
such as rule-based systems have largely become ineffective. AI and machine
learning solutions using graph computing principles have gained significant
interest in recent years. Graph-based techniques provide unique solution
opportunities for financial crime detection. However, implementing such
solutions at industrial-scale in real-time financial transaction processing
systems has brought numerous application challenges to light. In this paper, we
discuss the implementation difficulties current and next-generation graph
solutions face. Furthermore, financial crime and digital payments trends
indicate emerging challenges in the continued effectiveness of the detection
techniques. We analyze the threat landscape and argue that it provides key
insights for developing graph-based solutions.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.00821v1,2021-03-01T07:41:34Z,2021-03-01T07:41:34Z,"Rethinking complexity for software code structures: A pioneering study
  on Linux kernel code repository","The recent progress of artificial intelligence(AI) has shown great potentials
for alleviating human burden in various complex tasks. From the view of
software engineering, AI techniques can be seen in many fundamental aspects of
development, such as source code comprehension, in which state-of-the-art
models are implemented to extract and express the meaning of code snippets
automatically. However, such technologies are still struggling to tackle and
comprehend the complex structures within industrial code, thus far from
real-world applications. In the present work, we built an innovative and
systematical framework, emphasizing the problem of complexity in code
comprehension and further software engineering. Upon automatic data collection
from the latest Linux kernel source code, we modeled code structures as complex
networks through token extraction and relation parsing. Comprehensive analysis
of complexity further revealed the density and scale of network-based code
representations. Our work constructed the first large-scale dataset from
industrial-strength software code for downstream software engineering tasks
including code comprehension, and incorporated complex network theory into
code-level investigations of software development for the first time. In the
longer term, the proposed methodology could play significant roles in the
entire software engineering process, powering software design, coding,
debugging, testing, and sustaining by redefining and embracing complexity.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.12967v3,2022-03-31T09:29:13Z,2021-02-25T16:14:47Z,"A statistical framework for efficient out of distribution detection in
  deep neural networks","Background. Commonly, Deep Neural Networks (DNNs) generalize well on samples
drawn from a distribution similar to that of the training set. However, DNNs'
predictions are brittle and unreliable when the test samples are drawn from a
dissimilar distribution. This is a major concern for deployment in real-world
applications, where such behavior may come at a considerable cost, such as
industrial production lines, autonomous vehicles, or healthcare applications.
Contributions. We frame Out Of Distribution (OOD) detection in DNNs as a
statistical hypothesis testing problem. Tests generated within our proposed
framework combine evidence from the entire network. Unlike previous OOD
detection heuristics, this framework returns a $p$-value for each test sample.
It is guaranteed to maintain the Type I Error (T1E - incorrectly predicting OOD
for an actual in-distribution sample) for test data. Moreover, this allows to
combine several detectors while maintaining the T1E. Building on this
framework, we suggest a novel OOD procedure based on low-order statistics. Our
method achieves comparable or better results than state-of-the-art methods on
well-accepted OOD benchmarks, without retraining the network parameters or
assuming prior knowledge on the test distribution -- and at a fraction of the
computational cost.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.12429v1,2021-02-24T17:50:14Z,2021-02-24T17:50:14Z,Learning Off-By-One Mistakes: An Empirical Study,"Mistakes in binary conditions are a source of error in many software systems.
They happen when developers use, e.g., < or > instead of <= or >=. These
boundary mistakes are hard to find and impose manual, labor-intensive work for
software developers. While previous research has been proposing solutions to
identify errors in boundary conditions, the problem remains open. In this
paper, we explore the effectiveness of deep learning models in learning and
predicting mistakes in boundary conditions. We train different models on
approximately 1.6M examples with faults in different boundary conditions. We
achieve a precision of 85% and a recall of 84% on a balanced dataset, but lower
numbers in an imbalanced dataset. We also perform tests on 41 real-world
boundary condition bugs found from GitHub, where the model shows only a modest
performance. Finally, we test the model on a large-scale Java code base from
Adyen, our industrial partner. The model reported 36 buggy methods, but none of
them were confirmed by developers.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.11492v3,2022-04-05T09:05:30Z,2021-02-23T04:55:12Z,"DeepThermal: Combustion Optimization for Thermal Power Generating Units
  Using Offline Reinforcement Learning","Optimizing the combustion efficiency of a thermal power generating unit
(TPGU) is a highly challenging and critical task in the energy industry. We
develop a new data-driven AI system, namely DeepThermal, to optimize the
combustion control strategy for TPGUs. At its core, is a new model-based
offline reinforcement learning (RL) framework, called MORE, which leverages
historical operational data of a TGPU to solve a highly complex constrained
Markov decision process problem via purely offline training. In DeepThermal, we
first learn a data-driven combustion process simulator from the offline
dataset. The RL agent of MORE is then trained by combining real historical data
as well as carefully filtered and processed simulation data through a novel
restrictive exploration scheme. DeepThermal has been successfully deployed in
four large coal-fired thermal power plants in China. Real-world experiments
show that DeepThermal effectively improves the combustion efficiency of TPGUs.
We also report the superior performance of MORE by comparing with the
state-of-the-art algorithms on the standard offline RL benchmarks.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.10498v1,2021-02-21T03:25:26Z,2021-02-21T03:25:26Z,"Customized Slicing for 6G: Enforcing Artificial Intelligence on Resource
  Management","Next generation wireless networks are expected to support diverse vertical
industries and offer countless emerging use cases. To satisfy stringent
requirements of diversified services, network slicing is developed, which
enables service-oriented resource allocation by tailoring the infrastructure
network into multiple logical networks. However, there are still some
challenges in cross-domain multi-dimensional resource management for end-to-end
(E2E) slices under the dynamic and uncertain environment. Trading off the
revenue and cost of resource allocation while guaranteeing service quality is
significant to tenants. Therefore, this article introduces a hierarchical
resource management framework, utilizing deep reinforcement learning in
admission control of resource requests from different tenants and resource
adjustment within admitted slices for each tenant. Particularly, we first
discuss the challenges in customized resource management of 6G. Second, the
motivation and background are presented to explain why artificial intelligence
(AI) is applied in resource customization of multi-tenant slicing. Third, E2E
resource management is decomposed into two problems, multi-dimensional resource
allocation decision based on slice-level feedback and real-time slice adaption
aimed at avoiding service quality degradation. Simulation results demonstrate
the effectiveness of AI-based customized slicing. Finally, several significant
challenges that need to be addressed in practical implementation are
investigated.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.10430v1,2021-02-20T20:08:32Z,2021-02-20T20:08:32Z,"Cybersecurity Awareness Platform with Virtual Coach and Automated
  Challenge Assessment","Over the last years, the number of cyber-attacks on industrial control
systems has been steadily increasing. Among several factors, proper software
development plays a vital role in keeping these systems secure. To achieve
secure software, developers need to be aware of secure coding guidelines and
secure coding best practices. This work presents a platform geared towards
software developers in the industry that aims to increase awareness of secure
software development. The authors also introduce an interactive game component,
a virtual coach, which implements a simple artificial intelligence engine based
on the laddering technique for interviews. Through a survey, a preliminary
evaluation of the implemented artifact with real-world players (from academia
and industry) shows a positive acceptance of the developed platform.
Furthermore, the players agree that the platform is adequate for training their
secure coding skills. The impact of our work is to introduce a new automatic
challenge evaluation method together with a virtual coach to improve existing
cybersecurity awareness training programs. These training workshops can be
easily held remotely or off-line.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.09200v1,2021-02-18T07:47:43Z,2021-02-18T07:47:43Z,"Unsupervised Clustering of Time Series Signals using Neuromorphic
  Energy-Efficient Temporal Neural Networks","Unsupervised time series clustering is a challenging problem with diverse
industrial applications such as anomaly detection, bio-wearables, etc. These
applications typically involve small, low-power devices on the edge that
collect and process real-time sensory signals. State-of-the-art time-series
clustering methods perform some form of loss minimization that is extremely
computationally intensive from the perspective of edge devices. In this work,
we propose a neuromorphic approach to unsupervised time series clustering based
on Temporal Neural Networks that is capable of ultra low-power, continuous
online learning. We demonstrate its clustering performance on a subset of UCR
Time Series Archive datasets. Our results show that the proposed approach
either outperforms or performs similarly to most of the existing algorithms
while being far more amenable for efficient hardware implementation. Our
hardware assessment analysis shows that in 7 nm CMOS the proposed architecture,
on average, consumes only about 0.005 mm^2 die area and 22 uW power and can
process each signal with about 5 ns latency.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.08936v2,2021-04-02T20:06:57Z,2021-02-17T18:51:51Z,"Deep Learning Anomaly Detection for Cellular IoT with Applications in
  Smart Logistics","The number of connected Internet of Things (IoT) devices within
cyber-physical infrastructure systems grows at an increasing rate. This poses
significant device management and security challenges to current IoT networks.
Among several approaches to cope with these challenges, data-based methods
rooted in deep learning (DL) are receiving an increased interest. In this
paper, motivated by the upcoming surge of 5G IoT connectivity in industrial
environments, we propose to integrate a DL-based anomaly detection (AD) as a
service into the 3GPP mobile cellular IoT architecture. The proposed
architecture embeds autoencoder based anomaly detection modules both at the IoT
devices (ADM-EDGE) and in the mobile core network (ADM-FOG), thereby balancing
between the system responsiveness and accuracy. We design, integrate,
demonstrate and evaluate a testbed that implements the above service in a
real-world deployment integrated within the 3GPP Narrow-Band IoT (NB-IoT)
mobile operator network.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.10876v1,2021-01-26T15:33:22Z,2021-01-26T15:33:22Z,Blind Image Denoising and Inpainting Using Robust Hadamard Autoencoders,"In this paper, we demonstrate how deep autoencoders can be generalized to the
case of inpainting and denoising, even when no clean training data is
available. In particular, we show how neural networks can be trained to perform
all of these tasks simultaneously. While, deep autoencoders implemented by way
of neural networks have demonstrated potential for denoising and anomaly
detection, standard autoencoders have the drawback that they require access to
clean data for training. However, recent work in Robust Deep Autoencoders
(RDAEs) shows how autoencoders can be trained to eliminate outliers and noise
in a dataset without access to any clean training data. Inspired by this work,
we extend RDAEs to the case where data are not only noisy and have outliers,
but also only partially observed. Moreover, the dataset we train the neural
network on has the properties that all entries have noise, some entries are
corrupted by large mistakes, and many entries are not even known. Given such an
algorithm, many standard tasks, such as denoising, image inpainting, and
unobserved entry imputation can all be accomplished simultaneously within the
same framework. Herein we demonstrate these techniques on standard machine
learning tasks, such as image inpainting and denoising for the MNIST and
CIFAR10 datasets. However, these approaches are not only applicable to image
processing problems, but also have wide ranging impacts on datasets arising
from real-world problems, such as manufacturing and network processing, where
noisy, partially observed data naturally arise.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.07831v1,2021-01-19T19:29:38Z,2021-01-19T19:29:38Z,"Multi-Task Network Pruning and Embedded Optimization for Real-time
  Deployment in ADAS","Camera-based Deep Learning algorithms are increasingly needed for perception
in Automated Driving systems. However, constraints from the automotive industry
challenge the deployment of CNNs by imposing embedded systems with limited
computational resources. In this paper, we propose an approach to embed a
multi-task CNN network under such conditions on a commercial prototype
platform, i.e. a low power System on Chip (SoC) processing four surround-view
fisheye cameras at 10 FPS.
  The first focus is on designing an efficient and compact multi-task network
architecture. Secondly, a pruning method is applied to compress the CNN,
helping to reduce the runtime and memory usage by a factor of 2 without
lowering the performances significantly. Finally, several embedded optimization
techniques such as mixed-quantization format usage and efficient data transfers
between different memory areas are proposed to ensure real-time execution and
avoid bandwidth bottlenecks. The approach is evaluated on the hardware
platform, considering embedded detection performances, runtime and memory
bandwidth. Unlike most works from the literature that focus on classification
task, we aim here to study the effect of pruning and quantization on a compact
multi-task network with object detection, semantic segmentation and soiling
detection tasks.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.06175v1,2021-01-15T15:36:22Z,2021-01-15T15:36:22Z,PaddleSeg: A High-Efficient Development Toolkit for Image Segmentation,"Image Segmentation plays an essential role in computer vision and image
processing with various applications from medical diagnosis to autonomous car
driving. A lot of segmentation algorithms have been proposed for addressing
specific problems. In recent years, the success of deep learning techniques has
tremendously influenced a wide range of computer vision areas, and the modern
approaches of image segmentation based on deep learning are becoming prevalent.
In this article, we introduce a high-efficient development toolkit for image
segmentation, named PaddleSeg. The toolkit aims to help both developers and
researchers in the whole process of designing segmentation models, training
models, optimizing performance and inference speed, and deploying models.
Currently, PaddleSeg supports around 20 popular segmentation models and more
than 50 pre-trained models from real-time and high-accuracy levels. With
modular components and backbone networks, users can easily build over one
hundred models for different requirements. Furthermore, we provide
comprehensive benchmarks and evaluations to show that these segmentation
algorithms trained on our toolkit have more competitive accuracy. Also, we
provide various real industrial applications and practical cases based on
PaddleSeg. All codes and examples of PaddleSeg are available at
https://github.com/PaddlePaddle/PaddleSeg.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.05766v1,2021-01-14T18:17:11Z,2021-01-14T18:17:11Z,Ajalon: Simplifying the Authoring of Wearable Cognitive Assistants,"Wearable Cognitive Assistance (WCA) amplifies human cognition in real time
through a wearable device and low-latency wireless access to edge computing
infrastructure. It is inspired by, and broadens, the metaphor of GPS navigation
tools that provide real-time step-by-step guidance, with prompt error detection
and correction. WCA applications are likely to be transformative in education,
health care, industrial troubleshooting, manufacturing, and many other areas.
Today, WCA application development is difficult and slow, requiring skills in
areas such as machine learning and computer vision that are not widespread
among software developers. This paper describes Ajalon, an authoring toolchain
for WCA applications that reduces the skill and effort needed at each step of
the development pipeline. Our evaluation shows that Ajalon significantly
reduces the effort needed to create new WCA applications.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.04808v1,2021-01-13T00:02:49Z,2021-01-13T00:02:49Z,MLGO: a Machine Learning Guided Compiler Optimizations Framework,"Leveraging machine-learning (ML) techniques for compiler optimizations has
been widely studied and explored in academia. However, the adoption of ML in
general-purpose, industry strength compilers has yet to happen. We propose
MLGO, a framework for integrating ML techniques systematically in an industrial
compiler -- LLVM. As a case study, we present the details and results of
replacing the heuristics-based inlining-for-size optimization in LLVM with
machine learned models. To the best of our knowledge, this work is the first
full integration of ML in a complex compiler pass in a real-world setting. It
is available in the main LLVM repository. We use two different ML algorithms:
Policy Gradient and Evolution Strategies, to train the inlining-for-size model,
and achieve up to 7\% size reduction, when compared to state of the art LLVM
-Oz. The same model, trained on one corpus, generalizes well to a diversity of
real-world targets, as well as to the same set of targets after months of
active development. This property of the trained models is beneficial to deploy
ML techniques in real-world settings.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.04285v1,2021-01-12T04:12:18Z,2021-01-12T04:12:18Z,"Explainable Deep Behavioral Sequence Clustering for Transaction Fraud
  Detection","In e-commerce industry, user behavior sequence data has been widely used in
many business units such as search and merchandising to improve their products.
However, it is rarely used in financial services not only due to its 3V
characteristics - i.e. Volume, Velocity and Variety - but also due to its
unstructured nature. In this paper, we propose a Financial Service scenario
Deep learning based Behavior data representation method for Clustering
(FinDeepBehaviorCluster) to detect fraudulent transactions. To utilize the
behavior sequence data, we treat click stream data as event sequence, use time
attention based Bi-LSTM to learn the sequence embedding in an unsupervised
fashion, and combine them with intuitive features generated by risk experts to
form a hybrid feature representation. We also propose a GPU powered HDBSCAN
(pHDBSCAN) algorithm, which is an engineering optimization for the original
HDBSCAN algorithm based on FAISS project, so that clustering can be carried out
on hundreds of millions of transactions within a few minutes. The computation
efficiency of the algorithm has increased 500 times compared with the original
implementation, which makes flash fraud pattern detection feasible. Our
experimental results show that the proposed FinDeepBehaviorCluster framework is
able to catch missed fraudulent transactions with considerable business values.
In addition, rule extraction method is applied to extract patterns from risky
clusters using intuitive features, so that narrative descriptions can be
attached to the risky clusters for case investigation, and unknown risk
patterns can be mined for real-time fraud detection. In summary,
FinDeepBehaviorCluster as a complementary risk management strategy to the
existing real-time fraud detection engine, can further increase our fraud
detection and proactive risk defense capabilities.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.04086v1,2021-01-11T18:29:50Z,2021-01-11T18:29:50Z,"System Design for a Data-driven and Explainable Customer Sentiment
  Monitor","The most important goal of customer services is to keep the customer
satisfied. However, service resources are always limited and must be
prioritized. Therefore, it is important to identify customers who potentially
become unsatisfied and might lead to escalations. Today this prioritization of
customers is often done manually. Data science on IoT data (esp. log data) for
machine health monitoring, as well as analytics on enterprise data for customer
relationship management (CRM) have mainly been researched and applied
independently. In this paper, we present a framework for a data-driven decision
support system which combines IoT and enterprise data to model customer
sentiment. Such decision support systems can help to prioritize customers and
service resources to effectively troubleshoot problems or even avoid them. The
framework is applied in a real-world case study with a major medical device
manufacturer. This includes a fully automated and interpretable machine
learning pipeline designed to meet the requirements defined with domain experts
and end users. The overall framework is currently deployed, learns and
evaluates predictive models from terabytes of IoT and enterprise data to
actively monitor the customer sentiment for a fleet of thousands of high-end
medical devices. Furthermore, we provide an anonymized industrial benchmark
dataset for the research community.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.03747v1,2021-01-11T08:14:35Z,2021-01-11T08:14:35Z,Cognitive Visual Inspection Service for LCD Manufacturing Industry,"With the rapid growth of display devices, quality inspection via machine
vision technology has become increasingly important for flat-panel displays
(FPD) industry. This paper discloses a novel visual inspection system for
liquid crystal display (LCD), which is currently a dominant type in the FPD
industry. The system is based on two cornerstones: robust/high-performance
defect recognition model and cognitive visual inspection service architecture.
A hybrid application of conventional computer vision technique and the latest
deep convolutional neural network (DCNN) leads to an integrated defect
detection, classfication and impact evaluation model that can be economically
trained with only image-level class annotations to achieve a high inspection
accuracy. In addition, the properly trained model is robust to the variation of
the image qulity, significantly alleviating the dependency between the model
prediction performance and the image aquisition environment. This in turn
justifies the decoupling of the defect recognition functions from the front-end
device to the back-end serivce, motivating the design and realization of the
cognitive visual inspection service architecture. Empirical case study is
performed on a large-scale real-world LCD dataset from a manufacturing line
with different layers and products, which shows the promising utility of our
system, which has been deployed in a real-world LCD manufacturing line from a
major player in the world.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.02644v2,2021-01-08T12:26:17Z,2021-01-07T17:32:56Z,Data Poisoning Attacks to Deep Learning Based Recommender Systems,"Recommender systems play a crucial role in helping users to find their
interested information in various web services such as Amazon, YouTube, and
Google News. Various recommender systems, ranging from neighborhood-based,
association-rule-based, matrix-factorization-based, to deep learning based,
have been developed and deployed in industry. Among them, deep learning based
recommender systems become increasingly popular due to their superior
performance.
  In this work, we conduct the first systematic study on data poisoning attacks
to deep learning based recommender systems. An attacker's goal is to manipulate
a recommender system such that the attacker-chosen target items are recommended
to many users. To achieve this goal, our attack injects fake users with
carefully crafted ratings to a recommender system. Specifically, we formulate
our attack as an optimization problem, such that the injected ratings would
maximize the number of normal users to whom the target items are recommended.
However, it is challenging to solve the optimization problem because it is a
non-convex integer programming problem. To address the challenge, we develop
multiple techniques to approximately solve the optimization problem. Our
experimental results on three real-world datasets, including small and large
datasets, show that our attack is effective and outperforms existing attacks.
Moreover, we attempt to detect fake users via statistical analysis of the
rating patterns of normal and fake users. Our results show that our attack is
still effective and outperforms existing attacks even if such a detector is
deployed.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.02518v4,2021-05-19T13:38:09Z,2021-01-07T12:35:16Z,Towards Automating Code Review Activities,"Code reviews are popular in both industrial and open source projects. The
benefits of code reviews are widely recognized and include better code quality
and lower likelihood of introducing bugs. However, since code review is a
manual activity it comes at the cost of spending developers' time on reviewing
their teammates' code.
  Our goal is to make the first step towards partially automating the code
review process, thus, possibly reducing the manual costs associated with it. We
focus on both the contributor and the reviewer sides of the process, by
training two different Deep Learning architectures. The first one learns code
changes performed by developers during real code review activities, thus
providing the contributor with a revised version of her code implementing code
transformations usually recommended during code review before the code is even
submitted for review. The second one automatically provides the reviewer
commenting on a submitted code with the revised code implementing her comments
expressed in natural language.
  The empirical evaluation of the two models shows that, on the contributor
side, the trained model succeeds in replicating the code transformations
applied during code reviews in up to 16% of cases. On the reviewer side, the
model can correctly implement a comment provided in natural language in up to
31% of cases. While these results are encouraging, more research is needed to
make these models usable by developers.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.00703v1,2021-01-03T20:56:56Z,2021-01-03T20:56:56Z,"Automatic Defect Detection of Print Fabric Using Convolutional Neural
  Network","Automatic defect detection is a challenging task because of the variability
in texture and type of fabric defects. An effective defect detection system
enables manufacturers to improve the quality of processes and products.
Automation across the textile manufacturing systems would reduce fabric wastage
and increase profitability by saving cost and resources. There are different
contemporary research on automatic defect detection systems using image
processing and machine learning techniques. These techniques differ from each
other based on the manufacturing processes and defect types. Researchers have
also been able to establish real-time defect detection system during weaving.
Although, there has been research on patterned fabric defect detection, these
defects are related to weaving faults such as holes, and warp and weft defects.
But, there has not been any research that is designed to detect defects that
arise during such as spot and print mismatch. This research has fulfilled this
gap by developing a print fabric database and implementing deep convolutional
neural network (CNN).",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.00135v3,2021-11-28T14:06:46Z,2021-01-01T01:49:03Z,"Faults in Deep Reinforcement Learning Programs: A Taxonomy and A
  Detection Approach","A growing demand is witnessed in both industry and academia for employing
Deep Learning (DL) in various domains to solve real-world problems. Deep
Reinforcement Learning (DRL) is the application of DL in the domain of
Reinforcement Learning (RL). Like any software systems, DRL applications can
fail because of faults in their programs. In this paper, we present the first
attempt to categorize faults occurring in DRL programs. We manually analyzed
761 artifacts of DRL programs (from Stack Overflow posts and GitHub issues)
developed using well-known DRL frameworks (OpenAI Gym, Dopamine, Keras-rl,
Tensorforce) and identified faults reported by developers/users. We labeled and
taxonomized the identified faults through several rounds of discussions. The
resulting taxonomy is validated using an online survey with 19
developers/researchers. To allow for the automatic detection of faults in DRL
programs, we have defined a meta-model of DRL programs and developed DRLinter,
a model-based fault detection approach that leverages static analysis and graph
transformations. The execution flow of DRLinter consists in parsing a DRL
program to generate a model conforming to our meta-model and applying detection
rules on the model to identify faults occurrences. The effectiveness of
DRLinter is evaluated using 15 synthetic DRLprograms in which we injected
faults observed in the analyzed artifacts of the taxonomy. The results show
that DRLinter can successfully detect faults in all synthetic faulty programs.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.03889v2,2021-02-16T13:44:56Z,2020-12-21T10:25:27Z,A Comprehensive Survey of 6G Wireless Communications,"While fifth-generation (5G) communications are being rolled out worldwide,
sixth-generation (6G) communications have attracted much attention from both
the industry and the academia. Compared with 5G, 6G will have a wider frequency
band, higher transmission rate, spectrum efficiency, greater connection
capacity, shorter delay, broader coverage, and more robust anti-interference
capability to satisfy various network requirements. This survey presents an
insightful understanding of 6G wireless communications by introducing
requirements, features, critical technologies, challenges, and applications.
First, we give an overview of 6G from perspectives of technologies, security
and privacy, and applications. Subsequently, we introduce various 6G
technologies and their existing challenges in detail, e.g., artificial
intelligence (AI), intelligent surfaces, THz, space-air-ground-sea integrated
network, cell-free massive MIMO, etc. Because of these technologies, 6G is
expected to outperform existing wireless communication systems regarding the
transmission rate, latency, global coverage, etc. Next, we discuss security and
privacy techniques that can be applied to protect data in 6G. Since edge
devices are expected to gain popularity soon, the vast amount of generated data
and frequent data exchange make the leakage of data easily. Finally, we predict
real-world applications built on the technologies and features of 6G; for
example, smart healthcare, smart city, and smart manufacturing will be
implemented by taking advantage of AI.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.10610v3,2021-02-16T17:31:15Z,2020-12-19T07:00:09Z,"SpaceML: Distributed Open-source Research with Citizen Scientists for
  the Advancement of Space Technology for NASA","Traditionally, academic labs conduct open-ended research with the primary
focus on discoveries with long-term value, rather than direct products that can
be deployed in the real world. On the other hand, research in the industry is
driven by its expected commercial return on investment, and hence focuses on a
real world product with short-term timelines. In both cases, opportunity is
selective, often available to researchers with advanced educational
backgrounds. Research often happens behind closed doors and may be kept
confidential until either its publication or product release, exacerbating the
problem of AI reproducibility and slowing down future research by others in the
field. As many research organizations tend to exclusively focus on specific
areas, opportunities for interdisciplinary research reduce. Undertaking
long-term bold research in unexplored fields with non-commercial yet great
public value is hard due to factors including the high upfront risk, budgetary
constraints, and a lack of availability of data and experts in niche fields.
Only a few companies or well-funded research labs can afford to do such
long-term research. With research organizations focused on an exploding array
of fields and resources spread thin, opportunities for the maturation of
interdisciplinary research reduce. Apart from these exigencies, there is also a
need to engage citizen scientists through open-source contributors to play an
active part in the research dialogue. We present a short case study of SpaceML,
an extension of the Frontier Development Lab, an AI accelerator for NASA.
SpaceML distributes open-source research and invites volunteer citizen
scientists to partake in development and deployment of high social value
products at the intersection of space and AI.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.08174v2,2021-03-29T17:15:00Z,2020-12-15T09:49:22Z,"Towards open and expandable cognitive AI architectures for large-scale
  multi-agent human-robot collaborative learning","Learning from Demonstration (LfD) constitutes one of the most robust
methodologies for constructing efficient cognitive robotic systems. Despite the
large body of research works already reported, current key technological
challenges include those of multi-agent learning and long-term autonomy.
Towards this direction, a novel cognitive architecture for multi-agent LfD
robotic learning is introduced, targeting to enable the reliable deployment of
open, scalable and expandable robotic systems in large-scale and complex
environments. In particular, the designed architecture capitalizes on the
recent advances in the Artificial Intelligence (AI) field, by establishing a
Federated Learning (FL)-based framework for incarnating a multi-human
multi-robot collaborative learning environment. The fundamental
conceptualization relies on employing multiple AI-empowered cognitive processes
(implementing various robotic tasks) that operate at the edge nodes of a
network of robotic platforms, while global AI models (underpinning the
aforementioned robotic tasks) are collectively created and shared among the
network, by elegantly combining information from a large number of human-robot
interaction instances. Regarding pivotal novelties, the designed cognitive
architecture a) introduces a new FL-based formalism that extends the
conventional LfD learning paradigm to support large-scale multi-agent
operational settings, b) elaborates previous FL-based self-learning robotic
schemes so as to incorporate the human in the learning loop and c) consolidates
the fundamental principles of FL with additional sophisticated AI-enabled
learning methodologies for modelling the multi-level inter-dependencies among
the robotic tasks. The applicability of the proposed framework is explained
using an example of a real-world industrial case study for agile
production-based Critical Raw Materials (CRM) recovery.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.07938v1,2020-12-14T20:55:48Z,2020-12-14T20:55:48Z,NVIDIA SimNet^{TM}: an AI-accelerated multi-physics simulation framework,"We present SimNet, an AI-driven multi-physics simulation framework, to
accelerate simulations across a wide range of disciplines in science and
engineering. Compared to traditional numerical solvers, SimNet addresses a wide
range of use cases - coupled forward simulations without any training data,
inverse and data assimilation problems. SimNet offers fast turnaround time by
enabling parameterized system representation that solves for multiple
configurations simultaneously, as opposed to the traditional solvers that solve
for one configuration at a time. SimNet is integrated with parameterized
constructive solid geometry as well as STL modules to generate point clouds.
Furthermore, it is customizable with APIs that enable user extensions to
geometry, physics and network architecture. It has advanced network
architectures that are optimized for high-performance GPU computing, and offers
scalable performance for multi-GPU and multi-Node implementation with
accelerated linear algebra as well as FP32, FP64 and TF32 computations. In this
paper we review the neural network solver methodology, the SimNet architecture,
and the various features that are needed for effective solution of the PDEs. We
present real-world use cases that range from challenging forward multi-physics
simulations with turbulence and complex 3D geometries, to industrial design
optimization and inverse problems that are not addressed efficiently by the
traditional solvers. Extensive comparisons of SimNet results with open source
and commercial solvers show good correlation.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.05410v1,2020-12-10T02:08:47Z,2020-12-10T02:08:47Z,Artificial Intelligence at the Edge,"The Internet of Things (IoT) and edge computing applications aim to support a
variety of societal needs, including the global pandemic situation that the
entire world is currently experiencing and responses to natural disasters.
  The need for real-time interactive applications such as immersive video
conferencing, augmented/virtual reality, and autonomous vehicles, in education,
healthcare, disaster recovery and other domains, has never been higher. At the
same time, there have been recent technological breakthroughs in highly
relevant fields such as artificial intelligence (AI)/machine learning (ML),
advanced communication systems (5G and beyond), privacy-preserving
computations, and hardware accelerators. 5G mobile communication networks
increase communication capacity, reduce transmission latency and error, and
save energy -- capabilities that are essential for new applications. The
envisioned future 6G technology will integrate many more technologies,
including for example visible light communication, to support groundbreaking
applications, such as holographic communications and high precision
manufacturing. Many of these applications require computations and analytics
close to application end-points: that is, at the edge of the network, rather
than in a centralized cloud. AI techniques applied at the edge have tremendous
potential both to power new applications and to need more efficient operation
of edge infrastructure. However, it is critical to understand where to deploy
AI systems within complex ecosystems consisting of advanced applications and
the specific real-time requirements towards AI systems.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.04861v2,2021-02-23T03:41:10Z,2020-12-09T04:47:07Z,"Multi Agent Team Learning in Disaggregated Virtualized Open Radio Access
  Networks (O-RAN)","Starting from the Cloud Radio Access Network (C-RAN), continuing with the
virtual Radio Access Network (vRAN) and most recently with Open RAN (O-RAN)
initiative, Radio Access Network (RAN) architectures have significantly evolved
in the past decade. In the last few years, the wireless industry has witnessed
a strong trend towards disaggregated, virtualized and open RANs, with numerous
tests and deployments world wide. One unique aspect that motivates this paper
is the availability of new opportunities that arise from using machine learning
to optimize the RAN in closed-loop, i.e. without human intervention, where the
complexity of disaggregation and virtualization makes well-known Self-Organized
Networking (SON) solutions inadequate. In our view, Multi-Agent Systems (MASs)
with team learning, can play an essential role in the control and coordination
of controllers of O-RAN, i.e. near-real-time and non-real-time RAN Intelligent
Controller (RIC). In this article, we first present the state-of-the-art
research in multi-agent systems and team learning, then we provide an overview
of the landscape in RAN disaggregation and virtualization, as well as O-RAN
which emphasizes the open interfaces introduced by the O-RAN Alliance. We
present a case study for agent placement and the AI feedback required in O-RAN,
and finally, we identify challenges and open issues to provide a roadmap for
researchers.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.03631v1,2020-12-07T12:24:32Z,2020-12-07T12:24:32Z,"Exploitation of Channel-Learning for Enhancing 5G Blind Beam Index
  Detection","Proliferation of 5G devices and services has driven the demand for wide-scale
enhancements ranging from data rate, reliability, and compatibility to sustain
the ever increasing growth of the telecommunication industry. In this regard,
this work investigates how machine learning technology can improve the
performance of 5G cell and beam index search in practice. The cell search is an
essential function for a User Equipment (UE) to be initially associated with a
base station, and is also important to further maintain the wireless
connection. Unlike the former generation cellular systems, the 5G UE faces with
an additional challenge to detect suitable beams as well as the cell identities
in the cell search procedures. Herein, we propose and implement new
channel-learning schemes to enhance the performance of 5G beam index detection.
The salient point lies in the use of machine learning models and softwarization
for practical implementations in a system level. We develop the proposed
channel-learning scheme including algorithmic procedures and corroborative
system structure for efficient beam index detection. We also implement a
real-time operating 5G testbed based on the off-the-shelf Software Defined
Radio (SDR) platform and conduct intensive experiments with commercial 5G base
stations. The experimental results indicate that the proposed channel-learning
schemes outperform the conventional correlation-based scheme in real 5G channel
environments.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.01913v1,2020-12-03T13:51:05Z,2020-12-03T13:51:05Z,Transfer Learning as an Enabler of the Intelligent Digital Twin,"Digital Twins have been described as beneficial in many areas, such as
virtual commissioning, fault prediction or reconfiguration planning. Equipping
Digital Twins with artificial intelligence functionalities can greatly expand
those beneficial applications or open up altogether new areas of application,
among them cross-phase industrial transfer learning. In the context of machine
learning, transfer learning represents a set of approaches that enhance
learning new tasks based upon previously acquired knowledge. Here, knowledge is
transferred from one lifecycle phase to another in order to reduce the amount
of data or time needed to train a machine learning algorithm. Looking at common
challenges in developing and deploying industrial machinery with deep learning
functionalities, embracing this concept would offer several advantages: Using
an intelligent Digital Twin, learning algorithms can be designed, configured
and tested in the design phase before the physical system exists and real data
can be collected. Once real data becomes available, the algorithms must merely
be fine-tuned, significantly speeding up commissioning and reducing the
probability of costly modifications. Furthermore, using the Digital Twin's
simulation capabilities virtually injecting rare faults in order to train an
algorithm's response or using reinforcement learning, e.g. to teach a robot,
become practically feasible. This article presents several cross-phase
industrial transfer learning use cases utilizing intelligent Digital Twins. A
real cyber physical production system consisting of an automated welding
machine and an automated guided vehicle equipped with a robot arm is used to
illustrate the respective benefits.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.01153v1,2020-12-02T12:53:19Z,2020-12-02T12:53:19Z,Towards Intelligent Reconfigurable Wireless Physical Layer (PHY),"Next-generation wireless networks are getting significant attention because
they promise 10-factor enhancement in mobile broadband along with the potential
to enable new heterogeneous services. Services include massive machine type
communications desired for Industrial 4.0 along with ultra-reliable low latency
services for remote healthcare and vehicular communications. In this paper, we
present the design of an intelligent and reconfigurable physical layer (PHY) to
bring these services to reality. First, we design and implement the
reconfigurable PHY via a hardware-software co-design approach on system-on-chip
consisting of the ARM processor and field-programmable gate array (FPGA). The
reconfigurable PHY is then made intelligent by augmenting it with online
machine learning (OML) based decision-making algorithm. Such PHY can learn the
environment (for example, wireless channel) and dynamically adapt the
transceivers' configuration (i.e., modulation scheme, word-length) and select
the wireless channel on-the-fly. Since the environment is unknown and changes
with time, we make the OML architecture reconfigurable to enable dynamic switch
between various OML algorithms on-the-fly. We have demonstrated the functional
correctness of the proposed architecture for different environments and
word-lengths. The detailed throughput, latency, and complexity analysis
validate the feasibility and importance of the proposed intelligent and
reconfigurable PHY in next-generation networks.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.14925v1,2020-11-26T02:37:39Z,2020-11-26T02:37:39Z,"Autonomous Graph Mining Algorithm Search with Best Speed/Accuracy
  Trade-off","Graph data is ubiquitous in academia and industry, from social networks to
bioinformatics. The pervasiveness of graphs today has raised the demand for
algorithms that can answer various questions: Which products would a user like
to purchase given her order list? Which users are buying fake followers to
increase their public reputation? Myriads of new graph mining algorithms are
proposed every year to answer such questions - each with a distinct problem
formulation, computational time, and memory footprint. This lack of unity makes
it difficult for a practitioner to compare different algorithms and pick the
most suitable one for a specific application. These challenges - even more
severe for non-experts - create a gap in which state-of-the-art techniques
developed in academic settings fail to be optimally deployed in real-world
applications. To bridge this gap, we propose AUTOGM, an automated system for
graph mining algorithm development. We first define a unified framework
UNIFIEDGM that integrates various message-passing based graph algorithms,
ranging from conventional algorithms like PageRank to graph neural networks.
Then UNIFIEDGM defines a search space in which five parameters are required to
determine a graph algorithm. Under this search space, AUTOGM explicitly
optimizes for the optimal parameter set of UNIFIEDGM using Bayesian
Optimization. AUTOGM defines a novel budget-aware objective function for the
optimization to incorporate a practical issue - finding the best speed-accuracy
trade-off under a computation budget - into the graph algorithm generation
problem. Experiments on real-world benchmark datasets demonstrate that AUTOGM
generates novel graph mining algorithms with the best speed/accuracy trade-off
compared to existing models with heuristic parameters.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.02298v2,2021-06-15T06:28:13Z,2020-11-25T17:23:52Z,"Exploration in Online Advertising Systems with Deep Uncertainty-Aware
  Learning","Modern online advertising systems inevitably rely on personalization methods,
such as click-through rate (CTR) prediction. Recent progress in CTR prediction
enjoys the rich representation capabilities of deep learning and achieves great
success in large-scale industrial applications. However, these methods can
suffer from lack of exploration. Another line of prior work addresses the
exploration-exploitation trade-off problem with contextual bandit methods,
which are recently less studied in the industry due to the difficulty in
extending their flexibility with deep models. In this paper, we propose a novel
Deep Uncertainty-Aware Learning (DUAL) method to learn CTR models based on
Gaussian processes, which can provide predictive uncertainty estimations while
maintaining the flexibility of deep neural networks. DUAL can be easily
implemented on existing models and deployed in real-time systems with minimal
extra computational overhead. By linking the predictive uncertainty estimation
ability of DUAL to well-known bandit algorithms, we further present DUAL-based
Ad-ranking strategies to boost up long-term utilities such as the social
welfare in advertising systems. Experimental results on several public datasets
demonstrate the effectiveness of our methods. Remarkably, an online A/B test
deployed in the Alibaba display advertising platform shows an 8.2% social
welfare improvement and an 8.0% revenue lift.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.09747v2,2021-09-29T15:26:04Z,2020-11-19T09:53:27Z,"Energy Aware Deep Reinforcement Learning Scheduling for Sensors
  Correlated in Time and Space","Millions of battery-powered sensors deployed for monitoring purposes in a
multitude of scenarios, e.g., agriculture, smart cities, industry, etc.,
require energy-efficient solutions to prolong their lifetime. When these
sensors observe a phenomenon distributed in space and evolving in time, it is
expected that collected observations will be correlated in time and space. In
this paper, we propose a Deep Reinforcement Learning (DRL) based scheduling
mechanism capable of taking advantage of correlated information. We design our
solution using the Deep Deterministic Policy Gradient (DDPG) algorithm. The
proposed mechanism is capable of determining the frequency with which sensors
should transmit their updates, to ensure accurate collection of observations,
while simultaneously considering the energy available. To evaluate our
scheduling mechanism, we use multiple datasets containing environmental
observations obtained in multiple real deployments. The real observations
enable us to model the environment with which the mechanism interacts as
realistically as possible. We show that our solution can significantly extend
the sensors' lifetime. We compare our mechanism to an idealized, all-knowing
scheduler to demonstrate that its performance is near-optimal. Additionally, we
highlight the unique feature of our design, energy-awareness, by displaying the
impact of sensors' energy levels on the frequency of updates.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.09463v3,2021-08-20T07:24:05Z,2020-11-18T18:41:27Z,"EasyTransfer -- A Simple and Scalable Deep Transfer Learning Platform
  for NLP Applications","The literature has witnessed the success of leveraging Pre-trained Language
Models (PLMs) and Transfer Learning (TL) algorithms to a wide range of Natural
Language Processing (NLP) applications, yet it is not easy to build an
easy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the
EasyTransfer platform is designed to develop deep TL algorithms for NLP
applications. EasyTransfer is backended with a high-performance and scalable
engine for efficient training and inference, and also integrates comprehensive
deep TL algorithms, to make the development of industrial-scale TL applications
easier. In EasyTransfer, the built-in data and model parallelism strategies,
combined with AI compiler optimization, show to be 4.0x faster than the
community version of distributed training. EasyTransfer supports various NLP
models in the ModelZoo, including mainstream PLMs and multi-modality models. It
also features various in-house developed TL algorithms, together with the
AppZoo for NLP applications. The toolkit is convenient for users to quickly
start model training, evaluation, and online deployment. EasyTransfer is
currently deployed at Alibaba to support a variety of business scenarios,
including item recommendation, personalized search, conversational question
answering, etc. Extensive experiments on real-world datasets and online
applications show that EasyTransfer is suitable for online production with
cutting-edge performance for various applications. The source code of
EasyTransfer is released at Github (https://github.com/alibaba/EasyTransfer).",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.09926v3,2022-05-19T09:51:14Z,2020-11-18T16:20:28Z,Challenges in Deploying Machine Learning: a Survey of Case Studies,"In recent years, machine learning has transitioned from a field of academic
research interest to a field capable of solving real-world business problems.
However, the deployment of machine learning models in production systems can
present a number of issues and concerns. This survey reviews published reports
of deploying machine learning solutions in a variety of use cases, industries
and applications and extracts practical considerations corresponding to stages
of the machine learning deployment workflow. By mapping found challenges to the
steps of the machine learning deployment workflow we show that practitioners
face issues at each stage of the deployment process. The goal of this paper is
to lay out a research agenda to explore approaches addressing these challenges.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.08512v1,2020-11-17T08:55:14Z,2020-11-17T08:55:14Z,"Preventing Repeated Real World AI Failures by Cataloging Incidents: The
  AI Incident Database","Mature industrial sectors (e.g., aviation) collect their real world failures
in incident databases to inform safety improvements. Intelligent systems
currently cause real world harms without a collective memory of their failings.
As a result, companies repeatedly make the same mistakes in the design,
development, and deployment of intelligent systems. A collection of intelligent
system failures experienced in the real world (i.e., incidents) is needed to
ensure intelligent systems benefit people and society. The AI Incident Database
is an incident collection initiated by an industrial/non-profit cooperative to
enable AI incident avoidance and mitigation. The database supports a variety of
research and development use cases with faceted and full text search on more
than 1,000 incident reports archived to date.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.09902v1,2020-11-17T04:11:31Z,2020-11-17T04:11:31Z,"Low-latency Federated Learning and Blockchain for Edge Association in
  Digital Twin empowered 6G Networks","Emerging technologies such as digital twins and 6th Generation mobile
networks (6G) have accelerated the realization of edge intelligence in
Industrial Internet of Things (IIoT). The integration of digital twin and 6G
bridges the physical system with digital space and enables robust instant
wireless connectivity. With increasing concerns on data privacy, federated
learning has been regarded as a promising solution for deploying distributed
data processing and learning in wireless networks. However, unreliable
communication channels, limited resources, and lack of trust among users,
hinder the effective application of federated learning in IIoT. In this paper,
we introduce the Digital Twin Wireless Networks (DTWN) by incorporating digital
twins into wireless networks, to migrate real-time data processing and
computation to the edge plane. Then, we propose a blockchain empowered
federated learning framework running in the DTWN for collaborative computing,
which improves the reliability and security of the system, and enhances data
privacy. Moreover, to balance the learning accuracy and time cost of the
proposed scheme, we formulate an optimization problem for edge association by
jointly considering digital twin association, training data batch size, and
bandwidth allocation. We exploit multi-agent reinforcement learning to find an
optimal solution to the problem. Numerical results on real-world dataset show
that the proposed scheme yields improved efficiency and reduced cost compared
to benchmark learning method.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.07313v1,2020-11-14T14:56:26Z,2020-11-14T14:56:26Z,"Classification of Reverse-Engineered Class Diagram and
  Forward-Engineered Class Diagram using Machine Learning","UML Class diagram is very important to visualize the whole software we are
working on and helps understand the whole system in the easiest way possible by
showing the system classes, its attributes, methods, and relations with other
objects. In the real world, there are two types of Class diagram engineers work
with namely 1) Forward Engineered Class Diagram (FwCD) which are hand-made as
part of the forward-looking development process, and 2). Reverse Engineered
Class Diagram (RECD) which are those diagrams that are reverse engineered from
the source code. In the software industry while working with new open software
projects it is important to know which type of class diagram it is. Which UML
diagram was used in a particular project is an important factor to be known? To
solve this problem, we propose to build a classifier that can classify a UML
diagram into FwCD or RECD. We propose to solve this problem by using a
supervised Machine Learning technique. The approach in this involves analyzing
the features that are useful in classifying class diagrams. Different Machine
Learning models are used in this process and the Random Forest algorithm has
proved to be the best out of all. Performance testing was done on 999 Class
diagrams.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.02738v1,2020-11-05T10:16:54Z,2020-11-05T10:16:54Z,"Switching Scheme: A Novel Approach for Handling Incremental Concept
  Drift in Real-World Data Sets","Machine learning models nowadays play a crucial role for many applications in
business and industry. However, models only start adding value as soon as they
are deployed into production. One challenge of deployed models is the effect of
changing data over time, which is often described with the term concept drift.
Due to their nature, concept drifts can severely affect the prediction
performance of a machine learning system. In this work, we analyze the effects
of concept drift in the context of a real-world data set. For efficient concept
drift handling, we introduce the switching scheme which combines the two
principles of retraining and updating of a machine learning model. Furthermore,
we systematically analyze existing regular adaptation as well as triggered
adaptation strategies. The switching scheme is instantiated on New York City
taxi data, which is heavily influenced by changing demand patterns over time.
We can show that the switching scheme outperforms all other baselines and
delivers promising prediction results.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.09610v1,2020-10-30T20:33:05Z,2020-10-30T20:33:05Z,Validate and Enable Machine Learning in Industrial AI,"Industrial Artificial Intelligence (Industrial AI) is an emerging concept
which refers to the application of artificial intelligence to industry.
Industrial AI promises more efficient future industrial control systems.
However, manufacturers and solution partners need to understand how to
implement and integrate an AI model into the existing industrial control
system. A well-trained machine learning (ML) model provides many benefits and
opportunities for industrial control optimization; however, an inferior
Industrial AI design and integration limits the capability of ML models. To
better understand how to develop and integrate trained ML models into the
traditional industrial control system, test the deployed AI control system, and
ultimately outperform traditional systems, manufacturers and their AI solution
partners need to address a number of challenges. Six top challenges, which were
real problems we ran into when deploying Industrial AI, are explored in the
paper. The Petuum Optimum system is used as an example to showcase the
challenges in making and testing AI models, and more importantly, how to
address such challenges in an Industrial AI system.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.12837v4,2022-03-31T02:46:32Z,2020-10-24T08:37:04Z,"XDM: Improving Sequential Deep Matching with Unclicked User Behaviors
  for Recommender System","Deep learning-based sequential recommender systems have recently attracted
increasing attention from both academia and industry. Most of industrial
Embedding-Based Retrieval (EBR) system for recommendation share the similar
ideas with sequential recommenders. Among them, how to comprehensively capture
sequential user interest is a fundamental problem. However, most existing
sequential recommendation models take as input clicked or purchased behavior
sequences from user-item interactions. This leads to incomprehensive user
representation and sub-optimal model performance, since they ignore the
complete user behavior exposure data, i.e., items impressed yet unclicked by
users. In this work, we attempt to incorporate and model those unclicked item
sequences using a new learning approach in order to explore better sequential
recommendation technique. An efficient triplet metric learning algorithm is
proposed to appropriately learn the representation of unclicked items. Our
method can be simply integrated with existing sequential recommendation models
by a confidence fusion network and further gain better user representation. The
offline experimental results based on real-world E-commerce data demonstrate
the effectiveness and verify the importance of unclicked items in sequential
recommendation. Moreover we deploy our new model (named XDM) into EBR of
recommender system at Taobao, outperforming the deployed previous generation
SDM.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.09254v1,2020-10-19T06:48:40Z,2020-10-19T06:48:40Z,Query-aware Tip Generation for Vertical Search,"As a concise form of user reviews, tips have unique advantages to explain the
search results, assist users' decision making, and further improve user
experience in vertical search scenarios. Existing work on tip generation does
not take query into consideration, which limits the impact of tips in search
scenarios. To address this issue, this paper proposes a query-aware tip
generation framework, integrating query information into encoding and
subsequent decoding processes. Two specific adaptations of Transformer and
Recurrent Neural Network (RNN) are proposed. For Transformer, the query impact
is incorporated into the self-attention computation of both the encoder and the
decoder. As for RNN, the query-aware encoder adopts a selective network to
distill query-relevant information from the review, while the query-aware
decoder integrates the query information into the attention computation during
decoding. The framework consistently outperforms the competing methods on both
public and real-world industrial datasets. Last but not least, online
deployment experiments on Dianping demonstrate the advantage of the proposed
framework for tip generation as well as its online business values.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.02716v2,2021-06-02T12:55:40Z,2020-10-03T19:25:01Z,AI Lifecycle Models Need To Be Revised. An Exploratory Study in Fintech,"Tech-leading organizations are embracing the forthcoming artificial
intelligence revolution. Intelligent systems are replacing and cooperating with
traditional software components. Thus, the same development processes and
standards in software engineering ought to be complied in artificial
intelligence systems. This study aims to understand the processes by which
artificial intelligence-based systems are developed and how state-of-the-art
lifecycle models fit the current needs of the industry. We conducted an
exploratory case study at ING, a global bank with a strong European base. We
interviewed 17 people with different roles and from different departments
within the organization. We have found that the following stages have been
overlooked by previous lifecycle models: data collection, feasibility study,
documentation, model monitoring, and model risk assessment. Our work shows that
the real challenges of applying Machine Learning go much beyond sophisticated
learning algorithms - more focus is needed on the entire lifecycle. In
particular, regardless of the existing development tools for Machine Learning,
we observe that they are still not meeting the particularities of this field.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2009.13437v2,2021-08-17T10:53:45Z,2020-09-28T16:04:07Z,"A Human-in-the-Loop Approach based on Explainability to Improve NTL
  Detection","Implementing systems based on Machine Learning to detect fraud and other
Non-Technical Losses (NTL) is challenging: the data available is biased, and
the algorithms currently used are black-boxes that cannot be either easily
trusted or understood by stakeholders. This work explains our human-in-the-loop
approach to mitigate these problems in a real system that uses a supervised
model to detect Non-Technical Losses (NTL) for an international utility company
from Spain. This approach exploits human knowledge (e.g. from the data
scientists or the company's stakeholders) and the information provided by
explanatory methods to guide the system during the training process. This
simple, efficient method that can be easily implemented in other industrial
projects is tested in a real dataset and the results show that the derived
prediction model is better in terms of accuracy, interpretability, robustness
and flexibility.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2009.09926v1,2020-09-17T02:36:52Z,2020-09-17T02:36:52Z,"Cross-Modal Alignment with Mixture Experts Neural Network for
  Intral-City Retail Recommendation","In this paper, we introduce Cross-modal Alignment with mixture experts Neural
Network (CameNN) recommendation model for intral-city retail industry, which
aims to provide fresh foods and groceries retailing within 5 hours delivery
service arising for the outbreak of Coronavirus disease (COVID-19) pandemic
around the world. We propose CameNN, which is a multi-task model with three
tasks including Image to Text Alignment (ITA) task, Text to Image Alignment
(TIA) task and CVR prediction task. We use pre-trained BERT to generate the
text embedding and pre-trained InceptionV4 to generate image patch embedding
(each image is split into small patches with the same pixels and treat each
patch as an image token). Softmax gating networks follow to learn the weight of
each transformer expert output and choose only a subset of experts conditioned
on the input. Then transformer encoder is applied as the share-bottom layer to
learn all input features' shared interaction. Next, mixture of transformer
experts (MoE) layer is implemented to model different aspects of tasks. At top
of the MoE layer, we deploy a transformer layer for each task as task tower to
learn task-specific information. On the real word intra-city dataset,
experiments demonstrate CameNN outperform baselines and achieve significant
improvements on the image and text representation. In practice, we applied
CameNN on CVR prediction in our intra-city recommender system which is one of
the leading intra-city platforms operated in China.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2009.00351v1,2020-09-01T11:10:13Z,2020-09-01T11:10:13Z,"Advancing from Predictive Maintenance to Intelligent Maintenance with AI
  and IIoT","As Artificial Intelligent (AI) technology advances and increasingly large
amounts of data become readily available via various Industrial Internet of
Things (IIoT) projects, we evaluate the state of the art of predictive
maintenance approaches and propose our innovative framework to improve the
current practice. The paper first reviews the evolution of reliability
modelling technology in the past 90 years and discusses major technologies
developed in industry and academia. We then introduce the next generation
maintenance framework - Intelligent Maintenance, and discuss its key
components. This AI and IIoT based Intelligent Maintenance framework is
composed of (1) latest machine learning algorithms including probabilistic
reliability modelling with deep learning, (2) real-time data collection,
transfer, and storage through wireless smart sensors, (3) Big Data
technologies, (4) continuously integration and deployment of machine learning
models, (5) mobile device and AR/VR applications for fast and better
decision-making in the field. Particularly, we proposed a novel probabilistic
deep learning reliability modelling approach and demonstrate it in the Turbofan
Engine Degradation Dataset.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.13223v2,2020-10-25T17:47:42Z,2020-08-30T17:29:43Z,"Deep Reinforcement Learning for Contact-Rich Skills Using Compliant
  Movement Primitives","In recent years, industrial robots have been installed in various industries
to handle advanced manufacturing and high precision tasks. However, further
integration of industrial robots is hampered by their limited flexibility,
adaptability and decision making skills compared to human operators. Assembly
tasks are especially challenging for robots since they are contact-rich and
sensitive to even small uncertainties. While reinforcement learning (RL) offers
a promising framework to learn contact-rich control policies from scratch, its
applicability to high-dimensional continuous state-action spaces remains rather
limited due to high brittleness and sample complexity. To address those issues,
we propose different pruning methods that facilitate convergence and
generalization. In particular, we divide the task into free and contact-rich
sub-tasks, perform the control in Cartesian rather than joint space, and
parameterize the control policy. Those pruning methods are naturally
implemented within the framework of dynamic movement primitives (DMP). To
handle contact-rich tasks, we extend the DMP framework by introducing a
coupling term that acts like the human wrist and provides active compliance
under contact with the environment. We demonstrate that the proposed method can
learn insertion skills that are invariant to space, size, shape, and closely
related scenarios, while handling large uncertainties. Finally we demonstrate
that the learned policy can be easily transferred from simulations to real
world and achieve similar performance on UR5e robot.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.11856v1,2020-08-26T23:24:34Z,2020-08-26T23:24:34Z,Hybrid Deep Neural Networks to Infer State Models of Black-Box Systems,"Inferring behavior model of a running software system is quite useful for
several automated software engineering tasks, such as program comprehension,
anomaly detection, and testing. Most existing dynamic model inference
techniques are white-box, i.e., they require source code to be instrumented to
get run-time traces. However, in many systems, instrumenting the entire source
code is not possible (e.g., when using black-box third-party libraries) or
might be very costly. Unfortunately, most black-box techniques that detect
states over time are either univariate, or make assumptions on the data
distribution, or have limited power for learning over a long period of past
behavior. To overcome the above issues, in this paper, we propose a hybrid deep
neural network that accepts as input a set of time series, one per input/output
signal of the system, and applies a set of convolutional and recurrent layers
to learn the non-linear correlations between signals and the patterns, over
time. We have applied our approach on a real UAV auto-pilot solution from our
industry partner with half a million lines of C code. We ran 888 random recent
system-level test cases and inferred states, over time. Our comparison with
several traditional time series change point detection techniques showed that
our approach improves their performance by up to 102%, in terms of finding
state change points, measured by F1 score. We also showed that our state
classification algorithm provides on average 90.45% F1 score, which improves
traditional classification algorithms by up to 17%.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.13585v1,2020-08-26T14:03:25Z,2020-08-26T14:03:25Z,At Your Service: Coffee Beans Recommendation From a Robot Assistant,"With advances in the field of machine learning, precisely algorithms for
recommendation systems, robot assistants are envisioned to become more present
in the hospitality industry. Additionally, the COVID-19 pandemic has also
highlighted the need to have more service robots in our everyday lives, to
minimise the risk of human to-human transmission. One such example would be
coffee shops, which have become intrinsic to our everyday lives. However,
serving an excellent cup of coffee is not a trivial feat as a coffee blend
typically comprises rich aromas, indulgent and unique flavours and a lingering
aftertaste. Our work addresses this by proposing a computational model which
recommends optimal coffee beans resulting from the user's preferences.
Specifically, given a set of coffee bean properties (objective features), we
apply different supervised learning techniques to predict coffee qualities
(subjective features). We then consider an unsupervised learning method to
analyse the relationship between coffee beans in the subjective feature space.
Evaluated on a real coffee beans dataset based on digitised reviews, our
results illustrate that the proposed computational model gives up to 92.7
percent recommendation accuracy for coffee beans prediction. From this, we
propose how this computational model can be deployed on a service robot to
reliably predict customers' coffee bean preferences, starting from the user
inputting their coffee preferences to the robot recommending the coffee beans
that best meet the user's likings.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.11070v1,2020-08-25T14:43:30Z,2020-08-25T14:43:30Z,An Economic Perspective on Predictive Maintenance of Filtration Units,"This paper provides an economic perspective on the predictive maintenance of
filtration units. The rise of predictive maintenance is possible due to the
growing trend of industry 4.0 and the availability of inexpensive sensors.
However, the adoption rate for predictive maintenance by companies remains low.
The majority of companies are sticking to corrective and preventive
maintenance. This is not due to a lack of information on the technical
implementation of predictive maintenance, with an abundance of research papers
on state-of-the-art machine learning algorithms that can be used effectively.
The main issue is that most upper management has not yet been fully convinced
of the idea of predictive maintenance. The economic value of the implementation
has to be linked to the predictive maintenance program for better justification
by the management. In this study, three machine learning models were trained to
demonstrate the economic value of predictive maintenance. Data was collected
from a testbed located at the Singapore University of Technology and Design.
The testbed closely resembles a real-world water treatment plant. A
cost-benefit analysis coupled with Monte Carlo simulation was proposed. It
provided a structured approach to document potential costs and savings by
implementing a predictive maintenance program. The simulation incorporated
real-world risk into a financial model. Financial figures were adapted from
CITIC Envirotech Ltd, a leading membrane-based integrated environmental
solutions provider. Two scenarios were used to elaborate on the economic values
of predictive maintenance. Overall, this study seeks to bridge the gap between
technical and business domains of predictive maintenance.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.08525v1,2020-08-19T16:05:58Z,2020-08-19T16:05:58Z,"""Name that manufacturer"". Relating image acquisition bias with task
  complexity when training deep learning models: experiments on head CT","As interest in applying machine learning techniques for medical images
continues to grow at a rapid pace, models are starting to be developed and
deployed for clinical applications. In the clinical AI model development
lifecycle (described by Lu et al. [1]), a crucial phase for machine learning
scientists and clinicians is the proper design and collection of the data
cohort. The ability to recognize various forms of biases and distribution
shifts in the dataset is critical at this step. While it remains difficult to
account for all potential sources of bias, techniques can be developed to
identify specific types of bias in order to mitigate their impact. In this work
we analyze how the distribution of scanner manufacturers in a dataset can
contribute to the overall bias of deep learning models. We evaluate
convolutional neural networks (CNN) for both classification and segmentation
tasks, specifically two state-of-the-art models: ResNet [2] for classification
and U-Net [3] for segmentation. We demonstrate that CNNs can learn to
distinguish the imaging scanner manufacturer and that this bias can
substantially impact model performance for both classification and segmentation
tasks. By creating an original synthesis dataset of brain data mimicking the
presence of more or less subtle lesions we also show that this bias is related
to the difficulty of the task. Recognition of such bias is critical to develop
robust, generalizable models that will be crucial for clinical applications in
real-world data distributions.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.06933v2,2022-04-02T10:17:44Z,2020-08-16T15:10:39Z,"The reinforcement learning-based multi-agent cooperative approach for
  the adaptive speed regulation on a metallurgical pickling line","We present a holistic data-driven approach to the problem of productivity
increase on the example of a metallurgical pickling line. The proposed approach
combines mathematical modeling as a base algorithm and a cooperative
Multi-Agent Reinforcement Learning (MARL) system implemented such as to enhance
the performance by multiple criteria while also meeting safety and reliability
requirements and taking into account the unexpected volatility of certain
technological processes. We demonstrate how Deep Q-Learning can be applied to a
real-life task in a heavy industry, resulting in significant improvement of
previously existing automation systems.The problem of input data scarcity is
solved by a two-step combination of LSTM and CGAN, which helps to embrace both
the tabular representation of the data and its sequential properties. Offline
RL training, a necessity in this setting, has become possible through the
sophisticated probabilistic kinematic environment.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.06448v1,2020-08-14T16:17:54Z,2020-08-14T16:17:54Z,"Loghub: A Large Collection of System Log Datasets towards Automated Log
  Analytics","Logs have been widely adopted in software system development and maintenance
because of the rich system runtime information they contain. In recent years,
the increase of software size and complexity leads to the rapid growth of the
volume of logs. To handle these large volumes of logs efficiently and
effectively, a line of research focuses on intelligent log analytics powered by
AI (artificial intelligence) techniques. However, only a small fraction of
these techniques have reached successful deployment in industry because of the
lack of public log datasets and necessary benchmarking upon them. To fill this
significant gap between academia and industry and also facilitate more research
on AI-powered log analytics, we have collected and organized loghub, a large
collection of log datasets. In particular, loghub provides 17 real-world log
datasets collected from a wide range of systems, including distributed systems,
supercomputers, operating systems, mobile systems, server applications, and
standalone software. In this paper, we summarize the statistics of these
datasets, introduce some practical log usage scenarios, and present a case
study on anomaly detection to demonstrate how loghub facilitates the research
and practice in this field. Up to the time of this paper writing, loghub
datasets have been downloaded over 15,000 times by more than 380 organizations
from both industry and academia.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.05515v1,2020-08-12T18:19:30Z,2020-08-12T18:19:30Z,"Synergy between Machine/Deep Learning and Software Engineering: How Far
  Are We?","Since 2009, the deep learning revolution, which was triggered by the
introduction of ImageNet, has stimulated the synergy between Machine Learning
(ML)/Deep Learning (DL) and Software Engineering (SE). Meanwhile, critical
reviews have emerged that suggest that ML/DL should be used cautiously. To
improve the quality (especially the applicability and generalizability) of
ML/DL-related SE studies, and to stimulate and enhance future collaborations
between SE/AI researchers and industry practitioners, we conducted a 10-year
Systematic Literature Review (SLR) on 906 ML/DL-related SE papers published
between 2009 and 2018. Our trend analysis demonstrated the mutual impacts that
ML/DL and SE have had on each other. At the same time, however, we also
observed a paucity of replicable and reproducible ML/DL-related SE studies and
identified five factors that influence their replicability and reproducibility.
To improve the applicability and generalizability of research results, we
analyzed what ingredients in a study would facilitate an understanding of why a
ML/DL technique was selected for a specific SE problem. In addition, we
identified the unique trends of impacts of DL models on SE tasks, as well as
five unique challenges that needed to be met in order to better leverage DL to
improve the productivity of SE tasks. Finally, we outlined a road-map that we
believe can facilitate the transfer of ML/DL-based SE research results into
real-world industry practices.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.05221v4,2021-06-13T17:47:28Z,2020-08-12T10:42:14Z,Compression of Deep Learning Models for Text: A Survey,"In recent years, the fields of natural language processing (NLP) and
information retrieval (IR) have made tremendous progress thanksto deep learning
models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and
Long Short-Term Memory (LSTMs)networks, and Transformer [120] based models like
Bidirectional Encoder Representations from Transformers (BERT) [24],
GenerativePre-training Transformer (GPT-2) [94], Multi-task Deep Neural Network
(MT-DNN) [73], Extra-Long Network (XLNet) [134], Text-to-text transfer
transformer (T5) [95], T-NLG [98] and GShard [63]. But these models are
humongous in size. On the other hand,real world applications demand small model
size, low response times and low computational power wattage. In this survey,
wediscuss six different types of methods (Pruning, Quantization, Knowledge
Distillation, Parameter Sharing, Tensor Decomposition, andSub-quadratic
Transformer based methods) for compression of such models to enable their
deployment in real industry NLP projects.Given the critical need of building
applications with efficient and small models, and the large amount of recently
published work inthis area, we believe that this survey organizes the plethora
of work done by the 'deep learning for NLP' community in the past fewyears and
presents it as a coherent story.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.04461v1,2020-08-11T00:33:15Z,2020-08-11T00:33:15Z,SafetyOps,"Safety assurance is a paramount factor in the large-scale deployment of
various autonomous systems (e.g., self-driving vehicles). However, the
execution of safety engineering practices and processes have been challenged by
an increasing complexity of modern safety-critical systems. This attribute has
become more critical for autonomous systems that involve artificial
intelligence (AI) and data-driven techniques along with the complex
interactions of the physical world and digital computing platforms. In this
position paper, we highlight some challenges of applying current safety
processes to modern autonomous systems. Then, we introduce the concept of
SafetyOps - a set of practices, which combines DevOps, TestOps, DataOps, and
MLOps to provide an efficient, continuous and traceable system safety
lifecycle. We believe that SafetyOps can play a significant role in scalable
integration and adaptation of safety engineering into various industries
relying on AI and data.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.04130v1,2020-08-10T13:46:28Z,2020-08-10T13:46:28Z,Bilevel Learning Model Towards Industrial Scheduling,"Automatic industrial scheduling, aiming at optimizing the sequence of jobs
over limited resources, is widely needed in manufacturing industries. However,
existing scheduling systems heavily rely on heuristic algorithms, which either
generate ineffective solutions or compute inefficiently when job scale
increases. Thus, it is of great importance to develop new large-scale
algorithms that are not only efficient and effective, but also capable of
satisfying complex constraints in practice. In this paper, we propose a Bilevel
Deep reinforcement learning Scheduler, \textit{BDS}, in which the higher level
is responsible for exploring an initial global sequence, whereas the lower
level is aiming at exploitation for partial sequence refinements, and the two
levels are connected by a sliding-window sampling mechanism. In the
implementation, a Double Deep Q Network (DDQN) is used in the upper level and
Graph Pointer Network (GPN) lies within the lower level. After the theoretical
guarantee for the convergence of BDS, we evaluate it in an industrial automatic
warehouse scenario, with job number up to $5000$ in each production line. It is
shown that our proposed BDS significantly outperforms two most used heuristics,
three strong deep networks, and another bilevel baseline approach. In
particular, compared with the most used greedy-based heuristic algorithm in
real world which takes nearly an hour, our BDS can decrease the makespan by
27.5\%, 28.6\% and 22.1\% for 3 largest datasets respectively, with
computational time less than 200 seconds.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.00181v2,2021-01-14T00:15:23Z,2020-08-01T06:02:16Z,"Relation-aware Meta-learning for Market Segment Demand Prediction with
  Limited Records","E-commerce business is revolutionizing our shopping experiences by providing
convenient and straightforward services. One of the most fundamental problems
is how to balance the demand and supply in market segments to build an
efficient platform. While conventional machine learning models have achieved
great success on data-sufficient segments, it may fail in a large-portion of
segments in E-commerce platforms, where there are not sufficient records to
learn well-trained models. In this paper, we tackle this problem in the context
of market segment demand prediction. The goal is to facilitate the learning
process in the target segments by leveraging the learned knowledge from
data-sufficient source segments. Specifically, we propose a novel algorithm,
RMLDP, to incorporate a multi-pattern fusion network (MPFN) with a
meta-learning paradigm. The multi-pattern fusion network considers both local
and seasonal temporal patterns for segment demand prediction. In the
meta-learning paradigm, transferable knowledge is regarded as the model
parameter initialization of MPFN, which are learned from diverse source
segments. Furthermore, we capture the segment relations by combining
data-driven segment representation and segment knowledge graph representation
and tailor the segment-specific relations to customize transferable model
parameter initialization. Thus, even with limited data, the target segment can
quickly find the most relevant transferred knowledge and adapt to the optimal
parameters. We conduct extensive experiments on two large-scale industrial
datasets. The results justify that our RMLDP outperforms a set of
state-of-the-art baselines. Besides, RMLDP has been deployed in Taobao, a
real-world E-commerce platform. The online A/B testing results further
demonstrate the practicality of RMLDP.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.15724v1,2020-07-30T20:14:42Z,2020-07-30T20:14:42Z,"MAPPER: Multi-Agent Path Planning with Evolutionary Reinforcement
  Learning in Mixed Dynamic Environments","Multi-agent navigation in dynamic environments is of great industrial value
when deploying a large scale fleet of robot to real-world applications. This
paper proposes a decentralized partially observable multi-agent path planning
with evolutionary reinforcement learning (MAPPER) method to learn an effective
local planning policy in mixed dynamic environments. Reinforcement
learning-based methods usually suffer performance degradation on long-horizon
tasks with goal-conditioned sparse rewards, so we decompose the long-range
navigation task into many easier sub-tasks under the guidance of a global
planner, which increases agents' performance in large environments. Moreover,
most existing multi-agent planning approaches assume either perfect information
of the surrounding environment or homogeneity of nearby dynamic agents, which
may not hold in practice. Our approach models dynamic obstacles' behavior with
an image-based representation and trains a policy in mixed dynamic environments
without homogeneity assumption. To ensure multi-agent training stability and
performance, we propose an evolutionary training approach that can be easily
scaled to large and complex environments. Experiments show that MAPPER is able
to achieve higher success rates and more stable performance when exposed to a
large number of non-cooperative dynamic obstacles compared with traditional
reaction-based planner LRA* and the state-of-the-art learning-based method.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.15215v1,2020-07-30T03:54:32Z,2020-07-30T03:54:32Z,"Learner's Dilemma: IoT Devices Training Strategies in Collaborative Deep
  Learning","With the growth of Internet of Things (IoT) and mo-bile edge computing,
billions of smart devices are interconnected to develop applications used in
various domains including smart homes, healthcare and smart manufacturing. Deep
learning has been extensively utilized in various IoT applications which
require huge amount of data for model training. Due to privacy requirements,
smart IoT devices do not release data to a remote third party for their use. To
overcome this problem, collaborative approach to deep learning, also known as
Collaborative DeepLearning (CDL) has been largely employed in data-driven
applications. This approach enables multiple edge IoT devices to train their
models locally on mobile edge devices. In this paper,we address IoT device
training problem in CDL by analyzing the behavior of mobile edge devices using
a game-theoretic model,where each mobile edge device aims at maximizing the
accuracy of its local model at the same time limiting the overhead of
participating in CDL. We analyze the Nash Equilibrium in anN-player static game
model. We further present a novel cluster-based fair strategy to approximately
solve the CDL game to enforce mobile edge devices for cooperation. Our
experimental results and evaluation analysis in a real-world smart home
deployment show that 80% mobile edge devices are ready to cooperate in CDL,
while 20% of them do not train their local models collaboratively.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.13404v2,2020-10-29T16:23:12Z,2020-07-27T09:50:11Z,"YOLOpeds: Efficient Real-Time Single-Shot Pedestrian Detection for Smart
  Camera Applications","Deep Learning-based object detectors can enhance the capabilities of smart
camera systems in a wide spectrum of machine vision applications including
video surveillance, autonomous driving, robots and drones, smart factory, and
health monitoring. Pedestrian detection plays a key role in all these
applications and deep learning can be used to construct accurate
state-of-the-art detectors. However, such complex paradigms do not scale easily
and are not traditionally implemented in resource-constrained smart cameras for
on-device processing which offers significant advantages in situations when
real-time monitoring and robustness are vital. Efficient neural networks can
not only enable mobile applications and on-device experiences but can also be a
key enabler of privacy and security allowing a user to gain the benefits of
neural networks without needing to send their data to the server to be
evaluated. This work addresses the challenge of achieving a good trade-off
between accuracy and speed for efficient deployment of deep-learning-based
pedestrian detection in smart camera applications. A computationally efficient
architecture is introduced based on separable convolutions and proposes
integrating dense connections across layers and multi-scale feature fusion to
improve representational capacity while decreasing the number of parameters and
operations. In particular, the contributions of this work are the following: 1)
An efficient backbone combining multi-scale feature operations, 2) a more
elaborate loss function for improved localization, 3) an anchor-less approach
for detection, The proposed approach called YOLOpeds is evaluated using the
PETS2009 surveillance dataset on 320x320 images. Overall, YOLOpeds provides
real-time sustained operation of over 30 frames per second with detection rates
in the range of 86% outperforming existing deep learning models.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.12002v1,2020-07-23T13:25:36Z,2020-07-23T13:25:36Z,Grale: Designing Networks for Graph Learning,"How can we find the right graph for semi-supervised learning? In real world
applications, the choice of which edges to use for computation is the first
step in any graph learning process. Interestingly, there are often many types
of similarity available to choose as the edges between nodes, and the choice of
edges can drastically affect the performance of downstream semi-supervised
learning systems. However, despite the importance of graph design, most of the
literature assumes that the graph is static. In this work, we present Grale, a
scalable method we have developed to address the problem of graph design for
graphs with billions of nodes. Grale operates by fusing together different
measures of(potentially weak) similarity to create a graph which exhibits high
task-specific homophily between its nodes. Grale is designed for running on
large datasets. We have deployed Grale in more than 20 different industrial
settings at Google, including datasets which have tens of billions of nodes,
and hundreds of trillions of potential edges to score. By employing locality
sensitive hashing techniques,we greatly reduce the number of pairs that need to
be scored, allowing us to learn a task specific model and build the associated
nearest neighbor graph for such datasets in hours, rather than the days or even
weeks that might be required otherwise. We illustrate this through a case study
where we examine the application of Grale to an abuse classification problem on
YouTube with hundreds of million of items. In this application, we find that
Grale detects a large number of malicious actors on top of hard-coded rules and
content classifiers, increasing the total recall by 89% over those approaches
alone.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.10243v1,2020-07-20T16:32:27Z,2020-07-20T16:32:27Z,Inter-Homines: Distance-Based Risk Estimation for Human Safety,"In this document, we report our proposal for modeling the risk of possible
contagiousity in a given area monitored by RGB cameras where people freely move
and interact. Our system, called Inter-Homines, evaluates in real-time the
contagion risk in a monitored area by analyzing video streams: it is able to
locate people in 3D space, calculate interpersonal distances and predict risk
levels by building dynamic maps of the monitored area. Inter-Homines works both
indoor and outdoor, in public and private crowded areas. The software is
applicable to already installed cameras or low-cost cameras on industrial PCs,
equipped with an additional embedded edge-AI system for temporary measurements.
From the AI-side, we exploit a robust pipeline for real-time people detection
and localization in the ground plane by homographic transformation based on
state-of-the-art computer vision algorithms; it is a combination of a people
detector and a pose estimator. From the risk modeling side, we propose a
parametric model for a spatio-temporal dynamic risk estimation, that, validated
by epidemiologists, could be useful for safety monitoring the acceptance of
social distancing prevention measures by predicting the risk level of the
scene.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.03580v1,2020-07-07T15:58:10Z,2020-07-07T15:58:10Z,Using Semantic Web Services for AI-Based Research in Industry 4.0,"The transition to Industry 4.0 requires smart manufacturing systems that are
easily configurable and provide a high level of flexibility during
manufacturing in order to achieve mass customization or to support cloud
manufacturing. To realize this, Cyber-Physical Systems (CPSs) combined with
Artificial Intelligence (AI) methods find their way into manufacturing shop
floors. For using AI methods in the context of Industry 4.0, semantic web
services are indispensable to provide a reasonable abstraction of the
underlying manufacturing capabilities. In this paper, we present semantic web
services for AI-based research in Industry 4.0. Therefore, we developed more
than 300 semantic web services for a physical simulation factory based on Web
Ontology Language for Web Services (OWL-S) and Web Service Modeling Ontology
(WSMO) and linked them to an already existing domain ontology for intelligent
manufacturing control. Suitable for the requirements of CPS environments, our
pre- and postconditions are verified in near real-time by invoking other
semantic web services in contrast to complex reasoning within the knowledge
base. Finally, we evaluate our implementation by executing a cyber-physical
workflow composed of semantic web services using a workflow management system.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.01097v1,2020-07-01T08:47:46Z,2020-07-01T08:47:46Z,"PrototypeML: A Neural Network Integrated Design and Development
  Environment","Neural network architectures are most often conceptually designed and
described in visual terms, but are implemented by writing error-prone code.
PrototypeML is a machine learning development environment that bridges the
dichotomy between the design and development processes: it provides a highly
intuitive visual neural network design interface that supports (yet abstracts)
the full capabilities of the PyTorch deep learning framework, reduces model
design and development time, makes debugging easier, and automates many
framework and code writing idiosyncrasies. In this paper, we detail the deep
learning development deficiencies that drove the implementation of PrototypeML,
and propose a hybrid approach to resolve these issues without limiting network
expressiveness or reducing code quality. We demonstrate the real-world benefits
of a visual approach to neural network design for research, industry and
teaching. Available at https://PrototypeML.com",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2006.16911v1,2020-06-19T10:59:32Z,2020-06-19T10:59:32Z,"Turbulence on the Global Economy influenced by Artificial Intelligence
  and Foreign Policy Inefficiencies","It is said that Data and Information are the new oil. One, who handles the
data, handles the emerging future of the global economy. Complex algorithms and
intelligence-based filter programs are utilized to manage, store, handle and
maneuver vast amounts of data for the fulfillment of specific purposes. This
paper seeks to find the bridge between artificial intelligence and its impact
on the international policy implementation in the light of geopolitical
influence, global economy and the future of labor markets. We hypothesize that
the distortion in the labor markets caused by artificial intelligence can be
mitigated by a collaborative international foreign policy on the deployment of
AI in the industrial circles. We, in this paper, then proceed to propose a
disposition for the essentials of AI-based foreign policy and implementation,
while asking questions such as 'could AI become the real Invisible Hand
discussed by economists?'.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2006.06082v3,2021-02-13T23:23:08Z,2020-06-10T21:54:27Z,Towards Integrating Fairness Transparently in Industrial Applications,"Numerous Machine Learning (ML) bias-related failures in recent years have led
to scrutiny of how companies incorporate aspects of transparency and
accountability in their ML lifecycles. Companies have a responsibility to
monitor ML processes for bias and mitigate any bias detected, ensure business
product integrity, preserve customer loyalty, and protect brand image.
Challenges specific to industry ML projects can be broadly categorized into
principled documentation, human oversight, and need for mechanisms that enable
information reuse and improve cost efficiency. We highlight specific roadblocks
and propose conceptual solutions on a per-category basis for ML practitioners
and organizational subject matter experts. Our systematic approach tackles
these challenges by integrating mechanized and human-in-the-loop components in
bias detection, mitigation, and documentation of projects at various stages of
the ML lifecycle. To motivate the implementation of our system -- SIFT (System
to Integrate Fairness Transparently) -- we present its structural primitives
with an example real-world use case on how it can be used to identify potential
biases and determine appropriate mitigation strategies in a participatory
manner.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2006.03616v2,2020-10-26T15:31:56Z,2020-06-05T18:11:14Z,"High-level Modeling of Manufacturing Faults in Deep Neural Network
  Accelerators","The advent of data-driven real-time applications requires the implementation
of Deep Neural Networks (DNNs) on Machine Learning accelerators. Google's
Tensor Processing Unit (TPU) is one such neural network accelerator that uses
systolic array-based matrix multiplication hardware for computation in its
crux. Manufacturing faults at any state element of the matrix multiplication
unit can cause unexpected errors in these inference networks. In this paper, we
propose a formal model of permanent faults and their propagation in a TPU using
the Discrete-Time Markov Chain (DTMC) formalism. The proposed model is analyzed
using the probabilistic model checking technique to reason about the likelihood
of faulty outputs. The obtained quantitative results show that the
classification accuracy is sensitive to the type of permanent faults as well as
their location, bit position and the number of layers in the neural network.
The conclusions from our theoretical model have been validated using
experiments on a digit recognition-based DNN.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2006.00429v1,2020-05-31T03:55:41Z,2020-05-31T03:55:41Z,Pseudo-Representation Labeling Semi-Supervised Learning,"In recent years, semi-supervised learning (SSL) has shown tremendous success
in leveraging unlabeled data to improve the performance of deep learning
models, which significantly reduces the demand for large amounts of labeled
data. Many SSL techniques have been proposed and have shown promising
performance on famous datasets such as ImageNet and CIFAR-10. However, some
exiting techniques (especially data augmentation based) are not suitable for
industrial applications empirically. Therefore, this work proposes the
pseudo-representation labeling, a simple and flexible framework that utilizes
pseudo-labeling techniques to iteratively label a small amount of unlabeled
data and use them as training data. In addition, our framework is integrated
with self-supervised representation learning such that the classifier gains
benefits from representation learning of both labeled and unlabeled data. This
framework can be implemented without being limited at the specific model
structure, but a general technique to improve the existing model. Compared with
the existing approaches, the pseudo-representation labeling is more intuitive
and can effectively solve practical problems in the real world. Empirically, it
outperforms the current state-of-the-art semi-supervised learning methods in
industrial types of classification problems such as the WM-811K wafer map and
the MIT-BIH Arrhythmia dataset.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.05815v1,2020-05-12T14:30:03Z,2020-05-12T14:30:03Z,One-Shot Recognition of Manufacturing Defects in Steel Surfaces,"Quality control is an essential process in manufacturing to make the product
defect-free as well as to meet customer needs. The automation of this process
is important to maintain high quality along with the high manufacturing
throughput. With recent developments in deep learning and computer vision
technologies, it has become possible to detect various features from the images
with near-human accuracy. However, many of these approaches are data intensive.
Training and deployment of such a system on manufacturing floors may become
expensive and time-consuming. The need for large amounts of training data is
one of the limitations of the applicability of these approaches in real-world
manufacturing systems. In this work, we propose the application of a Siamese
convolutional neural network to do one-shot recognition for such a task. Our
results demonstrate how one-shot learning can be used in quality control of
steel by identification of defects on the steel surface. This method can
significantly reduce the requirements of training data and can also be run in
real-time.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.05287v2,2020-05-25T12:16:12Z,2020-05-11T17:40:58Z,"Using Computer Vision to enhance Safety of Workforce in Manufacturing in
  a Post COVID World","The COVID-19 pandemic forced governments across the world to impose lockdowns
to prevent virus transmissions. This resulted in the shutdown of all economic
activity and accordingly the production at manufacturing plants across most
sectors was halted. While there is an urgency to resume production, there is an
even greater need to ensure the safety of the workforce at the plant site.
Reports indicate that maintaining social distancing and wearing face masks
while at work clearly reduces the risk of transmission. We decided to use
computer vision on CCTV feeds to monitor worker activity and detect violations
which trigger real time voice alerts on the shop floor. This paper describes an
efficient and economic approach of using AI to create a safe environment in a
manufacturing setup. We demonstrate our approach to build a robust social
distancing measurement algorithm using a mix of modern-day deep learning and
classic projective geometry techniques. We have deployed our solution at
manufacturing plants across the Aditya Birla Group (ABG). We have also
described our face mask detection approach which provides a high accuracy
across a range of customized masks.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.04726v1,2020-05-10T17:37:38Z,2020-05-10T17:37:38Z,Knowledge Graph semantic enhancement of input data for improving AI,"Intelligent systems designed using machine learning algorithms require a
large number of labeled data. Background knowledge provides complementary, real
world factual information that can augment the limited labeled data to train a
machine learning algorithm. The term Knowledge Graph (KG) is in vogue as for
many practical applications, it is convenient and useful to organize this
background knowledge in the form of a graph. Recent academic research and
implemented industrial intelligent systems have shown promising performance for
machine learning algorithms that combine training data with a knowledge graph.
In this article, we discuss the use of relevant KGs to enhance input data for
two applications that use machine learning -- recommendation and community
detection. The KG improves both accuracy and explainability.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.06342v1,2020-05-09T05:54:28Z,2020-05-09T05:54:28Z,"sCrop: A Internet-of-Agro-Things (IoAT) Enabled Solar Powered Smart
  Device for Automatic Plant Disease Prediction","Internet-of-Things (IoT) is omnipresent, ranging from home solutions to
turning wheels for the fourth industrial revolution. This article presents the
novel concept of Internet-of-Agro-Things (IoAT) with an example of automated
plant disease prediction. It consists of solar enabled sensor nodes which help
in continuous sensing and automating agriculture. The existing solutions have
implemented a battery powered sensor node. On the contrary, the proposed system
has adopted the use of an energy efficient way of powering using solar energy.
It is observed that around 80% of the crops are attacked with microbial
diseases in traditional agriculture. To prevent this, a health maintenance
system is integrated with the sensor node, which captures the image of the crop
and performs an analysis with the trained Convolutional Neural Network (CNN)
model. The deployment of the proposed system is demonstrated in a real-time
environment using a microcontroller, solar sensor nodes with a camera module,
and an mobile application for the farmers visualization of the farms. The
deployed prototype was deployed for two months and has achieved a robust
performance by sustaining in varied weather conditions and continued to remain
rust-free. The proposed deep learning framework for plant disease prediction
has achieved an accuracy of 99.2% testing accuracy.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.03077v1,2020-05-06T19:06:51Z,2020-05-06T19:06:51Z,"AVAC: A Machine Learning based Adaptive RRAM Variability-Aware
  Controller for Edge Devices","Recently, the Edge Computing paradigm has gained significant popularity both
in industry and academia. Researchers now increasingly target to improve
performance and reduce energy consumption of such devices. Some recent efforts
focus on using emerging RRAM technologies for improving energy efficiency,
thanks to their no leakage property and high integration density. As the
complexity and dynamism of applications supported by such devices escalate, it
has become difficult to maintain ideal performance by static RRAM controllers.
Machine Learning provides a promising solution for this, and hence, this work
focuses on extending such controllers to allow dynamic parameter updates. In
this work we propose an Adaptive RRAM Variability-Aware Controller, AVAC, which
periodically updates Wait Buffer and batch sizes using on-the-fly learning
models and gradient ascent. AVAC allows Edge devices to adapt to different
applications and their stages, to improve computation performance and reduce
energy consumption. Simulations demonstrate that the proposed model can provide
up to 29% increase in performance and 19% decrease in energy, compared to
static controllers, using traces of real-life healthcare applications on a
Raspberry-Pi based Edge deployment.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.03459v4,2021-09-05T13:44:38Z,2020-05-06T01:24:25Z,AIBench Scenario: Scenario-distilling AI Benchmarking,"Modern real-world application scenarios like Internet services consist of a
diversity of AI and non-AI modules with huge code sizes and long and
complicated execution paths, which raises serious benchmarking or evaluating
challenges. Using AI components or micro benchmarks alone can lead to
error-prone conclusions. This paper presents a methodology to attack the above
challenge. We formalize a real-world application scenario as a Directed Acyclic
Graph-based model and propose the rules to distill it into a permutation of
essential AI and non-AI tasks, which we call a scenario benchmark. Together
with seventeen industry partners, we extract nine typical scenario benchmarks.
We design and implement an extensible, configurable, and flexible benchmark
framework. We implement two Internet service AI scenario benchmarks based on
the framework as proxies to two real-world application scenarios. We consider
scenario, component, and micro benchmarks as three indispensable parts for
evaluating. Our evaluation shows the advantage of our methodology against using
component or micro AI benchmarks alone. The specifications, source code,
testbed, and results are publicly available from
\url{https://www.benchcouncil.org/aibench/scenario/}.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.00936v1,2020-05-02T22:49:34Z,2020-05-02T22:49:34Z,"An Ensemble Deep Learning-based Cyber-Attack Detection in Industrial
  Control System","The integration of communication networks and the Internet of Things (IoT) in
Industrial Control Systems (ICSs) increases their vulnerability towards
cyber-attacks, causing devastating outcomes. Traditional Intrusion Detection
Systems (IDSs), which are mainly developed to support Information Technology
(IT) systems, count vastly on predefined models and are trained mostly on
specific cyber-attacks. Besides, most IDSs do not consider the imbalanced
nature of ICS datasets, thereby suffering from low accuracy and high false
positive on real datasets. In this paper, we propose a deep representation
learning model to construct new balanced representations of the imbalanced
dataset. The new representations are fed into an ensemble deep learning attack
detection model specifically designed for an ICS environment. The proposed
attack detection model leverages Deep Neural Network (DNN) and Decision Tree
(DT) classifiers to detect cyber-attacks from the new representations. The
performance of the proposed model is evaluated based on 10-fold
cross-validation on two real ICS datasets. The results show that the proposed
method outperforms conventional classifiers, including Random Forest (RF), DNN,
and AdaBoost, as well as recent existing models in the literature. The proposed
approach is a generalized technique, which can be implemented in existing ICS
infrastructures with minimum changes.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.14850v1,2020-04-30T15:02:08Z,2020-04-30T15:02:08Z,6G White Paper on Edge Intelligence,"In this white paper we provide a vision for 6G Edge Intelligence. Moving
towards 5G and beyond the future 6G networks, intelligent solutions utilizing
data-driven machine learning and artificial intelligence become crucial for
several real-world applications including but not limited to, more efficient
manufacturing, novel personal smart device environments and experiences, urban
computing and autonomous traffic settings. We present edge computing along with
other 6G enablers as a key component to establish the future 2030 intelligent
Internet technologies as shown in this series of 6G White Papers.
  In this white paper, we focus in the domains of edge computing infrastructure
and platforms, data and edge network management, software development for edge,
and real-time and distributed training of ML/AI algorithms, along with
security, privacy, pricing, and end-user aspects. We discuss the key enablers
and challenges and identify the key research questions for the development of
the Intelligent Edge services. As a main outcome of this white paper, we
envision a transition from Internet of Things to Intelligent Internet of
Intelligent Things and provide a roadmap for development of 6G Intelligent
Edge.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.13401v2,2022-03-27T00:18:20Z,2020-04-28T10:13:00Z,CmnRec: Sequential Recommendations with Chunk-accelerated Memory Network,"Recently, Memory-based Neural Recommenders (MNR) have demonstrated superior
predictive accuracy in the task of sequential recommendations, particularly for
modeling long-term item dependencies. However, typical MNR requires complex
memory access operations, i.e., both writing and reading via a controller
(e.g., RNN) at every time step. Those frequent operations will dramatically
increase the network training time, resulting in the difficulty in being
deployed on industrial-scale recommender systems. In this paper, we present a
novel general Chunk framework to accelerate MNR significantly. Specifically,
our framework divides proximal information units into chunks, and performs
memory access at certain time steps, whereby the number of memory operations
can be greatly reduced. We investigate two ways to implement effective
chunking, i.e., PEriodic Chunk (PEC) and Time-Sensitive Chunk (TSC), to
preserve and recover important recurrent signals in the sequence. Since
chunk-accelerated MNR models take into account more proximal information units
than that from a single timestep, it can remove the influence of noise in the
item sequence to a large extent, and thus improve the stability of MNR. In this
way, the proposed chunk mechanism can lead to not only faster training and
prediction, but even slightly better results. The experimental results on three
real-world datasets (weishi, ml-10M and ml-latest) show that our chunk
framework notably reduces the running time (e.g., with up to 7x for training &
10x for inference on ml-latest) of MNR, and meantime achieves competitive
performance.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.13094v1,2020-04-27T18:54:36Z,2020-04-27T18:54:36Z,Compact retail shelf segmentation for mobile deployment,"The recent surge of automation in the retail industries has rapidly increased
demand for applying deep learning models on mobile devices. To make the deep
learning models real-time on-device, a compact efficient network becomes
inevitable. In this paper, we work on one such common problem in the retail
industries - Shelf segmentation. Shelf segmentation can be interpreted as a
pixel-wise classification problem, i.e., each pixel is classified as to whether
they belong to visible shelf edges or not. The aim is not just to segment shelf
edges, but also to deploy the model on mobile devices. As there is no standard
solution for such dense classification problem on mobile devices, we look at
semantic segmentation architectures which can be deployed on edge. We modify
low-footprint semantic segmentation architectures to perform shelf
segmentation. In addressing this issue, we modified the famous U-net
architecture in certain aspects to make it fit for on-devices without impacting
significant drop in accuracy and also with 15X fewer parameters. In this paper,
we proposed Light Weight Segmentation Network (LWSNet), a small compact model
able to run fast on devices with limited memory and can train with less amount
(~ 100 images) of labeled data.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.11573v1,2020-04-24T07:29:47Z,2020-04-24T07:29:47Z,"Towards Characterizing Adversarial Defects of Deep Learning Software
  from the Lens of Uncertainty","Over the past decade, deep learning (DL) has been successfully applied to
many industrial domain-specific tasks. However, the current state-of-the-art DL
software still suffers from quality issues, which raises great concern
especially in the context of safety- and security-critical scenarios.
Adversarial examples (AEs) represent a typical and important type of defects
needed to be urgently addressed, on which a DL software makes incorrect
decisions. Such defects occur through either intentional attack or
physical-world noise perceived by input sensors, potentially hindering further
industry deployment. The intrinsic uncertainty nature of deep learning
decisions can be a fundamental reason for its incorrect behavior. Although some
testing, adversarial attack and defense techniques have been recently proposed,
it still lacks a systematic study to uncover the relationship between AEs and
DL uncertainty. In this paper, we conduct a large-scale study towards bridging
this gap. We first investigate the capability of multiple uncertainty metrics
in differentiating benign examples (BEs) and AEs, which enables to characterize
the uncertainty patterns of input data. Then, we identify and categorize the
uncertainty patterns of BEs and AEs, and find that while BEs and AEs generated
by existing methods do follow common uncertainty patterns, some other
uncertainty patterns are largely missed. Based on this, we propose an automated
testing technique to generate multiple types of uncommon AEs and BEs that are
largely missed by existing techniques. Our further evaluation reveals that the
uncommon data generated by our method is hard to be defended by the existing
defense techniques with the average defense success rate reduced by 35\%. Our
results call for attention and necessity to generate more diverse data for
evaluating quality assurance solutions of DL software.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.10908v4,2021-09-06T18:36:40Z,2020-04-23T00:21:05Z,"Taskflow: A Lightweight Parallel and Heterogeneous Task Graph Computing
  System","Taskflow aims to streamline the building of parallel and heterogeneous
applications using a lightweight task graph-based approach. Taskflow introduces
an expressive task graph programming model to assist developers in the
implementation of parallel and heterogeneous decomposition strategies on a
heterogeneous computing platform. Our programming model distinguishes itself as
a very general class of task graph parallelism with in-graph control flow to
enable end-to-end parallel optimization. To support our model with high
performance, we design an efficient system runtime that solves many of the new
scheduling challenges arising out of our models and optimizes the performance
across latency, energy efficiency, and throughput. We have demonstrated the
promising performance of Taskflow in real-world applications. As an example,
Taskflow solves a large-scale machine learning workload up to 29% faster, 1.5x
less memory, and 1.9x higher throughput than the industrial system, oneTBB, on
a machine of 40 CPUs and 4 GPUs. We have opened the source of Taskflow and
deployed it to large numbers of users in the open-source community.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.10251v1,2020-04-21T19:40:16Z,2020-04-21T19:40:16Z,"Industrial Robot Grasping with Deep Learning using a Programmable Logic
  Controller (PLC)","Universal grasping of a diverse range of previously unseen objects from heaps
is a grand challenge in e-commerce order fulfillment, manufacturing, and home
service robotics. Recently, deep learning based grasping approaches have
demonstrated results that make them increasingly interesting for industrial
deployments. This paper explores the problem from an automation systems
point-of-view. We develop a robotics grasping system using Dex-Net, which is
fully integrated at the controller level. Two neural networks are deployed on a
novel industrial AI hardware acceleration module close to a PLC with a power
footprint of less than 10 W for the overall system. The software is tightly
integrated with the hardware allowing for fast and efficient data processing
and real-time communication. The success rate of grasping an object form a bin
is up to 95 percent with more than 350 picks per hour, if object and receptive
bins are in close proximity. The system was presented at the Hannover Fair 2019
(world s largest industrial trade fair) and other events, where it performed
over 5,000 grasps per event.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.05898v1,2020-04-10T14:26:00Z,2020-04-10T14:26:00Z,Exposing Hardware Building Blocks to Machine Learning Frameworks,"There are a plethora of applications that demand high throughput and low
latency algorithms leveraging machine learning methods. This need for real time
processing can be seen in industries ranging from developing neural network
based pre-distortors for enhanced mobile broadband to designing FPGA-based
triggers in major scientific efforts by CERN for particle physics. In this
thesis, we explore how niche domains can benefit vastly if we look at neurons
as a unique boolean function of the form $f:B^{I} \rightarrow B^{O}$, where $B
= \{0,1\}$. We focus on how to design topologies that complement such a view of
neurons, how to automate such a strategy of neural network design, and
inference of such networks on Xilinx FPGAs. Major hardware borne constraints
arise when designing topologies that view neurons as unique boolean functions.
Fundamentally, realizing such topologies on hardware asserts a strict limit on
the 'fan-in' bits of a neuron due to the doubling of permutations possible with
every increment in input bit-length. We address this limit by exploring
different methods of implementing sparsity and explore activation quantization.
Further, we develop a library that supports training a neural network with
custom sparsity and quantization. This library also supports conversion of
trained Sparse Quantized networks from PyTorch to VERILOG code which is then
synthesized using Vivado, all of which is part of the LogicNet tool-flow. To
aid faster prototyping, we also support calculation of the worst-case hardware
cost of any given topology. We hope that our insights into the behavior of
extremely sparse quantized neural networks are of use to the research community
and by extension allow people to use the LogicNet design flow to deploy highly
efficient neural networks.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.04710v2,2021-01-21T16:05:23Z,2020-04-09T17:44:34Z,"Prune2Edge: A Multi-Phase Pruning Pipelines to Deep Ensemble Learning in
  IIoT","Most recently, with the proliferation of IoT devices, computational nodes in
manufacturing systems IIoT(Industrial-Internet-of-things) and the lunch of 5G
networks, there will be millions of connected devices generating a massive
amount of data. In such an environment, the controlling systems need to be
intelligent enough to deal with a vast amount of data to detect defects in a
real-time process. Driven by such a need, artificial intelligence models such
as deep learning have to be deployed into IIoT systems. However, learning and
using deep learning models are computationally expensive, so an IoT device with
limited computational power could not run such models. To tackle this issue,
edge intelligence had emerged as a new paradigm towards running Artificial
Intelligence models on edge devices. Although a considerable amount of studies
have been proposed in this area, the research is still in the early stages. In
this paper, we propose a novel edge-based multi-phase pruning pipelines to
ensemble learning on IIoT devices. In the first phase, we generate a diverse
ensemble of pruned models, then we apply integer quantisation, next we prune
the generated ensemble using a clustering-based technique. Finally, we choose
the best representative from each generated cluster to be deployed to a
distributed IoT environment. On CIFAR-100 and CIFAR-10, our proposed approach
was able to outperform the predictability levels of a baseline model (up to
7%), more importantly, the generated learners have small sizes (up to 90%
reduction in the model size) that minimise the required computational
capabilities to make an inference on the resource-constraint devices.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.03264v3,2020-08-21T04:12:15Z,2020-04-07T11:00:29Z,"Inspector Gadget: A Data Programming-based Labeling System for
  Industrial Images","As machine learning for images becomes democratized in the Software 2.0 era,
one of the serious bottlenecks is securing enough labeled data for training.
This problem is especially critical in a manufacturing setting where smart
factories rely on machine learning for product quality control by analyzing
industrial images. Such images are typically large and may only need to be
partially analyzed where only a small portion is problematic (e.g., identifying
defects on a surface). Since manual labeling these images is expensive, weak
supervision is an attractive alternative where the idea is to generate weak
labels that are not perfect, but can be produced at scale. Data programming is
a recent paradigm in this category where it uses human knowledge in the form of
labeling functions and combines them into a generative model. Data programming
has been successful in applications based on text or structured data and can
also be applied to images usually if one can find a way to convert them into
structured data. In this work, we expand the horizon of data programming by
directly applying it to images without this conversion, which is a common
scenario for industrial applications. We propose Inspector Gadget, an image
labeling system that combines crowdsourcing, data augmentation, and data
programming to produce weak labels at scale for image classification. We
perform experiments on real industrial image datasets and show that Inspector
Gadget obtains better performance than other weak-labeling techniques: Snuba,
GOGGLES, and self-learning baselines using convolutional neural networks (CNNs)
without pre-training.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.13174v1,2020-03-30T00:59:31Z,2020-03-30T00:59:31Z,"MIP An AI Distributed Architectural Model to Introduce Cognitive
  computing capabilities in Cyber Physical Systems (CPS)","This paper introduces the MIP Platform architecture model, a novel AI-based
cognitive computing platform architecture. The goal of the proposed application
of MIP is to reduce the implementation burden for the usage of AI algorithms
applied to cognitive computing and fluent HMI interactions within the
manufacturing process in a cyber-physical production system. The cognitive
inferencing engine of MIP is a deterministic cognitive module that processes
declarative goals, identifies Intents and Entities, selects suitable actions
and associated algorithms, and invokes for the execution a processing logic
(Function) configured in the internal Function-as-aService or Connectivity
Engine. Constant observation and evaluation against performance criteria assess
the performance of Lambda(s) for many and varying scenarios. The modular design
with well-defined interfaces enables the reusability and extensibility of FaaS
components. An integrated BigData platform implements this modular design
supported by technologies such as Docker, Kubernetes for virtualization and
orchestration of the individual components and their communication. The
implementation of the architecture is evaluated using a real-world use case
later discussed in this paper.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.13652v1,2020-03-18T01:26:36Z,2020-03-18T01:26:36Z,"Machine Learning enabled Spectrum Sharing in Dense LTE-U/Wi-Fi
  Coexistence Scenarios","The application of Machine Learning (ML) techniques to complex engineering
problems has proved to be an attractive and efficient solution. ML has been
successfully applied to several practical tasks like image recognition,
automating industrial operations, etc. The promise of ML techniques in solving
non-linear problems influenced this work which aims to apply known ML
techniques and develop new ones for wireless spectrum sharing between Wi-Fi and
LTE in the unlicensed spectrum. In this work, we focus on the LTE-Unlicensed
(LTE-U) specification developed by the LTE-U Forum, which uses the duty-cycle
approach for fair coexistence. The specification suggests reducing the duty
cycle at the LTE-U base-station (BS) when the number of co-channel Wi-Fi basic
service sets (BSSs) increases from one to two or more. However, without
decoding the Wi-Fi packets, detecting the number of Wi-Fi BSSs operating on the
channel in real-time is a challenging problem. In this work, we demonstrate a
novel ML-based approach which solves this problem by using energy values
observed during the LTE-U OFF duration. It is relatively straightforward to
observe only the energy values during the LTE-U BS OFF time compared to
decoding the entire Wi-Fi packet, which would require a full Wi-Fi receiver at
the LTE-U base-station. We implement and validate the proposed ML-based
approach by real-time experiments and demonstrate that there exist distinct
patterns between the energy distributions between one and many Wi-Fi AP
transmissions. The proposed ML-based approach results in a higher accuracy
(close to 99\% in all cases) as compared to the existing auto-correlation (AC)
and energy detection (ED) approaches.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.07583v1,2020-03-17T08:47:34Z,2020-03-17T08:47:34Z,"Reinforcement Learning Driven Adaptive VR Streaming with Optical Flow
  Based QoE","With the merit of containing full panoramic content in one camera, Virtual
Reality (VR) and 360-degree videos have attracted more and more attention in
the field of industrial cloud manufacturing and training. Industrial Internet
of Things (IoT), where many VR terminals needed to be online at the same time,
can hardly guarantee VR's bandwidth requirement. However, by making use of
users' quality of experience (QoE) awareness factors, including the relative
moving speed and depth difference between the viewpoint and other content,
bandwidth consumption can be reduced. In this paper, we propose OFB-VR (Optical
Flow Based VR), an interactive method of VR streaming that can make use of VR
users' QoE awareness to ease the bandwidth pressure. The Just-Noticeable
Difference through Optical Flow Estimation (JND-OFE) is explored to quantify
users' awareness of quality distortion in 360-degree videos. Accordingly, a
novel 360-degree videos QoE metric based on PSNR and JND-OFE (PSNR-OF) is
proposed. With the help of PSNR-OF, OFB-VR proposes a versatile-size tiling
scheme to lessen the tiling overhead. A Reinforcement Learning(RL) method is
implemented to make use of historical data to perform Adaptive BitRate(ABR).
For evaluation, we take two prior VR streaming schemes, Pano and Plato, as
baselines. Vast evaluations show that our system can increase the mean PSNR-OF
score by 9.5-15.8% while maintaining the same rebuffer ratio compared with Pano
and Plato in a fluctuate LTE bandwidth dataset. Evaluation results show that
OFB-VR is a promising prototype for actual interactive industrial VR. A
prototype of OFB-VR can be found in https://github.com/buptexplorers/OFB-VR.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.06700v3,2020-05-14T21:05:24Z,2020-03-14T20:53:05Z,"CoCoPIE: Making Mobile AI Sweet As PIE --Compression-Compilation
  Co-Design Goes a Long Way","Assuming hardware is the major constraint for enabling real-time mobile
intelligence, the industry has mainly dedicated their efforts to developing
specialized hardware accelerators for machine learning and inference. This
article challenges the assumption. By drawing on a recent real-time AI
optimization framework CoCoPIE, it maintains that with effective
compression-compiler co-design, it is possible to enable real-time artificial
intelligence on mainstream end devices without special hardware. CoCoPIE is a
software framework that holds numerous records on mobile AI: the first
framework that supports all main kinds of DNNs, from CNNs to RNNs, transformer,
language models, and so on; the fastest DNN pruning and acceleration framework,
up to 180X faster compared with current DNN pruning on other frameworks such as
TensorFlow-Lite; making many representative AI applications able to run in
real-time on off-the-shelf mobile devices that have been previously regarded
possible only with special hardware support; making off-the-shelf mobile
devices outperform a number of representative ASIC and FPGA solutions in terms
of energy efficiency and/or performance.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.02454v4,2020-03-16T01:23:30Z,2020-03-05T06:54:33Z,AGL: a Scalable System for Industrial-purpose Graph Machine Learning,"Machine learning over graphs have been emerging as powerful learning tools
for graph data. However, it is challenging for industrial communities to
leverage the techniques, such as graph neural networks (GNNs), and solve
real-world problems at scale because of inherent data dependency in the graphs.
As such, we cannot simply train a GNN with classic learning systems, for
instance parameter server that assumes data parallel. Existing systems store
the graph data in-memory for fast accesses either in a single machine or graph
stores from remote. The major drawbacks are in three-fold. First, they cannot
scale because of the limitations on the volume of the memory, or the bandwidth
between graph stores and workers. Second, they require extra development of
graph stores without well exploiting mature infrastructures such as MapReduce
that guarantee good system properties. Third, they focus on training but ignore
the optimization of inference over graphs, thus makes them an unintegrated
system.
  In this paper, we design AGL, a scalable, fault-tolerance and integrated
system, with fully-functional training and inference for GNNs. Our system
design follows the message passing scheme underlying the computations of GNNs.
We design to generate the $k$-hop neighborhood, an information-complete
subgraph for each node, as well as do the inference simply by merging values
from in-edge neighbors and propagating values to out-edge neighbors via
MapReduce. In addition, the $k$-hop neighborhood contains information-complete
subgraphs for each node, thus we simply do the training on parameter servers
due to data independency. Our system AGL, implemented on mature
infrastructures, can finish the training of a 2-layer graph attention network
on a graph with billions of nodes and hundred billions of edges in 14 hours,
and complete the inference in 1.2 hour.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.10853v1,2020-02-25T13:36:15Z,2020-02-25T13:36:15Z,Learning Machines from Simulation to Real World,"Learning Machines is developing a flexible, cross-industry, advanced
analytics platform, targeted during stealth-stage at a limited number of
specific vertical applications. In this paper, we aim to integrate a general
machine system to learn a variant of tasks from simulation to real world. In
such a machine system, it involves real-time robot vision, sensor fusion, and
learning algorithms (reinforcement learning). To this end, we demonstrate the
general machine system on three fundamental tasks including obstacle avoidance,
foraging, and predator-prey robot. The proposed solutions are implemented on
Robobo robots with mobile device (smartphone with camera) as interface and
built-in infrared (IR) sensors. The agent is trained in a virtual environment.
In order to assess its performance, the learned agent is tested in the virtual
environment and reproduce the same results in a real environment. The results
show that the reinforcement learning algorithm can be reliably used for a
variety of tasks in unknown environments.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.11637v1,2020-02-24T13:26:34Z,2020-02-24T13:26:34Z,Bio-inspired Optimization: metaheuristic algorithms for optimization,"In today's day and time solving real-world complex problems has become
fundamentally vital and critical task. Many of these are combinatorial
problems, where optimal solutions are sought rather than exact solutions.
Traditional optimization methods are found to be effective for small scale
problems. However, for real-world large scale problems, traditional methods
either do not scale up or fail to obtain optimal solutions or they end-up
giving solutions after a long running time. Even earlier artificial
intelligence based techniques used to solve these problems could not give
acceptable results. However, last two decades have seen many new methods in AI
based on the characteristics and behaviors of the living organisms in the
nature which are categorized as bio-inspired or nature inspired optimization
algorithms. These methods, are also termed meta-heuristic optimization methods,
have been proved theoretically and implemented using simulation as well used to
create many useful applications. They have been used extensively to solve many
industrial and engineering complex problems due to being easy to understand,
flexible, simple to adapt to the problem at hand and most importantly their
ability to come out of local optima traps. This local optima avoidance property
helps in finding global optimal solutions. This paper is aimed at understanding
how nature has inspired many optimization algorithms, basic categorization of
them, major bio-inspired optimization algorithms invented in recent time with
their applications.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.07443v1,2020-02-18T09:28:14Z,2020-02-18T09:28:14Z,"An Evaluation of Monte Carlo-Based Hyper-Heuristic for Interaction
  Testing of Industrial Embedded Software Applications","Hyper-heuristic is a new methodology for the adaptive hybridization of
meta-heuristic algorithms to derive a general algorithm for solving
optimization problems. This work focuses on the selection type of
hyper-heuristic, called the Exponential Monte Carlo with Counter (EMCQ).
Current implementations rely on the memory-less selection that can be
counterproductive as the selected search operator may not (historically) be the
best performing operator for the current search instance. Addressing this
issue, we propose to integrate the memory into EMCQ for combinatorial t-wise
test suite generation using reinforcement learning based on the Q-learning
mechanism, called Q-EMCQ. The limited application of combinatorial test
generation on industrial programs can impact the use of such techniques as
Q-EMCQ. Thus, there is a need to evaluate this kind of approach against
relevant industrial software, with a purpose to show the degree of interaction
required to cover the code as well as finding faults. We applied Q-EMCQ on 37
real-world industrial programs written in Function Block Diagram (FBD)
language, which is used for developing a train control management system at
Bombardier Transportation Sweden AB. The results of this study show that Q-EMCQ
is an efficient technique for test case generation. Additionally, unlike the
t-wise test suite generation, which deals with the minimization problem, we
have also subjected Q-EMCQ to a maximization problem involving the general
module clustering to demonstrate the effectiveness of our approach.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.04987v1,2020-02-17T18:48:55Z,2020-02-17T18:48:55Z,A Financial Service Chatbot based on Deep Bidirectional Transformers,"We develop a chatbot using Deep Bidirectional Transformer models (BERT) to
handle client questions in financial investment customer service. The bot can
recognize 381 intents, and decides when to say ""I don't know"" and escalates
irrelevant/uncertain questions to human operators. Our main novel contribution
is the discussion about uncertainty measure for BERT, where three different
approaches are systematically compared on real problems. We investigated two
uncertainty metrics, information entropy and variance of dropout sampling in
BERT, followed by mixed-integer programming to optimize decision thresholds.
Another novel contribution is the usage of BERT as a language model in
automatic spelling correction. Inputs with accidental spelling errors can
significantly decrease intent classification performance. The proposed approach
combines probabilities from masked language model and word edit distances to
find the best corrections for misspelled words. The chatbot and the entire
conversational AI system are developed using open-source tools, and deployed
within our company's intranet. The proposed approach can be useful for
industries seeking similar in-house solutions in their specific business
domains. We share all our code and a sample chatbot built on a public dataset
on Github.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.08333v1,2020-02-11T15:32:28Z,2020-02-11T15:32:28Z,"Towards Intelligent Pick and Place Assembly of Individualized Products
  Using Reinforcement Learning","Individualized manufacturing is becoming an important approach as a means to
fulfill increasingly diverse and specific consumer requirements and
expectations. While there are various solutions to the implementation of the
manufacturing process, such as additive manufacturing, the subsequent automated
assembly remains a challenging task. As an approach to this problem, we aim to
teach a collaborative robot to successfully perform pick and place tasks by
implementing reinforcement learning. For the assembly of an individualized
product in a constantly changing manufacturing environment, the simulated
geometric and dynamic parameters will be varied. Using reinforcement learning
algorithms capable of meta-learning, the tasks will first be trained in
simulation. They will then be performed in a real-world environment where new
factors are introduced that were not simulated in training to confirm the
robustness of the algorithms. The robot will gain its input data from tactile
sensors, area scan cameras, and 3D cameras used to generate heightmaps of the
environment and the objects. The selection of machine learning algorithms and
hardware components as well as further research questions to realize the
outlined production scenario are the results of the presented work.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.02741v1,2020-02-07T12:41:28Z,2020-02-07T12:41:28Z,"Can't Boil This Frog: Robustness of Online-Trained Autoencoder-Based
  Anomaly Detectors to Adversarial Poisoning Attacks","In recent years, a variety of effective neural network-based methods for
anomaly and cyber attack detection in industrial control systems (ICSs) have
been demonstrated in the literature. Given their successful implementation and
widespread use, there is a need to study adversarial attacks on such detection
methods to better protect the systems that depend upon them. The extensive
research performed on adversarial attacks on image and malware classification
has little relevance to the physical system state prediction domain, which most
of the ICS attack detection systems belong to. Moreover, such detection systems
are typically retrained using new data collected from the monitored system,
thus the threat of adversarial data poisoning is significant, however this
threat has not yet been addressed by the research community. In this paper, we
present the first study focused on poisoning attacks on online-trained
autoencoder-based attack detectors. We propose two algorithms for generating
poison samples, an interpolation-based algorithm and a back-gradient
optimization-based algorithm, which we evaluate on both synthetic and
real-world ICS data. We demonstrate that the proposed algorithms can generate
poison samples that cause the target attack to go undetected by the autoencoder
detector, however the ability to poison the detector is limited to a small set
of attack types and magnitudes. When the poison-generating algorithms are
applied to the popular SWaT dataset, we show that the autoencoder detector
trained on the physical system state data is resilient to poisoning in the face
of all ten of the relevant attacks in the dataset. This finding suggests that
neural network-based attack detectors used in the cyber-physical domain are
more robust to poisoning than in other problem domains, such as malware
detection and image processing.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.01711v5,2021-12-29T19:18:42Z,2020-02-05T10:25:02Z,"Dynamic Causal Effects Evaluation in A/B Testing with a Reinforcement
  Learning Framework","A/B testing, or online experiment is a standard business strategy to compare
a new product with an old one in pharmaceutical, technological, and traditional
industries. Major challenges arise in online experiments of two-sided
marketplace platforms (e.g., Uber) where there is only one unit that receives a
sequence of treatments over time. In those experiments, the treatment at a
given time impacts current outcome as well as future outcomes. The aim of this
paper is to introduce a reinforcement learning framework for carrying A/B
testing in these experiments, while characterizing the long-term treatment
effects. Our proposed testing procedure allows for sequential monitoring and
online updating. It is generally applicable to a variety of treatment designs
in different industries. In addition, we systematically investigate the
theoretical properties (e.g., size and power) of our testing procedure.
Finally, we apply our framework to both simulated data and a real-world data
example obtained from a technological company to illustrate its advantage over
the current practice. A Python implementation of our test is available at
https://github.com/callmespring/CausalRL.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.05648v3,2020-04-26T04:59:52Z,2020-02-01T01:15:39Z,Politics of Adversarial Machine Learning,"In addition to their security properties, adversarial machine-learning
attacks and defenses have political dimensions. They enable or foreclose
certain options for both the subjects of the machine learning systems and for
those who deploy them, creating risks for civil liberties and human rights. In
this paper, we draw on insights from science and technology studies,
anthropology, and human rights literature, to inform how defenses against
adversarial attacks can be used to suppress dissent and limit attempts to
investigate machine learning systems. To make this concrete, we use real-world
examples of how attacks such as perturbation, model inversion, or membership
inference can be used for socially desirable ends. Although the predictions of
this analysis may seem dire, there is hope. Efforts to address human rights
concerns in the commercial spyware industry provide guidance for similar
measures to ensure ML systems serve democratic, not authoritarian ends",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.11610v1,2020-01-30T23:49:15Z,2020-01-30T23:49:15Z,"UAV Autonomous Localization using Macro-Features Matching with a CAD
  Model","Research in the field of autonomous Unmanned Aerial Vehicles (UAVs) has
significantly advanced in recent years, mainly due to their relevance in a
large variety of commercial, industrial, and military applications. However,
UAV navigation in GPS-denied environments continues to be a challenging problem
that has been tackled in recent research through sensor-based approaches. This
paper presents a novel offline, portable, real-time in-door UAV localization
technique that relies on macro-feature detection and matching. The proposed
system leverages the support of machine learning, traditional computer vision
techniques, and pre-existing knowledge of the environment. The main
contribution of this work is the real-time creation of a macro-feature
description vector from the UAV captured images which are simultaneously
matched with an offline pre-existing vector from a Computer-Aided Design (CAD)
model. This results in a quick UAV localization within the CAD model. The
effectiveness and accuracy of the proposed system were evaluated through
simulations and experimental prototype implementation. Final results reveal the
algorithm's low computational burden as well as its ease of deployment in
GPS-denied environments.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.10249v1,2020-01-28T10:30:20Z,2020-01-28T10:30:20Z,"Online LiDAR-SLAM for Legged Robots with Robust Registration and
  Deep-Learned Loop Closure","In this paper, we present a factor-graph LiDAR-SLAM system which incorporates
a state-of-the-art deeply learned feature-based loop closure detector to enable
a legged robot to localize and map in industrial environments. These facilities
can be badly lit and comprised of indistinct metallic structures, thus our
system uses only LiDAR sensing and was developed to run on the quadruped
robot's navigation PC. Point clouds are accumulated using an inertial-kinematic
state estimator before being aligned using ICP registration. To close loops we
use a loop proposal mechanism which matches individual segments between clouds.
We trained a descriptor offline to match these segments. The efficiency of our
method comes from carefully designing the network architecture to minimize the
number of parameters such that this deep learning method can be deployed in
real-time using only the CPU of a legged robot, a major contribution of this
work. The set of odometry and loop closure factors are updated using pose graph
optimization. Finally we present an efficient risk alignment prediction method
which verifies the reliability of the registrations. Experimental results at an
industrial facility demonstrated the robustness and flexibility of our system,
including autonomous following paths derived from the SLAM map.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.08855v1,2020-01-24T01:06:25Z,2020-01-24T01:06:25Z,"Privacy for All: Demystify Vulnerability Disparity of Differential
  Privacy against Membership Inference Attack","Machine learning algorithms, when applied to sensitive data, pose a potential
threat to privacy. A growing body of prior work has demonstrated that
membership inference attack (MIA) can disclose specific private information in
the training data to an attacker. Meanwhile, the algorithmic fairness of
machine learning has increasingly caught attention from both academia and
industry. Algorithmic fairness ensures that the machine learning models do not
discriminate a particular demographic group of individuals (e.g., black and
female people). Given that MIA is indeed a learning model, it raises a serious
concern if MIA ``fairly'' treats all groups of individuals equally. In other
words, whether a particular group is more vulnerable against MIA than the other
groups. This paper examines the algorithmic fairness issue in the context of
MIA and its defenses. First, for fairness evaluation, it formalizes the
notation of vulnerability disparity (VD) to quantify the difference of MIA
treatment on different demographic groups. Second, it evaluates VD on four
real-world datasets, and shows that VD indeed exists in these datasets. Third,
it examines the impacts of differential privacy, as a defense mechanism of MIA,
on VD. The results show that although DP brings significant change on VD, it
cannot eliminate VD completely. Therefore, fourth, it designs a new mitigation
algorithm named FAIRPICK to reduce VD. An extensive set of experimental results
demonstrate that FAIRPICK can effectively reduce VD for both with and without
the DP deployment.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.05703v1,2020-01-16T09:13:31Z,2020-01-16T09:13:31Z,"A Markerless Deep Learning-based 6 Degrees of Freedom PoseEstimation for
  with Mobile Robots using RGB Data","Augmented Reality has been subject to various integration efforts within
industries due to its ability to enhance human machine interaction and
understanding. Neural networks have achieved remarkable results in areas of
computer vision, which bear great potential to assist and facilitate an
enhanced Augmented Reality experience. However, most neural networks are
computationally intensive and demand huge processing power thus, are not
suitable for deployment on Augmented Reality devices. In this work we propose a
method to deploy state of the art neural networks for real time 3D object
localization on augmented reality devices. As a result, we provide a more
automated method of calibrating the AR devices with mobile robotic systems. To
accelerate the calibration process and enhance user experience, we focus on
fast 2D detection approaches which are extracting the 3D pose of the object
fast and accurately by using only 2D input. The results are implemented into an
Augmented Reality application for intuitive robot control and sensor data
visualization. For the 6D annotation of 2D images, we developed an annotation
tool, which is, to our knowledge, the first open source tool to be available.
We achieve feasible results which are generally applicable to any AR device
thus making this work promising for further research in combining high
demanding neural networks with Internet of Things devices.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.05375v1,2020-01-15T15:30:29Z,2020-01-15T15:30:29Z,"AAAI FSS-19: Human-Centered AI: Trustworthiness of AI Models and Data
  Proceedings","To facilitate the widespread acceptance of AI systems guiding decision-making
in real-world applications, it is key that solutions comprise trustworthy,
integrated human-AI systems. Not only in safety-critical applications such as
autonomous driving or medicine, but also in dynamic open world systems in
industry and government it is crucial for predictive models to be
uncertainty-aware and yield trustworthy predictions. Another key requirement
for deployment of AI at enterprise scale is to realize the importance of
integrating human-centered design into AI systems such that humans are able to
use systems effectively, understand results and output, and explain findings to
oversight committees.
  While the focus of this symposium was on AI systems to improve data quality
and technical robustness and safety, we welcomed submissions from broadly
defined areas also discussing approaches addressing requirements such as
explainable models, human trust and ethical aspects of AI.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.02103v1,2020-01-06T13:13:21Z,2020-01-06T13:13:21Z,Self learning robot using real-time neural networks,"With the advancements in high volume, low precision computational technology
and applied research on cognitive artificially intelligent heuristic systems,
machine learning solutions through neural networks with real-time learning has
seen an immense interest in the research community as well the industry. This
paper involves research, development and experimental analysis of a neural
network implemented on a robot with an arm through which evolves to learn to
walk in a straight line or as required. The neural network learns using the
algorithms of Gradient Descent and Backpropagation. Both the implementation and
training of the neural network is done locally on the robot on a raspberry pi 3
so that its learning process is completely independent. The neural network is
first tested on a custom simulator developed on MATLAB and then implemented on
the raspberry computer. Data at each generation of the evolving network is
stored, and analysis both mathematical and graphical is done on the data.
Impact of factors like the learning rate and error tolerance on the learning
process and final output is analyzed.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1912.12397v1,2019-12-28T04:05:15Z,2019-12-28T04:05:15Z,"Natural language processing of MIMIC-III clinical notes for identifying
  diagnosis and procedures with neural networks","Coding diagnosis and procedures in medical records is a crucial process in
the healthcare industry, which includes the creation of accurate billings,
receiving reimbursements from payers, and creating standardized patient care
records. In the United States, Billing and Insurance related activities cost
around $471 billion in 2012 which constitutes about 25% of all the U.S hospital
spending. In this paper, we report the performance of a natural language
processing model that can map clinical notes to medical codes, and predict
final diagnosis from unstructured entries of history of present illness,
symptoms at the time of admission, etc. Previous studies have demonstrated that
deep learning models perform better at such mapping when compared to
conventional machine learning models. Therefore, we employed state-of-the-art
deep learning method, ULMFiT on the largest emergency department clinical notes
dataset MIMIC III which has 1.2M clinical notes to select for the top-10 and
top-50 diagnosis and procedure codes. Our models were able to predict the
top-10 diagnoses and procedures with 80.3% and 80.5% accuracy, whereas the
top-50 ICD-9 codes of diagnosis and procedures are predicted with 70.7% and
63.9% accuracy. Prediction of diagnosis and procedures from unstructured
clinical notes benefit human coders to save time, eliminate errors and minimize
costs. With promising scores from our present model, the next step would be to
deploy this on a small-scale real-world scenario and compare it with human
coders as the gold standard. We believe that further research of this approach
can create highly accurate predictions that can ease the workflow in a clinical
setting.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1912.11066v1,2019-12-23T19:11:50Z,2019-12-23T19:11:50Z,"FisheyeMultiNet: Real-time Multi-task Learning Architecture for
  Surround-view Automated Parking System","Automated Parking is a low speed manoeuvring scenario which is quite
unstructured and complex, requiring full 360{\deg} near-field sensing around
the vehicle. In this paper, we discuss the design and implementation of an
automated parking system from the perspective of camera based deep learning
algorithms. We provide a holistic overview of an industrial system covering the
embedded system, use cases and the deep learning architecture. We demonstrate a
real-time multi-task deep learning network called FisheyeMultiNet, which
detects all the necessary objects for parking on a low-power embedded system.
FisheyeMultiNet runs at 15 fps for 4 cameras and it has three tasks namely
object detection, semantic segmentation and soiling detection. To encourage
further research, we release a partial dataset of 5,000 images containing
semantic segmentation and bounding box detection ground truth via WoodScape
project \cite{yogamani2019woodscape}.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1912.04900v1,2019-12-10T13:26:53Z,2019-12-10T13:26:53Z,Datamorphic Testing: A Methodology for Testing AI Applications,"With the rapid growth of the applications of machine learning (ML) and other
artificial intelligence (AI) techniques, adequate testing has become a
necessity to ensure their quality. This paper identifies the characteristics of
AI applications that distinguish them from traditional software, and analyses
the main difficulties in applying existing testing methods. Based on this
analysis, we propose a new method called datamorphic testing and illustrate the
method with an example of testing face recognition applications. We also report
an experiment with four real industrial application systems of face recognition
to validate the proposed approach.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1912.03618v2,2020-06-06T03:56:29Z,2019-12-08T05:12:17Z,Efficient Black-box Assessment of Autonomous Vehicle Safety,"While autonomous vehicle (AV) technology has shown substantial progress, we
still lack tools for rigorous and scalable testing. Real-world testing, the
$\textit{de-facto}$ evaluation method, is dangerous to the public. Moreover,
due to the rare nature of failures, billions of miles of driving are needed to
statistically validate performance claims. Thus, the industry has largely
turned to simulation to evaluate AV systems. However, having a simulation stack
alone is not a solution. A simulation testing framework needs to prioritize
which scenarios to run, learn how the chosen scenarios provide coverage of
failure modes, and rank failure scenarios in order of importance. We implement
a simulation testing framework that evaluates an entire modern AV system as a
black box. This framework estimates the probability of accidents under a base
distribution governing standard traffic behavior. In order to accelerate
rare-event probability evaluation, we efficiently learn to identify and rank
failure scenarios via adaptive importance-sampling methods. Using this
framework, we conduct the first independent evaluation of a full-stack
commercial AV system, Comma AI's OpenPilot.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1912.02059v2,2020-06-28T20:20:33Z,2019-12-04T15:32:53Z,"Learning to Dynamically Coordinate Multi-Robot Teams in Graph Attention
  Networks","Increasing interest in integrating advanced robotics within manufacturing has
spurred a renewed concentration in developing real-time scheduling solutions to
coordinate human-robot collaboration in this environment. Traditionally, the
problem of scheduling agents to complete tasks with temporal and spatial
constraints has been approached either with exact algorithms, which are
computationally intractable for large-scale, dynamic coordination, or
approximate methods that require domain experts to craft heuristics for each
application. We seek to overcome the limitations of these conventional methods
by developing a novel graph attention network formulation to automatically
learn features of scheduling problems to allow their deployment. To learn
effective policies for combinatorial optimization problems via machine
learning, we combine imitation learning on smaller problems with deep
Q-learning on larger problems, in a non-parametric framework, to allow for
fast, near-optimal scheduling of robot teams. We show that our network-based
policy finds at least twice as many solutions over prior state-of-the-art
methods in all testing scenarios.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.10290v1,2019-11-23T01:11:29Z,2019-11-23T01:11:29Z,Scalable sim-to-real transfer of soft robot designs,"The manual design of soft robots and their controllers is notoriously
challenging, but it could be augmented---or, in some cases, entirely
replaced---by automated design tools. Machine learning algorithms can
automatically propose, test, and refine designs in simulation, and the most
promising ones can then be manufactured in reality (sim2real). However, it is
currently not known how to guarantee that behavior generated in simulation can
be preserved when deployed in reality. Although many previous studies have
devised training protocols that facilitate sim2real transfer of control
polices, little to no work has investigated the simulation-reality gap as a
function of morphology. This is due in part to an overall lack of tools capable
of systematically designing and rapidly manufacturing robots. Here we introduce
a low cost, open source, and modular soft robot design and construction kit,
and use it to simulate, fabricate, and measure the simulation-reality gap of
minimally complex yet soft, locomoting machines. We prove the scalability of
this approach by transferring an order of magnitude more robot designs from
simulation to reality than any other method. The kit and its instructions can
be found here: https://github.com/skriegman/sim2real4designs",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.08090v1,2019-11-19T04:33:05Z,2019-11-19T04:33:05Z,Deep Detector Health Management under Adversarial Campaigns,"Machine learning models are vulnerable to adversarial inputs that induce
seemingly unjustifiable errors. As automated classifiers are increasingly used
in industrial control systems and machinery, these adversarial errors could
grow to be a serious problem. Despite numerous studies over the past few years,
the field of adversarial ML is still considered alchemy, with no practical
unbroken defenses demonstrated to date, leaving PHM practitioners with few
meaningful ways of addressing the problem. We introduce turbidity detection as
a practical superset of the adversarial input detection problem, coping with
adversarial campaigns rather than statistically invisible one-offs. This
perspective is coupled with ROC-theoretic design guidance that prescribes an
inexpensive domain adaptation layer at the output of a deep learning model
during an attack campaign. The result aims to approximate the Bayes optimal
mitigation that ameliorates the detection model's degraded health. A
proactively reactive type of prognostics is achieved via Monte Carlo simulation
of various adversarial campaign scenarios, by sampling from the model's own
turbidity distribution to quickly deploy the correct mitigation during a
real-world campaign.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.06633v1,2019-11-15T13:50:27Z,2019-11-15T13:50:27Z,"HealthFog: An Ensemble Deep Learning based Smart Healthcare System for
  Automatic Diagnosis of Heart Diseases in Integrated IoT and Fog Computing
  Environments","Cloud computing provides resources over the Internet and allows a plethora of
applications to be deployed to provide services for different industries. The
major bottleneck being faced currently in these cloud frameworks is their
limited scalability and hence inability to cater to the requirements of
centralized Internet of Things (IoT) based compute environments. The main
reason for this is that latency-sensitive applications like health monitoring
and surveillance systems now require computation over large amounts of data
(Big Data) transferred to centralized database and from database to cloud data
centers which leads to drop in performance of such systems. The new paradigms
of fog and edge computing provide innovative solutions by bringing resources
closer to the user and provide low latency and energy-efficient solutions for
data processing compared to cloud domains. Still, the current fog models have
many limitations and focus from a limited perspective on either accuracy of
results or reduced response time but not both. We proposed a novel framework
called HealthFog for integrating ensemble deep learning in Edge computing
devices and deployed it for a real-life application of automatic Heart Disease
analysis. HealthFog delivers healthcare as a fog service using IoT devices and
efficiently manages the data of heart patients, which comes as user requests.
Fog-enabled cloud framework, FogBus is used to deploy and test the performance
of the proposed model in terms of power consumption, network bandwidth,
latency, jitter, accuracy and execution time. HealthFog is configurable to
various operation modes that provide the best Quality of Service or prediction
accuracy, as required, in diverse fog computation scenarios and for different
user requirements.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.05771v1,2019-11-13T19:25:53Z,2019-11-13T19:25:53Z,"Machine Learning Based Network Vulnerability Analysis of Industrial
  Internet of Things","It is critical to secure the Industrial Internet of Things (IIoT) devices
because of potentially devastating consequences in case of an attack. Machine
learning and big data analytics are the two powerful leverages for analyzing
and securing the Internet of Things (IoT) technology. By extension, these
techniques can help improve the security of the IIoT systems as well. In this
paper, we first present common IIoT protocols and their associated
vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the
utilization of machine learning in countering these susceptibilities. Following
that, a literature review of the available intrusion detection solutions using
machine learning models is presented. Finally, we discuss our case study, which
includes details of a real-world testbed that we have built to conduct
cyber-attacks and to design an intrusion detection system (IDS). We deploy
backdoor, command injection, and Structured Query Language (SQL) injection
attacks against the system and demonstrate how a machine learning based anomaly
detection system can perform well in detecting these attacks. We have evaluated
the performance through representative metrics to have a fair point of view on
the effectiveness of the methods.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.04099v1,2019-11-11T06:26:13Z,2019-11-11T06:26:13Z,"Beyond Similarity: Relation Embedding with Dual Attentions for
  Item-based Recommendation","Given the effectiveness and ease of use, Item-based Collaborative Filtering
(ICF) methods have been broadly used in industry in recent years. The key of
ICF lies in the similarity measurement between items, which however is a
coarse-grained numerical value that can hardly capture users' fine-grained
preferences toward different latent aspects of items from a representation
learning perspective. In this paper, we propose a model called REDA (latent
Relation Embedding with Dual Attentions) to address this challenge. REDA is
essentially a deep learning based recommendation method that employs an item
relation embedding scheme through a neural network structure for inter-item
relations representation. A relational user embedding is then proposed by
aggregating the relation embeddings between all purchased items of a user,
which not only better characterizes user preferences but also alleviates the
data sparsity problem. Moreover, to capture valid meta-knowledge that reflects
users' desired latent aspects and meanwhile suppress their explosive growth
towards overfitting, we further propose a dual attentions mechanism, including
a memory attention and a weight attention. A relation-wise optimization method
is finally developed for model inference by constructing a personalized ranking
loss for item relations. Extensive experiments are implemented on real-world
datasets and the proposed model is shown to greatly outperform state-of-the-art
methods, especially when the data is sparse.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.02987v1,2019-11-08T02:13:21Z,2019-11-08T02:13:21Z,The Pitfall of Evaluating Performance on Emerging AI Accelerators,"In recent years, domain-specific hardware has brought significant performance
improvements in deep learning (DL). Both industry and academia only focus on
throughput when evaluating these AI accelerators, which usually are custom
ASICs deployed in datacenter to speed up the inference phase of DL workloads.
Pursuing higher hardware throughput such as OPS (Operation Per Second) using
various optimizations seems to be their main design target. However, they
ignore the importance of accuracy in the DL nature. Motivated by this, this
paper argue that a single throughput metric can not comprehensively reflect the
real-world performance of AI accelerators. To reveal this pitfall, we evaluates
several frequently-used optimizations on a typical AI accelerator and
quantifies their impact on accuracy and throughout under representative DL
inference workloads. Based on our experimental results, we find that some
optimizations cause significant loss on accuracy in some workloads, although it
can improves the throughout. Furthermore, our results show the importance of
end-to-end evaluation in DL.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.05726v2,2020-05-14T22:57:58Z,2019-11-05T16:04:45Z,"Cyber Risk at the Edge: Current and future trends on Cyber Risk
  Analytics and Artificial Intelligence in the Industrial Internet of Things
  and Industry 4.0 Supply Chains","Digital technologies have changed the way supply chain operations are
structured. In this article, we conduct systematic syntheses of literature on
the impact of new technologies on supply chains and the related cyber risks. A
taxonomic/cladistic approach is used for the evaluations of progress in the
area of supply chain integration in the Industrial Internet of Things and
Industry 4.0, with a specific focus on the mitigation of cyber risks. An
analytical framework is presented, based on a critical assessment with respect
to issues related to new types of cyber risk and the integration of supply
chains with new technologies. This paper identifies a dynamic and self-adapting
supply chain system supported with Artificial Intelligence and Machine Learning
(AI/ML) and real-time intelligence for predictive cyber risk analytics. The
system is integrated into a cognition engine that enables predictive cyber risk
analytics with real-time intelligence from IoT networks at the edge. This
enhances capacities and assist in the creation of a comprehensive understanding
of the opportunities and threats that arise when edge computing nodes are
deployed, and when AI/ML technologies are migrated to the periphery of IoT
networks.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.12695v2,2019-11-25T17:07:00Z,2019-10-28T14:11:26Z,AI Ethics in Industry: A Research Framework,"Artificial Intelligence (AI) systems exert a growing influence on our
society. As they become more ubiquitous, their potential negative impacts also
become evident through various real-world incidents. Following such early
incidents, academic and public discussion on AI ethics has highlighted the need
for implementing ethics in AI system development. However, little currently
exists in the way of frameworks for understanding the practical implementation
of AI ethics. In this paper, we discuss a research framework for implementing
AI ethics in industrial settings. The framework presents a starting point for
empirical studies into AI ethics but is still being developed further based on
its practical utilization.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.11779v1,2019-10-25T15:03:11Z,2019-10-25T15:03:11Z,"Toward a better trade-off between performance and fairness with
  kernel-based distribution matching","As recent literature has demonstrated how classifiers often carry unintended
biases toward some subgroups, deploying machine learned models to users demands
careful consideration of the social consequences. How should we address this
problem in a real-world system? How should we balance core performance and
fairness metrics? In this paper, we introduce a MinDiff framework for
regularizing classifiers toward different fairness metrics and analyze a
technique with kernel-based statistical dependency tests. We run a thorough
study on an academic dataset to compare the Pareto frontier achieved by
different regularization approaches, and apply our kernel-based method to two
large-scale industrial systems demonstrating real-world improvements.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.03060v1,2019-10-07T20:06:38Z,2019-10-07T20:06:38Z,Impact of Inference Accelerators on hardware selection,"As opportunities for AI-assisted healthcare grow steadily, model deployment
faces challenges due to the specific characteristics of the industry. The
configuration choice for a production device can impact model performance while
influencing operational costs. Moreover, in healthcare some situations might
require fast, but not real time, inference. We study different configurations
and conduct a cost-performance analysis to determine the optimized hardware for
the deployment of a model subject to healthcare domain constraints. We observe
that a naive performance comparison may not lead to an optimal configuration
selection. In fact, given realistic domain constraints, CPU execution might be
preferable to GPU accelerators. Hence, defining beforehand precise expectations
for model deployment is crucial.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.02078v4,2020-08-13T12:21:07Z,2019-10-04T16:43:06Z,"I'm sorry Dave, I'm afraid I can't do that, Deep Q-learning from
  forbidden action","The use of Reinforcement Learning (RL) is still restricted to simulation or
to enhance human-operated systems through recommendations. Real-world
environments (e.g. industrial robots or power grids) are generally designed
with safety constraints in mind implemented in the shape of valid actions masks
or contingency controllers. For example, the range of motion and the angles of
the motors of a robot can be limited to physical boundaries. Violating
constraints thus results in rejected actions or entering in a safe mode driven
by an external controller, making RL agents incapable of learning from their
mistakes. In this paper, we propose a simple modification of a state-of-the-art
deep RL algorithm (DQN), enabling learning from forbidden actions. To do so,
the standard Q-learning update is enhanced with an extra safety loss inspired
by structured classification. We empirically show that it reduces the number of
hit constraints during the learning phase and accelerates convergence to
near-optimal policies compared to using standard DQN. Experiments are done on a
Visual Grid World Environment and Text-World domain.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.13343v2,2019-10-01T16:06:39Z,2019-09-29T19:15:08Z,"ISTHMUS: Secure, Scalable, Real-time and Robust Machine Learning
  Platform for Healthcare","In recent times, machine learning (ML) and artificial intelligence (AI) based
systems have evolved and scaled across different industries such as finance,
retail, insurance, energy utilities, etc. Among other things, they have been
used to predict patterns of customer behavior, to generate pricing models, and
to predict the return on investments. But the successes in deploying machine
learning models at scale in those industries have not translated into the
healthcare setting. There are multiple reasons why integrating ML models into
healthcare has not been widely successful, but from a technical perspective,
general-purpose commercial machine learning platforms are not a good fit for
healthcare due to complexities in handling data quality issues, mandates to
demonstrate clinical relevance, and a lack of ability to monitor performance in
a highly regulated environment with stringent security and privacy needs. In
this paper, we describe Isthmus, a turnkey, cloud-based platform which
addresses the challenges above and reduces time to market for operationalizing
ML/AI in healthcare. Towards the end, we describe three case studies which shed
light on Isthmus capabilities. These include (1) supporting an end-to-end
lifecycle of a model which predicts trauma survivability at hospital trauma
centers, (2) bringing in and harmonizing data from disparate sources to create
a community data platform for inferring population as well as patient level
insights for Social Determinants of Health (SDoH), and (3) ingesting
live-streaming data from various IoT sensors to build models, which can
leverage real-time and longitudinal information to make advanced time-sensitive
predictions.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.10976v1,2019-09-24T14:58:07Z,2019-09-24T14:58:07Z,"Synthetic dataset generation for object-to-model deep learning in
  industrial applications","The availability of large image data sets has been a crucial factor in the
success of deep learning-based classification and detection methods. While data
sets for everyday objects are widely available, data for specific industrial
use-cases (e.g. identifying packaged products in a warehouse) remains scarce.
In such cases, the data sets have to be created from scratch, placing a crucial
bottleneck on the deployment of deep learning techniques in industrial
applications.
  We present work carried out in collaboration with a leading UK online
supermarket, with the aim of creating a computer vision system capable of
detecting and identifying unique supermarket products in a warehouse setting.
To this end, we demonstrate a framework for using synthetic data to create an
end-to-end deep learning pipeline, beginning with real-world objects and
culminating in a trained model.
  Our method is based on the generation of a synthetic dataset from 3D models
obtained by applying photogrammetry techniques to real-world objects. Using
100k synthetic images generated from 60 real images per class, an InceptionV3
convolutional neural network (CNN) was trained, which achieved classification
accuracy of 95.8% on a separately acquired test set of real supermarket product
images. The image generation process supports automatic pixel annotation. This
eliminates the prohibitively expensive manual annotation typically required for
detection tasks. Based on this readily available data, a one-stage RetinaNet
detector was trained on the synthetic, annotated images to produce a detector
that can accurately localize and classify the specimen products in real-time.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.10270v1,2019-09-23T10:37:59Z,2019-09-23T10:37:59Z,"Pose Estimation for Texture-less Shiny Objects in a Single RGB Image
  Using Synthetic Training Data","In the industrial domain, the pose estimation of multiple texture-less shiny
parts is a valuable but challenging task. In this particular scenario, it is
impractical to utilize keypoints or other texture information because most of
them are not actual features of the target but the reflections of surroundings.
Moreover, the similarity of color also poses a challenge in segmentation. In
this article, we propose to divide the pose estimation process into three
stages: object detection, features detection and pose optimization. A
convolutional neural network was utilized to perform object detection.
Concerning the reliability of surface texture, we leveraged the contour
information for estimating pose. Since conventional contour-based methods are
inapplicable to clustered metal parts due to the difficulties in segmentation,
we use the dense discrete points along the metal part edges as semantic
keypoints for contour detection. Afterward, we exploit both keypoint
information and CAD model to calculate the 6D pose of each object in view. A
typical implementation of deep learning methods not only requires a large
amount of training data, but also relies on intensive human labor for labeling
the datasets. Therefore, we propose an approach to generate datasets and label
them automatically. Despite not using any real-world photos for training, a
series of experiments showed that the algorithm built on synthetic data perform
well in the real environment.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.10233v1,2019-09-23T09:09:12Z,2019-09-23T09:09:12Z,Machine Learning Optimization Algorithms & Portfolio Allocation,"Portfolio optimization emerged with the seminal paper of Markowitz (1952).
The original mean-variance framework is appealing because it is very efficient
from a computational point of view. However, it also has one well-established
failing since it can lead to portfolios that are not optimal from a financial
point of view. Nevertheless, very few models have succeeded in providing a real
alternative solution to the Markowitz model. The main reason lies in the fact
that most academic portfolio optimization models are intractable in real life
although they present solid theoretical properties. By intractable we mean that
they can be implemented for an investment universe with a small number of
assets using a lot of computational resources and skills, but they are unable
to manage a universe with dozens or hundreds of assets. However, the emergence
and the rapid development of robo-advisors means that we need to rethink
portfolio optimization and go beyond the traditional mean-variance optimization
approach. Another industry has faced similar issues concerning large-scale
optimization problems. Machine learning has long been associated with linear
and logistic regression models. Again, the reason was the inability of
optimization algorithms to solve high-dimensional industrial problems.
Nevertheless, the end of the 1990s marked an important turning point with the
development and the rediscovery of several methods that have since produced
impressive results. The goal of this paper is to show how portfolio allocation
can benefit from the development of these large-scale optimization algorithms.
Not all of these algorithms are useful in our case, but four of them are
essential when solving complex portfolio optimization problems. These four
algorithms are the coordinate descent, the alternating direction method of
multipliers, the proximal gradient method and the Dykstra's algorithm.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.06727v1,2019-09-15T04:01:39Z,2019-09-15T04:01:39Z,"An Empirical Study towards Characterizing Deep Learning Development and
  Deployment across Different Frameworks and Platforms","Deep Learning (DL) has recently achieved tremendous success. A variety of DL
frameworks and platforms play a key role to catalyze such progress. However,
the differences in architecture designs and implementations of existing
frameworks and platforms bring new challenges for DL software development and
deployment. Till now, there is no study on how various mainstream frameworks
and platforms influence both DL software development and deployment in
practice. To fill this gap, we take the first step towards understanding how
the most widely-used DL frameworks and platforms support the DL software
development and deployment. We conduct a systematic study on these frameworks
and platforms by using two types of DNN architectures and three popular
datasets. (1) For development process, we investigate the prediction accuracy
under the same runtime training configuration or same model weights/biases. We
also study the adversarial robustness of trained models by leveraging the
existing adversarial attack techniques. The experimental results show that the
computing differences across frameworks could result in an obvious prediction
accuracy decline, which should draw the attention of DL developers. (2) For
deployment process, we investigate the prediction accuracy and performance
(refers to time cost and memory consumption) when the trained models are
migrated/quantized from PC to real mobile devices and web browsers. The DL
platform study unveils that the migration and quantization still suffer from
compatibility and reliability issues. Meanwhile, we find several DL software
bugs by using the results as a benchmark. We further validate the results
through bug confirmation from stakeholders and industrial positive feedback to
highlight the implications of our study. Through our study, we summarize
practical guidelines, identify challenges and pinpoint new research directions.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.02129v1,2019-09-04T21:50:03Z,2019-09-04T21:50:03Z,"Towards Precise Robotic Grasping by Probabilistic Post-grasp
  Displacement Estimation","Precise robotic grasping is important for many industrial applications, such
as assembly and palletizing, where the location of the object needs to be
controlled and known. However, achieving precise grasps is challenging due to
noise in sensing and control, as well as unknown object properties. We propose
a method to plan robotic grasps that are both robust and precise by training
two convolutional neural networks - one to predict the robustness of a grasp
and another to predict a distribution of post-grasp object displacements. Our
networks are trained with depth images in simulation on a dataset of over 1000
industrial parts and were successfully deployed on a real robot without having
to be further fine-tuned. The proposed displacement estimator achieves a mean
prediction errors of 0.68cm and 3.42deg on novel objects in real world
experiments.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.11863v1,2019-08-30T17:48:05Z,2019-08-30T17:48:05Z,Systematic Analysis of Image Generation using GANs,"Generative Adversarial Networks have been crucial in the developments made in
unsupervised learning in recent times. Exemplars of image synthesis from text
or other images, these networks have shown remarkable improvements over
conventional methods in terms of performance. Trained on the adversarial
training philosophy, these networks aim to estimate the potential distribution
from the real data and then use this as input to generate the synthetic data.
Based on this fundamental principle, several frameworks can be generated that
are paragon implementations in several real-life applications such as art
synthesis, generation of high resolution outputs and synthesis of images from
human drawn sketches, to name a few. While theoretically GANs present better
results and prove to be an improvement over conventional methods in many
factors, the implementation of these frameworks for dedicated applications
remains a challenge. This study explores and presents a taxonomy of these
frameworks and their use in various image to image synthesis and text to image
synthesis applications. The basic GANs, as well as a variety of different niche
frameworks, are critically analyzed. The advantages of GANs for image
generation over conventional methods as well their disadvantages amongst other
frameworks are presented. The future applications of GANs in industries such as
healthcare, art and entertainment are also discussed.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.10001v1,2019-08-27T03:13:53Z,2019-08-27T03:13:53Z,Real-world Conversational AI for Hotel Bookings,"In this paper, we present a real-world conversational AI system to search for
and book hotels through text messaging. Our architecture consists of a
frame-based dialogue management system, which calls machine learning models for
intent classification, named entity recognition, and information retrieval
subtasks. Our chatbot has been deployed on a commercial scale, handling tens of
thousands of hotel searches every day. We describe the various opportunities
and challenges of developing a chatbot in the travel industry.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.08998v2,2019-10-23T14:39:47Z,2019-08-13T10:15:39Z,AIBench: An Industry Standard Internet Service AI Benchmark Suite,"Today's Internet Services are undergoing fundamental changes and shifting to
an intelligent computing era where AI is widely employed to augment services.
In this context, many innovative AI algorithms, systems, and architectures are
proposed, and thus the importance of benchmarking and evaluating them rises.
However, modern Internet services adopt a microservice-based architecture and
consist of various modules. The diversity of these modules and complexity of
execution paths, the massive scale and complex hierarchy of datacenter
infrastructure, the confidential issues of data sets and workloads pose great
challenges to benchmarking. In this paper, we present the first
industry-standard Internet service AI benchmark suite---AIBench with seventeen
industry partners, including several top Internet service providers. AIBench
provides a highly extensible, configurable, and flexible benchmark framework
that contains loosely coupled modules. We identify sixteen prominent AI problem
domains like learning to rank, each of which forms an AI component benchmark,
from three most important Internet service domains: search engine, social
network, and e-commerce, which is by far the most comprehensive AI benchmarking
effort. On the basis of the AIBench framework, abstracting the real-world data
sets and workloads from one of the top e-commerce providers, we design and
implement the first end-to-end Internet service AI benchmark, which contains
the primary modules in the critical paths of an industry scale application and
is scalable to deploy on different cluster scales. The specifications, source
code, and performance numbers are publicly available from the benchmark council
web site http://www.benchcouncil.org/AIBench/index.html.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.02427v1,2019-08-07T03:04:38Z,2019-08-07T03:04:38Z,"Strengthening the Case for a Bayesian Approach to Car-following Model
  Calibration and Validation using Probabilistic Programming","Compute and memory constraints have historically prevented traffic simulation
software users from fully utilizing the predictive models underlying them. When
calibrating car-following models, particularly, accommodations have included 1)
using sensitivity analysis to limit the number of parameters to be calibrated,
and 2) identifying only one set of parameter values using data collected from
multiple car-following instances across multiple drivers. Shortcuts are further
motivated by insufficient data set sizes, for which a driver may have too few
instances to fully account for the variation in their driving behavior. In this
paper, we demonstrate that recent technological advances can enable
transportation researchers and engineers to overcome these constraints and
produce calibration results that 1) outperform industry standard approaches,
and 2) allow for a unique set of parameters to be estimated for each driver in
a data set, even given a small amount of data. We propose a novel calibration
procedure for car-following models based on Bayesian machine learning and
probabilistic programming, and apply it to real-world data from a naturalistic
driving study. We also discuss how this combination of mathematical and
software tools can offer additional benefits such as more informative model
validation and the incorporation of true-to-data uncertainty into simulation
traces.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.01862v1,2019-08-05T21:10:12Z,2019-08-05T21:10:12Z,Semi-Automatic Labeling for Deep Learning in Robotics,"In this paper, we propose Augmented Reality Semi-automatic labeling (ARS), a
semi-automatic method which leverages on moving a 2D camera by means of a
robot, proving precise camera tracking, and an augmented reality pen to define
initial object bounding box, to create large labeled datasets with minimal
human intervention. By removing the burden of generating annotated data from
humans, we make the Deep Learning technique applied to computer vision, that
typically requires very large datasets, truly automated and reliable. With the
ARS pipeline, we created effortlessly two novel datasets, one on
electromechanical components (industrial scenario) and one on fruits
(daily-living scenario), and trained robustly two state-of-the-art object
detectors, based on convolutional neural networks, such as YOLO and SSD. With
respect to the conventional manual annotation of 1000 frames that takes us
slightly more than 10 hours, the proposed approach based on ARS allows
annotating 9 sequences of about 35000 frames in less than one hour, with a gain
factor of about 450. Moreover, both the precision and recall of object
detection is increased by about 15\% with respect to manual labeling. All our
software is available as a ROS package in a public repository alongside the
novel annotated datasets.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.02150v3,2019-10-22T02:23:42Z,2019-08-04T05:19:43Z,Industrial Artificial Intelligence,"Artificial Intelligence (AI) is a cognitive science to enables human to
explore many intelligent ways to model our sensing and reasoning processes.
Industrial AI is a systematic discipline to enable engineers to systematically
develop and deploy AI algorithms with repeating and consistent successes. In
this paper, the key enablers for this transformative technology along with
their significant advantages are discussed. In addition, this research explains
Lighthouse Factories as an emerging status applying to the top manufacturers
that have implemented Industrial AI in their manufacturing ecosystem and gained
significant financial benefits. It is believed that this research will work as
a guideline and roadmap for researchers and industries towards the real-world
implementation of Industrial AI.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.01853v1,2019-08-02T01:13:50Z,2019-08-02T01:13:50Z,DELTA: A DEep learning based Language Technology plAtform,"In this paper we present DELTA, a deep learning based language technology
platform. DELTA is an end-to-end platform designed to solve industry level
natural language and speech processing problems. It integrates most popular
neural network models for training as well as comprehensive deployment tools
for production. DELTA aims to provide easy and fast experiences for using,
deploying, and developing natural language processing and speech models for
both academia and industry use cases. We demonstrate the reliable performance
with DELTA on several natural language processing and speech tasks, including
text classification, named entity recognition, natural language inference,
speech recognition, speaker verification, etc. DELTA has been used for
developing several state-of-the-art algorithms for publications and delivering
real production to serve millions of users.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.10701v4,2019-10-22T06:07:55Z,2019-07-24T20:18:28Z,"Benchmarking TPU, GPU, and CPU Platforms for Deep Learning","Training deep learning models is compute-intensive and there is an
industry-wide trend towards hardware specialization to improve performance. To
systematically benchmark deep learning platforms, we introduce ParaDnn, a
parameterized benchmark suite for deep learning that generates end-to-end
models for fully connected (FC), convolutional (CNN), and recurrent (RNN)
neural networks. Along with six real-world models, we benchmark Google's Cloud
TPU v2/v3, NVIDIA's V100 GPU, and an Intel Skylake CPU platform. We take a deep
dive into TPU architecture, reveal its bottlenecks, and highlight valuable
lessons learned for future specialized system design. We also provide a
thorough comparison of the platforms and find that each has unique strengths
for some types of models. Finally, we quantify the rapid performance
improvements that specialized software stacks provide for the TPU and GPU
platforms.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.10554v2,2020-03-26T16:11:35Z,2019-07-24T16:47:47Z,"Development of a Real-time Indoor Location System using Bluetooth Low
  Energy Technology and Deep Learning to Facilitate Clinical Applications","An indoor, real-time location system (RTLS) can benefit both hospitals and
patients by improving clinical efficiency through data-driven optimization of
procedures. Bluetooth-based RTLS systems are cost-effective but lack accuracy
and robustness because Bluetooth signal strength is subject to fluctuation. We
developed a machine learning-based solution using a Long Short-Term Memory
(LSTM) network followed by a Multilayer Perceptron classifier and a posterior
constraint algorithm to improve RTLS performance. Training and validation
datasets showed that most machine learning models perform well in classifying
individual location zones, although LSTM was most reliable. However, when faced
with data indicating cross-zone trajectories, all models showed erratic zone
switching. Thus, we implemented a history-based posterior constraint algorithm
to reduce the variability in exchange for a slight decrease in responsiveness.
This network increases robustness at the expense of latency. When latency is
less of a concern, we computed the latency-corrected accuracy which is 100% for
our testing data, significantly improved from LSTM without constraint which is
96.2%. The balance between robustness and responsiveness can be considered and
adjusted on a case-by-case basis, according to the specific needs of downstream
clinical applications. This system was deployed and validated in an academic
medical center. Industry best practices enabled system scaling without
substantial compromises to performance or cost.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.09511v1,2019-07-22T18:17:56Z,2019-07-22T18:17:56Z,Universal Person Re-Identification,"Most state-of-the-art person re-identification (re-id) methods depend on
supervised model learning with a large set of cross-view identity labelled
training data. Even worse, such trained models are limited to only the
same-domain deployment with significantly degraded cross-domain generalization
capability, i.e. ""domain specific"". To solve this limitation, there are a
number of recent unsupervised domain adaptation and unsupervised learning
methods that leverage unlabelled target domain training data. However, these
methods need to train a separate model for each target domain as supervised
learning methods. This conventional ""{\em train once, run once}"" pattern is
unscalable to a large number of target domains typically encountered in
real-world deployments. We address this problem by presenting a ""train once,
run everywhere"" pattern industry-scale systems are desperate for. We formulate
a ""universal model learning' approach enabling domain-generic person re-id
using only limited training data of a ""{\em single}"" seed domain. Specifically,
we train a universal re-id deep model to discriminate between a set of
transformed person identity classes. Each of such classes is formed by applying
a variety of random appearance transformations to the images of that class,
where the transformations simulate the camera viewing conditions of any domains
for making the model training domain generic. Extensive evaluations show the
superiority of our method for universal person re-id over a wide variety of
state-of-the-art unsupervised domain adaptation and unsupervised learning re-id
methods on five standard benchmarks: Market-1501, DukeMTMC, CUHK03, MSMT17, and
VIPeR.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.02797v2,2020-03-14T14:25:49Z,2019-07-03T16:37:48Z,"Predicting e-commerce customer conversion from minimal temporal patterns
  on symbolized clickstream trajectories","Knowing if a user is a buyer or window shopper solely based on clickstream
data is of crucial importance for e-commerce platforms seeking to implement
real-time accurate NBA (next best action) policies. However, due to the low
frequency of conversion events and the noisiness of browsing data, classifying
user sessions is very challenging. In this paper, we address the clickstream
classification problem in the eCommerce industry and present three major
contributions to the burgeoning field of AI-for-retail: first, we collected,
normalized and prepared a novel dataset of live shopping sessions from a major
European e-commerce website; second, we use the dataset to test in a controlled
environment strong baselines and SOTA models from the literature; finally, we
propose a new discriminative neural model that outperforms neural architectures
recently proposed at Rakuten labs.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.01882v2,2019-08-05T15:51:47Z,2019-07-03T12:26:38Z,An Experimental Evaluation of Large Scale GBDT Systems,"Gradient boosting decision tree (GBDT) is a widely-used machine learning
algorithm in both data analytic competitions and real-world industrial
applications. Further, driven by the rapid increase in data volume, efforts
have been made to train GBDT in a distributed setting to support large-scale
workloads. However, we find it surprising that the existing systems manage the
training dataset in different ways, but none of them have studied the impact of
data management. To that end, this paper aims to study the pros and cons of
different data management methods regarding the performance of distributed
GBDT. We first introduce a quadrant categorization of data management policies
based on data partitioning and data storage. Then we conduct an in-depth
systematic analysis and summarize the advantageous scenarios of the quadrants.
Based on the analysis, we further propose a novel distributed GBDT system named
Vero, which adopts the unexplored composition of vertical partitioning and
row-store and suits for many large-scale cases. To validate our analysis
empirically, we implement different quadrants in the same code base and compare
them under extensive workloads, and finally compare Vero with other
state-of-the-art systems over a wide range of datasets. Our theoretical and
experimental results provide a guideline on choosing a proper data management
policy for a given workload.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.00400v1,2019-06-30T15:42:53Z,2019-06-30T15:42:53Z,"Prediction is very hard, especially about conversion. Predicting user
  purchases from clickstream data in fashion e-commerce","Knowing if a user is a buyer vs window shopper solely based on clickstream
data is of crucial importance for ecommerce platforms seeking to implement
real-time accurate NBA (next best action) policies. However, due to the low
frequency of conversion events and the noisiness of browsing data, classifying
user sessions is very challenging. In this paper, we address the clickstream
classification problem in the fashion industry and present three major
contributions to the burgeoning field of AI in fashion: first, we collected,
normalized and prepared a novel dataset of live shopping sessions from a major
European e-commerce fashion website; second, we use the dataset to test in a
controlled environment strong baselines and SOTA models from the literature;
finally, we propose a new discriminative neural model that outperforms neural
architectures recently proposed at Rakuten labs.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1906.08834v2,2019-06-24T07:57:58Z,2019-06-20T20:30:39Z,"Deep Learning in the Automotive Industry: Recent Advances and
  Application Examples","One of the most exciting technology breakthroughs in the last few years has
been the rise of deep learning. State-of-the-art deep learning models are being
widely deployed in academia and industry, across a variety of areas, from image
analysis to natural language processing. These models have grown from fledgling
research subjects to mature techniques in real-world use. The increasing scale
of data, computational power and the associated algorithmic innovations are the
main drivers for the progress we see in this field. These developments also
have a huge potential for the automotive industry and therefore the interest in
deep learning-based technology is growing. A lot of the product innovations,
such as self-driving cars, parking and lane-change assist or safety functions,
such as autonomous emergency braking, are powered by deep learning algorithms.
Deep learning is poised to offer gains in performance and functionality for
most ADAS (Advanced Driver Assistance System) solutions. Virtual sensing for
vehicle dynamics application, vehicle inspection/heath monitoring, automated
driving and data-driven product development are key areas that are expected to
get the most attention. This article provides an overview of the recent
advances and some associated challenges in deep learning techniques in the
context of automotive applications.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1906.08864v1,2019-06-01T18:49:57Z,2019-06-01T18:49:57Z,"Accurate and Energy-Efficient Classification with Spiking Random Neural
  Network: Corrected and Expanded Version","Artificial Neural Network (ANN) based techniques have dominated
state-of-the-art results in most problems related to computer vision, audio
recognition, and natural language processing in the past few years, resulting
in strong industrial adoption from all leading technology companies worldwide.
One of the major obstacles that have historically delayed large scale adoption
of ANNs is the huge computational and power costs associated with training and
testing (deploying) them. In the mean-time, Neuromorphic Computing platforms
have recently achieved remarkable performance running more bio-realistic
Spiking Neural Networks at high throughput and very low power consumption
making them a natural alternative to ANNs. Here, we propose using the Random
Neural Network (RNN), a spiking neural network with both theoretical and
practical appealing properties, as a general purpose classifier that can match
the classification power of ANNs on a number of tasks while enjoying all the
features of a spiking neural network. This is demonstrated on a number of
real-world classification datasets.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1905.13118v1,2019-05-30T15:50:14Z,2019-05-30T15:50:14Z,"Standing on the Shoulders of Giants: AI-driven Calibration of
  Localisation Technologies","High accuracy localisation technologies exist but are prohibitively expensive
to deploy for large indoor spaces such as warehouses, factories, and
supermarkets to track assets and people. However, these technologies can be
used to lend their highly accurate localisation capabilities to low-cost,
commodity, and less-accurate technologies. In this paper, we bridge this link
by proposing a technology-agnostic calibration framework based on artificial
intelligence to assist such low-cost technologies through highly accurate
localisation systems. A single-layer neural network is used to calibrate less
accurate technology using more accurate one such as BLE using UWB and UWB using
a professional motion tracking system. On a real indoor testbed, we demonstrate
an increase in accuracy of approximately 70% for BLE and 50% for UWB. Not only
the proposed approach requires a very short measurement campaign, the low
complexity of the single-layer neural network also makes it ideal for
deployment on constrained devices typically for localisation purposes.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1905.12443v1,2019-05-28T09:20:31Z,2019-05-28T09:20:31Z,"Implementing SCADA Scenarios and Introducing Attacks to Obtain Training
  Data for Intrusion Detection Methods","There are hardly any data sets publicly available that can be used to
evaluate intrusion detection algorithms. The biggest threat for industrial
applications arises from state-sponsored and criminal groups. Often, formerly
unknown exploits are employed by these attackers, so-called 0-day exploits.
They cannot be discovered with signature-based intrusion detection. Thus,
statistical or machine learning based anomaly detection lends itself readily.
These methods especially, however, need a large amount of labelled training
data. In this work, an exemplary industrial use case with real-world industrial
hardware is presented. Siemens S7 Programmable Logic Controllers are used to
control a real world-based control application using the OPC UA protocol: A
pump, filling and emptying water tanks. This scenario is used to generate
application specific network data. Furthermore, attacks are introduced into
this data set. This is done in three ways: First, the normal process is
monitored and captured. Common attacks are then synthetically introduced into
this data set. Second, malicious behaviour is implemented on the Programmable
Logic Controller program and executed live, the traffic is captured as well.
Third, malicious behaviour is implemented on the Programmable Logic Controller
while still keeping the same output behaviour as in normal operation. An
attacker could exploit an application but forge valid sensor output so that no
anomaly is detected. Sensors are employed, capturing temperature, sound and
flow of water to create data that can be correlated to the network data and
used to still detect the attack. All data is labelled, containing the ground
truth, meaning all attacks are known and no unknown attacks occur. This makes
them perfect for training of anomaly detection algorithms. The data is
published to enable security researchers to evaluate intrusion detection
solutions.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1905.10090v1,2019-05-24T08:45:56Z,2019-05-24T08:45:56Z,Deploying AI Frameworks on Secure HPC Systems with Containers,"The increasing interest in the usage of Artificial Intelligence techniques
(AI) from the research community and industry to tackle ""real world"" problems,
requires High Performance Computing (HPC) resources to efficiently compute and
scale complex algorithms across thousands of nodes. Unfortunately, typical data
scientists are not familiar with the unique requirements and characteristics of
HPC environments. They usually develop their applications with high-level
scripting languages or frameworks such as TensorFlow and the installation
process often requires connection to external systems to download open source
software during the build. HPC environments, on the other hand, are often based
on closed source applications that incorporate parallel and distributed
computing API's such as MPI and OpenMP, while users have restricted
administrator privileges, and face security restrictions such as not allowing
access to external systems. In this paper we discuss the issues associated with
the deployment of AI frameworks in a secure HPC environment and how we
successfully deploy AI frameworks on SuperMUC-NG with Charliecloud.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1905.06004v1,2019-05-15T07:48:21Z,2019-05-15T07:48:21Z,Domain Adaptive Transfer Learning for Fault Diagnosis,"Thanks to digitization of industrial assets in fleets, the ambitious goal of
transferring fault diagnosis models fromone machine to the other has raised
great interest. Solving these domain adaptive transfer learning tasks has the
potential to save large efforts on manually labeling data and modifying models
for new machines in the same fleet. Although data-driven methods have shown
great potential in fault diagnosis applications, their ability to generalize on
new machines and new working conditions are limited because of their tendency
to overfit to the training set in reality. One promising solution to this
problem is to use domain adaptation techniques. It aims to improve model
performance on the target new machine. Inspired by its successful
implementation in computer vision, we introduced Domain-Adversarial Neural
Networks (DANN) to our context, along with two other popular methods existing
in previous fault diagnosis research. We then carefully justify the
applicability of these methods in realistic fault diagnosis settings, and offer
a unified experimental protocol for a fair comparison between domain adaptation
methods for fault diagnosis problems.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1905.05984v2,2019-05-16T14:08:41Z,2019-05-15T07:15:32Z,"Modern Problems Require Modern Solutions: Hybrid Concepts for Industrial
  Intrusion Detection","The concept of Industry 4.0 brings a disruption into the processing industry.
It is characterised by a high degree of intercommunication, embedded
computation, resulting in a decentralised and distributed handling of data.
Additionally, cloud-storage and Software-as-a-Service (SaaS) approaches enhance
a centralised storage and handling of data. This often takes place in
third-party networks. Furthermore, Industry 4.0 is driven by novel business
cases. Lot sizes of one, customer individual production, observation of process
state and progress in real-time and remote maintenance, just to name a few. All
of these new business cases make use of the novel technologies. However, cyber
security has not been an issue in industry. Industrial networks have been
considered physically separated from public networks. Additionally, the high
level of uniqueness of any industrial network was said to prevent attackers
from exploiting flaws. Those assumptions are inherently broken by the concept
of Industry 4.0. As a result, an abundance of attack vectors is created. In the
past, attackers have used those attack vectors in spectacular fashions.
Especially Small and Mediumsized Enterprises (SMEs) in Germany struggle to
adapt to these challenges. Reasons are the cost required for technical
solutions and security professionals. In order to enable SMEs to cope with the
growing threat in the cyberspace, the research project IUNO Insec aims at
providing and improving security solutions that can be used without specialised
security knowledge. The project IUNO Insec is briefly introduced in this work.
Furthermore, contributions in the field of intrusion detection, especially
machine learning-based solutions, for industrial environments provided by the
authors are presented and set into context.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1905.02616v2,2019-06-16T23:00:46Z,2019-05-07T14:40:32Z,"An Integrated Multi-Time-Scale Modeling for Solar Irradiance Forecasting
  Using Deep Learning","For short-term solar irradiance forecasting, the traditional point
forecasting methods are rendered less useful due to the non-stationary
characteristic of solar power. The amount of operating reserves required to
maintain reliable operation of the electric grid rises due to the variability
of solar energy. The higher the uncertainty in the generation, the greater the
operating-reserve requirements, which translates to an increased cost of
operation. In this research work, we propose a unified architecture for
multi-time-scale predictions for intra-day solar irradiance forecasting using
recurrent neural networks (RNN) and long-short-term memory networks (LSTMs).
This paper also lays out a framework for extending this modeling approach to
intra-hour forecasting horizons thus, making it a multi-time-horizon
forecasting approach, capable of predicting intra-hour as well as intra-day
solar irradiance. We develop an end-to-end pipeline to effectuate the proposed
architecture. The performance of the prediction model is tested and validated
by the methodical implementation. The robustness of the approach is
demonstrated with case studies conducted for geographically scattered sites
across the United States. The predictions demonstrate that our proposed unified
architecture-based approach is effective for multi-time-scale solar forecasts
and achieves a lower root-mean-square prediction error when benchmarked against
the best-performing methods documented in the literature that use separate
models for each time-scale during the day. Our proposed method results in a
71.5% reduction in the mean RMSE averaged across all the test sites compared to
the ML-based best-performing method reported in the literature. Additionally,
the proposed method enables multi-time-horizon forecasts with real-time inputs,
which have a significant potential for practical industry applications in the
evolving grid.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1904.13001v1,2019-04-30T00:24:06Z,2019-04-30T00:24:06Z,"Encoding Categorical Variables with Conjugate Bayesian Models for WeWork
  Lead Scoring Engine","Applied Data Scientists throughout various industries are commonly faced with
the challenging task of encoding high-cardinality categorical features into
digestible inputs for machine learning algorithms. This paper describes a
Bayesian encoding technique developed for WeWork's lead scoring engine which
outputs the probability of a person touring one of our office spaces based on
interaction, enrichment, and geospatial data. We present a paradigm for
ensemble modeling which mitigates the need to build complicated preprocessing
and encoding schemes for categorical variables. In particular, domain-specific
conjugate Bayesian models are employed as base learners for features in a
stacked ensemble model. For each column of a categorical feature matrix we fit
a problem-specific prior distribution, for example, the Beta distribution for a
binary classification problem. In order to analytically derive the moments of
the posterior distribution, we update the prior with the conjugate likelihood
of the corresponding target variable for each unique value of the given
categorical feature. This function of column and value encodes the categorical
feature matrix so that the final learner in the ensemble model ingests
low-dimensional numerical input. Experimental results on both curated and real
world datasets demonstrate impressive accuracy and computational efficiency on
a variety of problem archetypes. Particularly, for the lead scoring engine at
WeWork -- where some categorical features have as many as 300,000 levels -- we
have seen an AUC improvement from 0.87 to 0.97 through implementing conjugate
Bayesian model encoding.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1904.01719v1,2019-04-03T00:56:14Z,2019-04-03T00:56:14Z,"Empirical Evaluations of Active Learning Strategies in Legal Document
  Review","One type of machine learning, text classification, is now regularly applied
in the legal matters involving voluminous document populations because it can
reduce the time and expense associated with the review of those documents. One
form of machine learning - Active Learning - has drawn attention from the legal
community because it offers the potential to make the machine learning process
even more effective. Active Learning, applied to legal documents, is considered
a new technology in the legal domain and is continuously applied to all
documents in a legal matter until an insignificant number of relevant documents
are left for review. This implementation is slightly different than traditional
implementations of Active Learning where the process stops once achieving
acceptable model performance. The purpose of this paper is twofold: (i) to
question whether Active Learning actually is a superior learning methodology
and (ii) to highlight the ways that Active Learning can be most effectively
applied to real legal industry data. Unlike other studies, our experiments were
performed against large data sets taken from recent, real-world legal matters
covering a variety of areas. We conclude that, although these experiments show
the Active Learning strategy popularly used in legal document review can
quickly identify informative training documents, it becomes less effective over
time. In particular, our findings suggest this most popular form of Active
Learning in the legal arena, where the highest-scoring documents are selected
as training examples, is in fact not the most efficient approach in most
instances. Ultimately, a different Active Learning strategy may be best suited
to initiate the predictive modeling process but not to continue through the
entire document review.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1903.12457v3,2019-06-05T07:35:08Z,2019-03-29T11:57:24Z,"Towards Knowledge-Based Personalized Product Description Generation in
  E-commerce","Quality product descriptions are critical for providing competitive customer
experience in an e-commerce platform. An accurate and attractive description
not only helps customers make an informed decision but also improves the
likelihood of purchase. However, crafting a successful product description is
tedious and highly time-consuming. Due to its importance, automating the
product description generation has attracted considerable interests from both
research and industrial communities. Existing methods mainly use templates or
statistical methods, and their performance could be rather limited. In this
paper, we explore a new way to generate the personalized product description by
combining the power of neural networks and knowledge base. Specifically, we
propose a KnOwledge Based pErsonalized (or KOBE) product description generation
model in the context of e-commerce. In KOBE, we extend the encoder-decoder
framework, the Transformer, to a sequence modeling formulation using
self-attention. In order to make the description both informative and
personalized, KOBE considers a variety of important factors during text
generation, including product aspects, user categories, and knowledge base,
etc. Experiments on real-world datasets demonstrate that the proposed method
out-performs the baseline on various metrics. KOBE can achieve an improvement
of 9.7% over state-of-the-arts in terms of BLEU. We also present several case
studies as the anecdotal evidence to further prove the effectiveness of the
proposed approach. The framework has been deployed in Taobao, the largest
online e-commerce platform in China.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1903.12110v1,2019-03-28T16:51:17Z,2019-03-28T16:51:17Z,Building Automated Survey Coders via Interactive Machine Learning,"Software systems trained via machine learning to automatically classify
open-ended answers (a.k.a. verbatims) are by now a reality. Still, their
adoption in the survey coding industry has been less widespread than it might
have been. Among the factors that have hindered a more massive takeup of this
technology are the effort involved in manually coding a sufficient amount of
training data, the fact that small studies do not seem to justify this effort,
and the fact that the process needs to be repeated anew when brand new coding
tasks arise. In this paper we will argue for an approach to building verbatim
classifiers that we will call ""Interactive Learning"", and that addresses all
the above problems. We will show that, for the same amount of training effort,
interactive learning delivers much better coding accuracy than standard
""non-interactive"" learning. This is especially true when the amount of data we
are willing to manually code is small, which makes this approach attractive
also for small-scale studies. Interactive learning also lends itself to reusing
previously trained classifiers for dealing with new (albeit related) coding
tasks. Interactive learning also integrates better in the daily workflow of the
survey specialist, and delivers a better user experience overall.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1903.09477v4,2020-12-30T10:34:47Z,2019-03-22T12:46:34Z,"Facilitating Rapid Prototyping in the OODIDA Data Analytics Platform via
  Active-Code Replacement","OODIDA (On-board/Off-board Distributed Data Analytics) is a platform for
distributed real-time analytics, targeting fleets of reference vehicles in the
automotive industry. Its users are data analysts. The bulk of the data
analytics tasks are performed by clients (on-board), while a central cloud
server performs supplementary tasks (off-board). OODIDA can be automatically
packaged and deployed, which necessitates restarting parts of the system, or
all of it. As this is potentially disruptive, we added the ability to execute
user-defined Python modules on clients as well as the server. These modules can
be replaced without restarting any part of the system; they can even be
replaced between iterations of an ongoing assignment. This feature is referred
to as active-code replacement. It facilitates use cases such as iterative A/B
testing of machine learning algorithms or modifying experimental algorithms
on-the-fly. Consistency of results is achieved by majority vote, which prevents
tainted state. Active-code replacement can be done in less than a second in an
idealized setting whereas a standard deployment takes many orders of magnitude
more time. The main contribution of this paper is the description of a
relatively straightforward approach to active-code replacement that is very
user-friendly. It enables a data analyst to quickly execute custom code on the
cloud server as well as on client devices. Sensible safeguards and design
decisions ensure that this feature can be used by non-specialists who are not
familiar with the implementation of OODIDA in general or this feature in
particular. As a consequence of adding the active-code replacement feature,
OODIDA is now very well-suited for rapid prototyping.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1904.09035v2,2019-04-22T04:07:17Z,2019-03-21T02:55:14Z,"Evolving Deep Neural Networks by Multi-objective Particle Swarm
  Optimization for Image Classification","In recent years, convolutional neural networks (CNNs) have become deeper in
order to achieve better classification accuracy in image classification.
However, it is difficult to deploy the state-of-the-art deep CNNs for
industrial use due to the difficulty of manually fine-tuning the
hyperparameters and the trade-off between classification accuracy and
computational cost. This paper proposes a novel multi-objective optimization
method for evolving state-of-the-art deep CNNs in real-life applications, which
automatically evolves the non-dominant solutions at the Pareto front. Three
major contributions are made: Firstly, a new encoding strategy is designed to
encode one of the best state-of-the-art CNNs; With the classification accuracy
and the number of floating point operations as the two objectives, a
multi-objective particle swarm optimization method is developed to evolve the
non-dominant solutions; Last but not least, a new infrastructure is designed to
boost the experiments by concurrently running the experiments on multiple GPUs
across multiple machines, and a Python library is developed and released to
manage the infrastructure. The experimental results demonstrate that the
non-dominant solutions found by the proposed algorithm form a clear Pareto
front, and the proposed infrastructure is able to almost linearly reduce the
running time.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1903.08536v3,2019-06-11T10:07:09Z,2019-03-20T15:03:17Z,Segmentation-Based Deep-Learning Approach for Surface-Defect Detection,"Automated surface-anomaly detection using machine learning has become an
interesting and promising area of research, with a very high and direct impact
on the application domain of visual inspection. Deep-learning methods have
become the most suitable approaches for this task. They allow the inspection
system to learn to detect the surface anomaly by simply showing it a number of
exemplar images. This paper presents a segmentation-based deep-learning
architecture that is designed for the detection and segmentation of surface
anomalies and is demonstrated on a specific domain of surface-crack detection.
The design of the architecture enables the model to be trained using a small
number of samples, which is an important requirement for practical
applications. The proposed model is compared with the related deep-learning
methods, including the state-of-the-art commercial software, showing that the
proposed approach outperforms the related methods on the specific domain of
surface-crack detection. The large number of experiments also shed light on the
required precision of the annotation, the number of required training samples
and on the required computational cost. Experiments are performed on a newly
created dataset based on a real-world quality control case and demonstrates
that the proposed approach is able to learn on a small number of defected
surfaces, using only approximately 25-30 defective training samples, instead of
hundreds or thousands, which is usually the case in deep-learning applications.
This makes the deep-learning method practical for use in industry where the
number of available defective samples is limited. The dataset is also made
publicly available to encourage the development and evaluation of new methods
for surface-defect detection.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1903.06278v2,2019-03-18T05:32:11Z,2019-03-14T22:05:20Z,"gym-gazebo2, a toolkit for reinforcement learning using ROS 2 and Gazebo","This paper presents an upgraded, real world application oriented version of
gym-gazebo, the Robot Operating System (ROS) and Gazebo based Reinforcement
Learning (RL) toolkit, which complies with OpenAI Gym. The content discusses
the new ROS 2 based software architecture and summarizes the results obtained
using Proximal Policy Optimization (PPO). Ultimately, the output of this work
presents a benchmarking system for robotics that allows different techniques
and algorithms to be compared using the same virtual conditions. We have
evaluated environments with different levels of complexity of the Modular
Articulated Robotic Arm (MARA), reaching accuracies in the millimeter scale.
The converged results show the feasibility and usefulness of the gym-gazebo 2
toolkit, its potential and applicability in industrial use cases, using modular
robots.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1903.04824v1,2019-03-12T10:28:36Z,2019-03-12T10:28:36Z,"Proceedings of the Fifth International Conference on Cloud and Robotics
  (ICCR2018)","The 5th edition of the International Conference on Cloud and Robotics (ICCR
2018 - http://cloudrobotics.info) will be held on November 12-14 2018 in Paris
and Saint-Quentin, France. The conference is a co-event with GDR ALROB and the
industry exposition Robonumerique (http://www.robonumerique.fr).
  The domain of cloud robotics aims to converge robots with computation,
storage and communication resources provided by the cloud. The cloud may
complement robotic resources in several ways, including crowd-sourcing
knowledge databases, context information, computational offloading or
data-intensive information processing for artificial intelligence. Today, the
paradigms of cloud/fog/edge computing propose software architecture solutions
for robots to share computations or offload them to ambiant and networked
resources. Yet, combining distant computations with the real time constraints
of robotics is very challenging. As the challenges in this domain are
multi-disciplinary and similar in other research areas, Cloud Robotics aims at
building bridges among experts from academia and industry working in different
fields, such as robotics, cyber-physical systems, automotive, aerospace,
machine learning, artificial intelligence, software architecture, big data
analytics, Internet-of-Things, networked control and distributed cloud systems.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1904.05724v1,2019-03-06T12:59:01Z,2019-03-06T12:59:01Z,"Improving SIEM for Critical SCADA Water Infrastructures Using Machine
  Learning","Network Control Systems (NAC) have been used in many industrial processes.
They aim to reduce the human factor burden and efficiently handle the complex
process and communication of those systems. Supervisory control and data
acquisition (SCADA) systems are used in industrial, infrastructure and facility
processes (e.g. manufacturing, fabrication, oil and water pipelines, building
ventilation, etc.) Like other Internet of Things (IoT) implementations, SCADA
systems are vulnerable to cyber-attacks, therefore, a robust anomaly detection
is a major requirement. However, having an accurate anomaly detection system is
not an easy task, due to the difficulty to differentiate between cyber-attacks
and system internal failures (e.g. hardware failures). In this paper, we
present a model that detects anomaly events in a water system controlled by
SCADA. Six Machine Learning techniques have been used in building and
evaluating the model. The model classifies different anomaly events including
hardware failures (e.g. sensor failures), sabotage and cyber-attacks (e.g. DoS
and Spoofing). Unlike other detection systems, our proposed work focuses on
notifying the operator when an anomaly occurs with a probability of the event
occurring. This additional information helps in accelerating the mitigation
process. The model is trained and tested using a real-world dataset.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1902.07316v2,2019-04-15T10:21:01Z,2019-02-17T19:37:33Z,Deep Modulation Embedding,"Deep neural network has recently shown very promising applications in
different research directions and attracted the industry attention as well.
Although the idea was introduced in the past but just recently the main
limitation of using this class of algorithms is solved by enabling parallel
computing on GPU hardware. Opening the possibility of hardware prototyping with
proven superiority of this class of algorithm, trigger several research
directions in communication system too. Among them cognitive radio, modulation
recognition, learning based receiver and transceiver are already given very
interesting result in simulation and real experimental evaluation implemented
on software defined radio. Specifically, modulation recognition is mostly
approached as a classification problem which is a supervised learning
framework. But it is here addressed as an unsupervised problem with introducing
new features for training, a new loss function and investigating the robustness
of the pipeline against several mismatch conditions.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1902.02236v1,2019-02-06T15:38:06Z,2019-02-06T15:38:06Z,Dynamic Pricing for Airline Ancillaries with Customer Context,"Ancillaries have become a major source of revenue and profitability in the
travel industry. Yet, conventional pricing strategies are based on business
rules that are poorly optimized and do not respond to changing market
conditions. This paper describes the dynamic pricing model developed by Deepair
solutions, an AI technology provider for travel suppliers. We present a pricing
model that provides dynamic pricing recommendations specific to each customer
interaction and optimizes expected revenue per customer. The unique nature of
personalized pricing provides the opportunity to search over the market space
to find the optimal price-point of each ancillary for each customer, without
violating customer privacy. In this paper, we present and compare three
approaches for dynamic pricing of ancillaries, with increasing levels of
sophistication: (1) a two-stage forecasting and optimization model using a
logistic mapping function; (2) a two-stage model that uses a deep neural
network for forecasting, coupled with a revenue maximization technique using
discrete exhaustive search; (3) a single-stage end-to-end deep neural network
that recommends the optimal price. We describe the performance of these models
based on both offline and online evaluations. We also measure the real-world
business impact of these approaches by deploying them in an A/B test on an
airline's internet booking website. We show that traditional machine learning
techniques outperform human rule-based approaches in an online setting by
improving conversion by 36% and revenue per offer by 10%. We also provide
results for our offline experiments which show that deep learning algorithms
outperform traditional machine learning techniques for this problem. Our
end-to-end deep learning model is currently being deployed by the airline in
their booking system.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1901.10281v1,2019-01-29T13:43:57Z,2019-01-29T13:43:57Z,Structural Material Property Tailoring Using Deep Neural Networks,"Advances in robotics, artificial intelligence, and machine learning are
ushering in a new age of automation, as machines match or outperform human
performance. Machine intelligence can enable businesses to improve performance
by reducing errors, improving sensitivity, quality and speed, and in some cases
achieving outcomes that go beyond current resource capabilities. Relevant
applications include new product architecture design, rapid material
characterization, and life-cycle management tied with a digital strategy that
will enable efficient development of products from cradle to grave. In
addition, there are also challenges to overcome that must be addressed through
a major, sustained research effort that is based solidly on both inferential
and computational principles applied to design tailoring of functionally
optimized structures. Current applications of structural materials in the
aerospace industry demand the highest quality control of material
microstructure, especially for advanced rotational turbomachinery in aircraft
engines in order to have the best tailored material property. In this paper,
deep convolutional neural networks were developed to accurately predict
processing-structure-property relations from materials microstructures images,
surpassing current best practices and modeling efforts. The models
automatically learn critical features, without the need for manual
specification and/or subjective and expensive image analysis. Further, in
combination with generative deep learning models, a framework is proposed to
enable rapid material design space exploration and property identification and
optimization. The implementation must take account of real-time decision cycles
and the trade-offs between speed and accuracy.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1901.07370v1,2019-01-18T00:53:39Z,2019-01-18T00:53:39Z,"SAML-QC: a Stochastic Assessment and Machine Learning based QC technique
  for Industrial Printing","Recently, the advancement in industrial automation and high-speed printing
has raised numerous challenges related to the printing quality inspection of
final products. This paper proposes a machine vision based technique to assess
the printing quality of text on industrial objects. The assessment is based on
three quality defects such as text misalignment, varying printing shades, and
misprinted text. The proposed scheme performs the quality inspection through
stochastic assessment technique based on the second-order statistics of
printing. First: the text-containing area on printed product is identified
through image processing techniques. Second: the alignment testing of the
identified text-containing area is performed. Third: optical character
recognition is performed to divide the text into different small boxes and only
the intensity value of each text-containing box is taken as a random variable
and second-order statistics are estimated to determine the varying printing
defects in the text under one, two and three sigma thresholds. Fourth: the
K-Nearest Neighbors based supervised machine learning is performed to provide
the stochastic process for misprinted text detection. Finally, the technique is
deployed on an industrial image for the printing quality assessment with
varying values of n and m. The results have shown that the proposed SAML-QC
technique can perform real-time automated inspection for industrial printing.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1901.05147v1,2019-01-16T06:10:45Z,2019-01-16T06:10:45Z,The Winning Solution to the IEEE CIG 2017 Game Data Mining Competition,"Machine learning competitions such as those organized by Kaggle or KDD
represent a useful benchmark for data science research. In this work, we
present our winning solution to the Game Data Mining competition hosted at the
2017 IEEE Conference on Computational Intelligence and Games (CIG 2017). The
contest consisted of two tracks, and participants (more than 250, belonging to
both industry and academia) were to predict which players would stop playing
the game, as well as their remaining lifetime. The data were provided by a
major worldwide video game company, NCSoft, and came from their successful
massively multiplayer online game Blade and Soul. Here, we describe the long
short-term memory approach and conditional inference survival ensemble model
that made us win both tracks of the contest, as well as the validation
procedure that we followed in order to prevent overfitting. In particular,
choosing a survival method able to deal with censored data was crucial to
accurately predict the moment in which each player would leave the game, as
censoring is inherent in churn. The selected models proved to be robust against
evolving conditions---since there was a change in the business model of the
game (from subscription-based to free-to-play) between the two sample datasets
provided---and efficient in terms of time cost. Thanks to these features and
also to their a ability to scale to large datasets, our models could be readily
implemented in real business settings.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1812.08273v1,2018-12-19T22:25:52Z,2018-12-19T22:25:52Z,Analog Signal Processing Using Stochastic Magnets,"We present a low barrier magnet based compact hardware unit for analog
stochastic neurons and demonstrate its use as a building-block for neuromorphic
hardware. By coupling circular magnetic tunnel junctions (MTJs) with a CMOS
based analog buffer, we show that these units can act as leaky-integrate-and
fire (LIF) neurons, a model of biological neural networks particularly suited
for temporal inferencing and pattern recognition. We demonstrate examples of
temporal sequence learning, processing, and prediction tasks in real time, as a
proof of concept demonstration of scalable and adaptive signal-processors.
Efficient non von-Neumann hardware implementation of such processors can open
up a pathway for integration of hardware based cognition in a wide variety of
emerging systems such as IoT, industrial controls, bio- and photo-sensors, and
Unmanned Autonomous Vehicles.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1812.03201v2,2018-12-18T23:40:20Z,2018-12-07T20:10:23Z,Residual Reinforcement Learning for Robot Control,"Conventional feedback control methods can solve various types of robot
control problems very efficiently by capturing the structure with explicit
models, such as rigid body equations of motion. However, many control problems
in modern manufacturing deal with contacts and friction, which are difficult to
capture with first-order physical modeling. Hence, applying control design
methodologies to these kinds of problems often results in brittle and
inaccurate controllers, which have to be manually tuned for deployment.
Reinforcement learning (RL) methods have been demonstrated to be capable of
learning continuous robot controllers from interactions with the environment,
even for problems that include friction and contacts. In this paper, we study
how we can solve difficult control problems in the real world by decomposing
them into a part that is solved efficiently by conventional feedback control
methods, and the residual which is solved with RL. The final control policy is
a superposition of both control signals. We demonstrate our approach by
training an agent to successfully perform a real-world block assembly task
involving contacts and unstable objects.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1812.01351v1,2018-12-04T11:57:46Z,2018-12-04T11:57:46Z,"Regularized Fuzzy Neural Networks to Aid Effort Forecasting in the
  Construction and Software Development","Predicting the time to build software is a very complex task for software
engineering managers. There are complex factors that can directly interfere
with the productivity of the development team. Factors directly related to the
complexity of the system to be developed drastically change the time necessary
for the completion of the works with the software factories. This work proposes
the use of a hybrid system based on artificial neural networks and fuzzy
systems to assist in the construction of an expert system based on rules to
support in the prediction of hours destined to the development of software
according to the complexity of the elements present in the same. The set of
fuzzy rules obtained by the system helps the management and control of software
development by providing a base of interpretable estimates based on fuzzy
rules. The model was submitted to tests on a real database, and its results
were promissory in the construction of an aid mechanism in the predictability
of the software construction.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1812.01029v1,2018-12-03T19:05:25Z,2018-12-03T19:05:25Z,Sensitivity based Neural Networks Explanations,"Although neural networks can achieve very high predictive performance on
various different tasks such as image recognition or natural language
processing, they are often considered as opaque ""black boxes"". The difficulty
of interpreting the predictions of a neural network often prevents its use in
fields where explainability is important, such as the financial industry where
regulators and auditors often insist on this aspect. In this paper, we present
a way to assess the relative input features importance of a neural network
based on the sensitivity of the model output with respect to its input. This
method has the advantage of being fast to compute, it can provide both global
and local levels of explanations and is applicable for many types of neural
network architectures. We illustrate the performance of this method on both
synthetic and real data and compare it with other interpretation techniques.
This method is implemented into an open-source Python package that allows its
users to easily generate and visualize explanations for their neural networks.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.07315v1,2018-11-18T11:28:24Z,2018-11-18T11:28:24Z,"Learning to infer: RL-based search for DNN primitive selection on
  Heterogeneous Embedded Systems","Deep Learning is increasingly being adopted by industry for computer vision
applications running on embedded devices. While Convolutional Neural Networks'
accuracy has achieved a mature and remarkable state, inference latency and
throughput are a major concern especially when targeting low-cost and low-power
embedded platforms. CNNs' inference latency may become a bottleneck for Deep
Learning adoption by industry, as it is a crucial specification for many
real-time processes. Furthermore, deployment of CNNs across heterogeneous
platforms presents major compatibility issues due to vendor-specific technology
and acceleration libraries. In this work, we present QS-DNN, a fully automatic
search based on Reinforcement Learning which, combined with an inference engine
optimizer, efficiently explores through the design space and empirically finds
the optimal combinations of libraries and primitives to speed up the inference
of CNNs on heterogeneous embedded devices. We show that, an optimized
combination can achieve 45x speedup in inference latency on CPU compared to a
dependency-free baseline and 2x on average on GPGPU compared to the best vendor
library. Further, we demonstrate that, the quality of results and time
""to-solution"" is much better than with Random Search and achieves up to 15x
better results for a short-time search.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.07112v2,2019-04-10T09:59:47Z,2018-11-17T07:09:13Z,Augmented LiDAR Simulator for Autonomous Driving,"In Autonomous Driving (AD), detection and tracking of obstacles on the roads
is a critical task. Deep-learning based methods using annotated LiDAR data have
been the most widely adopted approach for this. Unfortunately, annotating 3D
point cloud is a very challenging, time- and money-consuming task. In this
paper, we propose a novel LiDAR simulator that augments real point cloud with
synthetic obstacles (e.g., cars, pedestrians, and other movable objects).
Unlike previous simulators that entirely rely on CG models and game engines,
our augmented simulator bypasses the requirement to create high-fidelity
background CAD models. Instead, we can simply deploy a vehicle with a LiDAR
scanner to sweep the street of interests to obtain the background point cloud,
based on which annotated point cloud can be automatically generated. This
unique ""scan-and-simulate"" capability makes our approach scalable and
practical, ready for large-scale industrial applications. In this paper, we
describe our simulator in detail, in particular the placement of obstacles that
is critical for performance enhancement. We show that detectors with our
simulated LiDAR point cloud alone can perform comparably (within two percentage
points) with these trained with real data. Mixing real and simulated data can
achieve over 95% accuracy.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.04871v1,2018-11-12T17:32:24Z,2018-11-12T17:32:24Z,Characterizing machine learning process: A maturity framework,"Academic literature on machine learning modeling fails to address how to make
machine learning models work for enterprises. For example, existing machine
learning processes cannot address how to define business use cases for an AI
application, how to convert business requirements from offering managers into
data requirements for data scientists, and how to continuously improve AI
applications in term of accuracy and fairness, and how to customize general
purpose machine learning models with industry, domain, and use case specific
data to make them more accurate for specific situations etc. Making AI work for
enterprises requires special considerations, tools, methods and processes. In
this paper we present a maturity framework for machine learning model lifecycle
management for enterprises. Our framework is a re-interpretation of the
software Capability Maturity Model (CMM) for machine learning model development
process. We present a set of best practices from our personal experience of
building large scale real-world machine learning models to help organizations
achieve higher levels of maturity independent of their starting point.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.03934v1,2018-11-09T14:46:40Z,2018-11-09T14:46:40Z,"RadIoT: Radio Communications Intrusion Detection for IoT - A Protocol
  Independent Approach","Internet-of-Things (IoT) devices are nowadays massively integrated in daily
life: homes, factories, or public places. This technology offers attractive
services to improve the quality of life as well as new economic markets through
the exploitation of the collected data. However, these connected objects have
also become attractive targets for attackers because their current security
design is often weak or flawed, as illustrated by several vulnerabilities such
as Mirai, Blueborne, etc. This paper presents a novel approach for detecting
intrusions in smart spaces such as smarthomes, or smartfactories, that is based
on the monitoring and profiling of radio communications at the physical layer
using machine learning techniques. The approach is designed to be independent
of the large and heterogeneous set of wireless communication protocols
typically implemented by connected objects such as WiFi, Bluetooth, Zigbee,
Bluetooth-Low-Energy (BLE) or proprietary communication protocols. The main
concepts of the proposed approach are presented together with an experimental
case study illustrating its feasibility based on data collected during the
deployment of the intrusion detection approach in a smart home under real-life
conditions.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.02213v1,2018-11-06T08:05:24Z,2018-11-06T08:05:24Z,"Hybrid Approach to Automation, RPA and Machine Learning: a Method for
  the Human-centered Design of Software Robots","One of the more prominent trends within Industry 4.0 is the drive to employ
Robotic Process Automation (RPA), especially as one of the elements of the Lean
approach. The full implementation of RPA is riddled with challenges relating
both to the reality of everyday business operations, from SMEs to SSCs and
beyond, and the social effects of the changing job market. To successfully
address these points there is a need to develop a solution that would adjust to
the existing business operations and at the same time lower the negative social
impact of the automation process.
  To achieve these goals we propose a hybrid, human-centered approach to the
development of software robots. This design and implementation method combines
the Living Lab approach with empowerment through participatory design to
kick-start the co-development and co-maintenance of hybrid software robots
which, supported by variety of AI methods and tools, including interactive and
collaborative ML in the cloud, transform menial job posts into higher-skilled
positions, allowing former employees to stay on as robot co-designers and
maintainers, i.e. as co-programmers who supervise the machine learning
processes with the use of tailored high-level RPA Domain Specific Languages
(DSLs) to adjust the functioning of the robots and maintain operational
flexibility.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1810.07829v1,2018-10-17T23:06:06Z,2018-10-17T23:06:06Z,"Quality 4.0: Let's Get Digital - The many ways the fourth industrial
  revolution is reshaping the way we think about quality","The technology landscape is richer and more promising than ever before. In
many ways, cloud computing, big data, virtual reality (VR), augmented reality
(AR), blockchain, additive manufacturing, artificial intelligence (AI), machine
learning (ML), Internet Protocol Version 6 (IPv6), cyber-physical systems and
the Internet of Things (IoT) all represent new frontiers. These technologies
can help improve product and service quality, and organizational performance.
In many regions, the internet is now as ubiquitous as electricity. Components
are relatively cheap. A robust ecosystem of open-source software libraries
means that engineers can solve problems 100 times faster than just two decades
ago. This digital transformation is leading us toward connected intelligent
automation: smart, hyperconnected agents deployed in environments where humans
and machines cooperate, and leverage data, to achieve shared goals. This is not
the worlds first industrial revolution. In fact, it is its fourth, and the
disruptive changes it will bring suggest we will need a fresh perspective on
quality to adapt to it.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1810.04538v1,2018-10-10T14:04:08Z,2018-10-10T14:04:08Z,"Secure Deep Learning Engineering: A Software Quality Assurance
  Perspective","Over the past decades, deep learning (DL) systems have achieved tremendous
success and gained great popularity in various applications, such as
intelligent machines, image processing, speech processing, and medical
diagnostics. Deep neural networks are the key driving force behind its recent
success, but still seem to be a magic black box lacking interpretability and
understanding. This brings up many open safety and security issues with
enormous and urgent demands on rigorous methodologies and engineering practice
for quality enhancement. A plethora of studies have shown that the
state-of-the-art DL systems suffer from defects and vulnerabilities that can
lead to severe loss and tragedies, especially when applied to real-world
safety-critical applications. In this paper, we perform a large-scale study and
construct a paper repository of 223 relevant works to the quality assurance,
security, and interpretation of deep learning. We, from a software quality
assurance perspective, pinpoint challenges and future opportunities towards
universal secure deep learning engineering. We hope this work and the
accompanied paper repository can pave the path for the software engineering
community towards addressing the pressing industrial demand of secure
intelligent applications.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1810.09957v1,2018-10-08T04:30:44Z,2018-10-08T04:30:44Z,NSML: Meet the MLaaS platform with a real-world case study,"The boom of deep learning induced many industries and academies to introduce
machine learning based approaches into their concern, competitively. However,
existing machine learning frameworks are limited to sufficiently fulfill the
collaboration and management for both data and models. We proposed NSML, a
machine learning as a service (MLaaS) platform, to meet these demands. NSML
helps machine learning work be easily launched on a NSML cluster and provides a
collaborative environment which can afford development at enterprise scale.
Finally, NSML users can deploy their own commercial services with NSML cluster.
In addition, NSML furnishes convenient visualization tools which assist the
users in analyzing their work. To verify the usefulness and accessibility of
NSML, we performed some experiments with common examples. Furthermore, we
examined the collaborative advantages of NSML through three competitions with
real-world use cases.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1808.10134v1,2018-08-30T06:29:10Z,2018-08-30T06:29:10Z,"Baidu Apollo Auto-Calibration System - An Industry-Level Data-Driven and
  Learning based Vehicle Longitude Dynamic Calibrating Algorithm","For any autonomous driving vehicle, control module determines its road
performance and safety, i.e. its precision and stability should stay within a
carefully-designed range. Nonetheless, control algorithms require vehicle
dynamics (such as longitudinal dynamics) as inputs, which, unfortunately, are
obscure to calibrate in real time. As a result, to achieve reasonable
performance, most, if not all, research-oriented autonomous vehicles do manual
calibrations in a one-by-one fashion. Since manual calibration is not
sustainable once entering into mass production stage for industrial purposes,
we here introduce a machine-learning based auto-calibration system for
autonomous driving vehicles. In this paper, we will show how we build a
data-driven longitudinal calibration procedure using machine learning
techniques. We first generated offline calibration tables from human driving
data. The offline table serves as an initial guess for later uses and it only
needs twenty-minutes data collection and process. We then used an
online-learning algorithm to appropriately update the initial table (the
offline table) based on real-time performance analysis. This longitudinal
auto-calibration system has been deployed to more than one hundred Baidu Apollo
self-driving vehicles (including hybrid family vehicles and electronic
delivery-only vehicles) since April 2018. By August 27, 2018, it had been
tested for more than two thousands hours, ten thousands kilometers (6,213
miles) and yet proven to be effective.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1808.09794v2,2018-08-30T19:40:56Z,2018-08-29T13:25:11Z,"Correlated Time Series Forecasting using Deep Neural Networks: A Summary
  of Results","Cyber-physical systems often consist of entities that interact with each
other over time. Meanwhile, as part of the continued digitization of industrial
processes, various sensor technologies are deployed that enable us to record
time-varying attributes (a.k.a., time series) of such entities, thus producing
correlated time series. To enable accurate forecasting on such correlated time
series, this paper proposes two models that combine convolutional neural
networks (CNNs) and recurrent neural networks (RNNs). The first model employs a
CNN on each individual time series, combines the convoluted features, and then
applies an RNN on top of the convoluted features in the end to enable
forecasting. The second model adds additional auto-encoders into the individual
CNNs, making the second model a multi-task learning model, which provides
accurate and robust forecasting. Experiments on two real-world correlated time
series data set suggest that the proposed two models are effective and
outperform baselines in most settings.
  This report extends the paper ""Correlated Time Series Forecasting using
Multi-Task Deep Neural Networks,"" to appear in ACM CIKM 2018, by providing
additional experimental results.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1808.06352v1,2018-08-20T09:06:21Z,2018-08-20T09:06:21Z,"Navigating the Landscape for Real-time Localisation and Mapping for
  Robotics and Virtual and Augmented Reality","Visual understanding of 3D environments in real-time, at low power, is a huge
computational challenge. Often referred to as SLAM (Simultaneous Localisation
and Mapping), it is central to applications spanning domestic and industrial
robotics, autonomous vehicles, virtual and augmented reality. This paper
describes the results of a major research effort to assemble the algorithms,
architectures, tools, and systems software needed to enable delivery of SLAM,
by supporting applications specialists in selecting and configuring the
appropriate algorithm and the appropriate hardware, and compilation pathway, to
meet their performance, accuracy, and energy consumption goals. The major
contributions we present are (1) tools and methodology for systematic
quantitative evaluation of SLAM algorithms, (2) automated,
machine-learning-guided exploration of the algorithmic and implementation
design space with respect to multiple objectives, (3) end-to-end simulation
tools to enable optimisation of heterogeneous, accelerated architectures for
the specific algorithmic requirements of the various SLAM algorithmic
approaches, and (4) tools for delivering, where appropriate, accelerated,
adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1808.03454v1,2018-08-10T08:40:32Z,2018-08-10T08:40:32Z,AIQ: Measuring Intelligence of Business AI Software,"Focusing on Business AI, this article introduces the AIQ quadrant that
enables us to measure AI for business applications in a relative comparative
manner, i.e. to judge that software A has more or less intelligence than
software B. Recognizing that the goal of Business software is to maximize value
in terms of business results, the dimensions of the quadrant are the key
factors that determine the business value of AI software: Level of Output
Quality (Smartness) and Level of Automation. The use of the quadrant is
illustrated by several software solutions to support the real life business
challenge of field service scheduling. The role of machine learning and
conversational digital assistants in increasing the business value are also
discussed and illustrated with a recent integration of existing intelligent
digital assistants for factory floor decision making with the new version of
Google Glass. Such hands free AI solutions elevate the AIQ level to its
ultimate position.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1808.00601v1,2018-08-01T23:56:28Z,2018-08-01T23:56:28Z,"Classification of Building Information Model (BIM) Structures with Deep
  Learning","In this work we study an application of machine learning to the construction
industry and we use classical and modern machine learning methods to categorize
images of building designs into three classes: Apartment building, Industrial
building or Other. No real images are used, but only images extracted from
Building Information Model (BIM) software, as these are used by the
construction industry to store building designs. For this task, we compared
four different methods: the first is based on classical machine learning, where
Histogram of Oriented Gradients (HOG) was used for feature extraction and a
Support Vector Machine (SVM) for classification; the other three methods are
based on deep learning, covering common pre-trained networks as well as ones
designed from scratch. To validate the accuracy of the models, a database of
240 images was used. The accuracy achieved is 57% for the HOG + SVM model, and
above 89% for the neural networks.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1807.07506v2,2018-11-19T07:11:52Z,2018-07-19T15:58:14Z,Improving Simple Models with Confidence Profiles,"In this paper, we propose a new method called ProfWeight for transferring
information from a pre-trained deep neural network that has a high test
accuracy to a simpler interpretable model or a very shallow network of low
complexity and a priori low test accuracy. We are motivated by applications in
interpretability and model deployment in severely memory constrained
environments (like sensors). Our method uses linear probes to generate
confidence scores through flattened intermediate representations. Our transfer
method involves a theoretically justified weighting of samples during the
training of the simple model using confidence scores of these intermediate
layers. The value of our method is first demonstrated on CIFAR-10, where our
weighting method significantly improves (3-4%) networks with only a fraction of
the number of Resnet blocks of a complex Resnet model. We further demonstrate
operationally significant results on a real manufacturing problem, where we
dramatically increase the test accuracy of a CART model (the domain standard)
by roughly 13%.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1807.02340v2,2018-10-03T15:42:51Z,2018-07-06T10:17:44Z,Testing Untestable Neural Machine Translation: An Industrial Case,"Neural Machine Translation (NMT) has been widely adopted recently due to its
advantages compared with the traditional Statistical Machine Translation (SMT).
However, an NMT system still often produces translation failures due to the
complexity of natural language and sophistication in designing neural networks.
While in-house black-box system testing based on reference translations (i.e.,
examples of valid translations) has been a common practice for NMT quality
assurance, an increasingly critical industrial practice, named in-vivo testing,
exposes unseen types or instances of translation failures when real users are
using a deployed industrial NMT system. To fill the gap of lacking test oracle
for in-vivo testing of an NMT system, in this paper, we propose a new approach
for automatically identifying translation failures, without requiring reference
translations for a translation task; our approach can directly serve as a test
oracle for in-vivo testing. Our approach focuses on properties of natural
language translation that can be checked systematically and uses information
from both the test inputs (i.e., the texts to be translated) and the test
outputs (i.e., the translations under inspection) of the NMT system. Our
evaluation conducted on real-world datasets shows that our approach can
effectively detect targeted property violations as translation failures. Our
experiences on deploying our approach in both production and development
environments of WeChat (a messenger app with over one billion monthly active
users) demonstrate high effectiveness of our approach along with high industry
impact.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1807.00139v1,2018-06-30T08:31:54Z,2018-06-30T08:31:54Z,Harnessing constrained resources in service industry via video analytics,"Service industries contribute significantly to many developed and developing
- economies. As their business activities expand rapidly, many service
companies struggle to maintain customer's satisfaction due to sluggish service
response caused by resource shortages. Anticipating resource shortages and
proffering solutions before they happen is an effective way of reducing the
adverse effect on operations. However, this proactive approach is very
expensive in terms of capacity and labor costs. Many companies fall into
productivity conundrum as they fail to find sufficient strong arguments to
justify the cost of a new technology yet cannot afford not to invest in new
technologies to match up with competitors. The question is whether there is an
innovative solution to maximally utilize available resources and drastically
reduce the effect that the shortages of resources may cause yet achieving high
level of service quality at a low cost. This work demonstrates with a practical
analysis of a trolley tracking system we designed and deployed at Hong Kong
International Airport (HKIA) on how video analytics helps achieve management's
goal of satisfying customer's needs via real-time detection and prevention of
problems they may encounter during the service consumption process using
existing video technology rather than adopting new technologies. This paper
presents the integration of commercial video surveillance system with deep
learning algorithms for video analytics. We show that our system can provide
accurate decision when faced with total or partial occlusion with high accuracy
and it significantly improves daily operation. It is envisioned that this work
will heighten the appreciation of integrative technologies for resource
management within the service industries and as a measure for real-time
customer assistance.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1806.09057v1,2018-06-24T00:28:12Z,2018-06-24T00:28:12Z,In-situ Stochastic Training of MTJ Crossbar based Neural Networks,"Owing to high device density, scalability and non-volatility, Magnetic Tunnel
Junction-based crossbars have garnered significant interest for implementing
the weights of an artificial neural network. The existence of only two stable
states in MTJs implies a high overhead of obtaining optimal binary weights in
software. We illustrate that the inherent parallelism in the crossbar structure
makes it highly appropriate for in-situ training, wherein the network is taught
directly on the hardware. It leads to significantly smaller training overhead
as the training time is independent of the size of the network, while also
circumventing the effects of alternate current paths in the crossbar and
accounting for manufacturing variations in the device. We show how the
stochastic switching characteristics of MTJs can be leveraged to perform
probabilistic weight updates using the gradient descent algorithm. We describe
how the update operations can be performed on crossbars both with and without
access transistors and perform simulations on them to demonstrate the
effectiveness of our techniques. The results reveal that stochastically trained
MTJ-crossbar NNs achieve a classification accuracy nearly same as that of
real-valued-weight networks trained in software and exhibit immunity to device
variations.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1806.05434v1,2018-06-14T09:44:59Z,2018-06-14T09:44:59Z,"Transfer Learning for Context-Aware Question Matching in
  Information-seeking Conversations in E-commerce","Building multi-turn information-seeking conversation systems is an important
and challenging research topic. Although several advanced neural text matching
models have been proposed for this task, they are generally not efficient for
industrial applications. Furthermore, they rely on a large amount of labeled
data, which may not be available in real-world applications. To alleviate these
problems, we study transfer learning for multi-turn information seeking
conversations in this paper. We first propose an efficient and effective
multi-turn conversation model based on convolutional neural networks. After
that, we extend our model to adapt the knowledge learned from a resource-rich
domain to enhance the performance. Finally, we deployed our model in an
industrial chatbot called AliMe Assist
(https://consumerservice.taobao.com/online-help) and observed a significant
improvement over the existing online model.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1806.02424v1,2018-06-06T20:59:40Z,2018-06-06T20:59:40Z,Action4D: Real-time Action Recognition in the Crowd and Clutter,"Recognizing every person's action in a crowded and cluttered environment is a
challenging task. In this paper, we propose a real-time action recognition
method, Action4D, which gives reliable and accurate results in the real-world
settings. We propose to tackle the action recognition problem using a holistic
4D ""scan"" of a cluttered scene to include every detail about the people and
environment. Recognizing multiple people's actions in the cluttered 4D
representation is a new problem. In this paper, we propose novel methods to
solve this problem. We propose a new method to track people in 4D, which can
reliably detect and follow each person in real time. We propose a new deep
neural network, the Action4D-Net, to recognize the action of each tracked
person. The Action4D-Net's novel structure uses both the global feature and the
focused attention to achieve state-of-the-art result. Our real-time method is
invariant to camera view angles, resistant to clutter and able to handle crowd.
The experimental results show that the proposed method is fast, reliable and
accurate. Our method paves the way to action recognition in the real-world
applications and is ready to be deployed to enable smart homes, smart factories
and smart stores.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1805.01374v3,2018-06-19T02:00:32Z,2018-05-03T15:28:44Z,"RF-PUF: Enhancing IoT Security through Authentication of Wireless Nodes
  using In-situ Machine Learning","Traditional authentication in radio-frequency (RF) systems enable secure data
communication within a network through techniques such as digital signatures
and hash-based message authentication codes (HMAC), which suffer from key
recovery attacks. State-of-the-art IoT networks such as Nest also use Open
Authentication (OAuth 2.0) protocols that are vulnerable to cross-site-recovery
forgery (CSRF), which shows that these techniques may not prevent an adversary
from copying or modeling the secret IDs or encryption keys using invasive, side
channel, learning or software attacks. Physical unclonable functions (PUF), on
the other hand, can exploit manufacturing process variations to uniquely
identify silicon chips which makes a PUF-based system extremely robust and
secure at low cost, as it is practically impossible to replicate the same
silicon characteristics across dies. Taking inspiration from human
communication, which utilizes inherent variations in the voice signatures to
identify a certain speaker, we present RF- PUF: a deep neural network-based
framework that allows real-time authentication of wireless nodes, using the
effects of inherent process variation on RF properties of the wireless
transmitters (Tx), detected through in-situ machine learning at the receiver
(Rx) end. The proposed method utilizes the already-existing asymmetric RF
communication framework and does not require any additional circuitry for PUF
generation or feature extraction. Simulation results involving the process
variations in a standard 65 nm technology node, and features such as LO offset
and I-Q imbalance detected with a neural network having 50 neurons in the
hidden layer indicate that the framework can distinguish up to 4800
transmitters with an accuracy of 99.9% (~ 99% for 10,000 transmitters) under
varying channel conditions, and without the need for traditional preambles.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1804.05839v4,2019-11-05T13:12:43Z,2018-04-16T12:04:03Z,BigDL: A Distributed Deep Learning Framework for Big Data,"This paper presents BigDL (a distributed deep learning framework for Apache
Spark), which has been used by a variety of users in the industry for building
deep learning applications on production big data platforms. It allows deep
learning applications to run on the Apache Hadoop/Spark cluster so as to
directly process the production data, and as a part of the end-to-end data
analysis pipeline for deployment and management. Unlike existing deep learning
frameworks, BigDL implements distributed, data parallel training directly on
top of the functional compute model (with copy-on-write and coarse-grained
operations) of Spark. We also share real-world experience and ""war stories"" of
users that have adopted BigDL to address their challenges(i.e., how to easily
build end-to-end data analysis and deep learning pipelines for their production
data).",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1804.05497v1,2018-04-16T03:55:42Z,2018-04-16T03:55:42Z,"Deep Learning on Key Performance Indicators for Predictive Maintenance
  in SAP HANA","With a new era of cloud and big data, Database Management Systems (DBMSs)
have become more crucial in numerous enterprise business applications in all
the industries. Accordingly, the importance of their proactive and preventive
maintenance has also increased. However, detecting problems by predefined rules
or stochastic modeling has limitations, particularly when analyzing the data on
high-dimensional Key Performance Indicators (KPIs) from a DBMS. In recent
years, Deep Learning (DL) has opened new opportunities for this complex
analysis. In this paper, we present two complementary DL approaches to detect
anomalies in SAP HANA. A temporal learning approach is used to detect abnormal
patterns based on unlabeled historical data, whereas a spatial learning
approach is used to classify known anomalies based on labeled data. We
implement a system in SAP HANA integrated with Google TensorFlow. The
experimental results with real-world data confirm the effectiveness of the
system and models.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1804.00064v1,2018-03-30T21:56:38Z,2018-03-30T21:56:38Z,"Learning Beyond Human Expertise with Generative Models for Dental
  Restorations","Computer vision has advanced significantly that many discriminative
approaches such as object recognition are now widely used in real applications.
We present another exciting development that utilizes generative models for the
mass customization of medical products such as dental crowns. In the dental
industry, it takes a technician years of training to design synthetic crowns
that restore the function and integrity of missing teeth. Each crown must be
customized to individual patients, and it requires human expertise in a
time-consuming and labor-intensive process, even with computer-assisted design
software. We develop a fully automatic approach that learns not only from human
designs of dental crowns, but also from natural spatial profiles between
opposing teeth. The latter is hard to account for by technicians but important
for proper biting and chewing functions. Built upon a Generative Adversar-ial
Network architecture (GAN), our deep learning model predicts the customized
crown-filled depth scan from the crown-missing depth scan and opposing depth
scan. We propose to incorporate additional space constraints and statistical
compatibility into learning. Our automatic designs exceed human technicians'
standards for good morphology and functionality, and our algorithm is being
tested for production use.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1802.09756v2,2018-09-11T13:53:10Z,2018-02-27T07:52:35Z,"Real-Time Bidding with Multi-Agent Reinforcement Learning in Display
  Advertising","Real-time advertising allows advertisers to bid for each impression for a
visiting user. To optimize specific goals such as maximizing revenue and return
on investment (ROI) led by ad placements, advertisers not only need to estimate
the relevance between the ads and user's interests, but most importantly
require a strategic response with respect to other advertisers bidding in the
market. In this paper, we formulate bidding optimization with multi-agent
reinforcement learning. To deal with a large number of advertisers, we propose
a clustering method and assign each cluster with a strategic bidding agent. A
practical Distributed Coordinated Multi-Agent Bidding (DCMAB) has been proposed
and implemented to balance the tradeoff between the competition and cooperation
among advertisers. The empirical study on our industry-scaled real-world data
has demonstrated the effectiveness of our methods. Our results show
cluster-based bidding would largely outperform single-agent and bandit
approaches, and the coordinated bidding achieves better overall objectives than
purely self-interested bidding agents.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1802.08365v6,2018-10-23T15:20:56Z,2018-02-23T02:29:06Z,"Budget Constrained Bidding by Model-free Reinforcement Learning in
  Display Advertising","Real-time bidding (RTB) is an important mechanism in online display
advertising, where a proper bid for each page view plays an essential role for
good marketing results. Budget constrained bidding is a typical scenario in RTB
where the advertisers hope to maximize the total value of the winning
impressions under a pre-set budget constraint. However, the optimal bidding
strategy is hard to be derived due to the complexity and volatility of the
auction environment. To address these challenges, in this paper, we formulate
budget constrained bidding as a Markov Decision Process and propose a
model-free reinforcement learning framework to resolve the optimization
problem. Our analysis shows that the immediate reward from environment is
misleading under a critical resource constraint. Therefore, we innovate a
reward function design methodology for the reinforcement learning problems with
constraints. Based on the new reward design, we employ a deep neural network to
learn the appropriate reward so that the optimal policy can be learned
effectively. Different from the prior model-based work, which suffers from the
scalability problem, our framework is easy to be deployed in large-scale
industrial applications. The experimental evaluations demonstrate the
effectiveness of our framework on large-scale real datasets.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1802.02312v2,2018-06-05T03:12:06Z,2018-02-07T05:32:59Z,"Machine Learning-Based Prototyping of Graphical User Interfaces for
  Mobile Apps","It is common practice for developers of user-facing software to transform a
mock-up of a graphical user interface (GUI) into code. This process takes place
both at an application's inception and in an evolutionary context as GUI
changes keep pace with evolving features. Unfortunately, this practice is
challenging and time-consuming. In this paper, we present an approach that
automates this process by enabling accurate prototyping of GUIs via three
tasks: detection, classification, and assembly. First, logical components of a
GUI are detected from a mock-up artifact using either computer vision
techniques or mock-up metadata. Then, software repository mining, automated
dynamic analysis, and deep convolutional neural networks are utilized to
accurately classify GUI-components into domain-specific types (e.g.,
toggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates
a suitable hierarchical GUI structure from which a prototype application can be
automatically assembled. We implemented this approach for Android in a system
called ReDraw. Our evaluation illustrates that ReDraw achieves an average
GUI-component classification accuracy of 91% and assembles prototype
applications that closely mirror target mock-ups in terms of visual affinity
while exhibiting reasonable code structure. Interviews with industrial
practitioners illustrate ReDraw's potential to improve real development
workflows.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1801.05643v1,2018-01-17T12:51:01Z,2018-01-17T12:51:01Z,"The Case for Automatic Database Administration using Deep Reinforcement
  Learning","Like any large software system, a full-fledged DBMS offers an overwhelming
amount of configuration knobs. These range from static initialisation
parameters like buffer sizes, degree of concurrency, or level of replication to
complex runtime decisions like creating a secondary index on a particular
column or reorganising the physical layout of the store. To simplify the
configuration, industry grade DBMSs are usually shipped with various advisory
tools, that provide recommendations for given workloads and machines. However,
reality shows that the actual configuration, tuning, and maintenance is usually
still done by a human administrator, relying on intuition and experience.
Recent work on deep reinforcement learning has shown very promising results in
solving problems, that require such a sense of intuition. For instance, it has
been applied very successfully in learning how to play complicated games with
enormous search spaces. Motivated by these achievements, in this work we
explore how deep reinforcement learning can be used to administer a DBMS.
First, we will describe how deep reinforcement learning can be used to
automatically tune an arbitrary software system like a DBMS by defining a
problem environment. Second, we showcase our concept of NoDBA at the concrete
example of index selection and evaluate how well it recommends indexes for
given workloads.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1801.05627v2,2018-04-03T09:06:42Z,2018-01-17T11:48:18Z,"On the Reduction of Biases in Big Data Sets for the Detection of
  Irregular Power Usage","In machine learning, a bias occurs whenever training sets are not
representative for the test data, which results in unreliable models. The most
common biases in data are arguably class imbalance and covariate shift. In this
work, we aim to shed light on this topic in order to increase the overall
attention to this issue in the field of machine learning. We propose a scalable
novel framework for reducing multiple biases in high-dimensional data sets in
order to train more reliable predictors. We apply our methodology to the
detection of irregular power usage from real, noisy industrial data. In
emerging markets, irregular power usage, and electricity theft in particular,
may range up to 40% of the total electricity distributed. Biased data sets are
of particular issue in this domain. We show that reducing these biases
increases the accuracy of the trained predictors. Our models have the potential
to generate significant economic value in a real world application, as they are
being deployed in a commercial software for the detection of irregular power
usage.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1712.07452v2,2019-08-13T20:41:40Z,2017-12-20T12:47:39Z,"Self-Supervised Damage-Avoiding Manipulation Strategy Optimization via
  Mental Simulation","Everyday robotics are challenged to deal with autonomous product handling in
applications like logistics or retail, possibly causing damage on the items
during manipulation. Traditionally, most approaches try to minimize physical
interaction with goods. However, this paper proposes to take into account any
unintended object motion and to learn damage-minimizing manipulation strategies
in a self-supervised way. The presented approach consists of a simulation-based
planning method for an optimal manipulation sequence with respect to possible
damage. The planned manipulation sequences are generalized to new, unseen
scenes in the same application scenario using machine learning. This learned
manipulation strategy is continuously refined in a self-supervised,
simulation-in-the-loop optimization cycle during load-free times of the system,
commonly known as mental simulation. In parallel, the generated manipulation
strategies can be deployed in near-real time in an anytime fashion. The
approach is validated on an industrial container-unloading scenario and on a
retail shelf-replenishment scenario.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1712.06107v1,2017-12-17T13:00:25Z,2017-12-17T13:00:25Z,Railway Track Specific Traffic Signal Selection Using Deep Learning,"With the railway transportation Industry moving actively towards automation,
accurate location and inventory of wayside track assets like traffic signals,
crossings, switches, mileposts, etc. is of extreme importance. With the new
Positive Train Control (PTC) regulation coming into effect, many railway safety
rules will be tied directly to location of assets like mileposts and signals.
Newer speed regulations will be enforced based on location of the Train with
respect to a wayside asset. Hence it is essential for the railroads to have an
accurate database of the types and locations of these assets. This paper talks
about a real-world use-case of detecting railway signals from a camera mounted
on a moving locomotive and tracking their locations. The camera is engineered
to withstand the environment factors on a moving train and provide a consistent
steady image at around 30 frames per second. Using advanced image analysis and
deep learning techniques, signals are detected in these camera images and a
database of their locations is created. Railway signals differ a lot from road
signals in terms of shapes and rules for placement with respect to track. Due
to space constraint and traffic densities in urban areas signals are not placed
on the same side of the track and multiple lines can run in parallel. Hence
there is need to associate signal detected with the track on which the train
runs. We present a method to associate the signals to the specific track they
belong to using a video feed from the front facing camera mounted on the lead
locomotive. A pipeline of track detection, region of interest selection, signal
detection has been implemented which gives an overall accuracy of 94.7% on a
route covering 150km with 247 signals.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1712.02975v1,2017-12-08T08:06:03Z,2017-12-08T08:06:03Z,Recruitment Market Trend Analysis with Sequential Latent Variable Models,"Recruitment market analysis provides valuable understanding of
industry-specific economic growth and plays an important role for both
employers and job seekers. With the rapid development of online recruitment
services, massive recruitment data have been accumulated and enable a new
paradigm for recruitment market analysis. However, traditional methods for
recruitment market analysis largely rely on the knowledge of domain experts and
classic statistical models, which are usually too general to model large-scale
dynamic recruitment data, and have difficulties to capture the fine-grained
market trends. To this end, in this paper, we propose a new research paradigm
for recruitment market analysis by leveraging unsupervised learning techniques
for automatically discovering recruitment market trends based on large-scale
recruitment data. Specifically, we develop a novel sequential latent variable
model, named MTLVM, which is designed for capturing the sequential dependencies
of corporate recruitment states and is able to automatically learn the latent
recruitment topics within a Bayesian generative framework. In particular, to
capture the variability of recruitment topics over time, we design hierarchical
dirichlet processes for MTLVM. These processes allow to dynamically generate
the evolving recruitment topics. Finally, we implement a prototype system to
empirically evaluate our approach based on real-world recruitment data in
China. Indeed, by visualizing the results from MTLVM, we can successfully
reveal many interesting findings, such as the popularity of LBS related jobs
reached the peak in the 2nd half of 2014, and decreased in 2015.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1711.09279v1,2017-11-25T20:11:41Z,2017-11-25T20:11:41Z,A Big Data Analysis Framework Using Apache Spark and Deep Learning,"With the spreading prevalence of Big Data, many advances have recently been
made in this field. Frameworks such as Apache Hadoop and Apache Spark have
gained a lot of traction over the past decades and have become massively
popular, especially in industries. It is becoming increasingly evident that
effective big data analysis is key to solving artificial intelligence problems.
Thus, a multi-algorithm library was implemented in the Spark framework, called
MLlib. While this library supports multiple machine learning algorithms, there
is still scope to use the Spark setup efficiently for highly time-intensive and
computationally expensive procedures like deep learning. In this paper, we
propose a novel framework that combines the distributive computational
abilities of Apache Spark and the advanced machine learning architecture of a
deep multi-layer perceptron (MLP), using the popular concept of Cascade
Learning. We conduct empirical analysis of our framework on two real world
datasets. The results are encouraging and corroborate our proposed framework,
in turn proving that it is an improvement over traditional big data analysis
methods that use either Spark or Deep learning as individual elements.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1711.08149v3,2018-10-15T15:26:47Z,2017-11-22T06:32:13Z,"Accurate Real Time Localization Tracking in A Clinical Environment using
  Bluetooth Low Energy and Deep Learning","Deep learning has started to revolutionize several different industries, and
the applications of these methods in medicine are now becoming more
commonplace. This study focuses on investigating the feasibility of tracking
patients and clinical staff wearing Bluetooth Low Energy (BLE) tags in a
radiation oncology clinic using artificial neural networks (ANNs) and
convolutional neural networks (CNNs). The performance of these networks was
compared to relative received signal strength indicator (RSSI) thresholding and
triangulation. By utilizing temporal information, a combined CNN+ANN network
was capable of correctly identifying the location of the BLE tag with an
accuracy of 99.9%. It outperformed a CNN model (accuracy = 94%), a thresholding
model employing majority voting (accuracy = 95%), and a triangulation
classifier utilizing majority voting (accuracy = 95%). Future studies will seek
to deploy this affordable real time location system in hospitals to improve
clinical workflow, efficiency, and patient safety.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1710.11319v2,2018-06-07T21:16:06Z,2017-10-31T04:19:20Z,"Learning Motion Predictors for Smart Wheelchair using Autoregressive
  Sparse Gaussian Process","Constructing a smart wheelchair on a commercially available powered
wheelchair (PWC) platform avoids a host of seating, mechanical design and
reliability issues but requires methods of predicting and controlling the
motion of a device never intended for robotics. Analog joystick inputs are
subject to black-box transformations which may produce intuitive and adaptable
motion control for human operators, but complicate robotic control approaches;
furthermore, installation of standard axle mounted odometers on a commercial
PWC is difficult. In this work, we present an integrated hardware and software
system for predicting the motion of a commercial PWC platform that does not
require any physical or electronic modification of the chair beyond plugging
into an industry standard auxiliary input port. This system uses an RGB-D
camera and an Arduino interface board to capture motion data, including visual
odometry and joystick signals, via ROS communication. Future motion is
predicted using an autoregressive sparse Gaussian process model. We evaluate
the proposed system on real-world short-term path prediction experiments.
Experimental results demonstrate the system's efficacy when compared to a
baseline neural network model.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1710.08135v1,2017-10-23T08:12:45Z,2017-10-23T08:12:45Z,"An iterative closest point method for measuring the level of similarity
  of 3d log scans in wood industry","In the Canadian's lumber industry, simulators are used to predict the lumbers
resulting from the sawing of a log at a given sawmill. Giving a log or several
logs' 3D scans as input, simulators perform a real-time job to predict the
lumbers. These simulators, however, tend to be slow at processing large volume
of wood. We thus explore an alternative approximation techniques based on the
Iterative Closest Point (ICP) algorithm to identify the already processed log
to which an unseen log resembles the most. The main benefit of the ICP approach
is that it can easily handle 3D scans with a variable number of points. We
compare this ICP-based nearest neighbor predictor, to predictors built using
machine learning algorithms such as the K-nearest-neighbor (kNN) and Random
Forest (RF). The implemented ICP-based predictor enabled us to identify key
points in using the 3D scans directly for distance calculation. The long-term
goal of this ongoing research is to integrated ICP distance calculations and
machine learning.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1710.07709v1,2017-10-20T21:34:49Z,2017-10-20T21:34:49Z,"Solving the ""false positives"" problem in fraud prediction","In this paper, we present an automated feature engineering based approach to
dramatically reduce false positives in fraud prediction. False positives plague
the fraud prediction industry. It is estimated that only 1 in 5 declared as
fraud are actually fraud and roughly 1 in every 6 customers have had a valid
transaction declined in the past year. To address this problem, we use the Deep
Feature Synthesis algorithm to automatically derive behavioral features based
on the historical data of the card associated with a transaction. We generate
237 features (>100 behavioral patterns) for each transaction, and use a random
forest to learn a classifier. We tested our machine learning model on data from
a large multinational bank and compared it to their existing solution. On an
unseen data of 1.852 million transactions, we were able to reduce the false
positives by 54% and provide a savings of 190K euros. We also assess how to
deploy this solution, and whether it necessitates streaming computation for
real time scoring. We found that our solution can maintain similar benefits
even when historical features are computed once every 7 days.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1709.09480v2,2018-02-06T10:59:19Z,2017-09-27T13:03:52Z,A Benchmark Environment Motivated by Industrial Control Problems,"In the research area of reinforcement learning (RL), frequently novel and
promising methods are developed and introduced to the RL community. However,
although many researchers are keen to apply their methods on real-world
problems, implementing such methods in real industry environments often is a
frustrating and tedious process. Generally, academic research groups have only
limited access to real industrial data and applications. For this reason, new
methods are usually developed, evaluated and compared by using artificial
software benchmarks. On one hand, these benchmarks are designed to provide
interpretable RL training scenarios and detailed insight into the learning
process of the method on hand. On the other hand, they usually do not share
much similarity with industrial real-world applications. For this reason we
used our industry experience to design a benchmark which bridges the gap
between freely available, documented, and motivated artificial benchmarks and
properties of real industrial problems. The resulting industrial benchmark (IB)
has been made publicly available to the RL community by publishing its Java and
Python code, including an OpenAI Gym wrapper, on Github. In this paper we
motivate and describe in detail the IB's dynamics and identify prototypic
experimental settings that capture common situations in real-world industry
control problems.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1710.08299v1,2017-09-26T05:20:12Z,2017-09-26T05:20:12Z,An In-field Automatic Wheat Disease Diagnosis System,"Crop diseases are responsible for the major production reduction and economic
losses in agricultural industry world- wide. Monitoring for health status of
crops is critical to control the spread of diseases and implement effective
management. This paper presents an in-field automatic wheat disease diagnosis
system based on a weakly super- vised deep learning framework, i.e. deep
multiple instance learning, which achieves an integration of identification for
wheat diseases and localization for disease areas with only image-level
annotation for training images in wild conditions. Furthermore, a new in-field
image dataset for wheat disease, Wheat Disease Database 2017 (WDD2017), is
collected to verify the effectiveness of our system. Under two different
architectures, i.e. VGG-FCN-VD16 and VGG-FCN-S, our system achieves the mean
recognition accuracies of 97.95% and 95.12% respectively over 5-fold
cross-validation on WDD2017, exceeding the results of 93.27% and 73.00% by two
conventional CNN frameworks, i.e. VGG-CNN-VD16 and VGG-CNN-S. Experimental
results demonstrate that the proposed system outperforms conventional CNN
architectures on recognition accuracy under the same amount of parameters,
meanwhile main- taining accurate localization for corresponding disease areas.
Moreover, the proposed system has been packed into a real-time mobile app to
provide support for agricultural disease diagnosis.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1709.03008v1,2017-09-09T21:27:06Z,2017-09-09T21:27:06Z,"Identifying Irregular Power Usage by Turning Predictions into
  Holographic Spatial Visualizations","Power grids are critical infrastructure assets that face non-technical losses
(NTL) such as electricity theft or faulty meters. NTL may range up to 40% of
the total electricity distributed in emerging countries. Industrial NTL
detection systems are still largely based on expert knowledge when deciding
whether to carry out costly on-site inspections of customers. Electricity
providers are reluctant to move to large-scale deployments of automated systems
that learn NTL profiles from data due to the latter's propensity to suggest a
large number of unnecessary inspections. In this paper, we propose a novel
system that combines automated statistical decision making with expert
knowledge. First, we propose a machine learning framework that classifies
customers into NTL or non-NTL using a variety of features derived from the
customers' consumption data. The methodology used is specifically tailored to
the level of noise in the data. Second, in order to allow human experts to feed
their knowledge in the decision loop, we propose a method for visualizing
prediction results at various granularity levels in a spatial hologram. Our
approach allows domain experts to put the classification results into the
context of the data and to incorporate their knowledge for making the final
decisions of which customers to inspect. This work has resulted in appreciable
results on a real-world data set of 3.6M customers. Our system is being
deployed in a commercial NTL detection software.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1708.09099v1,2017-08-30T03:32:56Z,2017-08-30T03:32:56Z,"Watch Me, but Don't Touch Me! Contactless Control Flow Monitoring via
  Electromagnetic Emanations","Trustworthy operation of industrial control systems depends on secure and
real-time code execution on the embedded programmable logic controllers (PLCs).
The controllers monitor and control the critical infrastructures, such as
electric power grids and healthcare platforms, and continuously report back the
system status to human operators. We present Zeus, a contactless embedded
controller security monitor to ensure its execution control flow integrity.
Zeus leverages the electromagnetic emission by the PLC circuitry during the
execution of the controller programs. Zeus's contactless execution tracking
enables non-intrusive monitoring of security-critical controllers with tight
real-time constraints. Those devices often cannot tolerate the cost and
performance overhead that comes with additional traditional hardware or
software monitoring modules. Furthermore, Zeus provides an air-gap between the
monitor (trusted computing base) and the target (potentially compromised) PLC.
This eliminates the possibility of the monitor infection by the same attack
vectors. Zeus monitors for control flow integrity of the PLC program execution.
Zeus monitors the communications between the human-machine interface and the
PLC, and captures the control logic binary uploads to the PLC. Zeus exercises
its feasible execution paths, and fingerprints their emissions using an
external electromagnetic sensor. Zeus trains a neural network for legitimate
PLC executions, and uses it at runtime to identify the control flow based on
PLC's electromagnetic emissions. We implemented Zeus on a commercial Allen
Bradley PLC, which is widely used in industry, and evaluated it on real-world
control program executions. Zeus was able to distinguish between different
legitimate and malicious executions with 98.9% accuracy and with zero overhead
on PLC execution by design.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1708.08559v2,2018-03-20T06:10:24Z,2017-08-28T23:26:14Z,"DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous
  Cars","Recent advances in Deep Neural Networks (DNNs) have led to the development of
DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can
drive without any human intervention. Most major manufacturers including Tesla,
GM, Ford, BMW, and Waymo/Google are working on building and testing different
types of autonomous vehicles. The lawmakers of several US states including
California, Texas, and New York have passed new legislation to fast-track the
process of testing and deployment of autonomous vehicles on their roads.
  However, despite their spectacular progress, DNNs, just like traditional
software, often demonstrate incorrect or unexpected corner case behaviors that
can lead to potentially fatal collisions. Several such real-world accidents
involving autonomous cars have already happened including one which resulted in
a fatality. Most existing testing techniques for DNN-driven vehicles are
heavily dependent on the manual collection of test data under different driving
conditions which become prohibitively expensive as the number of test
conditions increases.
  In this paper, we design, implement and evaluate DeepTest, a systematic
testing tool for automatically detecting erroneous behaviors of DNN-driven
vehicles that can potentially lead to fatal crashes. First, our tool is
designed to automatically generated test cases leveraging real-world changes in
driving conditions like rain, fog, lighting conditions, etc. DeepTest
systematically explores different parts of the DNN logic by generating test
inputs that maximize the numbers of activated neurons. DeepTest found thousands
of erroneous behaviors under different realistic driving conditions (e.g.,
blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in
three top performing DNNs in the Udacity self-driving car challenge.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1708.02884v1,2017-08-09T15:51:28Z,2017-08-09T15:51:28Z,"Predicting and Evaluating Software Model Growth in the Automotive
  Industry","The size of a software artifact influences the software quality and impacts
the development process. In industry, when software size exceeds certain
thresholds, memory errors accumulate and development tools might not be able to
cope anymore, resulting in a lengthy program start up times, failing builds, or
memory problems at unpredictable times. Thus, foreseeing critical growth in
software modules meets a high demand in industrial practice. Predicting the
time when the size grows to the level where maintenance is needed prevents
unexpected efforts and helps to spot problematic artifacts before they become
critical.
  Although the amount of prediction approaches in literature is vast, it is
unclear how well they fit with prerequisites and expectations from practice. In
this paper, we perform an industrial case study at an automotive manufacturer
to explore applicability and usability of prediction approaches in practice. In
a first step, we collect the most relevant prediction approaches from
literature, including both, approaches using statistics and machine learning.
Furthermore, we elicit expectations towards predictions from practitioners
using a survey and stakeholder workshops. At the same time, we measure software
size of 48 software artifacts by mining four years of revision history,
resulting in 4,547 data points. In the last step, we assess the applicability
of state-of-the-art prediction approaches using the collected data by
systematically analyzing how well they fulfill the practitioners' expectations.
  Our main contribution is a comparison of commonly used prediction approaches
in a real world industrial setting while considering stakeholder expectations.
We show that the approaches provide significantly different results regarding
prediction accuracy and that the statistical approaches fit our data best.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1707.06959v1,2017-07-21T16:15:31Z,2017-07-21T16:15:31Z,"A Framework for Easing the Development of Applications Embedding Answer
  Set Programming","Answer Set Programming (ASP) is a well-established declarative problem
solving paradigm which became widely used in AI and recognized as a powerful
tool for knowledge representation and reasoning (KRR), especially for its high
expressiveness and the ability to deal also with incomplete knowledge.
  Recently, thanks to the availability of a number of robust and efficient
implementations, ASP has been increasingly employed in a number of different
domains, and used for the development of industrial-level and enterprise
applications. This made clear the need for proper development tools and
interoperability mechanisms for easing interaction and integration with
external systems in the widest range of real-world scenarios, including mobile
applications and educational contexts.
  In this work we present a framework for integrating the KRR capabilities of
ASP into generic applications. We show the use of the framework by illustrating
proper specializations for some relevant ASP systems over different platforms,
including the mobile setting; furthermore, the potential of the framework for
educational purposes is illustrated by means of the development of several
ASP-based applications.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1706.06978v4,2018-09-13T04:37:06Z,2017-06-21T16:05:17Z,Deep Interest Network for Click-Through Rate Prediction,"Click-through rate prediction is an essential task in industrial
applications, such as online advertising. Recently deep learning based models
have been proposed, which follow a similar Embedding\&MLP paradigm. In these
methods large scale sparse input features are first mapped into low dimensional
embedding vectors, and then transformed into fixed-length vectors in a
group-wise manner, finally concatenated together to fed into a multilayer
perceptron (MLP) to learn the nonlinear relations among features. In this way,
user features are compressed into a fixed-length representation vector, in
regardless of what candidate ads are. The use of fixed-length vector will be a
bottleneck, which brings difficulty for Embedding\&MLP methods to capture
user's diverse interests effectively from rich historical behaviors. In this
paper, we propose a novel model: Deep Interest Network (DIN) which tackles this
challenge by designing a local activation unit to adaptively learn the
representation of user interests from historical behaviors with respect to a
certain ad. This representation vector varies over different ads, improving the
expressive ability of model greatly. Besides, we develop two techniques:
mini-batch aware regularization and data adaptive activation function which can
help training industrial deep networks with hundreds of millions of parameters.
Experiments on two public datasets as well as an Alibaba real production
dataset with over 2 billion samples demonstrate the effectiveness of proposed
approaches, which achieve superior performance compared with state-of-the-art
methods. DIN now has been successfully deployed in the online display
advertising system in Alibaba, serving the main traffic.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1705.00346v1,2017-04-30T17:17:44Z,2017-04-30T17:17:44Z,Deep Learning in the Automotive Industry: Applications and Tools,"Deep Learning refers to a set of machine learning techniques that utilize
neural networks with many hidden layers for tasks, such as image
classification, speech recognition, language understanding. Deep learning has
been proven to be very effective in these domains and is pervasively used by
many Internet services. In this paper, we describe different automotive uses
cases for deep learning in particular in the domain of computer vision. We
surveys the current state-of-the-art in libraries, tools and infrastructures
(e.\,g.\ GPUs and clouds) for implementing, training and deploying deep neural
networks. We particularly focus on convolutional neural networks and computer
vision use cases, such as the visual inspection process in manufacturing plants
and the analysis of social media data. To train neural networks, curated and
labeled datasets are essential. In particular, both the availability and scope
of such datasets is typically very limited. A main contribution of this paper
is the creation of an automotive dataset, that allows us to learn and
automatically recognize different vehicle properties. We describe an end-to-end
deep learning application utilizing a mobile app for data collection and
process support, and an Amazon-based cloud backend for storage and training.
For training we evaluate the use of cloud and on-premises infrastructures
(including multiple GPUs) in conjunction with different neural network
architectures and frameworks. We assess both the training times as well as the
accuracy of the classifier. Finally, we demonstrate the effectiveness of the
trained classifier in a real world setting during manufacturing process.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1612.07448v6,2017-06-27T02:05:09Z,2016-12-22T05:41:27Z,Towards Linear Algebra over Normalized Data,"Providing machine learning (ML) over relational data is a mainstream
requirement for data analytics systems. While almost all the ML tools require
the input data to be presented as a single table, many datasets are
multi-table, which forces data scientists to join those tables first, leading
to data redundancy and runtime waste. Recent works on ""factorized"" ML mitigate
this issue for a few specific ML algorithms by pushing ML through joins. But
their approaches require a manual rewrite of ML implementations. Such piecemeal
methods create a massive development overhead when extending such ideas to
other ML algorithms. In this paper, we show that it is possible to mitigate
this overhead by leveraging a popular formal algebra to represent the
computations of many ML algorithms: linear algebra. We introduce a new logical
data type to represent normalized data and devise a framework of algebraic
rewrite rules to convert a large set of linear algebra operations over
denormalized data into operations over normalized data. We show how this
enables us to automatically ""factorize"" several popular ML algorithms, thus
unifying and generalizing several prior works. We prototype our framework in
the popular ML environment R and an industrial R-over-RDBMS tool. Experiments
with both synthetic and real normalized data show that our framework also
yields significant speed-ups, up to 36x on real data.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1610.04872v1,2016-10-16T15:14:36Z,2016-10-16T15:14:36Z,"Fault Detection Engine in Intelligent Predictive Analytics Platform for
  DCIM","With the advancement of huge data generation and data handling capability,
Machine Learning and Probabilistic modelling enables an immense opportunity to
employ predictive analytics platform in high security critical industries
namely data centers, electricity grids, utilities, airport etc. where downtime
minimization is one of the primary objectives. This paper proposes a novel,
complete architecture of an intelligent predictive analytics platform, Fault
Engine, for huge device network connected with electrical/information flow.
Three unique modules, here proposed, seamlessly integrate with available
technology stack of data handling and connect with middleware to produce online
intelligent prediction in critical failure scenarios. The Markov Failure module
predicts the severity of a failure along with survival probability of a device
at any given instances. The Root Cause Analysis model indicates probable
devices as potential root cause employing Bayesian probability assignment and
topological sort. Finally, a community detection algorithm produces correlated
clusters of device in terms of failure probability which will further narrow
down the search space of finding route cause. The whole Engine has been tested
with different size of network with simulated failure environments and shows
its potential to be scalable in real-time implementation.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1609.09296v1,2016-09-29T11:03:21Z,2016-09-29T11:03:21Z,"Comprehensive Evaluation of OpenCL-based Convolutional Neural Network
  Accelerators in Xilinx and Altera FPGAs","Deep learning has significantly advanced the state of the art in artificial
intelligence, gaining wide popularity from both industry and academia. Special
interest is around Convolutional Neural Networks (CNN), which take inspiration
from the hierarchical structure of the visual cortex, to form deep layers of
convolutional operations, along with fully connected classifiers. Hardware
implementations of these deep CNN architectures are challenged with memory
bottlenecks that require many convolution and fully-connected layers demanding
large amount of communication for parallel computation. Multi-core CPU based
solutions have demonstrated their inadequacy for this problem due to the memory
wall and low parallelism. Many-core GPU architectures show superior performance
but they consume high power and also have memory constraints due to
inconsistencies between cache and main memory. FPGA design solutions are also
actively being explored, which allow implementing the memory hierarchy using
embedded BlockRAM. This boosts the parallel use of shared memory elements
between multiple processing units, avoiding data replicability and
inconsistencies. This makes FPGAs potentially powerful solutions for real-time
classification of CNNs. Both Altera and Xilinx have adopted OpenCL co-design
framework from GPU for FPGA designs as a pseudo-automatic development solution.
In this paper, a comprehensive evaluation and comparison of Altera and Xilinx
OpenCL frameworks for a 5-layer deep CNN is presented. Hardware resources,
temporal performance and the OpenCL architecture for CNNs are discussed. Xilinx
demonstrates faster synthesis, better FPGA resource utilization and more
compact boards. Altera provides multi-platforms tools, mature design community
and better execution times.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1609.08018v1,2016-09-26T15:15:09Z,2016-09-26T15:15:09Z,"Small near-Earth asteroids in the Palomar Transient Factory survey: A
  real-time streak-detection system","Near-Earth asteroids (NEAs) in the 1-100 meter size range are estimated to be
$\sim$1,000 times more numerous than the $\sim$15,000 currently-catalogued
NEAs, most of which are in the 0.5-10 kilometer size range. Impacts from 10-100
meter size NEAs are not statistically life-threatening but may cause
significant regional damage, while 1-10 meter size NEAs with low velocities
relative to Earth are compelling targets for space missions. We describe the
implementation and initial results of a real-time NEA-discovery system
specialized for the detection of small, high angular rate (visually-streaked)
NEAs in Palomar Transient Factory (PTF) images. PTF is a 1.2-m aperture,
7.3-deg$^2$ field-of-view optical survey designed primarily for the discovery
of extragalactic transients (e.g., supernovae) in 60-second exposures reaching
$\sim$20.5 visual magnitude. Our real-time NEA discovery pipeline uses a
machine-learned classifier to filter a large number of false-positive streak
detections, permitting a human scanner to efficiently and remotely identify
real asteroid streaks during the night. Upon recognition of a streaked NEA
detection (typically within an hour of the discovery exposure), the scanner
triggers follow-up with the same telescope and posts the observations to the
Minor Planet Center for worldwide confirmation. We describe our ten initial
confirmed discoveries, all small NEAs that passed 0.3-15 lunar distances from
Earth. Lastly, we derive useful scaling laws for comparing
streaked-NEA-detection capabilities of different surveys as a function of their
hardware and survey-pattern characteristics. This work most directly informs
estimates of the streak-detection capabilities of the Zwicky Transient Facility
(ZTF, planned to succeed PTF in 2017), which will apply PTF's current
resolution and sensitivity over a 47-deg$^2$ field-of-view.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1611.00315v1,2016-08-28T20:04:25Z,2016-08-28T20:04:25Z,"Rapid Prototyping of a Text Mining Application for Cryptocurrency Market
  Intelligence","Blockchain represents a technology for establishing a shared, immutable
version of the truth between a network of participants that do not trust one
another, and therefore has the potential to disrupt any financial or other
industries that rely on third-parties to establish trust. Recent trends in
computing including: prevalence of Free and Open Source Software (FOSS); easy
access to High Performance Computing (HPC i.e. 'The Cloud'); and increasingly
advanced analytics capabilities such as Natural Language Processing (NLP) and
Machine Learning (ML) allow for rapidly prototyping applications for analysis
of trends in the emergence of Blockchain technology. A scaleable
proof-of-concept pipeline that lays the groundwork for analysis of multiple
streams of semi-structured data posted on social media is demonstrated.
Preliminary analysis and performance metrics are presented and discussed.
Future work is described that will scale the system to cloud-based, real-time,
analysis of multiple data streams, with Information Extraction (IE) (ex.
sentiment analysis) and Machine Learning capability.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1604.06195v1,2016-04-21T06:55:42Z,2016-04-21T06:55:42Z,Articulated Hand Pose Estimation Review,"With the increase number of companies focusing on commercializing Augmented
Reality (AR), Virtual Reality (VR) and wearable devices, the need for a hand
based input mechanism is becoming essential in order to make the experience
natural, seamless and immersive. Hand pose estimation has progressed
drastically in recent years due to the introduction of commodity depth cameras.
  Hand pose estimation based on vision is still a challenging problem due to
its complexity from self-occlusion (between fingers), close similarity between
fingers, dexterity of the hands, speed of the pose and the high dimension of
the hand kinematic parameters. Articulated hand pose estimation is still an
open problem and under intensive research from both academia and industry.
  The 2 approaches used for hand pose estimation are: discriminative and
generative. Generative approach is a model based that tries to fit a hand model
to the observed data. Discriminative approach is appearance based, usually
implemented with machine learning (ML) and require a large amount of training
data. Recent hand pose estimation uses hybrid approach by combining both
discriminative and generative methods into a single hand pipeline.
  In this paper, we focus on reviewing recent progress of hand pose estimation
from depth sensor. We will survey discriminative methods, generative methods
and hybrid methods. This paper is not a comprehensive review of all hand pose
estimation techniques, it is a subset of some of the recent state-of-the-art
techniques.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1602.08350v2,2017-07-25T04:44:12Z,2016-02-26T14:49:29Z,Large-Scale Detection of Non-Technical Losses in Imbalanced Data Sets,"Non-technical losses (NTL) such as electricity theft cause significant harm
to our economies, as in some countries they may range up to 40% of the total
electricity distributed. Detecting NTLs requires costly on-site inspections.
Accurate prediction of NTLs for customers using machine learning is therefore
crucial. To date, related research largely ignore that the two classes of
regular and non-regular customers are highly imbalanced, that NTL proportions
may change and mostly consider small data sets, often not allowing to deploy
the results in production. In this paper, we present a comprehensive approach
to assess three NTL detection models for different NTL proportions in large
real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and
Support Vector Machine. This work has resulted in appreciable results that are
about to be deployed in a leading industry solution. We believe that the
considerations and observations made in this contribution are necessary for
future smart meter research in order to report their effectiveness on
imbalanced and large real world data sets.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1610.04494v1,2016-02-07T19:28:08Z,2016-02-07T19:28:08Z,Localization for Wireless Sensor Networks: A Neural Network Approach,"As Wireless Sensor Networks are penetrating into the industrial domain, many
research opportunities are emerging. One such essential and challenging
application is that of node localization. A feed-forward neural network based
methodology is adopted in this paper. The Received Signal Strength Indicator
(RSSI) values of the anchor node beacons are used. The number of anchor nodes
and their configurations has an impact on the accuracy of the localization
system, which is also addressed in this paper. Five different training
algorithms are evaluated to find the training algorithm that gives the best
result. The multi-layer Perceptron (MLP) neural network model was trained using
Matlab. In order to evaluate the performance of the proposed method in real
time, the model obtained was then implemented on the Arduino microcontroller.
With four anchor nodes, an average 2D localization error of 0.2953 m has been
achieved with a 12-12-2 neural network structure. The proposed method can also
be implemented on any other embedded microcontroller system.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1602.02339v1,2016-02-07T04:01:55Z,2016-02-07T04:01:55Z,"Dynamic Selection of Virtual Machines for Application Servers in Cloud
  Environments","Autoscaling is a hallmark of cloud computing as it allows flexible
just-in-time allocation and release of computational resources in response to
dynamic and often unpredictable workloads. This is especially important for web
applications whose workload is time dependent and prone to flash crowds. Most
of them follow the 3-tier architectural pattern, and are divided into
presentation, application/domain and data layers. In this work we focus on the
application layer. Reactive autoscaling policies of the type ""Instantiate a new
Virtual Machine (VM) when the average server CPU utilisation reaches X%"" have
been used successfully since the dawn of cloud computing. But which VM type is
the most suitable for the specific application at the moment remains an open
question. In this work, we propose an approach for dynamic VM type selection.
It uses a combination of online machine learning techniques, works in real time
and adapts to changes in the users' workload patterns, application changes as
well as middleware upgrades and reconfigurations. We have developed a
prototype, which we tested with the CloudStone benchmark deployed on AWS EC2.
Results show that our method quickly adapts to workload changes and reduces the
total cost compared to the industry standard approach.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1601.06473v2,2016-01-26T04:43:20Z,2016-01-25T03:31:24Z,Teaching Robots to Do Object Assembly using Multi-modal 3D Vision,"The motivation of this paper is to develop a smart system using multi-modal
vision for next-generation mechanical assembly. It includes two phases where in
the first phase human beings teach the assembly structure to a robot and in the
second phase the robot finds objects and grasps and assembles them using AI
planning. The crucial part of the system is the precision of 3D visual
detection and the paper presents multi-modal approaches to meet the
requirements: AR markers are used in the teaching phase since human beings can
actively control the process. Point cloud matching and geometric constraints
are used in the robot execution phase to avoid unexpected noises. Experiments
are performed to examine the precision and correctness of the approaches. The
study is practical: The developed approaches are integrated with graph
model-based motion planning, implemented on an industrial robots and applicable
to real-world scenarios.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1310.5221v1,2013-10-19T12:25:00Z,2013-10-19T12:25:00Z,Soft computing techniques for software effort estimation,"The effort invested in a software project is probably one of the most
important and most analyzed variables in recent years in the process of project
management. The limitation of algorithmic effort prediction models is their
inability to cope with uncertainties and imprecision surrounding software
projects at the early development stage. More recently attention has turned to
a variety of machine learning methods, and soft computing in particular to
predict software development effort. Soft computing is a consortium of
methodologies centering in fuzzy logic, artificial neural networks, and
evolutionary computation. It is important, to mention here, that these
methodologies are complementary and synergistic, rather than competitive. They
provide in one form or another flexible information processing capability for
handling real life ambiguous situations. These methodologies are currently used
for reliable and accurate estimate of software development effort, which has
always been a challenge for both the software industry and academia. The aim of
this study is to analyze soft computing techniques in the existing models and
to provide in depth review of software and project estimation techniques
existing in industry and literature based on the different test datasets along
with their strength and weaknesses",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1208.6310v1,2012-08-16T12:14:46Z,2012-08-16T12:14:46Z,"Automated Marble Plate Classification System Based On Different Neural
  Network Input Training Sets and PLC Implementation","The process of sorting marble plates according to their surface texture is an
important task in the automated marble plate production. Nowadays some
inspection systems in marble industry that automate the classification tasks
are too expensive and are compatible only with specific technological equipment
in the plant. In this paper a new approach to the design of an Automated Marble
Plate Classification System (AMPCS),based on different neural network input
training sets is proposed, aiming at high classification accuracy using simple
processing and application of only standard devices. It is based on training a
classification MLP neural network with three different input training sets:
extracted texture histograms, Discrete Cosine and Wavelet Transform over the
histograms. The algorithm is implemented in a PLC for real-time operation. The
performance of the system is assessed with each one of the input training sets.
The experimental test results regarding classification accuracy and quick
operation are represented and discussed.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1204.1653v1,2012-04-07T16:34:20Z,2012-04-07T16:34:20Z,Machine Cognition Models: EPAM and GPS,"Through history, the human being tried to relay its daily tasks to other
creatures, which was the main reason behind the rise of civilizations. It
started with deploying animals to automate tasks in the field of
agriculture(bulls), transportation (e.g. horses and donkeys), and even
communication (pigeons). Millenniums after, come the Golden age with
""Al-jazari"" and other Muslim inventors, which were the pioneers of automation,
this has given birth to industrial revolution in Europe, centuries after. At
the end of the nineteenth century, a new era was to begin, the computational
era, the most advanced technological and scientific development that is driving
the mankind and the reason behind all the evolutions of science; such as
medicine, communication, education, and physics. At this edge of technology
engineers and scientists are trying to model a machine that behaves the same as
they do, which pushed us to think about designing and implementing ""Things
that-Thinks"", then artificial intelligence was. In this work we will cover each
of the major discoveries and studies in the field of machine cognition, which
are the ""Elementary Perceiver and Memorizer""(EPAM) and ""The General Problem
Solver""(GPS). The First one focus mainly on implementing the human-verbal
learning behavior, while the second one tries to model an architecture that is
able to solve problems generally (e.g. theorem proving, chess playing, and
arithmetic). We will cover the major goals and the main ideas of each model, as
well as comparing their strengths and weaknesses, and finally giving their
fields of applications. And Finally, we will suggest a real life implementation
of a cognitive machine.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1204.0262v2,2012-04-26T23:17:34Z,2012-04-01T20:17:32Z,"Managing contextual artificial neural networks with a service-based
  mediator","Today, a wide variety of probabilistic and expert AI systems used to analyze
real world inputs such as unstructured text, sounds, images, and statistical
data. However, all these systems exist on different platforms, with different
implementations, and with very different, often very specific goals in mind.
This paper introduces a concept for a mediator framework for such systems and
seeks to show several architectures which would support it, potential benefits
in combining the signals of disparate networks for formalized, high level logic
and signal processing, and its possible academic and industrial uses.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1107.5462v1,2011-07-27T13:07:39Z,2011-07-27T13:07:39Z,HyFlex: A Benchmark Framework for Cross-domain Heuristic Search,"Automating the design of heuristic search methods is an active research field
within computer science, artificial intelligence and operational research. In
order to make these methods more generally applicable, it is important to
eliminate or reduce the role of the human expert in the process of designing an
effective methodology to solve a given computational search problem.
Researchers developing such methodologies are often constrained on the number
of problem domains on which to test their adaptive, self-configuring
algorithms; which can be explained by the inherent difficulty of implementing
their corresponding domain specific software components.
  This paper presents HyFlex, a software framework for the development of
cross-domain search methodologies. The framework features a common software
interface for dealing with different combinatorial optimisation problems, and
provides the algorithm components that are problem specific. In this way, the
algorithm designer does not require a detailed knowledge the problem domains,
and thus can concentrate his/her efforts in designing adaptive general-purpose
heuristic search algorithms. Four hard combinatorial problems are fully
implemented (maximum satisfiability, one dimensional bin packing, permutation
flow shop and personnel scheduling), each containing a varied set of instance
data (including real-world industrial applications) and an extensive set of
problem specific heuristics and search operators. The framework forms the basis
for the first International Cross-domain Heuristic Search Challenge (CHeSC),
and it is currently in use by the international research community. In summary,
HyFlex represents a valuable new benchmark of heuristic search generality, with
which adaptive cross-domain algorithms are being easily developed, and reliably
compared.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/0706.1051v1,2007-06-07T18:13:59Z,2007-06-07T18:13:59Z,"Improved Neural Modeling of Real-World Systems Using Genetic Algorithm
  Based Variable Selection","Neural network models of real-world systems, such as industrial processes,
made from sensor data must often rely on incomplete data. System states may not
all be known, sensor data may be biased or noisy, and it is not often known
which sensor data may be useful for predictive modelling. Genetic algorithms
may be used to help to address this problem by determining the near optimal
subset of sensor variables most appropriate to produce good models. This paper
describes the use of genetic search to optimize variable selection to determine
inputs into the neural network model. We discuss genetic algorithm
implementation issues including data representation types and genetic operators
such as crossover and mutation. We present the use of this technique for neural
network modelling of a typical industrial application, a liquid fed ceramic
melter, and detail the results of the genetic search to optimize the neural
network model for this application.",arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
