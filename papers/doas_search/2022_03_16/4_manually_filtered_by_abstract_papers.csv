id,doi,publisher,database,url,domain,publication_date,algorithm_type,training_schema,algorithm_goal,architecture,title,abstract,status
1,http://arxiv.org/abs/1807.05211v1,arxiv,arxiv,http://arxiv.org/abs/1807.05211v1,space,2018-07-11 00:00:00,,,,,"learning deployable navigation policies at kilometer scale from a single
  traversal","Model-free reinforcement learning has recently been shown to be effective at
learning navigation policies from complex image input. However, these
algorithms tend to require large amounts of interaction with the environment,
which can be prohibitively costly to obtain on robots in the real world. We
present an approach for efficiently learning goal-directed navigation policies
on a mobile robot, from only a single coverage traversal of recorded data. The
navigation agent learns an effective policy over a diverse action space in a
large heterogeneous environment consisting of more than 2km of travel, through
buildings and outdoor regions that collectively exhibit large variations in
visual appearance, self-similarity, and connectivity. We compare pretrained
visual encoders that enable precomputation of visual embeddings to achieve a
throughput of tens of thousands of transitions per second at training time on a
commodity desktop computer, allowing agents to learn from millions of
trajectories of experience in a matter of hours. We propose multiple forms of
computationally efficient stochastic augmentation to enable the learned policy
to generalise beyond these precomputed embeddings, and demonstrate successful
deployment of the learned policy on the real robot without fine tuning, despite
environmental appearance differences at test time. The dataset and code
required to reproduce these results and apply the technique to other datasets
and robots is made publicly available at rl-navigation.github.io/deployable.",not classified
2,http://arxiv.org/abs/1908.00754v1,arxiv,arxiv,http://arxiv.org/abs/1908.00754v1,space,2019-08-02 00:00:00,,,,,"a visual technique to analyze flow of information in a machine learning
  system","Machine learning (ML) algorithms and machine learning based software systems
implicitly or explicitly involve complex flow of information between various
entities such as training data, feature space, validation set and results.
Understanding the statistical distribution of such information and how they
flow from one entity to another influence the operation and correctness of such
systems, especially in large-scale applications that perform classification or
prediction in real time. In this paper, we propose a visual approach to
understand and analyze flow of information during model training and serving
phases. We build the visualizations using a technique called Sankey Diagram -
conventionally used to understand data flow among sets - to address various use
cases of in a machine learning system. We demonstrate how the proposed
technique, tweaked and twisted to suit a classification problem, can play a
critical role in better understanding of the training data, the features, and
the classifier performance. We also discuss how this technique enables
diagnostic analysis of model predictions and comparative analysis of
predictions from multiple classifiers. The proposed concept is illustrated with
the example of categorization of millions of products in the e-commerce domain
- a multi-class hierarchical classification problem.",not classified
3,10.1109/icra40945.2020.9196540,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9196540/,space,2020-08-31 00:00:00,,,,,meta reinforcement learning for sim-to-real domain adaptation,"Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.",not classified
4,10.1109/robot.2000.844768,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/844768/,space,2000-04-28 00:00:00,,,,,application of automatic action planning for several work cells to the german ets-vii space robotics experiments,"Experiences in space robotics show, that the user normally has to cope with a huge amount of data. So, only robot and mission specialists are able to control the robot arm directly in teleoperation mode. By means of an intelligent robot control in cooperation with virtual reality methods, it is possible for non-robot specialists to generate tasks for a robot or an automation component intuitively. Furthermore, the intelligent robot control improves the safety of the entire system. The on-ground robot control and command station for the robot arm ERA onboard the satellite ETS-VII builds on a new resource-based action planning approach to manage robot manipulators and other automation components. In the case of ERA, the action planning system also takes care of the ""real"" robot onboard the satellite and the ""virtual"" robot in the simulation system. By means of the simulation system, the user can plan tasks ahead as well as analyze and visualize different strategies. The paper describes the mechanism of resource-based action planning, its application to different work cells, the practical experiences gained from the implementation for the on-ground robot control and command station for the robot arm ERA developed in the GETEX project as well as the services it provides to support VR-based man machine interfaces.",not classified
5,http://arxiv.org/abs/1702.03488v2,arxiv,arxiv,http://arxiv.org/abs/1702.03488v2,e-commerce,2017-02-12 00:00:00,,,,,octopus: a framework for cost-quality-time optimization in crowdsourcing,"We present Octopus, an AI agent to jointly balance three conflicting task
objectives on a micro-crowdsourcing marketplace - the quality of work, total
cost incurred, and time to completion. Previous control agents have mostly
focused on cost-quality, or cost-time tradeoffs, but not on directly
controlling all three in concert. A naive formulation of three-objective
optimization is intractable; Octopus takes a hierarchical POMDP approach, with
three different components responsible for setting the pay per task, selecting
the next task, and controlling task-level quality. We demonstrate that Octopus
significantly outperforms existing state-of-the-art approaches on real
experiments. We also deploy Octopus on Amazon Mechanical Turk, showing its
ability to manage tasks in a real-world dynamic setting.",not classified
6,10.1109/dasc52595.2021.9594351,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9594351/,space,2021-10-07 00:00:00,,,,,seda: a self-explaining decision architecture implemented using deep learning for on-board command and control,"Machine learning (ML) is a powerful tool for solving stochastic optimization problems. The aerospace and defense sectors have a number of stochastic optimization problems that would benefit from the application of ML; however, people often have difficulties interpreting solutions arrived at via ML, which undermines trust, producing an obstacle to widespread adoption in these sectors. This paper introduces the Self-Explaining Decision Architecture (SEDA) for ML-based decision-making systems capable of generating intuitive explanations for their decisions in real time. SEDA makes use of a feature extraction subsystem and a sequence interpretation subsystem to identify patterns in data followed by a decision generation subsystem that determines appropriate actions based on those patterns. Internal state information from each of these subsystems is used to generate explanations of the system’s decisions. Using this information to create explanations provides insight as to the data elements the system focused on when making decisions as well as the reasoning that was used. As a proof-of-concept, we present a first implementation of SEDA using start-of-the-art deep learning components including a combined convolutional neural network and long short-term memory network with attention mechanisms and demonstrate its use on both standard and custom datasets.",not classified
7,http://arxiv.org/abs/1609.08018v1,arxiv,arxiv,http://arxiv.org/abs/1609.08018v1,space,2016-09-26 00:00:00,,,,,"small near-earth asteroids in the palomar transient factory survey: a
  real-time streak-detection system","Near-Earth asteroids (NEAs) in the 1-100 meter size range are estimated to be
$\sim$1,000 times more numerous than the $\sim$15,000 currently-catalogued
NEAs, most of which are in the 0.5-10 kilometer size range. Impacts from 10-100
meter size NEAs are not statistically life-threatening but may cause
significant regional damage, while 1-10 meter size NEAs with low velocities
relative to Earth are compelling targets for space missions. We describe the
implementation and initial results of a real-time NEA-discovery system
specialized for the detection of small, high angular rate (visually-streaked)
NEAs in Palomar Transient Factory (PTF) images. PTF is a 1.2-m aperture,
7.3-deg$^2$ field-of-view optical survey designed primarily for the discovery
of extragalactic transients (e.g., supernovae) in 60-second exposures reaching
$\sim$20.5 visual magnitude. Our real-time NEA discovery pipeline uses a
machine-learned classifier to filter a large number of false-positive streak
detections, permitting a human scanner to efficiently and remotely identify
real asteroid streaks during the night. Upon recognition of a streaked NEA
detection (typically within an hour of the discovery exposure), the scanner
triggers follow-up with the same telescope and posts the observations to the
Minor Planet Center for worldwide confirmation. We describe our ten initial
confirmed discoveries, all small NEAs that passed 0.3-15 lunar distances from
Earth. Lastly, we derive useful scaling laws for comparing
streaked-NEA-detection capabilities of different surveys as a function of their
hardware and survey-pattern characteristics. This work most directly informs
estimates of the streak-detection capabilities of the Zwicky Transient Facility
(ZTF, planned to succeed PTF in 2017), which will apply PTF's current
resolution and sensitivity over a 47-deg$^2$ field-of-view.",not classified
8,http://arxiv.org/abs/2111.07171v1,arxiv,arxiv,http://arxiv.org/abs/2111.07171v1,space,2021-11-13 00:00:00,,,,,"deep reinforcement learning with shallow controllers: an experimental
  application to pid tuning","Deep reinforcement learning (RL) is an optimization-driven framework for
producing control strategies for general dynamical systems without explicit
reliance on process models. Good results have been reported in simulation. Here
we demonstrate the challenges in implementing a state of the art deep RL
algorithm on a real physical system. Aspects include the interplay between
software and existing hardware; experiment design and sample efficiency;
training subject to input constraints; and interpretability of the algorithm
and control law. At the core of our approach is the use of a PID controller as
the trainable RL policy. In addition to its simplicity, this approach has
several appealing features: No additional hardware needs to be added to the
control system, since a PID controller can easily be implemented through a
standard programmable logic controller; the control law can easily be
initialized in a ""safe'' region of the parameter space; and the final product
-- a well-tuned PID controller -- has a form that practitioners can reason
about and deploy with confidence.",not classified
9,,Artificial intelligence and the space station software support environment,core,,space,1986-01-01 00:00:00,,,,,https://core.ac.uk/download/pdf/42829617.pdf,"In a software system the size of the Space Station Software Support Environment (SSE), no one software development or implementation methodology is presently powerful enough to provide safe, reliable, maintainable, cost effective real time or near real time software. In an environment that must survive one of the most harsh and long life times, software must be produced that will perform as predicted, from the first time it is executed to the last. Many of the software challenges that will be faced will require strategies borrowed from Artificial Intelligence (AI). AI is the only development area mentioned as an example of a legitimate reason for a waiver from the overall requirement to use the Ada programming language for software development. The limits are defined of the applicability of the Ada language Ada Programming Support Environment (of which the SSE is a special case), and software engineering to AI solutions by describing a scenario that involves many facets of AI methodologies",not classified
10,10.1109/tase.2020.3032075,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/9246671/,space,2021-10-01 00:00:00,,,,,a virtual mechanism approach for exploiting functional redundancy in finishing operations,"We propose a new approach to programming by the demonstration of finishing operations. Such operations can be carried out by industrial robots in multiple ways because an industrial robot is typically functionally redundant with respect to a finishing task. In the proposed system, a human expert demonstrates a finishing operation, and the demonstrated motion is recorded in the Cartesian space. The robot’s kinematic model is augmented with a virtual mechanism, which is defined according to the applied finishing tool. This way, the kinematic model is expanded with additional degrees of freedom that can be exploited to compute the optimal joint space motion of the robot without altering the essential aspects of the Cartesian space task execution as demonstrated by the human expert. Finishing operations, such as polishing and grinding, occur in contact with the treated workpiece. Since information about the contact point position is needed to control the robot during the operation, we have developed a novel approach for accurate estimation of contact points using the measured forces and torques. Finally, we applied iterative learning control to refine the demonstrated operations and compensate for inaccurate calibration and different dynamics of the robot and human demonstrator. The proposed method was verified on real robots and real polishing and grinding tasks. <i>Note to Practitioners</i>—This work was motivated by the need for automation of finishing operations, such as polishing and grinding, on contemporary industrial robots. Existing approaches are both too complex and too time-consuming to be applied in flexible and small-scale production, which often requires the frequent deployment of new applications. Our approach is based on programming by demonstration and enables the programming of finishing operations also for users who are not specialists in robot programming. Programming by demonstration is especially useful for teaching finishing operations because it enables the transfer of expert knowledge about finishing skills to robots without providing lengthy task descriptions or manual coding. Besides the human demonstration of the desired operation, the proposed approach also requires the availability of the kinematic model for the machine tool applied to carry out the finishing operation. We provide several practical examples of grinding and polishing tools and how to integrate them into our approach. Another feature of the proposed system is that user demonstrations of finishing operations can be transferred between different combinations of robots and machine tools.",not classified
11,https://core.ac.uk/download/346440943.pdf,Western CEDAR,core,,space,2020-05-18 00:00:00,,,,,vikingbot: the starcraft artificial intelligence,"VikingBot is an automated AI that plays StarCraft by using a combination of machine learning and artificial intelligence. High level strategies are planned using the Brown-UMBC Reinforcement Learning and Planning (BURLAP), library which implements planning algorithms and provides interfaces for defining a domain and models of that domain for planning. For the planning, we used the BURLAP implementation of the sparse sampling algorithm because the time complexity is independent of the size of the state space, and we have to plan quickly in real time. SARSA reinforcement learning is used for a machine learning model that controls combat units. Various other helping functions are distributed to agent classes that aid in the AI in the different areas of the game. These agents are categorized as strategy, economy, combat, and intelligence. By using these parts in tandem VikingBot aims to use less training resources and still be able to play games at a high enough level to beat a human player",not classified
12,10.1109/smc-it.2009.40,IEEE,ieeexplore,https://ieeexplore.ieee.org/document/5226820/,space,2009-07-23 00:00:00,,,,,rapid prototyping of planning &amp; scheduling tools,"The Advanced Planning and Scheduling Initiative, or APSI, is an ESA programme to design and implement an Artificial Intelligence (AI) software infrastructure for planning and scheduling that can generically support different types and classes of space mission operations. The goal of the APSI is twofold: (1)~creating a software framework to improve the cost-effectiveness and flexibility of mission planning support tool development; (2)~bridging the gap between AI planning and scheduling technology and the world of space mission planning. A key aspect of the success of this project is the presence of a flexible timeline representation module that allows to exploit alternatives in the modeling of mission features. This paper shows an example of such a flexibility by using a real problem in the space realm - the HERSCHEL Science Long Term Planning process.",not classified
