doi,title,publisher,content_type,abstract,html_url,publication_title,publication_date,database
10.1109/CODESISSS51650.2020.9244038,A Fast Design Space Exploration Framework for the Deep Learning Accelerators: Work-in-Progress,IEEE,Conferences,"The Capsule Networks (CapsNets) is an advanced form of Convolutional Neural Network (CNN), capable of learning spatial relations and being invariant to transformations. CapsNets requires complex matrix operations which current accelerators are not optimized for, concerning both <sub>training</sub> and <sub>inference</sub> passes. Current state-of-the-art simulators and design space exploration (DSE) tools for DNN hardware neglect the modeling of training operations, while requiring long exploration times that slow down the complete design flow. These impediments restrict the real-world applications of CapsNets (e.g., autonomous driving and robotics) as well as the further development of DNNs in life-long learning scenarios that require training on low-power embedded devices. Towards this, we present <sub>XploreDL</sub>, a novel framework to perform fast yet high-fidelity DSE for both inference and training accelerators, supporting both CNNs and CapsNets operations. <sub>XploreDL</sub> enables a resource-efficient DSE for accelerators, focusing on power, area, and latency, highlighting Pareto-optimal solutions which can be a green-lit to expedite the design flow. <sub>XploreDL</sub> can reach the same fidelity as ARM's SCALE-sim, while providing 600x speedup and having a 50x lower memory-footprint. Preliminary results with a deep CapsNet model on MNIST for training accelerators show promising Pareto-optimal architectures with up to 0.4 TOPS/squared-mm and 800 fJ/op efficiency. With inference accelerators for AlexNet the Pareto-optimal solutions reach up to 1.8 TOPS/squared-mm and 200 fJ/op efficiency.",https://ieeexplore.ieee.org/document/9244038/,2020 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS),20-25 Sept. 2020,ieeexplore
10.1109/ISAI.2016.0070,A Method to Guarantee Real-Time for Software-Defined Radio in User-Space,IEEE,Conferences,"With the rapid development of computer technology, Software-Defined Radio has become more and more popular. We can now transfer the communication system from hardware to software, which makes it more flexible and helps us virtualize the communication system. In this paper, we creatively introduce a method to guarantee hard real-time in user-space on Linux. Unlike the usual methods, Our method(real time with RDTSC) provides real-time feature by soft ware clock with RDTSC and isolating CPUs instead of interrupts from the hardware. Our method is highly lightweight and give the SDR systems more flexibility when compared to the usual methods with interrupts. The experiment and validation results show that real time with RDTSC has a significantperformance and can be used in physical communication systems.",https://ieeexplore.ieee.org/document/7816723/,2016 International Conference on Information System and Artificial Intelligence (ISAI),24-26 June 2016,ieeexplore
10.23919/ACC.1990.4791047,A Neural Net Approach to Space Vehicle Guidance,IEEE,Conferences,"The on-line implementation of numerical algorithms for solving the optimum trajectory/guidance problem for advanced space vehicles such as ALS, HLLV, AOTV, transatmospheric vehicles and interplanetary spacecraft is not possible due to their complexity. Hence, the current approach to the development of real-time guidance laws for these advanced space vehicles is to use approximation theory to obtain closed-loop guidance laws. Neural networks offer an alternative to the derivation and implementation of guidance laws. In this paper, we formulate the space vehicle guidance problem using a neural network approach and investigate the appropriate neural net architecture for modelling optimum guidance trajectories. In particular, we investigate the incorporation of a priori knowledge about the characteristics of the optimal guidance solution into the neural network architecture. The online classification performance of the developed network is demonstrated using a synthesized network trained with a data base of optimum guidance trajectories. Such a neural network based guidance approach can readily adapt to environment uncertainties such as those encountered by an AOTV during atmospheric maneuvers.",https://ieeexplore.ieee.org/document/4791047/,1990 American Control Conference,23-25 May 1990,ieeexplore
10.1109/i-PACT52855.2021.9696457,A Novel Deep Recurrent Neural Network for Assessment of Role of Space Charge Effect in Partial Discharge Recognition and Diagnosis,IEEE,Conferences,"Partial Discharge (PD) has evolved into an inevitable tool for diagnosis of insulation of power equipment due to its inherent non-intrusive testing methodology. Since accurate recognition of complex overlapped PD sources is essential for effective diagnosis, recently the focus of research has shifted to challenges related to real-time PD measurement that involve complexities in discriminating multi-source pulse signatures, variations in applied voltage, dynamics of PD patterns related to space charge effect etc. Though research studies have successfully utilized a gamut of machine intelligence techniques such as neural networks, Hidden Markov Models, Support Vector Machines etc, effectiveness of recognition of complex overlapped PD sources based on space charge effect associated with PD has not been comprehensively established. This research focuses on implementation of Deep Recurrent Neural Network (DRNN) as a novel strategy for assessing the role of space charge and its associated memory propagation effect in PD signatures. Since Long Short-Term Memory (LSTM) based DRNN architecture augurs well for data involving sequence similarity recognition, the objective of this research is on establishing an indigenous approach of formulating identification markers to ascertain the role of space charge in PD patterns. The second objective is on establishing a unique approach to decipher state transitions of pulses using transition labels that describes the dynamics of PD signatures due to space charge effect. Detailed case studies based on laboratory benchmark models that replicate complex PD patterns clearly demonstrate the excellent capability of DRNN in deciphering the role of space charge in discriminating complex PD patterns.",https://ieeexplore.ieee.org/document/9696457/,2021 Innovations in Power and Advanced Computing Technologies (i-PACT),27-29 Nov. 2021,ieeexplore
10.1109/ICSS.2010.65,A Novel End-User Oriented Service Composition Model Based on Quotient Space Theory,IEEE,Conferences,"Nowadays, the services on the World Wide Web (WWW) are proliferating dramatically. The relationships of invoking among these services are becoming more and more complicated. The performances of traditional automated service composition methods suffer from the complexity of the service composition problem. Hence it is hardly to apply these automated methods in real cases. Semi-automated service composition is an alternative for coping with the complexity of service composition. A novel end-user oriented service composition model based on quotient space theory and service relation diagram is presented. This model proposes a hierarchical service composition from coarse-grain to fine-grain. The performance analysis and simulation showed that this model can reduce the complexity of service composition efficiently.",https://ieeexplore.ieee.org/document/5494284/,2010 International Conference on Service Sciences,13-14 May 2010,ieeexplore
10.1109/ICET52293.2021.9563174,A Physical-Cyber Dual Space Fusion Learning Assistant Method Based on Mixed Reality Implementation,IEEE,Conferences,"The learning assistant method of the Physical-Cyber dual space is an effective method to improve learning interaction and learning effect. In this paper, we proposed a learning assistant method based on mixed reality technology and the fusion of physical space and cyber space, including two classical learning scenes. Firstly, we integrated physical real classroom scene with online three-dimensional resource scene, which effectively expanded the imagination of students. It allows students to learn abstract knowledge of macro or micro scenes more intuitively. Secondly, we explored the online search functions to extend resources related to offline teaching knowledge. Finally, we constructed the instructing scene and application example of the dual space fusion learning companion system using HoloLens. We have come to the necessity, rationality and technical feasibility of the method, and carry out learning activities on the basis of HoloLens, which provides new ideas for promoting the innovation of teaching and learning forms in the future.",https://ieeexplore.ieee.org/document/9563174/,2021 IEEE International Conference on Educational Technology (ICET),18-20 June 2021,ieeexplore
10.1109/ICSLP.1996.607836,A comparison of modified k-means (MKM) and NN based real time adaptive clustering algorithms for articulatory space codebook formation,IEEE,Conferences,"The paper proposes the use of a neural network based real time adaptive clustering algorithm for the formation of a codebook of limited set of acoustical representation of finite set of vocal tract shapes from an articulatory space. A modified k-means algorithm (MKM) used for clustering nearly 10000 vocal tract shapes into 1000 cluster centers to form a codebook of articulatory shapes is computationally intensive for the application. An investigative study on the use of the NN based algorithm over the MKM algorithm at the peripheral level, for an application on computer aided pronunciation education, suggests the former for less intensive computation, with the possibility of improving the performance of the system by implementing the algorithm using a dedicated neural computer. Preliminary results of this study are reported.",https://ieeexplore.ieee.org/document/607836/,Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96,3-6 Oct. 1996,ieeexplore
10.1109/ICRA.2012.6225245,A depth space approach to human-robot collision avoidance,IEEE,Conferences,"In this paper a real-time collision avoidance approach is presented for safe human-robot coexistence. The main contribution is a fast method to evaluate distances between the robot and possibly moving obstacles (including humans), based on the concept of depth space. The distances are used to generate repulsive vectors that are used to control the robot while executing a generic motion task. The repulsive vectors can also take advantage of an estimation of the obstacle velocity. In order to preserve the execution of a Cartesian task with a redundant manipulator, a simple collision avoidance algorithm has been implemented where different reaction behaviors are set up for the end-effector and for other control points along the robot structure. The complete collision avoidance framework, from perception of the environment to joint-level robot control, is presented for a 7-dof KUKA Light-Weight-Robot IV using the Microsoft Kinect sensor. Experimental results are reported for dynamic environments with obstacles and a human.",https://ieeexplore.ieee.org/document/6225245/,2012 IEEE International Conference on Robotics and Automation,14-18 May 2012,ieeexplore
10.1109/ICSMC.2008.4811556,A digital circuit design of state-space recurrent neural networks,IEEE,Conferences,"This paper presents a digital circuit design of a state-space recurrent neural network (RNN). The proposed digital circuit design separates the datapath of the state-space RNN into a linear subcircuit and a nonlinear subcircuit. The linear subcircuit is realized by a matrix-vector multiplier while the nonlinear subcircuit by a customized nonlinear function computing unit. The throughput rate of the proposed RNN circuit is 36060.5 times faster than that of the software simulation using MATLAB<sup>reg</sup>. The proposed state-space RNN digital design methodology not only possesses the advantages including high computing speed, small area and portability, but also increases the possibility of using the digital RNN circuit in real-world dynamic problems.",https://ieeexplore.ieee.org/document/4811556/,"2008 IEEE International Conference on Systems, Man and Cybernetics",12-15 Oct. 2008,ieeexplore
10.1109/CISP.2012.6469706,A fast and highly accurate carrier acquisition for deep space applications,IEEE,Conferences,"On-board carrier acquisition in deep space telecommunications depends on the residual carrier, to overcome the detection difficulty of low-level received signal. Nowadays, it becomes reality for the receiver itself acquiring Doppler frequency offset. Frequency acquisition based on fast Fourier transform (FFT) has been applied extensively. In this paper, we introduce an algorithm of refined estimation after FFT frequency coarse estimation, which is theoretically analyzed to be maximum likelihood (ML) estimation. Its accuracy can achieve 10-4 when the signal-to-noise ratio (SNR) is 5dB. By the aid of data, the estimator performs well at low SNR ranges. In the design of carrier acquisition, the accurate estimation of frequency offset aids the phase-locked loop (PLL) into locked state. If the acquisition bandwidth of PLL is fixed, accurate estimation value helps to decrease the FFT size. Due to simple implementation and good performance, the algorithm can be applied in burst communications.",https://ieeexplore.ieee.org/document/6469706/,2012 5th International Congress on Image and Signal Processing,16-18 Oct. 2012,ieeexplore
10.1109/RAST.2003.1303948,A model of autonomous control system for deep space missions,IEEE,Conferences,"The particularities of autonomous control system for deep space missions are described. Some models are analyzed and compared. The team approach is analyzed in details. The general formal model is based on the theory of communicating sequential processes (CSP) and is set of communicating processes. Methods for reconfiguration, verification and trace control are described.",https://ieeexplore.ieee.org/document/1303948/,"International Conference on Recent Advances in Space Technologies, 2003. RAST '03. Proceedings of",20-22 Nov. 2003,ieeexplore
10.1109/MLSP.2015.7324376,A multi-layer discriminative framework for parking space detection,IEEE,Conferences,"In this paper, we proposed a new multi-layer discriminative framework for vacant parking space detection. From bottom to top, the framework consists of an image feature extraction layer, a patch classification layer, a weighted combination layer, and a status inference layer. In the feature extraction layer, the framework extracts lighting-invariant features to relieve the effects from lighting and shadow. In the patch classification layer, image patches are selected. In order To overcome perspective distortion, each patch was normalized. For different patch, we trained classifiers to recognize the occlusion patterns, which are treated as the middle-level feature of the parking status. In the weighted combination layer, three spaces are grouped as a unit to easily handle inter-object occlusion. Based on the middle-level features, a boosted space classifier was trained to determine the local status of a 3-space unit. In the status inference layer, we regarded these local status decisions as high-level evidences and inferred the final status of the parking lot. The results in an outdoor parking lot show our system can well handle inter-object occlusion and achieve robust vacant space detection under many environmental variations. A real-time system was also implemented to demonstrate its computing efficiency.",https://ieeexplore.ieee.org/document/7324376/,2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP),17-20 Sept. 2015,ieeexplore
10.1109/CCWC47524.2020.9031182,A proposed Crypto-Ransomware Early Detection(CRED) Model using an Integrated Deep Learning and Vector Space Model Approach,IEEE,Conferences,"Crypto-ransomware is a malware category that targets user-related files to encrypt them and hold them to ransom. The irreversible effect of crypto-ransomware attacks entails early detection before it starts encrypting the files. Although several works have been proposed to detect such attacks at the pre-encryption phase before the encryption takes place, the main limitation of these works is the way in which they define the boundaries of the pre-encryption phase. That is, these studies determine the pre-encryption boundaries based on tracking the first call of any cryptography-related Application Programming Interface (API). However, relying on the first call of cryptography-related APIs to delineate the pre-encryption boundaries is not accurate as these APIs might be related to other (normal) tasks done by the crypto-ransomware, such as unpacking and/or decrypting the metamorphic payload, before the ransomware starts the malicious activities. In that case, the collected pre-encryption data lack many relevant pre-encryption attack patterns that come after the mistakenly-identified pre-encryption boundary. Such data insufficiency adversely affects the accuracy of the detection model and increases the rate of false alarms. To overcome such limitations, this paper proposes an early detection model (CRED) that can determine the pre-encryption boundaries and collect the data related to this phase more accurately. Unlike the extant research, the CRED model employs data-centric and process-centric detection approaches to combine both IRP and API data. These data will then be used to train a deep learning-based model. The CRED model will be evaluated using a data-benchmark collected by executing real-world crypto-ransomware samples downloaded from a widely-used repository. The performance of the detection model will be validated using the k-fold cross validation and compared against the models proposed by the existing works.",https://ieeexplore.ieee.org/document/9031182/,2020 10th Annual Computing and Communication Workshop and Conference (CCWC),6-8 Jan. 2020,ieeexplore
10.1109/IECON.2005.1569052,A real-time neuro-computing three-dimensional space vector algorithm for three-phase four-leg converters,IEEE,Conferences,"Four-leg voltage source converters have successfully been used to nullify the zero-sequence current generated by unbalanced or nonlinear loads. This paper introduces an on-line, simple, intelligent, and computationally efficient neuro-computing classification algorithm for the implementation of three-dimensional space vector modulation (SVM) on four-leg voltage-source inverters. The proposed technique uses the concepts of counter propagation neural networks (CPN) for prism identification, and employs a nonlinear classifier network for tetrahedron identification. Nonlinear function approximations and bulky look up tables are successfully avoided, and exact positioning of the switching instants is obtained. Analytical analysis and simulations on a four-leg voltage-source converter validate the proposed scheme",https://ieeexplore.ieee.org/document/1569052/,"31st Annual Conference of IEEE Industrial Electronics Society, 2005. IECON 2005.",6-10 Nov. 2005,ieeexplore
10.1109/SICE.2002.1195611,A reinforcement learning using adaptive state space construction strategy for real autonomous mobile robots,IEEE,Conferences,"In the recent robotics, much attention has been focused on utilizing reinforcement learning for designing robot controllers. However, there still exists difficulties, one of them is well known as state space explosion problem. As the state space for a learning system becomes continuous and high dimensional, its combinational state space exponentially explodes and the learning process is time consuming. In this paper, we propose an adaptive state space recruitment strategy for reinforcement learning, which enables the system to divide state space gradually according to task complexity and progress of learning. Some simulation results and real robot implementation show the validity of the method.",https://ieeexplore.ieee.org/document/1195611/,Proceedings of the 41st SICE Annual Conference. SICE 2002.,5-7 Aug. 2002,ieeexplore
10.1109/IRDS.2002.1041504,A reinforcement learning with adaptive state space recruitment strategy for real autonomous mobile robots,IEEE,Conferences,"In the recent robotics, much attention has been focused on utilizing reinforcement learning for designing robot controllers. However, there still exists difficulties, one of them is well known as state space explosion problem. As the state space for learning system becomes continuous and high dimensional, the learning process results in time-consuming since its combinational states explodes exponentially. In order to adopt reinforcement learning for such complicated systems, it should be taken not only ""adaptability"" but ""computational efficiencies"" into account. In the paper, we propose an adaptive state space recruitment strategy for reinforcement learning, which enables the system to divide state space gradually according to task complexity and progress of learning. Some simulation results and real robot implementation show the validity of the method.",https://ieeexplore.ieee.org/document/1041504/,IEEE/RSJ International Conference on Intelligent Robots and Systems,30 Sept.-4 Oct. 2002,ieeexplore
10.1109/ROMAN.1995.531969,A shape knowledge representation scheme and its application on a multi-modal interface for a virtual space teleconferencing system,IEEE,Conferences,"This paper introduces recent advances in research on 3D shape ontologies. The purpose of shape ontologies is to formalize the characteristics of 3D geometric primitives so as to bridge the gap between knowledge-level representation of shapes in the human mind and their low-level computer graphics representations. In the formalizations described in this paper, an intermediate symbolic representation in the form of implicit, parametric functions is used for this purpose. With this intermediate level representation of shape knowledge it becomes possible to interactively change the ""meaning"" of a primitive by transforming its parametric space according to information extracted from natural language and/or hand gestures. Furthermore, ""meaningful"" hierarchical combinations of shapes into complex shapes also becomes possible because they can also be parameterized and thus formalized.",https://ieeexplore.ieee.org/document/531969/,Proceedings 4th IEEE International Workshop on Robot and Human Communication,5-7 July 1995,ieeexplore
10.1109/CEC.2005.1555005,A space saving digital VLSI evolutionary engine for CTRNN-EH devices,IEEE,Conferences,"Continuous time recurrent neural network - evolvable hardware (CTRNN-EH) control devices are composed of an analog continuous time recurrent neural network (CTRNN) with an onboard evolutionary algorithm (EA) engine that evolves the parameters of the neural network. These control devices have been demonstrated to be useful in a variety of real time control applications and are amenable to mixed-signal VLSI implementation for the control applications under stringent size and power constraints. Unlike the CTRNNs, which are analog in nature, the EA engine has to be implemented using digital VLSI techniques. Because these techniques do not offer the advantages of small area and power directly, the task of adhering to size and power constraints is challenging and must be accomplished at the algorithmic level. In this paper, the authors discussed the aforementioned issues in detail and also propose a space-saving digital EA engine for the CTRNN-EH device. The EA engine has been modeled in Verilog HDL. The synthesis results are presented and the functionality of the EA is demonstrated on a small test problem.",https://ieeexplore.ieee.org/document/1555005/,2005 IEEE Congress on Evolutionary Computation,2-5 Sept. 2005,ieeexplore
10.1109/VRAIS.1993.380739,A study of an operator assistant for virtual space,IEEE,Conferences,"A method to assist an operator to place an object on a surface in a virtual environment without any force feed-back tool is described and evaluated. The method used is to apply an attractive power to two faces that are likely to be attached. The system checks the distance of these faces and assists the operator by attaching two faces whose distance is less than a certain threshold. The real time calculation is achieved by limiting the number of faces that can be attached. Using this interface, the operator can move an object in some predefined planes. An experiment shows that the method is effective when the operator is requested to precisely place a virtual object in a certain location.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/380739/,Proceedings of IEEE Virtual Reality Annual International Symposium,18-22 Sept. 1993,ieeexplore
10.1109/ROMAN.1995.531967,A study of real time facial expression detection for virtual space teleconferencing,IEEE,Conferences,"A new method for real-time detection of facial expressions from time-sequential images is proposed. The proposed method does not need the tape marks that were pasted to the face for detecting expressions in real-time in the current implementation for the virtual space teleconferencing. In the proposed method, four windows are applied to the four areas in the face image: the left and right eyes, mouth and forehead. Each window is divided into blocks that consist of 8 by 8 pixels. The discrete cosine transform (DCT) is applied to each block, and the feature vector of each window is obtained from taking the summations of the DCT energies in the horizontal, vertical and diagonal directions. By a conversion table, the feature vectors are related to real 3D movements in the face. Experiment show some promising results for accurate expression detection and for the realization of real-time hardware implementation of the proposed method.",https://ieeexplore.ieee.org/document/531967/,Proceedings 4th IEEE International Workshop on Robot and Human Communication,5-7 July 1995,ieeexplore
10.1109/IICAIET49801.2020.9257862,ASIC Layout Design-Space Exploration of Pan-and-Tompkins Pre-Processing Algorithm for High Efficiency Electrocardiogram Monitor,IEEE,Conferences,"Cardiovascular diseases (CVDs) is the leading cause of the death globally. Ambulatory Electrocardiogram (ECG) and mobile monitoring is very important for early heart disease detection and prevention, but its measurement normally contains various types of noise which affect the analysis accuracy. Moreover, long hour ECG monitoring requires an efficient architecture to support real-time processing and low power consumption. This paper presents an application specific integrated circuit (ASIC) design of Pan-and-Tompkins ECG pre-processing algorithm which aims to remove several unwanted noise to increase analysis accuracy. The complete design flow covers high-level algorithm modelling in Matlab, followed by synthesizable design at Register Transfer Level (RTL) until logic synthesis, physical synthesis and static timing analysis to produce VLSI layout. Several power optimization techniques as well as different ASIC process technology libraries in terms of SilTerra's 180nm CMOS Logic Generic Library (CL180G) and Synopsys 32nm Generic Library (SAED32) are deployed for design-space exploration to study the design trade-off in terms of power consumption, timing performance, and the logic area usage. Results show that the clock gating technique is able to reduce 32.4% of dynamic power in design using CL180G generic library, whereas the integration of several power optimization techniques using SAED32 generic library is able to reduce 43.82% of dynamic power, 91.21% of leakage power and 91.25% of total power.",https://ieeexplore.ieee.org/document/9257862/,2020 IEEE 2nd International Conference on Artificial Intelligence in Engineering and Technology (IICAIET),26-27 Sept. 2020,ieeexplore
10.1109/ROBOT.2003.1242236,Ada - intelligent space: an artificial creature for the SwissExpo.02,IEEE,Conferences,"Ada is an entertainment exhibit that is able to interact with many people simultaneously, using a language of light and sound. ""She "" received 553,700 visitors over 5 months during the Swiss Expo.02 in 2002. In this paper we present the broad motivations, design and technologies behind Ada, and a first overview of the outcomes of the exhibit.",https://ieeexplore.ieee.org/document/1242236/,2003 IEEE International Conference on Robotics and Automation (Cat. No.03CH37422),14-19 Sept. 2003,ieeexplore
10.1109/GrC.2010.114,An Algorithm and Hardware Design for Very Fast Similarity Search in High Dimensional Space,IEEE,Conferences,"Similarity search in very high dimensions is vital for many scientific research activities as well as real applications. A high performance, scalable, and optimal quality solution to the problem still remains challenging. We propose a vote count based algorithm using p-stable distribution for approximate similarity search. Approximate similarity search effectively serves purpose for many real applications. Our algorithm is efficient and scalable with both dimension and database size. We also propose a novel hardware implementation of the algorithm using simple modification to Random Access Memory (RAM). The hardware design gives real time search for millions of points at practical cost. We empirically achieve high accuracy for query results using our algorithm on 128 dimensional synthetic and real datasets.",https://ieeexplore.ieee.org/document/5575953/,2010 IEEE International Conference on Granular Computing,14-16 Aug. 2010,ieeexplore
10.1109/WorldS4.2019.8904020,"An Integrated Framework for Autonomous Driving: Object Detection, Lane Detection, and Free Space Detection",IEEE,Conferences,"In this paper, we present a deep neural network based real-time integrated framework to detect objects, lane markings, and drivable space using a monocular camera for advanced driver assistance systems. The object detection framework detects and tracks objects on the road such as cars, trucks, pedestrians, bicycles, motorcycles, and traffic signs. The lane detection framework identifies the different lane markings on the road and also distinguishes between the ego lane and adjacent lane boundaries. The free space detection framework estimates the drivable space in front of the vehicle. In our integrated framework, we propose a pipeline combining the three deep neural networks into a single framework, for object detection, lane detection, and free space detection simultaneously. The integrated framework is implemented in C++ and runs real-time on the Nvidia's Drive PX 2 platform.",https://ieeexplore.ieee.org/document/8904020/,2019 Third World Conference on Smart Trends in Systems Security and Sustainablity (WorldS4),30-31 July 2019,ieeexplore
10.1109/FSKD.2011.6019741,An efficient clustering approach using ant colony algorithm in mutidimensional search space,IEEE,Conferences,"Clustering is an important data analysis technique and it widely used in many field such as data mining, machine learning and pattern recognition. Ant colony optimization clustering is one of the popular partition algorithm. However, in mutidimensional search space, its results is usually ordinary as the disturbing of redundant information. To address the problem, this paper presents MD-ACO clustering algorithm which improves the ant structure to implement attribute reduction. Four real data sets from UCI machine learning repository are used to evaluate MD-ACO with ACO. The results show that MD-ACO is more competitive.",https://ieeexplore.ieee.org/document/6019741/,2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD),26-28 July 2011,ieeexplore
10.1109/CDC.2012.6426325,An improved Predictive Optimal Controller with elastic search space for steam temperature control of large-scale supercritical power unit,IEEE,Conferences,"Predictive optimal control (POC) combined with artificial neural networks (ANNs) modeling and advanced heuristic optimization is a powerful technique for intelligent control. But actual implementation of the POC in complex industrial processes is limited by its known drawbacks, including the oscillation resulting from random search direction, difficulty in meeting the real-time requirement, and unresolved adaptability and generalization ability of the ANN predictive model. In resolving these problems, an improved Intelligent Predictive Optimal Controller (IPOC) with elastic search space is proposed in this paper. A new simpler and high-efficiency Particle Swarm Optimization (PSO) algorithm is adopted to find the optimal solution in fewer epochs to meet the real-time control requirements. The system output error in each control step is fed back to adjust the search space dynamically to prevent control oscillation and also make it easier to find the optimal solution. An improved recurrent neural network with external delayed inputs and outputs is constructed to model the dynamic response of the highly nonlinear system. The proposed IPOC is used to superheater steam temperature control of a 600MW supercritical power unit. Extensive control simulation tests are made to verify the validity of the new control scheme in a full-scope simulator.",https://ieeexplore.ieee.org/document/6426325/,2012 IEEE 51st IEEE Conference on Decision and Control (CDC),10-13 Dec. 2012,ieeexplore
10.1109/ICMLC.2014.7009153,An intelligent space location identification system based on passive RFID tags,IEEE,Conferences,"The development of context-aware control for smart space applications becomes popular recently, in which radio frequency identification (RFID) plays an important role. For an indoor context-aware smart space system, RFID tags and readers are utilized to locate object coordinates, cf. GPS utilizes GNSS for capture outdoor location information. Previous researches proposed to deploy regularly spaced reference tags, based on which the space location of target object tag can be estimated. Due to tag signal collision and received signal errors, the estimation accuracy is limited. We propose to utilize more than one RFID readers and control the reading power to eliminate estimation error. Passive tags that are cost effective are deployed for dense tag grid, e.g., for high accuracy location estimation. Experiments show that the proposed method can significantly reduce the estimation error from 69 cm to be under 15 cm. This RFID-based object location system can be applied to help people to locate personal belongings. In addition, a video surveillance system can be integrated with the object location method to provide context-awareness service.",https://ieeexplore.ieee.org/document/7009153/,2014 International Conference on Machine Learning and Cybernetics,13-16 July 2014,ieeexplore
10.1109/CASE49439.2021.9551267,Anomaly Detection Based on Selection and Weighting in Latent Space,IEEE,Conferences,"With the high requirements of automation in the era of Industry 4.0, anomaly detection plays an increasingly important role in high safety and reliability in the production and manufacturing industry. Recently, autoencoders have been widely used as a backend algorithm for anomaly detection. Different techniques have been developed to improve the anomaly detection performance of autoencoders. Nonetheless, little attention has been paid to the latent representations learned by autoencoders. In this paper, we propose a novel selection-and-weighting-based anomaly detection framework called SWAD. In particular, the learned latent representations are individually selected and weighted. Experiments on both benchmark and real-world datasets have shown the effectiveness and superiority of SWAD. On the benchmark datasets, the SWAD framework has reached comparable or even better performance than the state-of-the-art approaches.",https://ieeexplore.ieee.org/document/9551267/,2021 IEEE 17th International Conference on Automation Science and Engineering (CASE),23-27 Aug. 2021,ieeexplore
10.1109/ROBOT.2000.844768,Application of automatic action planning for several work cells to the German ETS-VII space robotics experiments,IEEE,Conferences,"Experiences in space robotics show, that the user normally has to cope with a huge amount of data. So, only robot and mission specialists are able to control the robot arm directly in teleoperation mode. By means of an intelligent robot control in cooperation with virtual reality methods, it is possible for non-robot specialists to generate tasks for a robot or an automation component intuitively. Furthermore, the intelligent robot control improves the safety of the entire system. The on-ground robot control and command station for the robot arm ERA onboard the satellite ETS-VII builds on a new resource-based action planning approach to manage robot manipulators and other automation components. In the case of ERA, the action planning system also takes care of the ""real"" robot onboard the satellite and the ""virtual"" robot in the simulation system. By means of the simulation system, the user can plan tasks ahead as well as analyze and visualize different strategies. The paper describes the mechanism of resource-based action planning, its application to different work cells, the practical experiences gained from the implementation for the on-ground robot control and command station for the robot arm ERA developed in the GETEX project as well as the services it provides to support VR-based man machine interfaces.",https://ieeexplore.ieee.org/document/844768/,Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),24-28 April 2000,ieeexplore
10.1109/RAST.2019.8767447,Artificial Intelligence Implementation on Voice Command and Sensor Anomaly Detection for Enhancing Human Habitation in Space Mission,IEEE,Conferences,"The work in this paper describes implementation of Artificial Neural Network (ANN) on space processor LEON3. The ANN has been tested for training voice signal and for detecting anomaly signal on multiple analog sensors. The build-in radiation hardened UART 115200 interface of standard Space Hardware was utilized to receive compressed data from Artificial Intelligence (AI) Kit. The AI Kit was built to acquire human voice (as voice command) or to process input signals from multiple sensors concurrently. The Kit enables voice intuitively by pressing a training button and selecting proper command type from the keypad. The experiment results show that voice commands were detected successfully with accuracy of more than 95%. The second experiment was carried out by using analog sensor signals mixer to allow AI to learn and determine type of sensor data anomalies when some failures occur. The anomalies types were generated by adding unexpected stimulus signals to AI Kit analog input terminals. The result shows that the anomalies can be detected with accuracy of 80%. The size of AI Kit is relatively small and it was built with commercial components that enable replacement with space radiation hardened components. The space computer platform is based on LEON3 processor core and synthesized on Xilinx Virtex-5QV Field Programmable Logic Arrays (FPGA). The core runs at 100 MHz. The feed forward artificial neural networks Algorithm was implemented on Real-Time Executive for Multiprocessor Systems (RTEMS) operating system. The AI Kit consists of audio signal pre-amplifier, automatic gain control circuit, analog signal buffer circuit, high pass filter circuit (HPF), analog to digital converter ADC which is integrated in the 8 bit microcontroller. The modified FFT algorithm that runs on microcontroller is used for data compression and for increasing uniqueness of the data acquired. A DC/DC converter for battery usage is included, when 5V voltage supply is not available.",https://ieeexplore.ieee.org/document/8767447/,2019 9th International Conference on Recent Advances in Space Technologies (RAST),11-14 June 2019,ieeexplore
10.1109/ICC.2019.8761821,Authentication Scheme Based on Hashchain for Space-Air-Ground Integrated Network,IEEE,Conferences,"With the development of artificial intelligence and self-driving, vehicular ad-hoc network (VANET) has become an irreplaceable part of the Intelligent Transportation Systems (ITSs). However, the traditional network of the ground cannot meet the requirements of transmission, processing, and storage among vehicles. Under this circumstance, integrating space and air nodes into the whole network can provide comprehensive traffic information and reduce the transmission delay. The high mobility and low latency in the Space-Air-Ground Integrated Network (SAGIN) put forward higher requirements for security issues such as identity authentication, privacy protection and data security. This paper simplifies the Blockchain and proposes an identity authentication and privacy protection scheme based on the Hashchain in the SAGIN. The scheme focuses on the characteristics of the wireless signal to identify and authenticate the nodes. The verification and backup of the records on the block are implemented with the distributed streaming platform, Kafka algorithm, instead of the consensus. Furthermore, this paper analyzes the security of this scheme. Afterward, the experimental results reveal the delay brought by the scheme using the simulation of SUMO, OMNeT++, and Veins.",https://ieeexplore.ieee.org/document/8761821/,ICC 2019 - 2019 IEEE International Conference on Communications (ICC),20-24 May 2019,ieeexplore
10.1109/RCAR49640.2020.9303043,Autobot for Effective Design Space Exploration and Agile Generation of RBFNN Hardware Accelerator in Embedded Real-time Computing,IEEE,Conferences,"This paper presents a method of employing Auto-bot to replace humans in the task of efficient hardware design for radial basis function neural network (RBFNN) in real-time computing applications. Autobot applies quick iterations using hardware generation and supports various number systems such as floating-point, half-floating point, and mixed-precision and hardware architectures to perform possible design space exploration, enabling an agile analysis for those requests. We have implemented and employed Autobot to successfully test with the applications of RBFNN-based Mackey-Glass chaotic time series prediction, servo motor control, and data classification. Analysis of these results shows that Autobot is able to deliver the hardware accelerator with less execution time than previous works, which also shortens the design time from days to minutes. Therefore, the proposed methodology is a useful alternative for agile real-time hardware development on FPGA.",https://ieeexplore.ieee.org/document/9303043/,2020 IEEE International Conference on Real-time Computing and Robotics (RCAR),28-29 Sept. 2020,ieeexplore
10.1109/CONECCT52877.2021.9622586,Benchmarking Transformer-Based Transcription on Embedded GPUs for Space Applications,IEEE,Conferences,"Speech transcription is a necessary tool for backend applications commonly found in voice assistants. Transcription is typically performed using cloud-based servers or custom hardware, but those resources are not always amenable to space environments due to size, weight, power, and cost constraints. Therefore, it is important to determine the performance of and optimal conditions for running transcription on hardware that is feasible for deployment in a space application. This research investigates and evaluates the performance of the wav2vec2 speech transcription engine, the current state-of-the-art model for this domain with and without optimizations. The target hardware, the NVIDIA Xavier NX Jetson embedded GPU, was chosen for its modern GPU architecture and small form factor. In addition to examining the input scaling behavior, we evaluate the hyperparameters of the clustered attention optimization, and average power and energy for inference relative to the operating power mode of the device. The clustered attention model outperformed the improved-clustered model for large input sizes, but the wav2vec2 model without clustering performed better for small input sizes. The clustered model energy per inference (13.90 J) was less than energy per inference of the improved-cluster model (15.03 J) and the vanilla softmax model (15.85 J). All models meet real-time speech processing requirements necessary to perform onboard inference entirely on a space system.",https://ieeexplore.ieee.org/document/9622586/,"2021 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",9-11 July 2021,ieeexplore
10.1109/ISNCC.2018.8530988,Building an Intelligent and Efficient Smart Space to Detect Human Behavior in Common Areas,IEEE,Conferences,"Smart spaces have become an integral part of our daily routines to improve quality of life for many different groups of people. The use of embedded systems to build these smart spaces, in combination with data analytics, can provide real-time information about the environment and how it interacts with the people in it. In this paper, we demonstrate how one embedded system that acquires data based on a 2-dimensional positional-grid, movement, temperature and vibration is used to build a smart and pervasive space. Data collected from these sensors is used for real time localization in conjunction with machine learning mechanisms to analyze human activities. We evaluate five machine learning algorithms, namely Logistic Regression, Support Vector Machine, Decision Tree, Random Forest, Naive Bayes and Artificial Neural Network applied on a dataset collected in our lab. Results show high classification performance for all methods giving up-to 99.95% classification accuracy. These patterns provide useful information about occupancy patterns, movement patterns, etc., which will be later used to allocate computational resources in the smart space accordingly. Furthermore, our implementation does not use any camera or microphone deployment, hence addressing potential privacy issues.",https://ieeexplore.ieee.org/document/8530988/,"2018 International Symposium on Networks, Computers and Communications (ISNCC)",19-21 June 2018,ieeexplore
10.1109/AERO.2017.7943916,Classification of multi-failure mechanisms in space operations in using novel PLS-DA approach,IEEE,Conferences,"The article addresses an innovative statistical classification methodology to the real spacecraft telemetry based on statistical multivariate latent technique called projection to latent structure discriminant analysis (PLS-DA). The models are generated via using a well-known statistical multivariate software called soft independent modeling for class analogy (SIMCA-P) developed by Umetrics which is used for data exploration and classification. Models are used to detect and classify multi-failure mechanisms in the attitude determination and control subsystem (ADCS) of Egypt-Sat1 for the first time. Models taken altogether the ADCS two faults mechanisms for both angular velocity meters and reaction wheel malfunctions in one model, as well as to identify key contributors to inconsistent events autonomously which lead to characterize the spacecraft (ADCS) behavior. The analysis results lead to give a deep explanation to the system state-of-health (SOH) and characterize its operation behavior during the mission phases.",https://ieeexplore.ieee.org/document/7943916/,2017 IEEE Aerospace Conference,4-11 March 2017,ieeexplore
10.1109/CNNA.2002.1035095,Colour space transformation and exact colour reproduction with CNN technology,IEEE,Conferences,"Nowadays many problems requiring huge computing power have risen. Although the performance of digital processors doubles every year, there are certain tasks where the computation cannot be carried out within a reasonable time interval. Such hard problems are the analysis of big dynamical systems or real-time exact colour reproduction. The exact colour visualization of motion pictures is necessary in industrial, medical and scientific research areas. Thus, for example, exact colour reproduction is required for remote medical diagnosis or remote operation. The doctor has to see the same image that appears in reality. Device dependent colour appearance may cause faulty decisions. Nowadays these problems cannot be solved perfectly because many steps of the transformation are not completely known and the huge number of computations cannot be done in real-time even by the fastest PC. In this article we describe some methods to produce exact colours in a remote medical diagnostic system.",https://ieeexplore.ieee.org/document/1035095/,Proceedings of the 2002 7th IEEE International Workshop on Cellular Neural Networks and Their Applications,24-24 July 2002,ieeexplore
10.1109/ISIC.2007.4450916,Construction of Expanded Input Space for Modeling Hysteretic Systems,IEEE,Conferences,"A neural model for hysteresis based on expanded input space is proposed in this article. In this method, the behavior of hysteresis is considered as a dynamic system that can be described by a nonlinear state space equation containing hysteretic state. In order to transfer the multi-valued mapping of hysteresis into a one-to-one mapping, an expanded input space involving the original input variable and a so-called Duhem operator is constructed. Thus, the neural networks can be employed to approximate the relation between the hysteretic state and the output of the system that is also the output of hysteresis. The proposed model has a simple architecture that can be easily implemented for on-line adaptation for the model in case of the unexpected change of operating environment. Furthermore, the dynamic performance of the model is improved because of the existence of Duhem operator. Finally, the method is used to the modeling of hysteresis in a piezoelectric actuator.",https://ieeexplore.ieee.org/document/4450916/,2007 IEEE 22nd International Symposium on Intelligent Control,1-3 Oct. 2007,ieeexplore
10.1109/ICMIM48759.2020.9299052,Deep Open Space Segmentation using Automotive Radar,IEEE,Conferences,"In this work, we propose the use of radar with advanced deep segmentation models to identify open space in parking scenarios. A publically available dataset of radar observations called SCORP was collected. Deep models are evaluated with various radar input representations. Our proposed approach achieves low memory usage and real-time processing speeds, and is thus very well suited for embedded deployment.",https://ieeexplore.ieee.org/document/9299052/,2020 IEEE MTT-S International Conference on Microwaves for Intelligent Mobility (ICMIM),23-23 Nov. 2020,ieeexplore
10.1109/CAIA.1990.89189,Demonstrating artificial intelligence for space systems-integration and project management issues,IEEE,Conferences,"As part of its systems autonomy demonstration project (SADP), the National Aeronautics and Space Administration (NASA) has recently demonstrated the Thermal Expert System (TEXSYS). Advanced real-time expert system and human interface technology was successfully developed and integrated with conventional controllers of prototype space hardware to provide intelligent fault detection, isolation and recovery capability. Many specialized skills were required, and responsibility for the various phases of the project therefore spanned multiple NASA centers, internal departments and contractor organizations. The test environment required communication among many types of hardware and software as well as between many people. The integration, testing, and configuration management tools and methodologies which were applied to the TEXSYS project to assure its safe and successful completion are detailed. The project demonstrated that artificial intelligence technology, including model-based reasoning, is capable of the monitoring and control of a large, complex system in real time.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/89189/,Sixth Conference on Artificial Intelligence for Applications,5-9 May 1990,ieeexplore
10.1109/ICRA.2016.7487691,Denoising auto-encoders for learning of objects and tools affordances in continuous space,IEEE,Conferences,"The concept of affordances facilitates the encoding of relations between actions and effects in an environment centered around the agent. Such an interpretation has important impacts on several cognitive capabilities and manifestations of intelligence, such as prediction and planning. In this paper, a new framework based on denoising Auto-encoders (dA) is proposed which allows an agent to explore its environment and actively learn the affordances of objects and tools by observing the consequences of acting on them. The dA serves as a unified framework to fuse multi-modal data and retrieve an entire missing modality or a feature within a modality given information about other modalities. This work has two major contributions. First, since training the dA is done in continuous space, there will be no need to discretize the dataset and higher accuracies in inference can be achieved with respect to approaches in which data discretization is required (e.g. Bayesian networks). Second, by fixing the structure of the dA, knowledge can be added incrementally making the architecture particularly useful in online learning scenarios. Evaluation scores of real and simulated robotic experiments show improvements over previous approaches while the new model can be applied in a wider range of domains.",https://ieeexplore.ieee.org/document/7487691/,2016 IEEE International Conference on Robotics and Automation (ICRA),16-21 May 2016,ieeexplore
10.1109/IJCNN.2019.8852005,Design Space Evaluation of a Memristor Crossbar Based Multilayer Perceptron for Image Processing,IEEE,Conferences,"This paper describes a simulated memristor-based neuromorphic system that can be used for ex-situ training of a multi-layer perceptron algorithm. The presented programming technique can be used to map the weights required of a neural algorithm directly onto the grid of resistances in a memristor crossbar. Using this weight-to-crossbar mapping approach along with the dot product calculation circuit, neural algorithms can be easily implemented using this system. To show the effectiveness of this circuit, a Multilayer Perceptron is trained to perform Sobel edge detection. Following these simulations, an analysis was presented that shows how memristor programming accuracy and network size are related to output error; the results show that network size can be increased to reduce testing error. In some cases, the memristors in the circuit may be capable of operating with at lower precision if the network size is increased. This means that less precise (or lower resolution) memristor devices may be used to implement the proposed system. Furthermore, a power, timing, and energy analysis shows that this circuit has a computation throughput that allows it to process 4K UHD video in real time at approximately 337mW.",https://ieeexplore.ieee.org/document/8852005/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore
10.1109/ETFA.2015.7301549,Design and implementation for multiple-robot deployment in intelligent space,IEEE,Conferences,"This paper presents the problem of robot deployment for a number of scattered tasks. We aim to minimize the duration it takes for all robots to reach their assigned task locations. In previous work, we have proposed a team composed of one carrier robot (CR) and several servant robots to accomplish the mission. Then we have suggested an algorithm that determines a path of the CR for an efficient deployment under a few constraints, which is verified by simulations. Assuming that the servant robots are unmanned aerial vehicles (UAVs), the present paper extends the discussion to a real robot experiment. We design and implement a deployment system in intelligent space. The feasibility of the study is demonstrated through an experiment.",https://ieeexplore.ieee.org/document/7301549/,2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA),8-11 Sept. 2015,ieeexplore
10.1109/ICCS.2008.4737145,Design of new minimum decoding complexity quasi-orthogonal Space-Time Block Code for four transmit antennas,IEEE,Conferences,"A new Space-Time Block Code (STBC) achieving full rate and full diversity for general QAM and four transmit antennas is proposed. This code also possesses a quasi-orthogonal (QO) property like the conventional Minimum Decoding Complexity QO-STBC (MDC-QO-STBC), leading to joint ML detection of only two real symbols. The proposed code is shown to exhibit the identical error performance with the existing MDC-QO-STBC. However, the proposed code has an advantage in the transceiver implementation since this code can be modified so that the increase of PAPR occurs at only two transmit antennas, but the MDC-QO-STBC at all of transmit antennas.",https://ieeexplore.ieee.org/document/4737145/,2008 11th IEEE Singapore International Conference on Communication Systems,19-21 Nov. 2008,ieeexplore
10.1109/ASPDAC.2016.7428073,Design space exploration of FPGA-based Deep Convolutional Neural Networks,IEEE,Conferences,"Deep Convolutional Neural Networks (DCNN) have proven to be very effective in many pattern recognition applications, such as image classification and speech recognition. Due to their computational complexity, DCNNs demand implementations that utilize custom hardware accelerators to meet performance and energy-efficiency constraints. In this paper we propose an FPGA-based accelerator architecture which leverages all sources of parallelism in DCNNs. We develop analytical feasibility and performance estimation models that take into account various design and platform parameters. We also present a design space exploration algorithm for obtaining the implementation with the highest performance on a given platform. Simulation results with a real-life DCNN demonstrate that our accelerator outperforms other competing approaches, which disregard some sources of parallelism in the application. Most notably, our accelerator runs 1.9 faster than the state-of-the-art DCNN accelerator on the same FPGA device.",https://ieeexplore.ieee.org/document/7428073/,2016 21st Asia and South Pacific Design Automation Conference (ASP-DAC),25-28 Jan. 2016,ieeexplore
10.1109/ISCAS.2018.8351685,Design-Space Exploration of Pareto-Optimal Architectures for Deep Learning with DVFS,IEEE,Conferences,"Specialized computing engines are required to accelerate the execution of Deep Learning (DL) algorithms in an energy-efficient way. To adapt the processing throughput of these accelerators to the workload requirements while saving power, Dynamic Voltage and Frequency Scaling (DVFS) seems the natural solution. However, DL workloads need to frequently access the off-chip memory, which tends to make the performance of these accelerators memory-bound rather than computation-bound, hence reducing the effectiveness of DVFS. In this work we use a performance-power analytical model fitted on a parametrized implementation of a DL accelerator in a 28-nm FDSOI technology to explore a large design space and to obtain the Pareto points that maximize the effectiveness of DVFS in the sub-space of throughput and energy efficiency. In our model we consider the impact on performance and power of the off-chip memory using real data of a commercial low-power DRAM.",https://ieeexplore.ieee.org/document/8351685/,2018 IEEE International Symposium on Circuits and Systems (ISCAS),27-30 May 2018,ieeexplore
10.1109/3DIM.2005.33,Discrete pose space estimation to improve ICP-based tracking,IEEE,Conferences,"Iterative closest point (ICP)-based tracking works well when the interframe motion is within the ICP minimum well space. For large interframe motions resulting from a limited sensor acquisition rate relative to the speed of the object motion, it suffers from slow convergence and a tendency to be stalled by local minima. A novel method is proposed to improve the performance of ICP-based tracking. The method is based upon the bounded Hough transform (BHT) which estimates the object pose in a coarse discrete pose space. Given an initial pose estimate, and assuming that the interframe motion is bounded in all 6 pose dimensions, the BHT estimates the current frame's pose. On its own, the BHT is able to track an object's pose in sparse range data both efficiently and reliably, albeit with a limited precision. Experiments on both simulated and real data show the BHT to be more efficient than a number of variants of the ICP for a similar degree of reliability. A hybrid method has also been implemented wherein at each frame the BHT is followed by a few ICP iterations. This hybrid method is more efficient than the ICP, and is more reliable than either the BHT or ICP separately.",https://ieeexplore.ieee.org/document/1443287/,Fifth International Conference on 3-D Digital Imaging and Modeling (3DIM'05),13-16 June 2005,ieeexplore
10.1109/ICPADS.2015.110,Dynamic Web Service Composition Based on State Space Searching,IEEE,Conferences,"Web service composition problem was considered as a planning problem by previous research. However, many factors constantly affect the QoS and results of invocation of web services, thus the environment of web services is dynamic. As result, web service composition problem should be considered as an uncertain planning problem. This paper uses Markov property to deal with the uncertain planning problem for service composition. According to the uncertainty model, we propose a reinforcement learning method to compose web services. Without knowing the transition function and reward function, our uncertain planning method uses an estimated value function to approach a real function and is able to obtain a composite service. The results of experiments show that our method can effectively reduce computing time of the service composition.",https://ieeexplore.ieee.org/document/7384373/,2015 IEEE 21st International Conference on Parallel and Distributed Systems (ICPADS),14-17 Dec. 2015,ieeexplore
10.1109/BDCAT50828.2020.00018,Edge-enhanced analytics via latent space dimensionality reduction,IEEE,Conferences,"With the Internet of Things technology, almost any remote sensing devices, wearables, and smart objects are equipped to transmit large volumes of data in continuous streams. In conventional cloud-centric analytics, all the raw data is transferred to a data centre and processed in real-time, near real-time, or in batches. However, this approach is usually not very responsive to real-time analytics due to the latency in transmission alongside network traffic, bandwidth and data transmission costs. To tackle this, edge-enhanced analytics ensures that raw data can be preprocessed at the edge and sent across the network channel in a more compact form. A specific category of deep learning model, autoencoder, can help to achieve this by transforming high-dimensional data into compact representation. We propose an edge-enhanced framework which deploys a deep autoencoder model on the network edge for data compression. After training of models in the cloud, the encoder part of the autoencoder is deployed on the edge for data reduction while the decoder remains on the cloud to reconstruct the data for an image classification task on the cloud. We applied supervised fine-tuning using the intrinsic dimensionality of the data to achieve an accuracy that surpasses the baseline cloud model. The solution was explored in the context of an image recognition problem using the MNIST and FASHION-MNIST datasets. The framework was validated on an event simulator to estimate the network savings of the proposed method in terms of bandwidth and latency. The edge-enhanced approach saves up to 74% bandwidth compared to the centralised analytics. In addition, real-time analytics is further improved by taking 25% less time to complete the task.",https://ieeexplore.ieee.org/document/9302537/,"2020 IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT)",7-10 Dec. 2020,ieeexplore
10.1109/ROMAN.2011.6005223,Effect of human guidance and state space size on Interactive Reinforcement Learning,IEEE,Conferences,"The Interactive Reinforcement Learning algorithm enables a human user to train a robot by providing rewards in response to past actions and anticipatory guidance to guide the selection of future actions. Past work with software agents has shown that incorporating user guidance into the policy learning process through Interactive Reinforcement Learning significantly improves the policy learning time by reducing the number of states the agent explores. We present the first study of Interactive Reinforcement Learning in real-world robotic systems. We report on four experiments that study the effects that teacher guidance and state space size have on policy learning performance. We discuss modifications made to apply Interactive Reinforcement Learning to a real-world system and show that guidance significantly reduces the learning rate, and that its positive effects increase with state space size.",https://ieeexplore.ieee.org/document/6005223/,2011 RO-MAN,31 July-3 Aug. 2011,ieeexplore
10.1109/IJCNN.1999.831156,Efficient training techniques for classification with vast input space,IEEE,Conferences,"Strategies to efficiently train a neural network for an aerospace problem with a large multidimensional input space are developed and demonstrated. The neural network provides classification for over 100,000,000 data points. A query-based strategy is used that initiates training using a small input set, and then augments the set in multiple stages to include important data around the network decision boundary. Neural network inversion and oracle query are used to generate the additional data, jitter is added to the query data to improve the results, and an extended Kalman filter algorithm is used for training. A causality index is discussed as a means to reduce the dimensionality of the problem based on the relative importance of the inputs.",https://ieeexplore.ieee.org/document/831156/,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),10-16 July 1999,ieeexplore
10.1109/TAI.1993.633958,Elastic version space: a knowledge acquisition method with background knowledge adjustment,IEEE,Conferences,"Similarity based learning (SBL) is efficient in knowledge acquisition process, and it uses training examples to generate rules and refine them. Training examples collected in the real world are very often constructed with numerical attributes. In order to deal with these training examples, SBL needs background knowledge. Especially segments which specify value ranges of numerical attributes are discussed in the background knowledge. Elastic version space method is proposed here which integrates the version space method with the functions of segments adjustment. By defining the segments structure using margin segments in the background knowledge, the version space method itself adjusts the segments. In consequence, this method expands the region of application of version space. Empirical results applying to the man-power allocation problem are presented which shows that the elastic version space method is an effective SBL in the knowledge acquisition process.",https://ieeexplore.ieee.org/document/633958/,Proceedings of 1993 IEEE Conference on Tools with Al (TAI-93),8-11 Nov. 1993,ieeexplore
10.1109/WCICA.2000.863468,Estimated force emulation for space robot using neural networks,IEEE,Conferences,"This paper introduces the telerobotic system estimated force emulation using neural networks. A delay-compensating 3D stereo-graphic simulator is implemented in SGI ONYX/4 RE/sup 2/. The estimated force emulation can protect the real robot in time from being damaged in collision. The neural network is used to learn the mapping between the contact force error and the accommodated position command to the controller of the space robot. Finally, the controller can feel the emulated force with a two-hand 6-DOF master arm using the force feedback interface.",https://ieeexplore.ieee.org/document/863468/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore
10.1109/SNPD.2012.99,Evaluation of Realism of Dynamic Sound Space Using a Virtual Auditory Display,IEEE,Conferences,"We can perceive a sound position from binaural signals using mainly head-related transfer functions (HRTFs). Using the theorem presented herein, we can display a sound image to a specific position in virtual auditory space by HRTFs. However, HRTF is defined commonly in a free-field, and a virtual sound image is perceived as a dry source without reflection, reverberation, or ambient noise. Therefore, the virtual sound space might be unnatural. The authors developed a software-based virtual auditory display (VAD) that outputs audio signals for a set of headphones with a three-dimensional position sensor. The VAD software can display a dynamic virtual auditory space that is responsive to a listener's head movement. Subjective evaluations were conducted to clarify the relation between the perceived reality of virtual sound space and ambient sound. Evaluation results of the reality of the virtual sound space displayed by the VAD software are introduced.",https://ieeexplore.ieee.org/document/6299338/,"2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",8-10 Aug. 2012,ieeexplore
10.1109/SCORED.2009.5443323,FPGA implementation of a space-time trellis decoder,IEEE,Conferences,This paper describes the real-time implementation of a space-time trellis encoder and decoder using the Xilinx Virtex-4FX12 FPGA. The code uses a generator matrix designed for 4-state space-time trellis (STT) that uses Quadrature Phase Shift Keying (QPSK) modulation scheme. The decoding process was done using Maximum Likelihood (ML) through the Viterbi Algorithm. The results show that the STT decoder can successfully decipher the encoded symbols from the STT encoder and that it can fully recover the original data in the absence of noise. The data rate of the decoder was 6.25 Msymbols/s. It was shown that 14% of the logic elements in Virtex 4 FPGA were used in implementing an encoder-decoder system.,https://ieeexplore.ieee.org/document/5443323/,2009 IEEE Student Conference on Research and Development (SCOReD),16-18 Nov. 2009,ieeexplore
10.1109/IJCNN.2013.6707020,Feature construction approach for email categorization based on term space partition,IEEE,Conferences,"This paper proposes a novel feature construction approach based on term space partition (TSP) aiming to establish a mechanism to make terms play more sufficient and rational roles in email categorization. Dominant terms and general terms are separated by performing a vertical partition of the original term space with respect to feature selection metrics, while spam terms and ham terms are separated by a transverse partition with respect to class tendency. Strategies for constructing discriminative features, named term ratio and term density, are designed on corresponding subspaces. Motivation and principle of the TSP approach is presented in detail, as well as the implementation. Experiments are conducted on five benchmark corpora using cross-validation to evaluate the proposed TSP approach. Comprehensive experimental results suggest that the TSP approach far outperforms the traditional and most widely used feature construction approach in spam filtering, which is named bag-of-words, in both performance and efficiency. In comparison with the heuristic and state-of-the-art approaches, namely CFC and LC, the proposed TSP approach shows obvious advantage in terms of accuracy and <sub>1</sub> measure, as well as high precision, which is warmly welcomed in real spam filtering. Furthermore, the TSP approach performs quite similar with CFC in efficiency of processing incoming emails, while much faster than LC. In addition, it is shown that the TSP approach cooperates well with both unsupervised and supervised feature selection metrics, which endows it with flexible capability in the real world.",https://ieeexplore.ieee.org/document/6707020/,The 2013 International Joint Conference on Neural Networks (IJCNN),4-9 Aug. 2013,ieeexplore
10.1109/IJCNN.2001.939561,Fuzzy clusters identification in the feature space using neural networks,IEEE,Conferences,Deals with the development of ARTMAP-like neural networks to analyze feature space for classification purposes. The proposed tool provides information about the value of membership functions of the unknown input vector to each class of interest. The designed ARTMAP-like system is called MF-ARTMAP based on the fact that membership functions are calculated. The functions shape is predefined as Gaussian with adaptation of mean value and variance in each feature space dimension during the training procedure. The parallel version of this approach is designed and implemented too. The parallel MF ARTMAP have some advantages over regular MF ARTMAP. The usefulness of this approach is presented on the benchmark classification problems e.g. circle in the square and spiral and on real-world data from satellite images over Slovakia. Classification accuracy is calculated using the contingency tables approach on actual and predicted classes of interest.,https://ieeexplore.ieee.org/document/939561/,IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222),15-19 July 2001,ieeexplore
10.1109/INES52918.2021.9512921,Game Feature Validation of a Real-Time Game Space with an eXtended Classifier System,IEEE,Conferences,"The objective of this paper is the proposal of a new approach for the game feature validation of a game space with the eXtended Classifier System (XCS) algorithm. For initial ""proof-of-concept"" evaluation we used the game space of the Tic-Tac-Toe game, which was placed in a context of real-time characteristics. Evaluation was done with the XCS algorithm without pre-processing internal knowledge programmed (online mode). Evaluation data results were acquired under real-time constraints. No-loss-strategy was implemented for the game, with various scenarios tested to find out if the algorithm has the capability of finding invalid game feature design flaws. Results indicated that the XCS algorithm is able to deliver stable validation of game feature testing results, can be a powerful tool and, therefore, worthy of further research in the Gaming domain. Confirmation of this core concept testing with this type of algorithm (and similar ones) is necessary, since it paves the way for further research in more complex game environments, where the dimension size, the number of aspects, game states and game actions rises intensely.",https://ieeexplore.ieee.org/document/9512921/,2021 IEEE 25th International Conference on Intelligent Engineering Systems (INES),7-9 July 2021,ieeexplore
10.1109/VRAIS.1996.490517,Gaze-directed adaptive rendering for interacting with virtual space,IEEE,Conferences,"This paper presents a new method of rendering for interaction with 3D virtual space with the use of gaze detection devices. In this method, hierarchical geometric models of graphic objects are constructed prior to the rendering process. The rendering process first calculates the visual acuity, which represents the importance of a graphic object for a human operator, from the gaze position of the operator. Second, the process selects a level from the set of hierachical geometric models depending on the value of visual acuity. That is, a simpler level of detail is selected where the visual acuity is lower, and a more complicated level is used where it is higher. Then, the selected graphic models are rendered on the display. This paper examines three visual characteristics to calculate the visual acuity: the central/peripheral vision, the kinetic vision, and the fusional vision. The actual implementation and our testbed system are described, as well as the details of the visual acuity model.",https://ieeexplore.ieee.org/document/490517/,Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium,30 March-3 April 1996,ieeexplore
10.1109/BioCAS.2015.7348397,General-purpose LSM learning processor architecture and theoretically guided design space exploration,IEEE,Conferences,"This paper presents a general-purpose liquid state machine based neuromorphic learning processor with integrated training and recognition for real world pattern recognition problems. The proposed architecture consists of a generic preprocessor and one or multiple task processors. The pre-processor, or the reservoir, consists of a recurrent spiking neural network with fixed synaptic weights. Task processors are light weight and comprise a set of readout spiking neurons with plastic weights, which are tuned by a biologically plausible supervised learning rule. Importantly, we leverage the unique computational structure of the reservoir for highly efficient implementation of multiple tasks on the same learning processor. A novel theoretical measure of computational power, which is strongly correlated with the true learning performance, is proposed to facilitate fast design space exploration of the recurrent reservoir. We demonstrate the application of our processor architecture by mapping four recognition tasks onto a reconfigurable FPGA processor platform.",https://ieeexplore.ieee.org/document/7348397/,2015 IEEE Biomedical Circuits and Systems Conference (BioCAS),22-24 Oct. 2015,ieeexplore
10.1109/TAI.1995.479372,Genetic algorithms as a tool for restructuring feature space representations,IEEE,Conferences,"This paper describes an approach being explored to improve the usefulness of machine learning techniques to classify complex, real world data. The approach involves the use of genetic algorithms as a ""front end"" to a traditional tree induction system (ID3) in order to find the best feature set to be used by the induction system. This approach has been implemented and tested on difficult texture classification problems. The results are encouraging and indicate significant advantages of the presented approach.",https://ieeexplore.ieee.org/document/479372/,Proceedings of 7th IEEE International Conference on Tools with Artificial Intelligence,5-8 Nov. 1995,ieeexplore
10.1109/IROS.2008.4651150,High-dimensional underactuated motion planning via task space control,IEEE,Conferences,"Kinodynamic planning algorithms have the potential to find feasible control trajectories which accomplish a task even in very nonlinear or constrained dynamical systems. Underactuation represents a particular form of a dynamic constraint, inherently present in many machines of interest (e.g., walking robots), and necessitates planning for long-term control solutions. A major limitation in motion planning techniques, especially for real-time implementation, is that they are only practical for relatively low degree-of-freedom problems. Here we present a model-based dimensionality reduction technique based on an extension of partial feedback linearization control into a task-space framework. This allows one to plan motions for a complex underactuated robot directly in a low-dimensional task-space, and to resolve redundancy with lower-priority tasks. We illustrate the potential of this approach with an extremely simple motion planning system which solves the swing-up problem for multi-link underactuated pendula, and discuss extensions to the control of walking.",https://ieeexplore.ieee.org/document/4651150/,2008 IEEE/RSJ International Conference on Intelligent Robots and Systems,22-26 Sept. 2008,ieeexplore
10.1109/ALLERTON.2009.5394528,Highly parallel decoding of space-time codes on graphics processing units,IEEE,Conferences,Graphics processing units (GPUs) with a few hundred extremely simple processors represent a paradigm shift for highly parallel computations. We use this emergent GPU architecture to provide a first demonstration of the feasibility of real time ML decoding (in software) of a high rate space-time block code that is representative of codes incorporated in 4th generation wireless standards such as WiMAX and LTE. The decoding algorithm is conditional optimization which reduces to a parallel calculation that is a natural fit to the architecture of low cost GPUs. Experimental results demonstrate that asymptotically the GPU implementation is more than 700 times faster than a standard serial implementation. These results suggest that GPU architectures have the potential to improve the cost / performance tradeoff of 4th generation wireless base stations. Additional benefits might include reducing the time required for system development and the time required for configuration and testing of wireless base stations.,https://ieeexplore.ieee.org/document/5394528/,"2009 47th Annual Allerton Conference on Communication, Control, and Computing (Allerton)",30 Sept.-2 Oct. 2009,ieeexplore
10.1109/VRAIS.1995.512487,Human figure synthesis and animation for virtual space teleconferencing,IEEE,Conferences,"Human figure animation is it widely researched area with many applications. This paper addresses specific issues that deal with the synthesis, animation and environmental interaction of human figures within a virtual space teleconferencing system. A layered representation of the human figure is adopted. Skeletal posture is determined from magnetic sensors on the body, using heuristics and inverse kinematics. This paper describes the use of implicit function techniques in the synthesis and animation of a polymesh geometric skin over the skeletal structure. Implicit functions perform detection and handling of collisions with an optimal worst case time complexity that is linear in the number polymesh vertices. Body deformations resulting from auto-collisions are handled elegantly and homogeneously as part of the environment. Further, implicit functions generate precise collision contact surfaces and have the capability to model the physical characteristics of muscles in systems that employ force feedback. The real time implementation within a virtual space teleconferencing system, illustrates this new approach, coupling polymesh and implicit surface based modeling and animation techniques.",https://ieeexplore.ieee.org/document/512487/,Proceedings Virtual Reality Annual International Symposium '95,11-15 March 1995,ieeexplore
10.1109/CCAAW.2017.8001607,Implementation of a space communications cognitive engine,IEEE,Conferences,"Although communications-based cognitive engines have been proposed, very few have been implemented in a full system, especially in a space communications system. In this paper, we detail the implementation of a multi-objective reinforcement-learning algorithm and deep artificial neural networks for the use as a radio-resource-allocation controller. The modular software architecture presented encourages re-use and easy modification for trying different algorithms. Various trade studies involved with the system implementation and integration are discussed. These include the choice of software libraries that provide platform flexibility and promote reusability, choices regarding the deployment of this cognitive engine within a system architecture using the DVB-S2 standard and commercial hardware, and constraints placed on the cognitive engine caused by real-world radio constraints. The implemented radio-resource-allocation-management controller was then integrated with the larger space-ground system developed by NASA Glenn Research Center (GRC).",https://ieeexplore.ieee.org/document/8001607/,2017 Cognitive Communications for Aerospace Applications Workshop (CCAA),27-28 June 2017,ieeexplore
10.1109/ASIANCON51346.2021.9544615,Indian Sign language Recognition Using Color Space Model and Thresholding,IEEE,Conferences,"Sign language is for deaf and mute people. Population as large as India's provides for an enormous section of people using Indian sign language to communicate. Unfortunately, a major chunk of the population does not understand sign language which limits communication between those with disabilities and wider population. We are proposing a system to harness this communication chasm. First the images are taken from webcam in RGB color space. Preprocessing and semantic segmentation are applied on the input image. By employing a simple background, the RGB segmented image is transformed to gray scale and background noise is removed. Otsu's segmentation method is used to segment the image. Convolutional Neural Networks were employed, utilizing skin segmented hand images as the input. On the training data, a classification accuracy of 99.33% was attained for 36 static hand gestures from Indian Sign Language. The above mentioned model performed significantly well in real time implementation.",https://ieeexplore.ieee.org/document/9544615/,2021 Asian Conference on Innovation in Technology (ASIANCON),27-29 Aug. 2021,ieeexplore
10.1109/SP40000.2020.00073,Intriguing Properties of Adversarial ML Attacks in the Problem Space,IEEE,Conferences,"Recent research efforts on adversarial ML have investigated problem-space attacks, focusing on the generation of real evasive objects in domains where, unlike images, there is no clear inverse mapping to the feature space (e.g., software). However, the design, comparison, and real-world implications of problem-space attacks remain underexplored.This paper makes two major contributions. First, we propose a novel formalization for adversarial ML evasion attacks in the problem-space, which includes the definition of a comprehensive set of constraints on available transformations, preserved semantics, robustness to preprocessing, and plausibility. We shed light on the relationship between feature space and problem space, and we introduce the concept of side-effect features as the byproduct of the inverse feature-mapping problem. This enables us to define and prove necessary and sufficient conditions for the existence of problem-space attacks. We further demonstrate the expressive power of our formalization by using it to describe several attacks from related literature across different domains.Second, building on our formalization, we propose a novel problem-space attack on Android malware that overcomes past limitations. Experiments on a dataset with 170K Android apps from 2017 and 2018 show the practical feasibility of evading a state-of-the-art malware classifier along with its hardened version. Our results demonstrate that ""adversarial-malware as a service"" is a realistic threat, as we automatically generate thousands of realistic and inconspicuous adversarial applications at scale, where on average it takes only a few minutes to generate an adversarial app. Yet, out of the 1600+ papers on adversarial ML published in the past six years, roughly 40 focus on malware [15]-and many remain only in the feature space.Our formalization of problem-space attacks paves the way to more principled research in this domain. We responsibly release the code and dataset of our novel attack to other researchers, to encourage future work on defenses in the problem space.",https://ieeexplore.ieee.org/document/9152781/,2020 IEEE Symposium on Security and Privacy (SP),18-21 May 2020,ieeexplore
10.1109/ICASSP39728.2021.9414951,Kernel Regression on Graphs in Random Fourier Features Space,IEEE,Conferences,"This work proposes an efficient batch-based implementation for kernel regression on graphs (KRG) using random Fourier features (RFF) and a low-complexity online implementation. Kernel regression has proven to be an efficient learning tool in the graph signal processing framework. However, it suffers from poor scalability inherent to kernel methods. We employ RFF to overcome this issue and derive a batch-based KRG whose model size is independent of the training sample size. We then combine it with a stochastic gradient-descent approach to propose an online algorithm for KRG, namely the stochastic-gradient KRG (SGKRG). We also derive sufficient conditions for convergence in the mean sense of the online algorithms. We validate the performance of the proposed algorithms through numerical experiments using both synthesized and real data. Results show that the proposed batch-based implementation can match the performance of conventional KRG while having reduced complexity. Moreover, the online implementations effectively learn the target model and achieve competitive performance compared to the batch implementations.",https://ieeexplore.ieee.org/document/9414951/,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",6-11 June 2021,ieeexplore
10.1109/RADAR.2017.7944428,Localized random projections for space-time adaptive processing,IEEE,Conferences,"High-dimensional multi-sensor radar data suffers from the well known curse of dimensionality. For example, in radar space time adaptive processing (STAP), training data from neighboring range cells is limited, since the statistical properties vary significantly over range and azimuth. Therefore, precluding straightforward implementation of standard detectors, for example, the whitening minimum variance distortionless response filter. Using random projections, we can reduce the dimension of the radar problem by random sampling, i.e. by projecting the data into a random d-dimensional subspace. The Johnson-Lindenstrauss (JL) theorem provides theoretical guarantees which explicitly states that the low dimensional data after random projections is only very slightly perturbed when compared to the data from the original problem in an l<sub>2</sub> norm sense. Random projections offers significant computational savings permitting possible real time solutions, however, at the cost of reducing the clairvoyant SINR for radar STAP. To alleviate this issue of SINR loss, we use localized random projections where the random projection matrix incorporates the look angle information, thereby minimizing the noise and interference effects from other angles, and increasing the SINR. We show that the resulting detector is CFAR, and the transformation matrix satisfies all the necessary conditions for the the JL theorem to hold.",https://ieeexplore.ieee.org/document/7944428/,2017 IEEE Radar Conference (RadarConf),8-12 May 2017,ieeexplore
,Maximum likelihood identification of countercurrent rare earth extraction process based on state-space model,IEEE,Conferences,"In this paper, the maximum likelihood identification problem of linear state-space models is considered for the rare earth extraction process. The maximum likelihood estimation is computed based on the expectation maximization algorithm. The mathematic expression of the conditional expectation is derived and the parameter choice for maximizing the conditional expectation is also presented. Finally, an real industrial experiment of rare earth extraction process is implemented and the results show the efficiency of the proposed method.",https://ieeexplore.ieee.org/document/5573611/,Proceedings of the 29th Chinese Control Conference,29-31 July 2010,ieeexplore
10.1109/CSPA.2018.8368706,Modeling the affective space of 360 virtual reality videos based on arousal and valence for wearable EEG-based VR emotion classification,IEEE,Conferences,"This study attempts to produce a novel database for emotional analysis which uses virtual reality (VR) contents, obtained from third party sources such as YouTube, Discovery VR, Jaunt VR, NYT VR, Veer VR and Google Cardboard, as the visual stimuli in the classification of emotion using commercial-of-the-shelf (COTS) wearable electroencephalography (EEG) headsets. While there are available sources for emotional analysis such as Dataset for Emotion Analysis using EEG, Physiological and video signals (DEAP) dataset presented by Koelstra et al. and Database for Emotional Analysis in Music (DEAM) dataset by Soleymani et al, their contents are focused on using music stimuli and music video stimuli. The database which will be presented here will consist of novel affective taggings using virtual reality content, specifically on Youtube 360 videos, as evaluated by 15 participants based on the Arousal-Valence emotion model (AVS). The feedback obtained from these evaluations will serve as the underlying dataset for the next stage of machine learning implementation, which is the targeted emotion classification of virtual reality stimuli using wearable EEG headsets.",https://ieeexplore.ieee.org/document/8368706/,2018 IEEE 14th International Colloquium on Signal Processing & Its Applications (CSPA),9-10 March 2018,ieeexplore
10.1109/NCVPRIPG.2013.6776185,Monitoring a large surveillance space through distributed face matching,IEEE,Conferences,Large space with many cameras require huge storage and computational power to process these data for surveillance applications. In this paper we propose a distributed camera and processing based face detection and recognition system which can generate information for finding spatiotemporal movement pattern of individuals over a large monitored space. The system is built upon Hadoop Distributed File System using map reduce programming model. A novel key generation scheme using distance based hashing technique has been used for distribution of the face matching task. Experimental results have established effectiveness of the technique.,https://ieeexplore.ieee.org/document/6776185/,"2013 Fourth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)",18-21 Dec. 2013,ieeexplore
10.1109/ISCAS.1994.409629,Neural networks using bit stream arithmetic: a space efficient implementation,IEEE,Conferences,"In this paper an expandable digital architecture that provides an efficient implementation base for large neural networks, is presented. The architecture uses the circuit for arithmetic operations on delta encoded signals to carry out the large number of required parallel synaptic calculations. All real valued quantities are encoded on delta bit streams. The actual digital circuitry is simple and highly regular, thus allowing very efficient space usage of fine grained FPGAs.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/409629/,Proceedings of IEEE International Symposium on Circuits and Systems - ISCAS '94,30 May-2 June 1994,ieeexplore
10.1109/ICDM.2003.1250919,OP-cluster: clustering by tendency in high dimensional space,IEEE,Conferences,"Clustering is the process of grouping a set of objects into classes of similar objects. Because of unknownness of the hidden patterns in the data sets, the definition of similarity is very subtle. Until recently, similarity measures are typically based on distances, e.g Euclidean distance and cosine distance. We propose a flexible yet powerful clustering model, namely OP-cluster (Order Preserving Cluster). Under this new model, two objects are similar on a subset of dimensions if the values of these two objects induce the same relative order of those dimensions. Such a cluster might arise when the expression levels of (coregulated) genes can rise or fall synchronously in response to a sequence of environment stimuli. Hence, discovery of OP-Cluster is essential in revealing significant gene regulatory networks. A deterministic algorithm is designed and implemented to discover all the significant OP-Clusters. A set of extensive experiments has been done on several real biological data sets to demonstrate its effectiveness and efficiency in detecting coregulated patterns.",https://ieeexplore.ieee.org/document/1250919/,Third IEEE International Conference on Data Mining,22-22 Nov. 2003,ieeexplore
10.1109/VETECS.2012.6239909,On the Effect of Gaussian Imperfect Channel Estimations on the Performance of Space Modulation Techniques,IEEE,Conferences,"Space modulation techniques, such as spatial modulation (SM) and space shift keying (SSK), are efficient low complexity implementation of multiple input multiple output (MIMO) systems. In such techniques, a single transmit-antenna is activated during each time instant and the activated antenna index is used to convey information. Due to the novel method of conveying information, a major criticism arises on the practicality of such techniques in the presence of real-time imperfections such as channel estimation errors. Therefore, the aim of this paper is to shed light on this issue. The performance of such systems are analyzed in the presence of Gaussian imperfect channel estimations. More specifically, the performance of SSK system consisting of N<sub>t</sub> transmit and N<sub>r</sub> receive antennas with maximum-likelihood (ML) detection and imperfect channel state information (CSI) at the receiver is studied. The exact average bit error probability (ABEP) over Rayleigh fading channels is obtained in closed-form for N<sub>t</sub> = 2 and arbitrary N<sub>r</sub>; while union upper bound is used to compute the ABEP when N<sub>t</sub> &gt;; 2 and arbitrary N<sub>r</sub>. Furthermore, simple and general asymptotic expression for the ABEP is derived and analyzed. Besides, the effect of imperfect CSI on the performance of SM, Alamouti and SSK schemes considering different number of channel estimation pilots are studied and compared via numerical Monte Carlo simulations. It is shown that, on the contrary to the raised criticism, space modulation techniques are more robust to channel estimation errors than Alamouti since the probability of error is determined by the differences between channels associated with the different transmit antennas rather than the actual channel realization.",https://ieeexplore.ieee.org/document/6239909/,2012 IEEE 75th Vehicular Technology Conference (VTC Spring),6-9 May 2012,ieeexplore
10.1109/CEC.2002.1007008,On the similarities between binary-coded GA and real-coded GA in wide search space,IEEE,Conferences,"This study sets out to identify a real-world system by employing binary-coded and real-coded genetic algorithms (GAs). However, in a nascent stage of setting configurations of GAs, it is difficult to determine the feasible boundaries of each parameter of the system. In this paper, both GAs are implemented based on the similar mechanisms of crossover and mutation, and performed on discretized linear, logarithmic, and hybrid search spaces with corresponding encoding methods. Through simple probabilistic analysis, it follows that logarithmic space and hybrid space searches are far more advantageous to general linear space search in wide bound searching. The identification results of a simple electrical circuit support this expectation and confirm that real-coded GA (RCGA) of logarithmic and hybrid space searches are the best way to tackle the real-world problem.",https://ieeexplore.ieee.org/document/1007008/,Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600),12-17 May 2002,ieeexplore
10.1109/IPDPSW.2017.131,On-FPGA real-time processing of biological signals from high-density MEAs: a design space exploration,IEEE,Conferences,"High-density microelectrode arrays (HDMEAs) are promising tools to tackle fundamental questions in neuroscience and brain diseases with unprecedented experimental capabilities. The acquisition of the biological signals sampled by such MEAs, that usually involves filtering, preliminary processing and finally data storage, is an intrinsically parallel and computation-intensive activity, particularly in systems targeting thousands of recording channels acquired with sub-millisecond time resolution. Within several applications, these operations need to be performed in real-time. A promising solution offering an adequate performance level relies on parallel hardware structures, making FPGA devices the perfect target technology. In this paper, we present an evaluation of an acquisition and processing system, to be implemented on an FPGA device, which is conceived to be connected to multi-channel CMOS-MEAs and is specifically designed for in-vitro and in-vivo recordings of neural activity. The template, implemented on reconfigurable logic, performs the first steps of the computing chain: filtering and adaptive detection of neural spikes. The filtered samples together with information regarding the presence of spikes are stored in an external DDR memory, for further elaboration and communication with the external environment. We performed a design space exploration measuring resource utilization and precision of the detection algorithm for different use-cases, corresponding to different state-of-the-art HDMEAs, and for different application parameters, such as the filtering scheme, number of parallel input channels, and sampling frequency. A prototype instance of the proposed platform, implemented on a low-end Xilinx Zynq SoC, allows to process more than 1 Gbps of data coming from up to 4096 18-kHz channels, within a time latency of 1.8 ms.",https://ieeexplore.ieee.org/document/7965040/,2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),29 May-2 June 2017,ieeexplore
10.1109/AISIG.1989.47322,PX1: a space shuttle mission operations knowledge-based system project,IEEE,Conferences,"A knowledge-based system (KBS) prototyping project is examined. The work entails reasoning about the impact of component failures in space shuttle orbiter subsystems. The effort is directed toward the development of a system that will recognize passive component failures as potential safety hazards, rather than toward an active failure identification or diagnostic tool. The system was designed to be integrated as a knowledge-based processing system utilizing input from a procedure-based processing system. Implementation of the consolidated system will occur when real-time telemetry data is available at the workstations. The system may be used standalone in the meantime. Flight controllers at the Johnson Space Center will use this prototype to help develop requirements for a space shuttle mission operations tool.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/47322/,[1989] Proceedings. The Annual AI Systems in Government Conference,27-31 March 1989,ieeexplore
10.1109/MASCOTS.2019.00045,Practical Design Space Exploration,IEEE,Conferences,"Multi-objective optimization is a crucial matter in computer systems design space exploration because real-world applications often rely on a trade-off between several objectives. Derivatives are usually not available or impractical to compute and the feasibility of an experiment can not always be determined in advance. These problems are particularly difficult when the feasible region is relatively small, and it may be prohibitive to even find a feasible experiment, let alone an optimal one. We introduce a new methodology and corresponding software framework, HyperMapper 2.0, which handles multi-objective optimization, unknown feasibility constraints, and categorical/ordinal variables. This new methodology also supports injection of the user prior knowledge in the search when available. All of these features are common requirements in computer systems but rarely exposed in existing design space exploration systems. The proposed methodology follows a white-box model which is simple to understand and interpret (unlike, for example, neural networks) and can be used by the user to better understand the results of the automatic search. We apply and evaluate the new methodology to the automatic static tuning of hardware accelerators within the recently introduced Spatial programming language, with minimization of design run-time and compute logic under the constraint of the design fitting in a target field-programmable gate array chip. Our results show that HyperMapper 2.0 provides better Pareto fronts compared to state-of-the-art baselines, with better or competitive hypervolume indicator and with 8x improvement in sampling budget for most of the benchmarks explored.",https://ieeexplore.ieee.org/document/8843094/,"2019 IEEE 27th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",21-25 Oct 2019,ieeexplore
10.1109/UIC-ATC-ScalCom.2014.75,Private Smart Space: Cost-Effective ADLs (Activities of Daily Livings) Recognition Based on Superset Transformation,IEEE,Conferences,"Aging population inspired the market on advanced real time caring for the elder in home setting, accurately recognizing human activities is a challenging task. Activities of daily living are good indicators for behavior recognition. In this paper, we describe a new method to deploy a cost-effective solution which can be run on embedded device as smart router. We use the open dataset, map the raw dataset into a sparse binary matrix, unique by the time line and activity tags. Decision tree algorithm is applied to train the model, in order to achieve the goal that simple comparison work to implement the model and get a quick respond at high accuracy. We evaluate our approach by 3-fold cross validation and achieve a time-slice accuracy of 98.45%.",https://ieeexplore.ieee.org/document/7307038/,2014 IEEE 11th Intl Conf on Ubiquitous Intelligence and Computing and 2014 IEEE 11th Intl Conf on Autonomic and Trusted Computing and 2014 IEEE 14th Intl Conf on Scalable Computing and Communications and Its Associated Workshops,9-12 Dec. 2014,ieeexplore
10.1109/CCAAW.2019.8904903,Quantifying Degradations of Convolutional Neural Networks in Space Environments,IEEE,Conferences,"Advances in machine learning applications for image processing, natural language processing, and direct ingestion of radio frequency signals continue to accelerate. Less attention, however, has been paid to the resilience of these machine learning algorithms when implemented on real hardware and subjected to unintentional and/or malicious errors during execution, such as those occurring from space-based single event upsets (SEU). This paper presents a series of results quantifying the rate and level of performance degradation that occurs when convolutional neural nets (CNNs) are subjected to selected bit errors in single-precision number representations. This paper provides results that are conditioned upon ten different error case events to isolate the impacts showing that CNN performance can be gradually degraded or reduced to random guessing based on where errors arise. The degradations are then translated into expected operational lifetimes for each of four CNNs when deployed to space radiation environments. The discussion also provides a foundation for ongoing research that enhances the overall resilience of neural net architectures and implementations in space under both random and malicious error events, offering significant improvements over current implementations. Future work to extend these CNN resilience evaluations, conditioned upon architectural design elements and well-known error correction methods, is also introduced.",https://ieeexplore.ieee.org/document/8904903/,2019 IEEE Cognitive Communications for Aerospace Applications Workshop (CCAAW),25-26 June 2019,ieeexplore
10.1109/DS-RT.2016.9,RA2: Predicting Simulation Execution Time for Cloud-Based Design Space Explorations,IEEE,Conferences,"Design space exploration refers to the evaluation of implementation alternatives for many engineering and design problems. A popular exploration approach is to run a large number of simulations of the actual system with varying sets of configuration parameters to search for the optimal ones. Due to the potentially huge resource requirements, cloud-based simulation execution strategies should be considered in many cases. In this paper, we look at the issue of running large-scale simulation-based design space exploration problems on commercial Infrastructure-as-a-Service clouds, namely Amazon EC2, Microsoft Azure and Google Compute Engine. To efficiently manage cloud resources used for execution, the key problem would be to accurately predict the running time for each simulation instance in advance. This is not trivial due to the currently wide range of cloud resource types which offer varying levels of performance. In addition, the widespread use of virtualization techniques in most cloud providers often introduces unpredictable performance interference. In this paper, we propose a resource and application-aware (RA2) prediction approach to combat performance variability on clouds. In particular, we employ neural network based techniques coupled with non-intrusive monitoring of resource availability to obtain more accurate predictions. We conducted extensive experiments on commercial cloud platforms using an evacuation planning design problem over a month-long period. The results demonstrate that it is possible to predict simulation execution times in most cases with high accuracy. The experiments also provide some interesting insights on how we should run similar simulation problems on various commercially available clouds.",https://ieeexplore.ieee.org/document/7789881/,2016 IEEE/ACM 20th International Symposium on Distributed Simulation and Real Time Applications (DS-RT),21-23 Sept. 2016,ieeexplore
10.1109/ROBOT.1993.291973,Real-time implementation of neural network learning control of a flexible Space manipulator,IEEE,Conferences,"A neural network approach to online learning control and real-time implementation for a flexible space robot manipulator is presented. An overview of the motivation and system development of the self-mobile space modulator (SM/sup 2/) is given. The neural network learns control by updating feedforward dynamics based on feedback control input. Implementation issues associated with online training strategies are addressed and a single stochastic training scheme is presented. A recurrent neural network architecture with improved performance is proposed. Using the proposed learning scheme, the manipulator tracking error is reduced by 85% compared to that of conventional proportional-integral-derivative (PID) control. The approach possesses a high degree of generality and adaptability to various applications. It will be a valuable learning control method for robots working in unconstructed environments.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/291973/,[1993] Proceedings IEEE International Conference on Robotics and Automation,2-6 May 1993,ieeexplore
10.1109/ICARM52023.2021.9536145,Reducing the Dimension of the Configuration Space with Self Organizing Neural Networks,IEEE,Conferences,"For robotics, especially industrial applications, it is crucial to reactively plan safe motions through efficient algorithms. Planning is more powerful in the configuration space than the task space. However, for robots with many degrees of freedom, this is challenging and computationally expensive. Sophisticated techniques for motion planning such as the Wavefront algorithm are limited by the high dimensionality of the configuration space, especially for robots with many degrees of freedom. For a neural implementation of the Wavefront algorithm in the configuration space, neurons represent discrete configurations and synapses are used for path planning. In order to decrease the complexity, we reduce the search space by pruning superfluous neurons and synapses. We present different models of self-organizing neural networks for this reduction. The approach takes real-life human motion data as input and creates a representation with reduced dimension. We compare six different neural network models and adapt the Wavefront algorithm to the different structures of the reduced output spaces. The method is backed up by an extensive evaluation of the reduced spaces, including their suitability for path planning by the Wavefront algorithm.",https://ieeexplore.ieee.org/document/9536145/,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),3-5 July 2021,ieeexplore
10.1109/ISGTAsia49270.2021.9715603,Reinforcement Learning Based EV Charging Scheduling: A Novel Action Space Representation,IEEE,Conferences,"In recent years, several optimization techniques have been proposed for electric vehicle (EV) charging scheduling. A common approach to intelligent scheduling is day-ahead planning, assuming full arrival time, departure time and energy demand knowledge or having them forecasted. However, the result from the day-ahead scheduling is limitedly applicable due to the uncertainties from the charging behaviors. With the deployment of the EV charging communication protocol defined in ISO 15118, it is realistic to assume that the EV will publish the departure time and the energy demand upon arrival. Thus, real-time scheduling, making decisions at each decision timeslot, can adapt to the new information and increase scheduling performance. Traditional model-based approaches like model predictive control (MPC) still require models, for example, for the future arrival times to solve the scheduling problem. Reinforcement learning (RL), a model-free approach, has also been successfully applied to real-time scheduling. RL can learn how to make decisions without relying on any system knowledge. This paper proposes a new action space construction method for an RL as proposed in a preceding work. The resulting action space size is significantly reduced compared to the original approach. Further, we compare the performance of a novel prioritized RL method to the original method. A publicly available charging session dataset is used for performance comparison in contrast to the original method. It is shown, that the prioritized RL performs better.",https://ieeexplore.ieee.org/document/9715603/,2021 IEEE PES Innovative Smart Grid Technologies - Asia (ISGT Asia),5-8 Dec. 2021,ieeexplore
10.1109/CASE48305.2020.9249227,Reinforcement Learning with Converging Goal Space and Binary Reward Function,IEEE,Conferences,"Usage of a sparse and binary reward function remains one of the most challenging problems in reinforcement learning. In particular, when the environments wherein robotic agents learn are sufficiently vast, it is much more difficult to learn tasks because the probability of reaching the goal is minimal. A Hindsight Experience Replay algorithm was proposed to overcome these difficulties; however, problems persist that affect the learning speed and delay learning when a learning agent cannot receive proper rewards at the beginning of the learning process. In this paper, we present a simple method called Converging Goal Space and Binary Reward Function, which helps agents learn tasks easily and efficiently in large environments while providing a binary reward. At an early stage in training, a larger goal space margin facilitates the reward function for a more rapid policy learning. As the number of successes increases, the goal space is gradually reduced to the size used to the size used in the test. We apply this reward function to two different task experiments: sliding and throwing, which must be explored at a wider range than the reach of the robotic arms, and then compare the learning efficiency to that of experiments that only employ a sparse and binary reward function. We show that the proposed reward function performs better in large environments using physics simulation, and we demonstrate that the function is applicable to real world robotic arms.",https://ieeexplore.ieee.org/document/9249227/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/ICSE43902.2021.00028,Resource-Guided Configuration Space Reduction for Deep Learning Models,IEEE,Conferences,"Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity. In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.",https://ieeexplore.ieee.org/document/9402095/,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),22-30 May 2021,ieeexplore
10.1109/CVPR46437.2021.00844,Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments,IEEE,Conferences,"Localizing the camera in a known indoor environment is a key building block for scene mapping, robot navigation, AR, etc. Recent advances estimate the camera pose via optimization over the 2D/3D-3D correspondences established between the coordinates in 2D/3D camera space and 3D world space. Such a mapping is estimated with either a convolution neural network or a decision tree using only the static input image sequence, which makes these approaches vulnerable to dynamic indoor environments that are quite common yet challenging in the real world. To address the aforementioned issues, in this paper, we propose a novel outlier-aware neural tree which bridges the two worlds, deep learning and decision tree approaches. It builds on three important blocks: (a) a hierarchical space partition over the indoor scene to construct the decision tree; (b) a neural routing function, implemented as a deep classification network, employed for better 3D scene understanding; and (c) an outlier rejection module used to filter out dynamic points during the hierarchical routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark developed for camera relocalization in dynamic indoor environments. It achieves robust neural routing through space partitions and outperforms the state-of-the-art approaches by around 30% on camera pose accuracy, while running comparably fast for evaluation.",https://ieeexplore.ieee.org/document/9577932/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore
10.1109/ISCAS.1994.409584,Searching over DOA parameter space via neural networks,IEEE,Conferences,"In this paper, we propose a neural method to solve the orthogonality search problem arising in direction-of-arrival (DOA) estimation. The most important feature of this method hinges upon the fact that it can offer the potential of real-time solutions to the above problem by utilizing the fast relaxation properties of the Hopfield's linear programming neural network. Theoretical analysis and simulation results show that the performance of neural method is exactly equivalent to that of the standard MUSIC method or the Real Domain DOA estimation method (RD method). That is to say, the method proposed in this paper is a neural implementation of the MUSIC method and RD method.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/409584/,1994 IEEE International Symposium on Circuits and Systems (ISCAS),30 May-2 June 1994,ieeexplore
10.1109/IV48863.2021.9575822,Self-Supervised Action-Space Prediction for Automated Driving,IEEE,Conferences,"Making informed driving decisions requires reliable prediction of other vehicles' trajectories. In this paper, we present a novel learned multi-modal trajectory prediction architecture for automated driving. It achieves kinematically feasible predictions by casting the learning problem into the space of accelerations and steering angles - by performing action-space prediction, we can leverage valuable model knowledge. Additionally, the dimensionality of the action manifold is lower than that of the state manifold, whose intrinsically correlated states are more difficult to capture in a learned manner. For the purpose of action-space prediction, we present the simple Feed-Forward Action-Space Prediction (FFW-ASP) architecture. Then, we build on this notion and introduce the novel Self-Supervised Action-Space Prediction (SSP-ASP) architecture that outputs future environment context features in addition to trajectories. A key element in the self-supervised architecture is that, based on an observed action history and past context features, future context features are predicted prior to future trajectories. The proposed methods are evaluated on real-world datasets containing urban intersections and roundabouts, and show accurate predictions, outperforming state-of-the-art for kinematically feasible predictions in several prediction metrics.",https://ieeexplore.ieee.org/document/9575822/,2021 IEEE Intelligent Vehicles Symposium (IV),11-17 July 2021,ieeexplore
10.1109/ICWAPR.2007.4421653,Small-shaped space target recognition based on wavelet decomposition and support vector machine,IEEE,Conferences,"A kind of method for small-shaped space target recognition was proposed in this paper based on feature extraction with wavelet decomposition and formative support vector machine (FSVM) with sequential minimal optimization (SMO) algorithm. Firstly, the significance and characteristics of space target recognition were discussed and a two-stage recognition strategy was designed. And then aiming at the characteristics of small-shaped space target recognition, a new method was implemented based on feature extraction with wavelet decomposition and FSVM with SMO algorithm. Simulation results show the good performance of the algorithm proposed in this paper: the correct rate is more than 97% within 1360 simulation samples of ten classes of small shaped space targets; meanwhile the algorithm is characterized with high speed of near real time in both implementation of training and testing.",https://ieeexplore.ieee.org/document/4421653/,2007 International Conference on Wavelet Analysis and Pattern Recognition,2-4 Nov. 2007,ieeexplore
10.23919/FRUCT.2012.8253108,Smart space logistic service for real-time ridesharing,IEEE,Conferences,The paper describes a logistic service-based approach to real-time ridesharing based on smartspace concept. Smart-M3 information platform is used as smart space infrastructure for presented approach. The service is based on Smart-M3 RDF ontology which is formed by ontology slices of participants' mobile devices. The paper presents an algorithm for finding appropriate fellow- travelers for drivers as well as definition of acceptable pick-up and drop-off points for them.,https://ieeexplore.ieee.org/document/8253108/,2012 11th Conference of Open Innovations Association (FRUCT),23-27 April 2012,ieeexplore
10.1109/ICCD.1995.528945,Smart-pixel array processors based on optimal cellular neural networks for space sensor applications,IEEE,Conferences,"A smart-pixel cellular neural network with hardware annealing capability, digitally programmable synaptic weights, and multisensor parallel interface has been under development for advanced space sensor applications. The smart-pixel CNN architecture is a programmable multi-dimensional array of optoelectronic neurons which are locally connected with their local neurons and associated active-pixel sensors. Integration of the neuroprocessor in each processor node of a scalable multiprocessor system offers orders-of-magnitude computing performance enhancements for on-board real-time intelligent multisensor processing and control tasks of advanced small satellites. The smart-pixel CNN operation theory, architecture, design and implementation, and system applications are investigated in detail. The VLSI implementation feasibility was illustrated by a prototype smart-pixel 5/spl times/5-neuroprocessor array chip of active dimensions 1380 /spl mu/m/spl times/746 /spl mu/m in a 2-/spl mu/m CMOS technology.",https://ieeexplore.ieee.org/document/528945/,Proceedings of ICCD '95 International Conference on Computer Design. VLSI in Computers and Processors,2-4 Oct. 1995,ieeexplore
10.1109/SSCI50451.2021.9660133,Space and Time Efficiency Analysis of Data-Driven Methods Applied to Embedded Systems,IEEE,Conferences,"One of the applications of data-driven methods in the industry is the creation of real-time, embedded measurements, whether to monitor or replace sensor signals. As the number of embedded systems in products raises over time, the energy efficiency of such systems must be considered in the design. The time (processor) efficiency of the embedded software is directly related to the energy efficiency of the embedded system. Therefore, when considering some embedded software solutions, such as data-driven methods, time efficiency must be taken into account to improve energy efficiency. In this work, the energy efficiency of three data-driven methods: the Sparse Identification of Nonlinear Dynamics (SINDy), the Extreme Learning Machine (ELM), and the Random-Vector Functional Link (RVFL) network were assessed by using the creation of a real-time in-cylinder pressure sensor for diesel engines as a task. The three methods were kept with equivalent performances, whereas their relative execution time was tested and classified by their statistical rankings. Additionally, the space (memory) efficiency of the methods was assessed. The contribution of this work is to provide a guide to choose the best data-driven method to be used in an embedded system in terms of efficiency.",https://ieeexplore.ieee.org/document/9660133/,2021 IEEE Symposium Series on Computational Intelligence (SSCI),5-7 Dec. 2021,ieeexplore
10.1109/ICACI.2018.8377475,Space-time constrained optimization for deep locomotion controller,IEEE,Conferences,"Physics-based methods synthesize motion for virtual characters following physics principles in real world. Given the high dimensions and continuity of joint actuation, designing a controller for virtual characters is a challenging task. Existing methods normally construct the controller based on Finite State Machine, which is a manual process and requires expert knowledge. This paper uses deep neural network, as the locomotion controller, to control the motion of virtual characters. The network learning is conducted with the Deep Deterministic Policy Gradient. We propose to integrate space-time constraints, as part of the reward function, during the learning process. The experiment results confirm that the introduction of the space-time constraints avoids the problem of generating awkward gait (as observed in existing methods).",https://ieeexplore.ieee.org/document/8377475/,2018 Tenth International Conference on Advanced Computational Intelligence (ICACI),29-31 March 2018,ieeexplore
10.1109/INFOCOMWKSHPS50562.2020.9162585,Sparser: Secure Nearest Neighbor Search with Space-filling Curves,IEEE,Conferences,"Nearest neighbor search, a classic way of identifying similar data, can be applied to various areas, including database, machine learning, natural language processing, software engineering, etc. Secure nearest neighbor search aims to find nearest neighbors to a given query point over encrypted data without accessing data in plaintext. It provides privacy protection to datasets when nearest neighbor queries need to be operated by an untrusted party (e.g., a public server). While different solutions have been proposed to support nearest neighbor queries on encrypted data, these existing solutions still encounter critical drawbacks either in efficiency or privacy. In light of the limitations in the current literature, we propose a novel approximate nearest neighbor search solution, referred to as Sparser, by leveraging a combination of space-filling curves, perturbation, and Order-Preserving Encryption. The advantages of Sparser are twofold, strengthening privacy and improving efficiency. Specifically, Sparser pre-processes plaintext data with space-filling curves and perturbation, such that data is sparse, which mitigates leakage abuse attacks and renders stronger privacy. In addition to privacy enhancement, Sparser can efficiently find approximate nearest neighbors over encrypted data with logarithmic time. Through extensive experiments over real-world datasets, we demonstrate that Sparser can achieve strong privacy protection under leakage abuse attacks and minimize search time.",https://ieeexplore.ieee.org/document/9162585/,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),6-9 July 2020,ieeexplore
10.1109/NNSP.2000.890146,Stable fuzzy state space controllers for an AGV,IEEE,Conferences,"The primary focus of this paper is on the development of an intelligent control scheme which is insensitive to parametric uncertainty and load and parameter fluctuations, and, most importantly, is amenable to real-time implementation. We present a stable, uncoupled, direct fuzzy PD/PI control scheme for an outdoor automatic guided vehicle (AGV), which is a converted electrically powered golf-car. The controller performance was assessed both through simulations and experimental results. It was established that the fuzzy logic controller (FLC) yielded good performance even under uncertain and variable parameters in the model, unlike the computed torque technique (CTT) or conventional PID control. In terms of real-time implementation, the availability of custom fuzzy chips and the reduced computational complexity of the fuzzy controller, as against that of the CTT, makes the fuzzy controller an ideal choice between the two schemes.",https://ieeexplore.ieee.org/document/890146/,Neural Networks for Signal Processing X. Proceedings of the 2000 IEEE Signal Processing Society Workshop (Cat. No.00TH8501),11-13 Dec. 2000,ieeexplore
10.23919/EUSIPCO.2019.8902815,"State Space Models with Dynamical and Sparse Variances, and Inference by EM Message Passing",IEEE,Conferences,"Sparse Bayesian learning (SBL) is a probabilistic approach to estimation problems based on representing sparsity-promoting priors by Normals with Unknown Variances. This representation blends well with linear Gaussian state space models (SSMs). However, in classical SBL the unknown variances are a priori independent, which is not suited for modeling group sparse signals, or signals whose variances have structure. To model signals with, e.g., exponentially decaying or piecewise-constant (in particular block-sparse) variances, we propose SSMs with dynamical and sparse variances (SSM-DSV). These are two-layer SSMs, where the bottom layer models physical signals, and the top layer models dynamical variances that are subject to abrupt changes. Inference and learning in these hierarchical models is performed with a message passing version of the expectation maximization (EM) algorithm, which is a special instance of the more general class of variational message passing algorithms. We validated the proposed model and estimation algorithm with two applications, using both simulated and real data. First, we implemented a block-outlier insensitive Kalman smoother by modeling the disturbance process with a SSM-DSV. Second, we used SSM-DSV to model the oculomotor system and employed EM-message passing for estimating neural controller signals from eye position data.",https://ieeexplore.ieee.org/document/8902815/,2019 27th European Signal Processing Conference (EUSIPCO),2-6 Sept. 2019,ieeexplore
10.1109/IRSEC.2015.7455135,State space neural network control (SSNNC) of UPFC for compensation power,IEEE,Conferences,"In our present communication, we present the effectiveness of the controller's Unified Power Flow Controller UPFC with the choice of a control strategy. This Unified Power Flow Controller (UPFC) is used to control the power flow in the transmission systems by controlling the impedance, voltage magnitude and phase angle. This controller offers advantages in terms of static and dynamic operation of the power system. It also brings in new challenges in power electronics and power system design. To evaluate the performance and robustness of the system, we proposed a hybrid control combining the concept of identification neural networks with conventional regulators (SSNNC) and with the changes in characteristics of the transmission line in order to improve the stability of the electrical power network. With its unique capability to control simultaneously real and reactive power flows on a transmission line as well as to regulate voltage at the bus where it is connected, this device creates a tremendous quality impact on power system stability. The result which has been obtained from using Matlab and Simulink software showed a good agreement with the simulation result.",https://ieeexplore.ieee.org/document/7455135/,2015 3rd International Renewable and Sustainable Energy Conference (IRSEC),10-13 Dec. 2015,ieeexplore
10.1109/ICPR48806.2021.9412040,Supervised Classification Using Graph-based Space Partitioning for Multiclass Problems,IEEE,Conferences,"We introduce and investigate in multiclass setting an efficient classifier which partitions the training data by means of multidimensional parallelepipeds called boxes. We show that multiclass classification problem at hand can be solved by combining the heuristic minimum clique cover approach and the k-nearest neighbor rule. Our algorithm is motivated by an algorithm for partitioning a graph into a minimal number of cliques. The main advantage of a new classifier called Box classifier is that it optimally utilizes the geometrical structure of the training set by reducing the 1-class classification problem to a single nearest neighbor problem. We discuss computational complexity of the proposed Box classifier. The extensive experiments performed on the simulated and real data from UCI Machine Learning Repository for binary and multiclass problems show that in almost all cases the Box classifier performs significantly better than k-NN, SVM and decision trees.",https://ieeexplore.ieee.org/document/9412040/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore
10.1109/C5.2012.11,The Development of a Programming-Project Sharing Environment on Virtual Space,IEEE,Conferences,"Recently, there are classes using GUI programming environment such as Squeak Etoys or Scratch in primary education. When children study in such environment, children do not have enough opportunity to view the other's works and show their own work to parents and it is difficult to perform mutual evaluation and review lesson. In this paper, we propose an environment where children or parents can view the children's works on virtual space. It can provide opportunities for children to do mutual evaluation or review the past lesson. We have deployed this system to classes using GUI-programming environment and conducted a questionnaire for children. As a result, seven out of eleven children could view the other's works well. Moreover, eight out of ten children could find a scene in which other children watch the work together, and among these children, six of them viewed this attracting work.",https://ieeexplore.ieee.org/document/6195219/,"2012 10th International Conference on Creating, Connecting and Collaborating through Computing",18-20 Jan. 2012,ieeexplore
10.1109/CAIA.1988.196126,The NASA systems autonomy program: applying AI in space,IEEE,Conferences,"Summary form only given. The National Aeronautics and Space Administration (NASA) has initiated an aggressive program to develop, integrate, and implement autonomous systems technologies starting with knowledge-based (expert) systems and evolving towards 'autonomous', intelligent systems. Research thrusts to achieve these capabilities are centered around machine learning; coordinated real-time decision-making by multiple, cooperating intelligent agents; management, maintenance, and real-time control of distributed databases; software verification and validation; and fault-tolerant, multiprocessor architectures capable of operating in a heterogeneous environment. A summary of the research plans and the progress made in each of these areas is discussed.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/196126/,[1988] Proceedings. The Fourth Conference on Artificial Intelligence Applications,14-18 March 1988,ieeexplore
10.1109/ICRMS.2009.5270085,The analysis and modeling for the input space of real-time embedded software,IEEE,Conferences,"Software reliability testing is one of the important tasks in software reliability engineering, in which the failure data can be used to evaluate and validate the software reliability. In this paper, the input space of real-time embedded software is analyzed first. And the model of input space constructed with usage space and input value space is presented, with which a formal modeling method, the usage profile in network graph form, is presented. With the usage profile, the constraint conditions of operations and the dynamic actions of software users can be expressed closely to actual situation. The software reliability testing cases can be generated with random sampling according to the software usage profile.",https://ieeexplore.ieee.org/document/5270085/,"2009 8th International Conference on Reliability, Maintainability and Safety",20-24 July 2009,ieeexplore
10.1109/ECAI.2016.7861123,The approach of wavelength dense multiplexing using free space optical systems,IEEE,Conferences,"This paper aims to explore the ways of implementation for WDM technology that is specific for high capacity fiber optic systems, using FSO terrestrial links. It will study the peculiarities of the CWDM-SO versus DWDM-SO transmission, stating their limits (transmission distance versus BER for constant bitrate and atmospheric attenuation). It will calculate the BER value for each CWDM/DWDM channel, idealized and real conditions (with attenuation). The simulations will be made under Optiwave environment for a system with 8 CWDM/DWDM channels, performance evaluation being made using the eye diagram, WDM and optical spectrum analyzers.",https://ieeexplore.ieee.org/document/7861123/,"2016 8th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",30 June-2 July 2016,ieeexplore
10.1109/ChiCC.2014.6896010,The design and optimization method of near space intelligent target generator,IEEE,Conferences,"This paper presents the design and optimization method of near space intelligent target generator to simulate the physical characteristics of the near space vehicle. Combined with High Level Architecture distributed simulation technology, a common, repeatable and verified platform for the near space vehicle has been provided. This method used 3D modeling software Creator and 3D visual rendering software Vega, two-dimensional map and three-dimensional vision were constructed to form a simulation environment, which enhanced the authenticity of the simulation. Based on particle swarm optimization, the intelligent path planning study of near space vehicle was conducted in this environment to make up for the inadequate intelligence of traditional target generators.",https://ieeexplore.ieee.org/document/6896010/,Proceedings of the 33rd Chinese Control Conference,28-30 July 2014,ieeexplore
10.1109/POWERI.2006.1632564,The development of artificial neural network space vector PWM and diagnostic controller for voltage source inverter,IEEE,Conferences,"This paper presents the development of neural-network-based controller of space vector modulation (ANN-SVPWM) for voltage-source inverters (VSI). This ANN-SVPWM controller completely covers the undermodulation and overmodulation modes with operation extended linearly and smoothly up to square wave (six-step) by using theory of modulation between the limit trajectories. The ANN controller has the advantage of the very fast implementation of an SVM algorithm that can increase the switching frequency of power switches of the static converter. Furthermore, a ANN diagnosis method for real-time fault detection of power switches is proposed in this paper. The ANN controller uses the individual training strategy with the fixed weight and supervised models. The complete ANN-SVPWM and diagnostic controller can be used in power applications such as APF, STATCOM, UPFC and motor drives. A computer simulation program is developed using Matlab/Simulink together with the neural network toolbox for training the ANN-controller.",https://ieeexplore.ieee.org/document/1632564/,2006 IEEE Power India Conference,10-12 April 2006,ieeexplore
10.1109/ICIT.2009.4939636,The investigation of ANN space vector PWM and diagnostic controller for four switch three phase inverter fed induction motor drive,IEEE,Conferences,"This paper is to present the investigation of neural-network-based controller of space vector modulation (ANN-SVPWM) for four switch three phase inverter (FSTPI) fed induction motor drive. This ANN-SVPWM controller completely covers the under modulation and over modulation modes with operation extended linearly and smoothly up to square wave (six-step). The ANN controller uses the individual training strategy with the fixed weight and supervised models. Furthermore, ANN diagnosis method for real-time fault detection of power switches is proposed in this paper. The complete ANN-SVPWM and diagnostic controller can be used in power applications such as motor drives. A computer simulation program is developed using Matlab/Simulink together with the neural network toolbox for training the ANN-controller.This method has been validated experimentally using kit ACE 1104 (DSPACE) .",https://ieeexplore.ieee.org/document/4939636/,2009 IEEE International Conference on Industrial Technology,10-13 Feb. 2009,ieeexplore
10.1109/WI-IAT.2013.63,Towards Adaptive Multi-agent Planning in Cyber Physical Space,IEEE,Conferences,"Cyber physical space is potentially hosting innumerable spatio-temporal data streams due to increasing use of social networking platforms as real-time information dissemination system and world-wide deployment of sensors for continuous monitoring of physical phenomena. In this paper we address the problem of how cyber physical space can be used for sensing and responding to global calamities such as earthquake. The paper proposes a novel approach of generating online adaptive response in assisting search-and-rescue operations using situation awareness built from real-time heterogeneous spatio-temporal data streams. Online adaptive response is achieved by using agent-based cooperative task sharing and modeling agent decision making as self-organized emergent behavior based on concepts of complex adaptive system. An implemented simulation platform use concepts of situation modeling, domain task network, contract net protocol based negotiation and complex adaptive system to generate adaptive plans. Preliminary simulation results are promising as we have been able to demonstrate a repertoire of self-organized emergent behaviors.",https://ieeexplore.ieee.org/document/6690049/,2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT),17-20 Nov. 2013,ieeexplore
10.1109/ICDCSW53096.2021.00009,Towards Understanding the Adaptation Space of AI-Assisted Data Protection for Video Analytics at the Edge,IEEE,Conferences,"Edge computing facilitates the deployment of distributed AI applications, capable of processing video data in real time. AI-assisted video analytics can provide valuable information and benefits in various domains. Face recognition, object detection, or movement tracing are prominent examples enabled by this technology. However, such mechanisms also entail threats regarding privacy and security, for example if the video contains identifiable persons. Therefore, adequate data protection is an increasing concern in video analytics. AI-assisted data protection mechanisms, such as face blurring, can help, but are often computationally expensive. Additionally, the heterogeneous hardware of end devices and the time-varying load on edge services need to be considered. Therefore, such systems need to adapt to react to changes during their operation, ensuring that conflicting requirements on data protection, performance, and accuracy are addressed in the best possible way. Sound adaptation decisions require an understanding of the adaptation options and their impact on different quality attributes. In this paper, we identify factors that can be adapted in AI-assisted data protection for video analytics using the example of a face blurring pipeline. We measure the impact of these factors using a heterogeneous edge computing hardware testbed. The results show a large and complex adaptation space, with varied impacts on data protection, performance, and accuracy.",https://ieeexplore.ieee.org/document/9545916/,2021 IEEE 41st International Conference on Distributed Computing Systems Workshops (ICDCSW),7-10 July 2021,ieeexplore
10.1109/AIM43001.2020.9158908,Towards accelerated robotic deployment by supervised learning of latent space observer and policy from simulated experiments with expert policies,IEEE,Conferences,"Up until today robotic tasks in highly variable environments remain very difficult to solve. We propose accelerated robotic deployment through task solving on low-level sensor data in simulation. A simulation allows for a lot of data, which is usually not available in a real world robotic setup due to cost and feasibility. Solving tasks in simulation is safe and a lot easier due to the huge amount of feedback from virtual sensory data. We present a novel sim2real architecture for converting simulated low level sensor data policies to high level real world policies. After solving a task we let the robot complete it a number of times in simulation using domain randomization, while doing so we save the simulated sensor data corresponding to the real robotic setup and actions taken. Given these sensor data and actions a task specific policy can be trained using our architecture. In this paper we work towards a proof of concept by simulating a simple low cost manipulator in pybullet to pick and place an object based on image observations.",https://ieeexplore.ieee.org/document/9158908/,2020 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM),6-9 July 2020,ieeexplore
10.1109/IEMBS.1997.757650,Two dimensional interleaved differential k-space sampling for fluoroscopic triggering of contrast-enhanced three dimensional MR angiography,IEEE,Conferences,"The purpose of this work is to demonstrate how 2D differential k-space sampling can be implemented with interleaving of the view (phase encoding) order, providing smoother temporal behavior and no spatial resolution penalty when compared to standard sequential k-space ordering. Implementation in a clinical setting provides a method for real-time bolus tracking and timing for contrast-enhanced 3D MR angiography.",https://ieeexplore.ieee.org/document/757650/,Proceedings of the 19th Annual International Conference of the IEEE Engineering in Medicine and Biology Society. 'Magnificent Milestones and Emerging Opportunities in Medical Engineering' (Cat. No.97CH36136),30 Oct.-2 Nov. 1997,ieeexplore
10.1109/IEIT53597.2021.00077,UAV Control in Smart City Based on Space-Air-Ground Integrated Network,IEEE,Conferences,"Unmanned Aerial Vehicle (UAV) is an important part of the wireless network system of the future smart city. As a difficult point in the large-scale application of UAV, UAV control gradually attracts people's attention. Aiming at the problems of UAV control in smart city application, a near real time online learning architecture of UAV control based on the software-defined space-air-ground integrated network (SSAG) was proposed. This architecture uses the two-layer software defined network (SDN) controller architecture of SSAG framework to separate UAV control. The upper-tier SDN controller is responsible for the scheduling of UAV configuration, while the lower-tier SDN controller is responsible for regional coordination of UAV. The upper-tier SDN controller updates the tendency of network states by acquiring network states information in time interval. By simulating the network state in the next time interval, the optimal strategy of UAV scheduling of the next time interval is obtained by using the strategy iteration algorithm. Finally, an example is given to verify that the near real-time online learning architecture can accurately predict the UAV requirement, and increase the throughput of the network system compared with the traditional approach.",https://ieeexplore.ieee.org/document/9526159/,"2021 International Conference on Internet, Education and Information Technology (IEIT)",16-18 April 2021,ieeexplore
10.1109/COMSWA.2007.382562,Ubiquitous Semantic Space: A context-aware and coordination middleware for Ubiquitous Computing,IEEE,Conferences,"Ubiquitous computing poses the challenge of increased communication, context-awareness and functionality. In a highly dynamic and weekly connected ubiquitous environment, continuous access to the network (synchronous communication) is very difficult. So it's necessary to go for tuple space which provides asynchronous communication without any loss in data. Tuple space offers a coordination infrastructure for communication between autonomous entities by providing a logically shared memory along with data persistence, transactional security as well as temporal and spatial decoupling properties that make it desirable for distributed systems [2] such as ubiquitous computing. In order to automate the task and the system to be intelligent, context awareness is a must. This can be achieved by using semantic web technology. Existing middleware's for ubiquitous computing concentrates on RPC communication paradigm and deals with context-awareness separately. In our approach of constructing the middleware we provide common solution to both communication and context-awareness using ubiquitous semantic space. Ubiquitous semantic space [5] brings together tuple space, semantic web technologies and ubiquitous computing. Hence in this paper, we introduce a context-aware and co-ordination middleware framework for ubiquitous environment using ubiquitous semantic space. Ubiquitous semantic space uses ontologies to define the semantics of various concepts. Using ontologies facilitates different agents in the environments to have a common semantic understanding of different contexts. Ontology is represented using ontology web language, OWL [6]. We have modeled a ubiquitous semantic space ontology structure suitable for communicating conceptual information among the agents. Our model also incorporates context-triggered action which is more useful for real-time ubiquitous application having reactive behavior. For enabling context-triggered action, our model has reactive space into ubiquitous semantic space. Reactive space, stores rules written in SWRL [22], semantic web rule language and fired using JESS [3] reasoner at the appropriate time. The middleware could easily adapt to changes in the environment. The structure of the ubiquitous semantic space is designed in a fashion to have privacy among the communicating devices and the agents. Hence our middleware uses a decentralized architecture which supports asynchronous communication, context-awareness, context-sensitive communication, Privacy sensitive, adaptive to context-changes and reactive to emergency situation.",https://ieeexplore.ieee.org/document/4267986/,2007 2nd International Conference on Communication Systems Software and Middleware,7-12 Jan. 2007,ieeexplore
10.1109/ITT51279.2020.9320781,Using Deep Learning And Machine Learning In Space Network,IEEE,Conferences,"The UAE have achieved the goal to be a country to host its potential satellites in the space i.e. Dubaiset1, Khalifa set 1 and others by using the technologies like deep space navigation, autonomous satellite and planetary spectrum generator. But, these technologies are currently dependent on the command center which are manually operated. Thus, increasing the chance of error. Due to this, the demand of software which integrate deep learning and machine learning in the commanding centers are coming under high demand. This will make the probability of an error to a very minute percentage. The Artificial Intelligence which will be used in the Planetary Spectrum Generator and autonomous satellite to create a safe passage of communication directly from the rovers on different planets and satellites in the orbit to the command center without any human interference. The software will work with deep learning which currently all the space stations work with and will combine it with machine learning to make it into a complete network of different data collected from different satellites into the planetary spectrum generator(PSG) and thus giving the correct information with less time and perfect efficiency.",https://ieeexplore.ieee.org/document/9320781/,2020 Seventh International Conference on Information Technology Trends (ITT),25-26 Nov. 2020,ieeexplore
10.1109/ICPR48806.2021.9412896,Variational Inference with Latent Space Quantization for Adversarial Resilience,IEEE,Conferences,"Despite their tremendous success in modelling high-dimensional data manifolds, deep neural networks suffer from the threat of adversarial attacks - Existence of perceptually valid input-like samples obtained through careful perturbation that lead to degradation in the performance of the underlying model. Major concerns with existing defense mechanisms include non-generalizability across different attacks, models and large inference time. In this paper, we propose a generalized defense mechanism capitalizing on the expressive power of regularized latent space based generative models. We design an adversarial filter, devoid of access to classifier and adversaries, which makes it usable in tandem with any classifier. The basic idea is to learn a Lipschitz constrained mapping from the data manifold, incorporating adversarial perturbations, to a quantized latent space and re-map it to the true data manifold. Specifically, we simultaneously auto-encode the data manifold and its perturbations implicitly through the perturbations of the regularized and quantized generative latent space, realized using variational inference. We demonstrate the efficacy of the proposed formulation in providing resilience against multiple attack types (black and white box) and methods, while being almost real-time. Our experiments show that the proposed method surpasses the state-of-the-art techniques in several cases. The implementation code is available at - https://github.com/mayank31398/lqvae.",https://ieeexplore.ieee.org/document/9412896/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore
10.1109/MMAR49549.2021.9528467,"Virtual Urban Space Simulator   Gliwice, Poland City Centre Example in the Context of COVID-19 pandemic",IEEE,Conferences,"Virtual environments (VE) are commonly used in learning or training in many different areas. We are focused on the problem when a group of artificial agents are simulated at the same time; their behavior is easily recognized as artificial. To avoid this undesired property presented work was aimed at creating a virtual world with agents equipped with psychosomatic elements that occur in the real world. It has been chosen the implementation of basic needs based on Maslow's hierarchy of needs (MHN), since they are one of necessary elements for the development of simulation of the real world. It is shown that the VE design based on artificial intelligence (AI) planning algorithms and the theory of MHN can be efficiently applied to generate semi-realistic behavior of agents population. Models of coronavirus spread have been introduced into the simulated environment. It allowed to build cases related to the number of infected people and the rate of infection depending on the level of avatar activity associated with basic needs.",https://ieeexplore.ieee.org/document/9528467/,2021 25th International Conference on Methods and Models in Automation and Robotics (MMAR),23-26 Aug. 2021,ieeexplore
10.1109/TCSVT.2017.2726564,A Hardware Architecture for Cell-Based Feature-Extraction and Classification Using Dual-Feature Space,IEEE,Journals,"Many computer-vision and machine-learning applications in robotics, mobile, wearable devices, and automotive domains are constrained by their real-time performance requirements. This paper reports a dual-feature-based object recognition coprocessor that exploits both histogram of oriented gradient (HOG) and Haar-like descriptors with a cell-based parallel sliding-window recognition mechanism. The feature extraction circuitry for HOG and Haar-like descriptors is implemented by a pixel-based pipelined architecture, which synchronizes to the pixel frequency from the image sensor. After extracting each cell feature vector, a cell-based sliding window scheme enables parallelized recognition for all windows, which contain this cell. The nearest neighbor search classifier is, respectively, applied to the HOG and Haar-like feature space. The complementary aspects of the two feature domains enable a hardware-friendly implementation of the binary classification for pedestrian detection with improved accuracy. A proof-of-concept prototype chip fabricated in a 65-nm SOI CMOS, having thin gate oxide and buried oxide layers (SOTB CMOS), with 3.22-mm<sup>2</sup> core area achieves an energy efficiency of 1.52 nJ/pixel and a processing speed of 30 fps for 1024  1616-pixel image frames at 200-MHz recognition working frequency and 1-V supply voltage. Furthermore, multiple chips can implement image scaling, since the designed chip has image-size flexibility attributable to the pixel-based architecture.",https://ieeexplore.ieee.org/document/7979565/,IEEE Transactions on Circuits and Systems for Video Technology,Oct. 2018,ieeexplore
10.1109/JSEN.2020.3042665,A Searching Space Constrained Partial to Full Registration Approach With Applications in Airport Trolley Deployment Robot,IEEE,Journals,"For airports with high passenger and luggage flows, a large number of staff members have to be hired to deploy the scattered passenger luggage trolleys. To release humans from the repetitive and laborious job, we develop an autonomous trolley deployment robot to detect, transport and collect the scattered idle trolleys to recycling points. This paper will firstly illustrate the entire collection pipeline of the deployment robot system and then address the key challenge: partial to full point set registration. With the perception framework, the robot can detect the idle trolleys and acquire the pose of the trolleys on the ground, and then capture the trolley from behind, along the same direction for subsequent grasping and manipulation. With RGB-D camera and a segmentation Convolutional Neural Network, the robot can generate a partial surface point cloud of the detected trolley. The resulting point cloud, data and a pre-scanned full trolley point cloud, model, are matched by an implicit pose. To tackle the low accuracy and long computation time issues, a novel searching space-constrained point set registration algorithm is proposed to register the two overlapping point sets. Based on Branch-and-Bound (BnB) mechanism, the error between data and model is iteratively optimized. The constraint of searching space speeds up the global searching of the optimal pose, by pruning the candidate spaces which is impossible to contain the optimal result. To evaluate the performance, an airport trolley segmentation dataset and a point cloud dataset for registration are constructed. Experimental results on the datasets and synthetic dataset show that our method achieves higher accuracy and success rate than the previous methods. The experiments demonstrated in video clips validate the developed system works in real-world applications.",https://ieeexplore.ieee.org/document/9281085/,IEEE Sensors Journal,"15 May15, 2021",ieeexplore
10.1109/TIE.2011.2159949,A TS Fuzzy System Learned Through a Support Vector Machine in Principal Component Space for Real-Time Object Detection,IEEE,Journals,"This paper proposes a Takagi-Sugeno (TS) fuzzy system learned through a support vector machine (SVM) in principal component space (TFS-SVMPC) for real-time object detection. The antecedent part of the TFS-SVMPC classifier is generated using an algorithm that is similar to fuzzy clustering. The dimension of the free parameter vector in the TS consequent part of the TFS-SVMPC is first reduced by principal component analysis (PCA). A linear SVM is then used to tune the subsequent parameters in the principal component space to give the system better generalization performance. The TFS-SVMPC is used as a classifier in a camera-based real-time object detection system. The object detection system consists of two stages. The first stage uses a color histogram of the global color appearance of an object as a detection feature for a TFS-SVMPC classifier. In particular, an efficient method for histogram extraction during the image scanning process is proposed for real-time implementation. The second stage uses the geometry-dependent local color appearance as a color feature for another TFS-SVMPC classifier. Comparisons with other types of classifiers and detection methods for the detection of different objects verify the performance of the proposed TFS-SVMPC-based detection method.",https://ieeexplore.ieee.org/document/5892887/,IEEE Transactions on Industrial Electronics,Aug. 2012,ieeexplore
10.23919/JSEE.2020.000080,A fast computational method for the landing footprints of space-to-ground vehicles,BIAI,Journals,"Fast computation of the landing footprint of a space-to- ground vehicle is a basic requirement for the deployment of parking orbits, as well as for enabling decision makers to develop real-time programs of transfer trajectories. In order to address the usually slow computational time for the determination of the landing footprint of a space-to-ground vehicle under finite thrust, this work proposes a method that uses polynomial equations to describe the boundaries of the landing footprint and uses back propagation (BP) neural networks to quickly determine the landing footprint of the space-to-ground vehicle. First, given orbital parameters and a manoeuvre moment, the solution model of the landing footprint of a space-to-ground vehicle under finite thrust is established. Second, given arbitrary orbital parameters and an arbitrary manoeuvre moment, a fast computational model for the landing footprint of a space-to-ground vehicle based on BP neural networks is provided. Finally, the simulation results demonstrate that under the premise of ensuring accuracy, the proposed method can quickly determine the landing footprint of a space-to-ground vehicle with arbitrary orbital parameters and arbitrary manoeuvre moments. The proposed fast computational method for determining a landing footprint lays a foundation for the parking-orbit configuration and supports the design of real-time transfer trajectories.",https://ieeexplore.ieee.org/document/9247427/,Journal of Systems Engineering and Electronics,Oct. 2020,ieeexplore
10.1109/TSMCB.2003.811114,Clustering and group selection of multiple criteria alternatives with application to space-based networks,IEEE,Journals,"In many real-world problems, the range of consequences of different alternatives are considerably different. In addition, sometimes, selection of a group of alternatives (instead of only one best alternative) is necessary. Traditional decision making approaches treat the set of alternatives with the same method of analysis and selection. In this paper, we propose clustering alternatives into different groups so that different methods of analysis, selection, and implementation for each group can be applied. As an example, consider the selection of a group of functions (or tasks) to be processed by a group of processors. The set of tasks can be grouped according to their similar criteria, and hence, each cluster of tasks to be processed by a processor. The selection of the best alternative for each clustered group can be performed using existing methods; however, the process of selecting groups is different than the process of selecting alternatives within a group. We develop theories and procedures for clustering discrete multiple criteria alternatives. We also demonstrate how the set of alternatives is clustered into mutually exclusive groups based on 1) similar features among alternatives; 2) ideal (or most representative) alternatives given by the decision maker; and 3) other preferential information of the decision maker. The clustering of multiple criteria alternatives also has the following advantages. 1) It decreases the set of alternatives to be considered by the decision maker (for example, different decision makers are assigned to different groups of alternatives). 2) It decreases the number of criteria. 3) It may provide a different approach for analyzing multiple decision makers problems. Each decision maker may cluster alternatives differently, and hence, clustering of alternatives may provide a basis for negotiation. The developed approach is applicable for solving a class of telecommunication networks problems where a set of objects (such as routers, processors, or intelligent autonomous vehicles) are to be clustered into similar groups. Objects are clustered based on several criteria and the decision maker's preferences.",https://ieeexplore.ieee.org/document/1262480/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",Feb. 2004,ieeexplore
10.1109/ACCESS.2021.3105136,Cocktail Glass Network: Fast Depth Estimation Using Channel to Space Unrolling,IEEE,Journals,"Depth-estimation from a single input image can be used in applications such as robotics and autonomous driving. Recently, depth-estimation networks with UNet encoder/decoder structures have been widely used. In these decoders, operations are repeated to gradually increase the image resolution, while decreasing the channel size. If the upsampling operation at a high magnification can be processed at once, the amount of computation in the decoder can be dramatically reduced. To achieve this, we propose a new network structure, i.e., a cocktail glass network. In this network, convolution layers in the decoder are reduced, and a novel fast upsampling method is used that is known as channel-to-space unrolling, which converts thick channel data into high-resolution data. The proposed method can be easily implemented using simple reshaping operations; therefore, it is suitable for reducing the depth-estimation network. Considering the experimental results based on the NYU V2 and KITTI datasets, we demonstrate that the proposed method reduces the amount of computation in the decoder by half, while maintaining the same level of accuracy; it can be used in both lightweight and large-model-capacity networks.",https://ieeexplore.ieee.org/document/9514839/,IEEE Access,2021,ieeexplore
10.1109/TITS.2018.2882439,Convolutional Neural Networks for On-Street Parking Space Detection in Urban Networks,IEEE,Journals,"The purpose of this paper is the development of data science models for the detection of empty on-street parking spaces in urban road networks based on data provided by in-vehicle cameras that are already, or soon will be, a standard vehicle equipment. A rolling spatial interval is used to identify the existence of an on-street parking space and the properties of empty spaces are used to determine the availability of the parking space. Convolutional neural networks are developed, trained, and evaluated with the use of images from a moving vehicle camera. The images are preprocessed and converted to suitable matrices, so that only the useful information for the empty on-street parking space detection problem is preserved. The optimized convolutional networks, in terms of structural and learning parameters, provided predictions for the detection of empty on-street parking spaces with approximately 90% average accuracy. The proposed model performs better than the relatively complex SVMs, which supports its appropriateness as an approach. Finally, the implementation of a framework, which integrates the developed models to produce meaningful parking information for drivers in real time, is discussed.",https://ieeexplore.ieee.org/document/8577026/,IEEE Transactions on Intelligent Transportation Systems,Dec. 2019,ieeexplore
10.1109/TITB.2004.832550,DBMap: a space-conscious data visualization and knowledge discovery framework for biomedical data warehouse,IEEE,Journals,"Advances in digital imaging modalities as well as other diagnosis and therapeutic techniques have generated a massive amount of diverse data for clinical research. The purpose of this study is to investigate and implement a new intuitive and space-conscious visualization framework, called DBMap, to facilitate efficient multidimensional data visualization and knowledge discovery against the large-scale data warehouses of integrated image and nonimage data. The DBMap framework is built upon the TreeMap concept. TreeMap is a space constrained graphical representation of large hierarchical data sets, mapped to a matrix of rectangles, whose size and color represent interested database fields. It allows the display of a large amount of numerical and categorical information in limited real estate of the computer screen with an intuitive user interface. DBMap has been implemented and integrated into a large brain research data warehouse to support neurologic and neuroradiologic research at the University of California, San Francisco Medical Center. For imaging specialists and clinical researchers, this novel DBMap framework facilitates another way to better explore and classify the hidden knowledge embedded in medical image data warehouses.",https://ieeexplore.ieee.org/document/1331412/,IEEE Transactions on Information Technology in Biomedicine,Sept. 2004,ieeexplore
10.1109/ACCESS.2021.3064928,Deep Space Network Scheduling via Mixed-Integer Linear Programming,IEEE,Journals,"NASAs Deep Space Network (DSN) is a globally-spanning communications network responsible for supporting the interplanetary spacecraft missions of NASA and other international users. The DSN is a highly utilized asset, and the large demand for its services makes the assignment of DSN resources a daunting computational problem. In this paper we study the DSN scheduling problem, which is the problem of assigning the DSNs limited resources to its users within a given time horizon. The DSN scheduling problem is oversubscribed, meaning that only a subset of the activities can be scheduled, and network operators must decide which activities to exclude from the schedule. We first formulate this challenging scheduling task as a Mixed-Integer Linear Programming (MILP) optimization problem. Next, we develop a sequential algorithm which solves the resulting MILP formulation to produce valid schedules for large-scale instances of the DSN scheduling problem. We use real world DSN data from week 44 of 2016 in order to evaluate our algorithms performance. We find that given a fixed run time, our algorithm outperforms a simple implementation of our MILP model, generating a feasible schedule in which 17% more activities are scheduled by the algorithm than by the simple implementation. We design a non-MILP based heuristic to further validate our results. We find that our algorithm also outperforms this heuristic, scheduling 8% more activities and 20% more tracking time than the best results achieved by the non-MILP implementation.",https://ieeexplore.ieee.org/document/9373338/,IEEE Access,2021,ieeexplore
10.1109/18.985979,Diagonal algebraic space-time block codes,IEEE,Journals,"We construct a new family of linear space-time (ST) block codes by the combination of rotated constellations and the Hadamard transform, and we prove them to achieve the full transmit diversity over a quasi-static or fast fading channels. The proposed codes transmit at a normalized rate of 1 symbol/s. When the number of transmit antennas n=1, 2, or n is a multiple of four, we spread a rotated version of the information symbol vector by the Hadamard transform and send it over n transmit antennas and n time periods; for other values of n, we construct the codes by sending the components of a rotated version of the information symbol vector over the diagonal of an n /spl times/ n ST code matrix. The codes maintain their rate, diversity, and coding gains for all real and complex constellations carved from the complex integers ring Z [i], and they outperform the codes from orthogonal design when using complex constellations for n &gt; 2. The maximum-likelihood (ML) decoding of the proposed codes can be implemented by the sphere decoder at a moderate complexity. It is shown that using the proposed codes in a multiantenna system yields good performances with high spectral efficiency and moderate decoding complexity.",https://ieeexplore.ieee.org/document/985979/,IEEE Transactions on Information Theory,March 2002,ieeexplore
10.1109/ACCESS.2019.2918480,Distribution Network Reconfiguration Using Selective Firefly Algorithm and a Load Flow Analysis Criterion for Reducing the Search Space,IEEE,Journals,"This paper proposes an alternative to solve the distribution network reconfiguration (DNR) problem, aiming real power losses' minimization. For being a problem that has complexity for its solution, approximate techniques are adequate for solving it. Here, the proposition is a technique based on the firefly metaheuristic, named selective firefly algorithm, where the positioning of these insects is compressed in a selective range of values. The algorithm is applied to the DNR, and all its implementation and adequacy to the problem studied are presented. To define the search space, the methodology presented initially considers a set of candidate switches for opening based on the studied systems' mesh analysis. To reduce these possibilities, a refinement through a load flow analysis criterion (LFAC) is proposed. This LFAC considers the real power losses on each branch for a configuration with all switches closed, then, selecting possible switches to elimination from the set previously established. To demonstrate the behavior and the viability of the LFAC, it was initially applied on a 5 buses' and 7 branches' system. Also, to avoid getting stuck on results that may be considered not good, a disturbance resetting the population is set to occur every time a counter reaches a pre-defined number of times that the best solution does not change. Results found for simulations with 33, 70, and 84 buses are presented and comparisons with selective particle swarm optimization (SPSO) and selective bat algorithm (SBAT) are made.",https://ieeexplore.ieee.org/document/8720166/,IEEE Access,2019,ieeexplore
10.1109/TCBB.2013.119,EEG/ERP Adaptive Noise Canceller Design with Controlled Search Space (CSS) Approach in Cuckoo and Other Optimization Algorithms,IEEE,Journals,"This paper explores the migration of adaptive filtering with swarm intelligence/evolutionary techniques employed in the field of electroencephalogram/event-related potential noise cancellation or extraction. A new approach is proposed in the form of controlled search space to stabilize the randomness of swarm intelligence techniques especially for the EEG signal. Swarm-based algorithms such as Particles Swarm Optimization, Artificial Bee Colony, and Cuckoo Optimization Algorithm with their variants are implemented to design optimized adaptive noise canceler. The proposed controlled search space technique is tested on each of the swarm intelligence techniques and is found to be more accurate and powerful. Adaptive noise canceler with traditional algorithms such as least-mean-square, normalized least-mean-square, and recursive least-mean-square algorithms are also implemented to compare the results. ERP signals such as simulated visual evoked potential, real visual evoked potential, and real sensorimotor evoked potential are used, due to their physiological importance in various EEG studies. Average computational time and shape measures of evolutionary techniques are observed 8.21E-01 sec and 1.73E-01, respectively. Though, traditional algorithms take negligible time consumption, but are unable to offer good shape preservation of ERP, noticed as average computational time and shape measure difference, 1.41E-02 sec and 2.60E+00, respectively.",https://ieeexplore.ieee.org/document/6606790/,IEEE/ACM Transactions on Computational Biology and Bioinformatics,Nov.-Dec. 2013,ieeexplore
10.1109/ACCESS.2021.3136138,Extending the Space of Software Test Monitoring: Practical Experience,IEEE,Journals,"Software reliability depends on the performed tests. Bug detection and diagnosis are based on test outcome (oracle) analysis. Most of practical test reports do not provide sufficient information for localizing and correcting bugs. We have found the need to extend the space of test result observation in data and time perspectives. This resulted in tracing supplementary test result features in event logs. They are explored with combined text mining and log parsing techniques. Another important point is correlating test life cycle with project development history journaled in issue tracking and software version control repositories. Dealing with the outlined problems, neglected in the literature, we have introduced original analysis schemes. They focus on assessing test coverage, reasons of low diagnosability, and test result profiles. Multidimensional investigation of test features and their management is supported with the developed test infrastructure. This assures a holistic insight into the test efficiency to identify test scheme deficiencies (e.g., functional inadequacy, aging, insufficient coverage) and possible improvements (test set updates). Our studies have been verified in relevance to a real commercial project and confronted with the experience of testers engaged in other projects.",https://ieeexplore.ieee.org/document/9652520/,IEEE Access,2021,ieeexplore
10.1109/81.661672,Gabor-type filtering in space and time with cellular neural networks,IEEE,Journals,"Gabor filters are preprocessing stages in image-processing and computer-vision applications. One drawback is that they are computationally intensive on a digital computer. This paper describes the design of cellular neural networks (CNNs) which compute the outputs of filters similar to Gabor filters. Analog VLSI implementations of these CNNs might eventually relieve the computational bottleneck associated with Gabor filtering image-processing algorithms. The CNNs compute both the real and imaginary parts of the filter outputs simultaneously, which is an important feature in applying them in algorithms utilizing the phase of the Gabor output.",https://ieeexplore.ieee.org/document/661672/,IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications,Feb. 1998,ieeexplore
10.1109/TCOMM.2005.847166,Generalized PSK in space-time coding,IEEE,Journals,"A wireless communication system using multiple antennas promises reliable transmission under Rayleigh flat fading assumptions. Design criteria and practical schemes have been presented for both coherent and noncoherent communication channels. In this paper, we generalize one-dimensional (1-D) phase-shift keying (PSK) signals and introduce space-time constellations from generalized PSK (GPSK) signals based on the complex and real orthogonal designs. The resulting space-time constellations reallocate the energy for each transmitting antenna and feature good diversity products; consequently, their performances are better than some of the existing comparable codes. Moreover, since the maximum-likelihood (ML) decoding of our proposed codes can be decomposed to 1-D PSK signal demodulation, the ML decoding of our codes can be implemented in a very efficient way.",https://ieeexplore.ieee.org/document/1431123/,IEEE Transactions on Communications,May 2005,ieeexplore
10.1109/TIP.2020.3045634,"HRSiam: High-Resolution Siamese Network, Towards Space-Borne Satellite Video Tracking",IEEE,Journals,"Tracking moving objects from space-borne satellite videos is a new and challenging task. The main difficulty stems from the extremely small size of the target of interest. First, because the target usually occupies only a few pixels, it is hard to obtain discriminative appearance features. Second, the small object can easily suffer from occlusion and illumination variation, making the features of objects less distinguishable from features in surrounding regions. Current state-of-the-art tracking approaches mainly consider high-level deep features of a single frame with low spatial resolution, and hardly benefit from inter-frame motion information inherent in videos. Thus, they fail to accurately locate such small objects and handle challenging scenarios in satellite videos. In this article, we successfully design a lightweight parallel network with a high spatial resolution to locate the small objects in satellite videos. This architecture guarantees real-time and precise localization when applied to the Siamese Trackers. Moreover, a pixel-level refining model based on online moving object detection and adaptive fusion is proposed to enhance the tracking robustness in satellite videos. It models the video sequence in time to detect the moving targets in pixels and has ability to take full advantage of tracking and detecting. We conduct quantitative experiments on real satellite video datasets, and the results show the proposed HIGH-RESOLUTION SIAMESE NETWORK (HRSiam) achieves state-of-the-art tracking performance while running at over 30 FPS.",https://ieeexplore.ieee.org/document/9350236/,IEEE Transactions on Image Processing,2021,ieeexplore
10.1109/TAMD.2011.2106781,Implicit Sensorimotor Mapping of the Peripersonal Space by Gazing and Reaching,IEEE,Journals,"Primates often perform coordinated eye and arm movements, contextually fixating and reaching towards nearby objects. This combination of looking and reaching to the same target is used by infants to establish an implicit visuomotor representation of the peripersonal space, useful for both oculomotor and arm motor control. In this work, taking inspiration from such behavior and from primate visuomotor mechanisms, a shared sensorimotor map of the environment, built on a radial basis function framework, is configured and trained by the coordinated control of eye and arm movements. Computational results confirm that the approach seems especially suitable for the problem at hand, and for its implementation on a real humanoid robot. By exploratory gazing and reaching actions, either free or goal-based, the artificial agent learns to perform direct and inverse transformations between stereo vision, oculomotor, and joint-space representations. The integrated sensorimotor map that allows to contextually represent the peripersonal space through different vision and motor parameters is never made explicit, but rather emerges thanks to the interaction of the agent with the environment.",https://ieeexplore.ieee.org/document/5703113/,IEEE Transactions on Autonomous Mental Development,March 2011,ieeexplore
10.1093/mnras/stab1879,Investigating cosmological GAN emulators using latent space interpolation,OUP,Journals,"Generative adversarial networks (GANs) have been recently applied as a novel emulation technique for large-scale structure simulations. Recent results show that GANs can be used as a fast and efficient emulator for producing novel weak lensing convergence maps as well as cosmic web data in 2D and 3D. However, like any algorithm, the GAN approach comes with a set of limitations, such as an unstable training procedure, inherent randomness of the produced outputs, and difficulties when training the algorithm on multiple data sets. In this work, we employ a number of techniques commonly used in the machine learning literature to address the mentioned limitations. Specifically, we train a GAN to produce weak lensing convergence maps and dark matter overdensity field data for multiple redshifts, cosmological parameters, and modified gravity models. In addition, we train a GAN using the newest Illustris data to emulate dark matter, gas, and internal energy distribution data simultaneously. Finally, we apply the technique of latent space interpolation as a tool for understanding the feature space of the GAN algorithm. We show that the latent space interpolation procedure allows the generation of outputs with intermediate cosmological parameters that were not included in the training data. Our results indicate a 120 percent difference between the power spectra of the GAN-produced and the test data samples depending on the data set used and whether Gaussian smoothing was applied. Similarly, the Minkowski functional analysis indicates a good agreement between the emulated and the real images for most of the studied data sets.",https://ieeexplore.ieee.org/document/9521040/,Monthly Notices of the Royal Astronomical Society,July 2021,ieeexplore
10.1109/ACCESS.2020.2999727,LPD-AE: Latent Space Representation of Large-Scale 3D Point Cloud,IEEE,Journals,"The effective latent space representation of point cloud provides a foremost and fundamental manner that can be used for challenging tasks, including point cloud based place recognition and reconstruction, especially in large-scale dynamic environments. In this paper, we present a novel deep neural network, LPD-AE(Large-scale Place Description AutoEncoder Network), to obtain meaningful local and contextual features for the generation of latent space from 3D point cloud directly. The encoder network constructs the discriminative global descriptors to realize high accuracy and robust place recognition, which contributed by extracting the local neighbor geometric features and aggregating neighborhood relationships both in feature space and physical space. The decoder network performs hierarchical reconstruction on coarse key points and ultimately produce dense point clouds, which shows that it is capable of reconstructing a full point cloud frame from a single compact but high dimensional descriptor. Our proposed network demonstrates performance that is comparable to the state-of-the-art approaches. With the benefit of the LPD-AE, many computationally complex tasks that rely directly on point clouds can be effortlessly conducted on latent space with lower memory costs, such as relocalization, loop closure detection, and map compression reconstruction. Comprehensive validations on Oxford RobotCar dataset, KITTI dataset, and our freshly collected dataset, which contains multiple trials of repeated routes in different weather and at different times, manifest its potency for real robotic and self-driving implementation. The source code is available at https://github.com/Suoivy/LPD-AE.",https://ieeexplore.ieee.org/document/9107146/,IEEE Access,2020,ieeexplore
10.1109/TPAMI.2008.157,Latent-Space Variational Bayes,IEEE,Journals,"Variational Bayesian expectation-maximization (VBEM), an approximate inference method for probabilistic models based on factorizing over latent variables and model parameters, has been a standard technique for practical Bayesian inference. In this paper, we introduce a more general approximate inference framework for conjugate-exponential family models, which we call latent-space variational Bayes (LSVB). In this approach, we integrate out model parameters in an exact way, leaving only the latent variables. It can be shown that the LSVB approach gives better estimates of the model evidence as well as the distribution over latent variables than the VBEM approach, but in practice, the distribution over latent variables has to be approximated. As a practical implementation, we present a first-order LSVB (FoLSVB) algorithm to approximate this distribution over latent variables. From this approximate distribution, one can estimate the model evidence and the posterior over model parameters. The FoLSVB algorithm is directly comparable to the VBEM algorithm and has the same computational complexity. We discuss how LSVB generalizes the recently proposed collapsed variational methods [20] to general conjugate-exponential families. Examples based on mixtures of Gaussians and mixtures of Bernoullis with synthetic and real-world data sets are used to illustrate some advantages of our method over VBEM.",https://ieeexplore.ieee.org/document/4670325/,IEEE Transactions on Pattern Analysis and Machine Intelligence,Dec. 2008,ieeexplore
10.1109/TNNLS.2013.2256797,Learning in the Model Space for Cognitive Fault Diagnosis,IEEE,Journals,"The emergence of large sensor networks has facilitated the collection of large amounts of real-time data to monitor and control complex engineering systems. However, in many cases the collected data may be incomplete or inconsistent, while the underlying environment may be time-varying or unformulated. In this paper, we develop an innovative cognitive fault diagnosis framework that tackles the above challenges. This framework investigates fault diagnosis in the model space instead of the signal space. Learning in the model space is implemented by fitting a series of models using a series of signal segments selected with a sliding window. By investigating the learning techniques in the fitted model space, faulty models can be discriminated from healthy models using a one-class learning algorithm. The framework enables us to construct a fault library when unknown faults occur, which can be regarded as cognitive fault isolation. This paper also theoretically investigates how to measure the pairwise distance between two models in the model space and incorporates the model distance into the learning algorithm in the model space. The results on three benchmark applications and one simulated model for the Barcelona water distribution network confirm the effectiveness of the proposed framework.",https://ieeexplore.ieee.org/document/6515601/,IEEE Transactions on Neural Networks and Learning Systems,Jan. 2014,ieeexplore
10.1093/comjnl/bxu058,Multi-Agent Architecture for Control of Heating and Cooling in a Residential Space,OUP,Journals,"Energy demand in a smart grid is directly related to energy consumption, as defined by user needs and comfort experience. This article presents a multi-agent architecture for smart control of space heating and cooling processes, in an attempt to enable flexible ways of monitoring and adjusting energy supply and demand. In this proposed system, control agents are implemented in order to perform temperature set-point delegation for heating and cooling systems in a building, offering a means to observe and learn from both the environment and the occupant. Operation of the proposed algorithms is compared with traditional algorithms utilized for room heating, using a simulated model of a residential building and real data about user behaviour. The results show (i) the performance of machine learning for the occupancy forecasting problem and for the problem of calculating the time to heat or cool a room; and (ii) the performance of the control algorithms, with respect to energy consumption and occupant comfort. The proposed control agents make it possible to significantly improve an occupant comfort with a relatively small increase in energy consumption, compared with simple control strategies that always maintain predefined temperatures. The findings enable the smart grid to anticipate the energy needs of the building.",https://ieeexplore.ieee.org/document/8131352/,The Computer Journal,June 2015,ieeexplore
10.1109/TSP.2021.3095709,Online Joint State Inference and Learning of Partially Unknown State-Space Models,IEEE,Journals,"A computationally efficient method for online joint state inference and dynamical model learning is presented. The dynamical model combines an a priori known, physically derived, state-space model with a radial basis function expansion representing unknown system dynamics and inherits properties from both physical and data-driven modeling. The method uses an extended Kalman filter approach to jointly estimate the state of the system and learn the unknown system dynamics, via the parameters of the basis function expansion. The key contribution is a computational complexity reduction compared to a similar approach with globally supported basis functions. By using compactly supported radial basis functions and an approximate Kalman gain, the computational complexity is considerably reduced and is essentially determined by the support of the basis functions. The approximation works well when the system dynamics exhibit limited correlation between points well separated in the state-space domain. The method is exemplified via two intelligent vehicle applications where it is shown to: (i) have competitive system dynamics estimation performance compared to the globally supported basis function method, and (ii) be real-time applicable to problems with a large-scale state-space.",https://ieeexplore.ieee.org/document/9479713/,IEEE Transactions on Signal Processing,2021,ieeexplore
10.1109/TGRS.2020.3045790,Physically Constrained Transfer Learning Through Shared Abundance Space for Hyperspectral Image Classification,IEEE,Journals,"Hyperspectral image (HSI) classification is one of the most active research topics and has achieved promising results boosted by the recent development of deep learning. However, most state-of-the-art approaches tend to perform poorly when the training and testing images are on different domains, e.g., the source domain and target domain, respectively, due to the spectral variability caused by different acquisition conditions. Transfer learning-based methods address this problem by pretraining in the source domain and fine-tuning on the target domain. Nonetheless, a considerable amount of data on the target domain has to be labeled and nonnegligible computational resources are required to retrain the whole network. In this article, we propose a new transfer learning scheme to bridge the gap between the source and target domains by projecting the HSI data from the source and target domains into a shared abundance space based on their own physical characteristics. In this way, the domain discrepancy would be largely reduced such that the model trained on the source domain could be applied to the target domain without extra efforts for data labeling or network retraining. The proposed method is referred to as physically constrained transfer learning through shared abundance space (PCTL-SAS). Extensive experimental results demonstrate the superiority of the proposed method as compared to the state of the art. The success of this endeavor would largely facilitate the deployment of HSI classification for real-world sensing scenarios.",https://ieeexplore.ieee.org/document/9318553/,IEEE Transactions on Geoscience and Remote Sensing,Dec. 2021,ieeexplore
10.1109/TDSC.2019.2903049,Real-Time Error Detection in Nonlinear Control Systems Using Machine Learning Assisted State-Space Encoding,IEEE,Journals,"Successful deployment of autonomous systems in a wide range of societal applications depends on error-free operation of the underlying signal processing and control functions. Real-time error detection in nonlinear systems has mostly relied on redundancy at the component or algorithmic level causing expensive area and power overheads. This paper describes a real-time error detection methodology for nonlinear control systems for detecting sensor and actuator degradations as well as malfunctions due to soft errors in the execution of the control algorithm on a digital processor. Our approach is based on creation of a redundant check state in such a way that its value can be computed from the current states of the system as well as from a history of prior observable state values and inputs (via machine learning algorithms). By checking for consistency between the two, errors are detected with low latency. The method is demonstrated on two test case simulations - an inverted pendulum balancing problem and a sliding mode controller driven brake-by-wire (BBW) system. In addition, hardware results from error injection experiments in an ARM core representation on an FPGA and artificial sensor degradations on a self-balancing robot prove the practical feasibility of implementation.",https://ieeexplore.ieee.org/document/8658148/,IEEE Transactions on Dependable and Secure Computing,1 March-April 2021,ieeexplore
10.1109/TVCG.2018.2849381,Realtime Hand-Object Interaction Using Learned Grasp Space for Virtual Environments,IEEE,Journals,"We present a realtime virtual grasping algorithm to model interactions with virtual objects. Our approach is designed for multi-fingered hands and makes no assumptions about the motion of the user's hand or the virtual objects. Given a model of the virtual hand, we use machine learning and particle swarm optimization to automatically pre-compute stable grasp configurations for that object. The learning pre-computation step is accelerated using GPU parallelization. At runtime, we rely on the pre-computed stable grasp configurations, and dynamics/non-penetration constraints along with motion planning techniques to compute plausible looking grasps. In practice, our realtime algorithm can perform virtual grasping operations in less than 20ms for complex virtual objects, including high genus objects with holes. We have integrated our grasping algorithm with Oculus Rift HMD and Leap Motion controller and evaluated its performance for different tasks corresponding to grabbing virtual objects and placing them at arbitrary locations. Our user evaluation suggests that our virtual grasping algorithm can increase the user's realism and participation in these tasks and offers considerable benefits over prior interaction algorithms, such as pinch grasping and raycast picking.",https://ieeexplore.ieee.org/document/8392385/,IEEE Transactions on Visualization and Computer Graphics,1 Aug. 2019,ieeexplore
10.1109/TPAMI.2012.82,Removing Atmospheric Turbulence via Space-Invariant Deconvolution,IEEE,Journals,"To correct geometric distortion and reduce space and time-varying blur, a new approach is proposed in this paper capable of restoring a single high-quality image from a given image sequence distorted by atmospheric turbulence. This approach reduces the space and time-varying deblurring problem to a shift invariant one. It first registers each frame to suppress geometric deformation through B-spline-based nonrigid registration. Next, a temporal regression process is carried out to produce an image from the registered frames, which can be viewed as being convolved with a space invariant near-diffraction-limited blur. Finally, a blind deconvolution algorithm is implemented to deblur the fused image, generating a final output. Experiments using real data illustrate that this approach can effectively alleviate blur and distortions, recover details of the scene, and significantly improve visual quality.",https://ieeexplore.ieee.org/document/6178259/,IEEE Transactions on Pattern Analysis and Machine Intelligence,Jan. 2013,ieeexplore
10.1109/ACCESS.2018.2880794,"Smart Space Concepts, Properties and Architectures",IEEE,Journals,"Smart spaces have been actively emerging recently, and researchers are working on developing and testing smart spaces in the real world. They facilitate smart applications that are adaptive to user preferences and contexts. In doing so they must satisfy applications' dynamically changing resource needs. These objectives are achievable by cooperation among connected devices and ubiquitous interaction. Smart space architecture designs in the literature are mostly application specific, their concepts and components defined based on the specific needs of one application. In this paper, we formally define general smart space concepts and architectural models rigorously and discuss related architectural components (both hardware and software) in detail. Based on a literature review we summarize the discriminating properties that a smart space must possess, and its basic components and services to realize these properties. We present a comparative analysis of the architectural designs proposed thus far. A comprehensive smart space architecture is proposed and its semantic interoperability is discussed in detail. In addition, we provide a case study of a smart lighting system, where the properties of smart spaces are analyzed. Finally, we provide a roadmap for future smart space development.",https://ieeexplore.ieee.org/document/8531616/,IEEE Access,2018,ieeexplore
10.1109/36.317435,Space instrument neural network for real-time data analysis,IEEE,Journals,"A simple software implementation of an artificial neural network (ANN) was used to analyze up to 200 autocorrelation functions (ACFs) per second within the Shuttle Potential and Return Electron Experiment (SPREE) flown on the Shuttle STS46 mission, July 31, 1992. As all ACF data are stored onboard until postmission, this facility provided ground-based experimenters with their only access to ACF data in real time for optimum instrument control. ACFs contain data either as waveforms or as radar echoes. Operating directly on the ACF, the neural network identifies the type of data, ascertains the wave frequency or radar peak separation, and provides a score or measure of significance of its decision. An effective 16:1 data reduction is achieved and the data interpretation performance is comparable to that achieved by an expert data analyst. Erroneous analysis accounts for less than 1% of data analyzed.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/317435/,IEEE Transactions on Geoscience and Remote Sensing,Nov. 1993,ieeexplore
10.1109/TVCG.2007.1019,Space-Time Light Field Rendering,IEEE,Journals,"In this paper, we propose a novel framework called space-time light field rendering, which allows continuous exploration of a dynamic scene in both space and time. Compared to existing light field capture/rendering systems, it offers the capability of using unsynchronized video inputs and the added freedom of controlling the visualization in the temporal domain, such as smooth slow motion and temporal integration. In order to synthesize novel views from any viewpoint at any time instant, we develop a two-stage rendering algorithm. We first interpolate in the temporal domain to generate globally synchronized images using a robust spatial-temporal image registration algorithm followed by edge-preserving image morphing. We then interpolate these software-synchronized images in the spatial domain to synthesize the final view. In addition, we introduce a very accurate and robust algorithm to estimate subframe temporal offsets among input video sequences. Experimental results from unsynchronized videos with or without time stamps show that our approach is capable of maintaining photorealistic quality from a variety of real scenes.",https://ieeexplore.ieee.org/document/4293014/,IEEE Transactions on Visualization and Computer Graphics,July-Aug. 2007,ieeexplore
10.1109/TPS.2013.2276537,Space-Varying Templates for Real-Time Applications of Cellular Nonlinear Networks to Pattern Recognition in Nuclear Fusion,IEEE,Journals,"In this paper, a new methodology consisting of space-varying templates in cellular nonlinear networks (CNNs) for real-time visual pattern recognition in nuclear fusion devices is presented. The development of space-varying templates is a new upgrade, driven by the need to process different parts of the images in different ways. The new approach has been applied to the identification in real time of various objects present in the Joint European Torus videos of both infrared (IR) and visible cameras. IR videos are here used to detect hot spots and the regions of the walls in which dangerously high temperatures are reached, whereas visible cameras provide information about multifaceted asymmetric radiations from the edge, which are dangerous instabilities that can lead to disruptions. Their identification is particularly difficult because of their movement and their shape which is similar to other objects present in the frames. Therefore, in addition to space-varying template CNNs, quite sophisticated morphological operators have to be deployed and their outputs processed by machine learning tools, such as support vector machines. The implementation of the whole methodology was performed in a field-programmable gate array board, obtaining, in both applications, a final success rate close to 100% and a frame rate higher than 200 frames/s.",https://ieeexplore.ieee.org/document/6582663/,IEEE Transactions on Plasma Science,Sept. 2013,ieeexplore
10.23919/JCIN.2019.8917873,TV White Space Spectrum Analysis Based on Machine Learning,PTP,Journals,"Exploration of TV white space (TVWS) is a promising solution to mitigate the spectrum shortage and provide opportunities for new applications. In this paper, we present a detailed analysis of spectrum utilisation over TVWS at different locations in London. Both short-term and long-term outdoor measurement campaigns are conducted over large scales to better understand the spectrum features and variations across multiple locations and time periods. Different from most fixed-location-only measurements, we also drive along the main streets of London with a portable moving node to measure the on-route spectrum density along with the corresponding geographical information, which allows us to study the features and variations of spectrum use through a continuous space. To better analyse the dynamic spectrum utilisation, a machine learning based analysis algorithm is developed over the real-world measurements. This approach allows us to characterise the similarity and variability in spectrum usage within and among different channels, locations, and time instances, which is critical for the secondary system deployment to efficiently exploit the white space.",https://ieeexplore.ieee.org/document/8917873/,Journal of Communications and Information Networks,June 2019,ieeexplore
10.1109/JIOT.2021.3051343,The Study of Urban Residentials Public Space Activeness Using Space-Centric Approach,IEEE,Journals,"With the advancement of the Internet of Things (IoT) and communication platform, large-scale sensor deployment can be easily implemented in an urban city to collect various information. To date, there are only a handful of research studies about understanding the usage of urban public spaces. Leveraging IoT, various sensors have been deployed in an urban residential area to monitor and study public space utilization patterns. In this article, we propose a data processing system to generate space-centric insights about the utilization of an urban residential region of multiple Points of Interests (PoIs) that consists of 190 000 m<sup>2</sup> real estate. We identify the activeness of each PoI based on the spectral clustering, and then study their corresponding static features, which are composed of transportation, commercial facilities, population density, along with other characteristics. Through the heuristic features inferring, the residential density and commercial facilities are the most significant factors affecting public place utilization.",https://ieeexplore.ieee.org/document/9321451/,IEEE Internet of Things Journal,"15 July15, 2021",ieeexplore
10.1109/ACCESS.2020.3048683,Transfer Learning Based on Hybrid Riemannian and Euclidean Space Data Alignment and Subject Selection in Brain-Computer Interfaces,IEEE,Journals,"Transfer learning is a promising approach for reducing training time in a brain-computer interface (BCI). However, how to effectively transfer data from previous users to a new user poses a huge challenge. This paper presents a novel transfer learning approach that combines data alignment and source subject selection for motor imagery (MI) based BCIs. The former is achieved by a reference matrix from the regularization of the two reference matrices estimated in Riemannian and Euclidean space respectively, whereas the latter is implemented by a modified sequential forward floating-point search algorithm. The aligned training data from chosen source subjects are used for creating a classification model based on either spatial covariance matrices in Riemannian space or common spatial pattern algorithm in Euclidean space. The proposed algorithms were evaluated on two MI based BCI data sets with different subjects and compared with existing transfer learning algorithms with sole data alignment or subject selection. The experimental results show that the hybrid-space data alignment methods for reducing the differences among subjects significantly outperform two single-space alignment methods, and the source subject selection method can substantially enhance the similarity between source subjects and the target subject. The combination of the two methods achieves superior classification performance compared to either one. The proposed algorithms will greatly facilitate the real-world applications of MI based BCIs.",https://ieeexplore.ieee.org/document/9312161/,IEEE Access,2021,ieeexplore
10.1109/ISSCC.2019.8662455,2.5 A 4040 Four-Neighbor Time-Based In-Memory Computing Graph ASIC Chip Featuring Wavefront Expansion and 2D Gradient Control,IEEE,Conferences,"Single-source shortest path (SSP) problems have a rich history of algorithm development [1-3]. SSP has many applications including AI decision making, robot navigation, VLSI signal routing, autonomous vehicles and many other classes of problems that can be mapped onto graphs. Conventional algorithms rely on sequentially traversing the search space, which is inherently limited by traditional computer architecture. In graphs which become very large, this slow processing time can become a bottleneck in real world applications. We propose a time-based ASIC to address this issue. Our design leverages a dedicated hardware implementation to solve these problems in linear time complexity with superior energy efficiency. A $40\times40$ four-neighbor grid implements a wavefront (WF) expansion with a first-in lockout mechanism to enable traceback. Outside the array, a programmable resistive ladder provides bias voltages to the edge cells, which enables pulse shaping reminiscent of the A* algorithm [3].",https://ieeexplore.ieee.org/document/8662455/,2019 IEEE International Solid- State Circuits Conference - (ISSCC),17-21 Feb. 2019,ieeexplore
10.1109/IIH-MSP.2007.3,2D(PC)2 A for Face Recognition with One Training Image per Person,IEEE,Conferences,"In the real-world application of face recognition system, owing to the difficulties of collecting samples or storage space of systems, only one sample image per person is stored in the system, which is so-called one sample per person problem. In this paper, we propose a novel algorithm, called 2D(PC)<sup>2</sup>A, to solve this problem. The procedure of 2D(PC)<sup>2</sup>A can be divided into the three stages: 1) creating the combined image from the original image 2) performing 2DPCA on the combined images; 3) classifying a new face based on assembled matrix distance (AMD). Experiments implemented on two real datasets show that 2D(PC)<sup>2</sup>A method is an efficient and practical approach for face recognition.",https://ieeexplore.ieee.org/document/4457509/,Third International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP 2007),26-28 Nov. 2007,ieeexplore
10.1109/BIBM.2018.8621468,3D Convolutional Neural Networks Fusion Model for Lung Nodule Detection onClinical CT Scans,IEEE,Conferences,"Automatically accurate pulmonary nodule detection plays an important role in lung cancer diagnosis and early treatment. We propose a three-dimensional (3D) Convolutional Neural Networks (ConvNets) fusion model for lung nodule detection on clinical CT scans. Two 3D ConvNets models are trained separately without any pre-training weights: One trained on the LUng Nodule Analysis 2016 dataset (LUNA) and additional augmented data to learn the nodules representative features in volumetric space, which may cause overfitting problems meanwhile, so we train another network on original data and fuse the results of the two best-performing models to reduce this risk. Both use reshaped objective function to solve the class imbalance problem and differentiate hard samples from easy samples. More importantly, 335 patients CT scans from the hospital are further used to evaluate and help optimize the performance of our approach in the real situation, and we develop a system based on this method. Experimental results show a sensitivity of 95.1% at 8 false positives per scan in Free Receiver Operating Characteristics (FROC) curve analysis, and our system has a pleasing generalization ability in clinical data.",https://ieeexplore.ieee.org/document/8621468/,2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),3-6 Dec. 2018,ieeexplore
10.1109/ICESS49830.2020.9301562,3D Hand Pose Estimation from Single Depth Images with Label Distribution Learning,IEEE,Conferences,"Reliable hand pose estimation enriches the way of human-computer interaction, such as sign language recognition and virtual reality. However, the task of estimating the hand pose faces two severe challenges. To be specific, it is difficult to learn spatial information from a 2D image and regress the location of a point in 3D space. And the highly non-linear correlation between the hand feature space and the joint location makes it hard to be modeled. To deal with the above problems, we propose a deep regression network, which learns the hand feature space from the point cloud and includes a specific label distribution learning network. Due to the point cloud contains more spatial information, it is beneficial for the neural network to extract the hand spatial geometric features. Utilizing the deep network to guide label learning actively reduces the negative effects of nonlinearity. According to the experimental results, our proposed network achieves the state-of-the-art performance on MSRA dataset.",https://ieeexplore.ieee.org/document/9301562/,2020 IEEE International Conference on Embedded Software and Systems (ICESS),10-11 Dec. 2020,ieeexplore
10.1109/IAAI54625.2021.9699957,3D Virtual Sand Table Display System of Industrial Park Based on Augmented Reality,IEEE,Conferences,"To improve the interactivity of 3D virtual sand table displays, an industrial park 3D virtual sand table display system based on augmented reality is designed. The system consists of multiple modules. The design of the animation browsing module is based on the roaming walking model environment. The walking and roaming module can enable participants to experience the real visual and tactile virtual space in the process of input browsing. In the surrounding environment and traffic condition module, the surrounding environment and traffic condition display are created by means of static pictures and picture buttons. The VRML scene space module is responsible for creating objects such as VRML viewpoints. Scenario module transformation completes event transmission through routing. The virtual sand table VRML optimization modeling module is responsible for constructing the virtual sand table VRML model. The function of the multiperson collaboration module is to provide collaboration capability for plotting and discussion functions. In the multiuser detection module, yolov2 is used for multiuser detection. The augmented reality hardware module is mainly composed of computers and hololens. Through the combination of hardware and software, a three-dimensional virtual sand table display of an industrial park is realized. The test results show that the performance of the designed system is good and can meet the design requirements.",https://ieeexplore.ieee.org/document/9699957/,2021 IEEE International Conference on Industrial Application of Artificial Intelligence (IAAI),24-26 Dec. 2021,ieeexplore
10.1109/ICTAI.2015.60,A 3D Frontier-Based Exploration Tool for MAVs,IEEE,Conferences,"This paper presents a 3D frontier-based exploration tool named 3D-FBET. Our tool runs onboard the MAV equipped with a 3D sensor. The 3D map of the environment explored is constructed incrementally from two consecutive point clouds obtained. Considering the computation and memory limitations of MAVs, the OctoMap is utilized to represent 3D models. A novel approach is designed to extract the 3D frontiers from the OctoMap. Different from existing extraction method, only state-changed space in the 3D map is processed in each iteration. We implement our approach on top of the well-known robot operating system (ROS) and demonstrate the effectiveness of our tool in real scenarios.",https://ieeexplore.ieee.org/document/7372156/,2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI),9-11 Nov. 2015,ieeexplore
10.1109/ADCONIP.2017.7983776,A Bayesian learning and data mining approach to reaction system identification: Application to biomass conversion,IEEE,Conferences,"The growing environmental concern over the use of fossil fuels calls for alternative sources of energy with smaller environmental footprint, and biomass-derived fuels have been extensively investigated as a substitute. In biofuels production, the development of reaction networks and kinetic models is unquestionably a major challenge due to the difficulty in characterizing the reaction products. Therefore, there is a need for a better way to retrieve the information about the reaction from the available experimental data. This study uses a data mining and Bayesian learning approach to estimate the reaction network of the acid and base catalyzed hydrous pyrolysis of hemicellulose from Fourier Transform Infrared (FTIR) spectroscopy. Cluster analysis is used to model the system in terms of lumps and a Bayesian network structure-learning algorithm is then used to device a reaction network. Three Bayesian network structure-learning algorithms were implemented to estimate the reaction network. The results from each were identical, indicating that the model representing the reaction network is most probably in the optimal equivalence space. The model was compared against expert-based reaction models and the agreement is encouraging. A useful aspect of this model is its self-updating capability, i.e., the reaction model can provide a quantitative description of the effect of the change in the operation condition from spectroscopic data. Hence, the model may be used for the real time analysis of the investigated process.",https://ieeexplore.ieee.org/document/7983776/,2017 6th International Symposium on Advanced Control of Industrial Processes (AdCONIP),28-31 May 2017,ieeexplore
10.1109/ACIT49673.2020.9208974,A Behaviour based Ransomware Detection using Neural Network Models,IEEE,Conferences,"This study proposes a behaviour based methodology for ransomware detection. Ransomware is the type of malware that restricts access to files or blocks an infected device asking victims to pay fees in order to remove the restriction. The proposed detection procedure is based on the usage of neural network methodologies for the ransomware detection assuming features that related only with the utilization of the device resources. In the first part of the study, the System Monitor Service is proposed, that records the utilisation of the workstations' resources and extracts the corresponding features that describe their behaviour. The above tool monitors in real time the CPU, the memory, the disk space, the rate of reads and writes, the number of changed, created and deleted files. The second part of the methodology concerns the development of a neural network model that detects ransomware. Based on real data that arose from the System Monitor service, a model that fulfils the modern needs regarding the performance of the agents has been developed. The proposed methodology is ideal for Small and Medium Enterprises (SMEs) that constitute a particular target of the ransomwares for financial reasons.",https://ieeexplore.ieee.org/document/9208974/,2020 10th International Conference on Advanced Computer Information Technologies (ACIT),16-18 Sept. 2020,ieeexplore
10.1109/ICAL.2007.4338916,A Bran-new Feature Extraction Method and its application to Surface Defect Recognition of Hot Rolled Strips,IEEE,Conferences,"Considering defects of hot rolled strips are hard to be recognized by human eyes under high speed and strong noise disturbance circumstances, automatic recognition technique is discussed, and traditional space-domain feature set is very hard to be extracted, spectrum images of defects are researched in depth and are firstly extracted three types of features including Crisscross Region Feature Set which contains 240 origin features, Sum of Valid Pixels (SVP) and Repletion Ratio of Centre Region (RRCR), secondly optimized the feature set by genetic algorithm to get a bran-new feature set including 54 features. Based on the bran-new feature set, several recognition experiments of neural network are implemented, and the results show that the recognition effects under this new feature set can meet the demands of surface defect recognition of hot rolled strips in real- world under hard circumstances. Meanwhile research orientation of surface defect recognition of hot rolled strips is pointed out in order to improve recognition effects.",https://ieeexplore.ieee.org/document/4338916/,2007 IEEE International Conference on Automation and Logistics,18-21 Aug. 2007,ieeexplore
10.1109/INDIANCC.2019.8715586,A Chance Constrained Programming Based Multi-Criteria Decision Making Under Uncertainty,IEEE,Conferences,"Multi-criteria decision making under uncertainty is a common practice followed in industries and academia. Among several types of uncertainty handling techniques, Chance Constrained Programming (CCP) is considered as an efficient and tractable approach provided one has accessibility to distribution of the data for uncertain parameters. However, the assumption that the uncertain parameters must follow some well-behaved probability distribution is a myth for most of the practical applications. This paper proposes a methodology to amalgamate machine learning algorithms with CCP and thereby make it data-driven. A novel fuzzy clustering mechanism is implemented to transcript the uncertain space such that the exact regions of uncertainty are identified. Subsequently, density based boundary point detection and Delaunay triangulation based boundary construction enable intelligent Sobol based sampling in these regions for use in CCP. The Fuzzy clustering mechanism used in the proposed method transforms the existing fuzzy C-means technique such that the decision variables are significantly reduced. This enables evolutionary optimizers to obtain better approximations of the uncertain space by identifying the true clusters. A highly nonlinear real life model for continuous casting from steelmaking industries is considered as a case study for testing the efficiency of data based CCP along with a comprehensive comparison between conventional CCP approach using box uncertainty set and proposed methodology. As the resulting CCP problem is multi-objective in nature, the Pareto solutions are obtained by NSGA II.",https://ieeexplore.ieee.org/document/8715586/,2019 Fifth Indian Control Conference (ICC),9-11 Jan. 2019,ieeexplore
10.1109/IJCNN52387.2021.9534250,A Comparative Study of Methods for Visualizable Semantic Embedding of Small Text Corpora,IEEE,Conferences,"Text embedding has recently emerged as a very useful and successful method for semantic representation. Following initial word-level embedding methods such as Latent Semantic Analysis (LSA) and topic-based bag-of-words approaches like Latent Dirichlet Allocation (LDA), the focus has turned to language models and text encoders implemented as neural networks - ranging from word-level models to those embedding whole documents. The distinctive feature of these models is their ability to infer semantic spaces at all levels based purely on data, with no need for complexities such as syntactic analysis or ontology building. Many of these models are available pre-trained on enormous amounts of data, providing downstream applications with general-purpose semantic spaces. In particular, embedding models at the sentence level or higher are most useful in applications because the meaning of text only becomes clear at that level. Most text embedding methods produce text embeddings in high-dimensional spaces, with a dimensionality ranging from a few hundred to thousands. However, it is often useful to visualize semantic spaces in very low dimension, which requires the use of dimensionality reduction methods. It is not clear what language models and what method of dimensionality reduction would work well in these cases. In this paper, we compare four text embedding methods in combination with three methods of dimensionality reduction to map three related real-world datasets comprising textual descriptions of items in a particular domain (sports) to a 2-dimensional semantic visualization space. The results provide several insights into the utility of these methods for data of this type.",https://ieeexplore.ieee.org/document/9534250/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore
10.1109/ICSTCEE54422.2021.9708548,A Convolutional Neural Network Based Approach for Computational Fluid Dynamics,IEEE,Conferences,"Computational fluid dynamics (CFD) is the visualisation of how a fluid moves and interacts with things as it passes by using applied mathematics, physics, and computational software. The project is designed to simulate fluid flow of a particle based on provided boundary conditions using High Performance Computing (HPC), with two-dimensional picture files as input to the software and fluid flow of a particle generated based on these image data. The Naiver Stokes Equation and the Lattice Boltzmann Equation are used to create these fluid flow particles.The governing equations based on the conservation law of fluid physical characteristics lead the primary structure of thermofluids investigations. Fluid flow is created depending on the item using the three governing equations from the conservation laws of physics. CFD simulation, on the other hand, which is a Iterative process is frequently computationally costly, memory-intensive, and time-consuming. A model based on convolutional neural networks, is proposed for predicting non-uniform flow in 2D to over come these disadvantages. The proposed approach thus aims to aid the behaviour of fluid particles on a certain system and to assist in the development of the system based on the fluid particles that travel through it. At the early stages of design, this technique can give quick feedback for real-time design revisions. In comparison to previous approximation methods in the aerodynamics domain, CNNs provide for efficient velocity field estimate and took less time then the previous approximation method. As CFD based CNN model is more effective to 2D design(2D aeroplane dataset) as it is in research stage lot more experiments have to be made for 3D designs. Designers and engineers may also use the CFD based CNN model directly in their 2D design space exploration.",https://ieeexplore.ieee.org/document/9708548/,"2021 Second International Conference on Smart Technologies in Computing, Electrical and Electronics (ICSTCEE)",16-17 Dec. 2021,ieeexplore
10.1109/ICDAR.2017.120,A Convolutional Neural Network Based Two-Stage Document Deblurring,IEEE,Conferences,"Blurring often happens when capturing documents with hand held cameras, which has negative effects on the Optical Character Recognition systems. In this paper, we propose a Convolutional Neural Network (CNN) based two-stage deblurring method. The method can deal with both real motion blur and focal blur situations, while it does not require exact estimation of the blur kernel. To achieve this, the whole blur kernel space is divided into several degradative sub-spaces. Firstly, a CNN classifier is trained to predict which sub-space the blurry image belongs to at the patch level. Then, several patches voting for the specific blur kernel sub-space is developed. Given the strong learning ability of CNN, only one CNN model corresponding to a degradative kernel sub-space is trained to restore the sharp images in the image restoration step. Experimental results show that the proposed approach performs well on the real blurring document images. In addition, we demonstrate that the proposed method could also handle the spatially-varying blurring.",https://ieeexplore.ieee.org/document/8270051/,2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR),9-15 Nov. 2017,ieeexplore
10.1049/cp.2012.0916,A DVGE service system for risk assessment of dam-break in barrier lake,IET,Conferences,"This paper employs theories and technologies of the distributed virtual reality and geographic information system (GIS) to construct a DVGE (Distributed Virtual Geographic Environment) system. The proposed DVGE System provides geographically distributed users with a shared virtual space and a collaborative platform in order to implement risk assessment work. Using five-layer service system architecture efficiently integrates and shares geographically distributed resources as well as modeling procedures. Meanwhile some key technologies including distributed virtual scene modelling and implementing mechanism of collaborative workflow are discussed. Finally, a DVGE prototype system is implemented to support risk assessment and impact analysis of dam-break in Barrier Lake. The experimental results show that the scheme developed in this paper is efficient and feasible.",https://ieeexplore.ieee.org/document/6492523/,International Conference on Automatic Control and Artificial Intelligence (ACAI 2012),3-5 March 2012,ieeexplore
10.1109/BIBM52615.2021.9669629,A Decoding algorithm for Non-invasive SSVEP-based Drone Flight Control,IEEE,Conferences,"Many advanced researches on natural user interfaces methods based on user-centered design have been using speech, gestures and vision to interact with environment and/or control internet of things (IoT) devices. Brain computer interfaces (BCIs) technology could make this interaction/control more natural, faster, and reliable, and effective. In this paper, we propose a decoding algorithm for controlling a drone in a three-dimensional (3D) space using steady state visually evoked potential (SSVEP)-based BCI modality. SSVEP-based BCI has the great potential for use in virtual reality environment, which enables the user to control the drone using his/her brain activity in an first-person-view mode. Therefore, the user will be in a full control over the flight using BCI system by commanding the drone to take off, land, go forward, stop, and turn right/left. This system yields a super convenient way for normal people with no prior experience to interact with the drone and control a flight mission in a little to no time, over traditional manual control which takes longer time to learn and perfect. in the decoding phase, a various convolutional neural networks (CNN) models were built to accommodate different control criteria such as the generality of the model. This proposed EEG-decode-pipeline has been implemented on an open-source data-set which consists of 8-channel EEG data from 10 subjects performing 12 target SSVEP-based BCI task. A high multi-class BCI classification results were achieved with an accuracy ranging around 80-90% for performing a successful online simulation of the drone control.",https://ieeexplore.ieee.org/document/9669629/,2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),9-12 Dec. 2021,ieeexplore
10.1109/ICISCE48695.2019.00014,A Deep Reinforcement Learning Malware Detection Method Based on PE Feature Distribution,IEEE,Conferences,"Existing anti-virus software and malware detection methods, including signature-based and the machine learning-based malware detection methods, are unable to update the virus database in real time, resulting in poor resistance to malware variants. To solve this problem, this paper proposes a novel malware detection method based on deep reinforcement learning, which combines the advantages of Q-learning and neural network. Q-learning action selection strategy is adopted while solving the problem of high dimensional state space. Theoretical analysis and experimental results show that the proposed method can not only detect malware variants efficiently, but also perform well in many well-known anti-virus software, which is a new direction in the field of malware detection.",https://ieeexplore.ieee.org/document/9107644/,2019 6th International Conference on Information Science and Control Engineering (ICISCE),20-22 Dec. 2019,ieeexplore
10.1109/IUCC-CIT-DSCI-SmartCNS55181.2021.00092,A Domain Model for SARS-Co V-2 Rational Vaccine Design to Engineer a Prediction Tool for Binding Affinity and Antigenicity,IEEE,Conferences,"One of our greatest present challenges are designing vaccines against SARS COV2 and its variants. Rational vaccine design uses computational methods prior to development of a vaccine for testing in animals and humans. The latest methods in rational vaccine design use machine learning techniques to predict binding affinity and antigenicity but offer the researchers only isolated stand-alone tools. A difficulty that software engineers and data scientist face in development of tools for doctors and researchers is their lack of knowledge of the medical domain. This paper presents a set of domain model developed in collaboration between software engineers and a medical researcher in the process of building a tool scientists could use to predict binding affinity and antigenicity of potential designs of SARS COV2 vaccines. A domain model visualizes the real-world entities and their interrelationships, that together define the domain space. This domain model will be useful to other software engineers trying to predict other characteristics of vaccines, such as potential autoimmunity response.",https://ieeexplore.ieee.org/document/9719628/,2021 20th International Conference on Ubiquitous Computing and Communications (IUCC/CIT/DSCI/SmartCNS),20-22 Dec. 2021,ieeexplore
10.1109/CCPR.2008.32,A Fast Algorithm for Image Euclidean Distance,IEEE,Conferences,"Determining, or selecting a distance measure over the input feature space is a fundamental problem in pattern recognition. A notable metric, called the image euclidean distance (IMED) was proposed by Wang et al. [5], which is demonstrated consistent performance improvements in many real-world problems. In this paper, we present a fast implementation of IMED, which is referred as the convolution standardizing transform (CST). It can reduce the space complexity from O(n<sub>1</sub> <sup>2</sup>n<sub>2</sub> <sup>2</sup> ) to O(1) , and the time complexity from O(n<sub>1</sub> <sup>2</sup>n<sub>2</sub> <sup>2</sup> ) to O(n<sub>1</sub>n<sub>2</sub>), for n<sub>1</sub> X n<sub>2</sub> images. Both theoretical analysis and experimental results show the efficiency of our algorithm.",https://ieeexplore.ieee.org/document/4662985/,2008 Chinese Conference on Pattern Recognition,22-24 Oct. 2008,ieeexplore
10.1109/CODESISSS51650.2020.9244038,A Fast Design Space Exploration Framework for the Deep Learning Accelerators: Work-in-Progress,IEEE,Conferences,"The Capsule Networks (CapsNets) is an advanced form of Convolutional Neural Network (CNN), capable of learning spatial relations and being invariant to transformations. CapsNets requires complex matrix operations which current accelerators are not optimized for, concerning both <sub>training</sub> and <sub>inference</sub> passes. Current state-of-the-art simulators and design space exploration (DSE) tools for DNN hardware neglect the modeling of training operations, while requiring long exploration times that slow down the complete design flow. These impediments restrict the real-world applications of CapsNets (e.g., autonomous driving and robotics) as well as the further development of DNNs in life-long learning scenarios that require training on low-power embedded devices. Towards this, we present <sub>XploreDL</sub>, a novel framework to perform fast yet high-fidelity DSE for both inference and training accelerators, supporting both CNNs and CapsNets operations. <sub>XploreDL</sub> enables a resource-efficient DSE for accelerators, focusing on power, area, and latency, highlighting Pareto-optimal solutions which can be a green-lit to expedite the design flow. <sub>XploreDL</sub> can reach the same fidelity as ARM's SCALE-sim, while providing 600x speedup and having a 50x lower memory-footprint. Preliminary results with a deep CapsNet model on MNIST for training accelerators show promising Pareto-optimal architectures with up to 0.4 TOPS/squared-mm and 800 fJ/op efficiency. With inference accelerators for AlexNet the Pareto-optimal solutions reach up to 1.8 TOPS/squared-mm and 200 fJ/op efficiency.",https://ieeexplore.ieee.org/document/9244038/,2020 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS),20-25 Sept. 2020,ieeexplore
10.1109/ICCSN.2018.8488287,A Fast Direct Position Determination for Multiple Sources Based on Radial Basis Function Neural Network,IEEE,Conferences,"Compared with the conventional two-step Iocalization mode, direct position determination (DPD) algorithm avoids the measurement-source association problem in multiple sources scenario, and has the advantages of higher Iocalization accuracy and stronger resolution capability. However, the existing DPD algorithms, e.g. maximum likelihood (ML)-based DPD algorithm, are unsuitable for real-time applications due to high computational complexity. In this paper, a fast DPD method using radial basis function (RBF) neural network (NN) has been proposed. To reduce the dimension of the input space, an effective pre-processing scheme is present. A reliable training process improves the generalization performance of NN. Simulation results show the feasibility of the proposed algorithm and demonstrate that the proposed method is more computationally efficient than the existing ML-based DPD algorithm.",https://ieeexplore.ieee.org/document/8488287/,2018 10th International Conference on Communication Software and Networks (ICCSN),6-9 July 2018,ieeexplore
10.1109/PESGM41954.2020.9281856,A Fault Classification Method for Medium Voltage Networks with a high Penetration of Photovoltaic Systems using Artificial Neural Networks,IEEE,Conferences,"With the rapid advancement of power electronic technologies and the reduction of photovoltaic cell price, the share of solar energy in the total power production has been booming recently. On the one hand, the increase in the amount of power delivered by solar energy can be beneficial in many economic and environmental aspects. On the other hand, this can cause various technical challenges to network operators. One of these issues is related to classifying faults located in distribution networks with high penetration of photovoltaic systems. Although many studies have paid significant attention to developing new algorithms applicable for a more active today distribution networks, there is still space for other improvements. Hence, after reviewing state-of-the-art researches, this paper was intended to develop a fault classification that is based on artificial neural networks. In particular, a technique so-called Multiplayer Perceptron Classifier was selected for the proposed algorithm. First, the authors generated a data set for the study by modeling and simulating a real distribution network with practical parameters provided by a local utility in the environment software PowerFactory/DigSILENT. Multiple fault scenarios were simulated. Second, a part of the generated data collection was used for network learning. Finally, the performance of the proposed methodology was demonstrated via testing on the remaining number of generated data.",https://ieeexplore.ieee.org/document/9281856/,2020 IEEE Power & Energy Society General Meeting (PESGM),2-6 Aug. 2020,ieeexplore
10.1109/TIME.2014.27,A Formal Account of Planning with Flexible Timelines,IEEE,Conferences,"Planning for real world problems with explicit temporal constraints is a challenging problem. Among several approaches, the use of flexible timelines in Planning and Scheduling (P&amp;S) has demonstrated to be successful in a number of concrete applications, such as, for instance, autonomous space systems. A flexible timeline describes an envelope of possible solutions which can be exploited by an executive system for robust on-line execution. A remarkable research effort has been dedicated to design, build and deploy software environments, like EUROPA, ASPEN, and APSI-TRF, for the synthesis of timeline-based P&amp;S applications. Several attempts have also been made to characterize the concept of timelines. Nevertheless, a formal characterization of flexible timelines and plans is still missing. This paper presents a formal account of flexible timelines aiming at providing a general semantics for related planning concepts such as domains, goals, problems, constraints and flexible plans. Some basic properties of the defined concepts are also stated and proved. A simple running example inspired by a real world planning domain is exploited to illustrate the proposed formal notions. Finally, a planning tool, called Extensible Planning and Scheduling Library (EPSL), is briefly presented, which is able to generate flexible plans that are compliant with the given semantics.",https://ieeexplore.ieee.org/document/6940372/,2014 21st International Symposium on Temporal Representation and Reasoning,8-10 Sept. 2014,ieeexplore
10.1109/ISORC.2008.70,A Framework for Extrusion Detection Using Machine Learning,IEEE,Conferences,"Machine learning deals with the issue of how to build programs that improve their performance at some task through experience. Machine learning algorithms have proven to be of great practical value in a variety of application domains. They are particularly useful for (a) poorly understood problem domains where little knowledge exists for the humans to develop effective algorithms; (b) domains where there are large databases containing valuable implicit regularities to be discovered; or (c) domains where programs must adapt to changing conditions. Not surprisingly, the field of Cyber space turns out to be a fertile ground where many software security problems could be formulated as learning problems and approached in terms of learning algorithms. This paper deals with the subject of applying machine learning in extraction detection. In the paper, we present our research work on design and implementation of an extrusion detection system for information security of big companies. The result shows a potential in real-world applications.",https://ieeexplore.ieee.org/document/4519564/,2008 11th IEEE International Symposium on Object and Component-Oriented Real-Time Distributed Computing (ISORC),5-7 May 2008,ieeexplore
10.1109/ICCAD51958.2021.9643557,A General Hardware and Software Co-Design Framework for Energy-Efficient Edge AI,IEEE,Conferences,"A huge number of edge applications including self-driving cars, mobile health, robotics, and augmented reality / virtual reality are enabled by deep neural networks (DNNs). Currently, much of this computation for these applications happens in the cloud, but there are several good reasons to perform the processing on local edge platforms such as smartphones: improved accessibility to different parts of the world, low latency, and data privacy. In this paper, we present a general hardware and software co-design framework for energy-efficient edge AI for both simple classification and structured output prediction tasks (e.g., 3D shapes from images). This framework relies on two key ideas. First, we design a space of DNNs of increasing complexity (coarse to fine) and perform input-specific adaptive inference by selecting a DNN of appropriate complexity depending on the hardness of input examples. Second, we execute the selected DNN on the target edge platform using a resource management policy to save energy. We also provide instantiations of our co-design framework for three qualitatively different problem settings: convolutional neural networks for image classification, graph convolutional networks for predicting 3D shapes from images, and generative adversarial networks on photo-realistic unconditional image generation. Our experiments on real-world benchmarks and mobile platforms show the effectiveness of our co-design framework in achieving significant gain in energy with little to no loss in accuracy of predictions.",https://ieeexplore.ieee.org/document/9643557/,2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD),1-4 Nov. 2021,ieeexplore
10.1109/ICIBA50161.2020.9276900,A Generative Model for Zero-Shot Learning via Wasserstein Auto-encoder,IEEE,Conferences,"Zero-shot learning aims to use the labeled instances to train the model, and then classifies the instances that belong to a class without labeled instances. However, the training instances and test instances are disjoint. Thus, the description of the classes (e.g. text description or class attribute information) is to establish a connection between the training set and the test set to make the model effective. Since real world image annotation requires a lot of manpower and material resources, this setting is very important in the real world. Zero-shot learning can effectively solve the problem of image annotation. Most of the previous methods explored the mapping between visual space and semantic space. Some recent generative methods attempt to generate image features of unseen classes based on auxiliary information, and have achieved good performances. In this paper, we propose to use Wasserstein Auto-encoder (WAE) as a generative model to establish data distribution. Then the generated samples from the generative model are used to classify unseen classes. We test our model on four benchmark datasets including CUB, SUN, AWA2 and aPY, the results of which demonstrate the effectiveness of our model.",https://ieeexplore.ieee.org/document/9276900/,"2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA)",6-8 Nov. 2020,ieeexplore
10.1109/ICDM50108.2020.00137,A Goal-Prioritized Algorithm for Additional Route Deployment on Existing Mass Transportation System,IEEE,Conferences,"Multi-criteria path planning is an important combinatorial optimization problem with broad real-world applications. Finding the Pareto-optimal set of paths ideal for all requiring features is time-consuming and unclear to obtain the subset of optimal paths efficiently for multiple origin states in the planning space. Meanwhile, due to the rise of deep learning, hybrid systems of computational intelligence thrive in recent years. When facing non-monotonic data or heuristics derived from pre-trained neural networks, most of the existing methods for the one-to-all path problem fail to find an ideal solution. We employ Gaussian mixture model to propose a target-prioritized searching algorithm called Multi-Source Bidirectional Gaussian-Prioritized Spanning Tree (BiasSpan) in solving this non-monotonic multi-criteria route planning problem given constraints including range, must-visit vertices, and the number of recommended vertices. Experimental results on mass transportation system in Tainan and Chicago cities show that BiasSpan outperforms comparative methods from 7% to 24<sup>%</sup> and runs in a reasonable time compared to state-of-art route-planning algorithms.",https://ieeexplore.ieee.org/document/9338323/,2020 IEEE International Conference on Data Mining (ICDM),17-20 Nov. 2020,ieeexplore
10.1109/IROS51168.2021.9636757,A Hierarchical Framework for Quadruped Locomotion Based on Reinforcement Learning,IEEE,Conferences,"Quadruped locomotion is a challenging task for learning-based algorithms. It requires tedious manual tuning and is difficult to deploy in reality due to the reality gap. In this paper, we propose a quadruped robot learning system for agile locomotion which does not require any pre-training and works well in various real-world terrains. We introduce a hierarchical learning framework that uses reinforcement learning as the high-level policy to adjust the low-level trajectory generator for better adaptability to the terrain. We compact the observation and action space of the reinforcement learning to deploy it on a host computer in reality. Besides, we design a trajectory generator guided by robot posture, which can generate adaptive foot trajectory to interact with the environment. Experimental results show that our system can be easily deployed in reality while only trained in simulation, and also has the advantages of fast convergence and good terrain adaptability. The supplementary video demonstration is available at https://vsislab.github.io/hfql/.",https://ieeexplore.ieee.org/document/9636757/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore
10.1109/AINA.2010.112,A Japanese Calligraphy Trainer Based on Skill Acquisition Through Haptization,IEEE,Conferences,"We present an approach for implementing a virtual reality system targeted at learning handwritten characters. We especially aim at enabling learners to acquire important writing skills required to be a good writer in Japanese calligraphy. The proposed system provides a haptic channel allowing the learners to intuitively master an instructor's fine motor skills through the sense of touch. We utilize a commercially available haptic device called PHANTOMTM for simulating a writing brush in a virtual learning space. The system implements a function for recording and replaying the instructor's hand motions via the PAHNTOM device. The instructor's writing techniques such as brush-strokes and pen pressures he/she performs/adds when writing characters are effectively presented to the learners via the PHANTOM device. Then, they can master how to write the recorded characters with feeling the instructor's style of handwritings. We also invented a simple yet powerful 3D brush model for real-time visualization of handwritten characters without compromising the quality and reality of the characters. The users can start learning at any time and iterate training without worrying about resource consumptions such as papers and ink as much as they like. We conducted an experiment to validate the effectiveness of the proposed system for learning calligraphy in a virtual environment.",https://ieeexplore.ieee.org/document/5474852/,2010 24th IEEE International Conference on Advanced Information Networking and Applications,20-23 April 2010,ieeexplore
10.1109/I2CT51068.2021.9417940,A Lightweight Classifier for Facial Expression Recognition based on Evolutionary SVM Ensembles,IEEE,Conferences,"Evaluation criteria for solutions to facial expression recognition usually bias to classification accuracy. Hence, the utilization of deep neural networks has become a straightforward and popular option in theoretical studies despite the limitations in real usage from data collection, storage space, and power consumption issues. Our work proposes a practical alternative that is consisted of a minimum model configuration and still matches the state-of-the-art performance of deep learning approaches. We establish a conventional two-stage procedure, where feature extraction of a facial subject depends on a universal filter, histogram of oriented gradients (HOG), and classification is implemented through an ensemble learning approach using basic binary classifiers, support vector machines (SVM). Our two designs considerably improve prediction accuracy. One is that we adopt post-hoc statistics, rather than a priori expectations, to interpret the outputs of weak classifiers. The other is we design a genetic algorithm to search for the optimal ensemble of weak classifiers efficiently. Our method demonstrates supreme performance in several benchmark datasets and even outperforms those based on deep learning from big data. Besides, from a practical viewpoint, our model shows the advantage and flexibility of its storage size and power consumption. Lastly, we further display how the evolutionary SVM ensembles in our model contain information about the dependency and similarity among facial expression categories.",https://ieeexplore.ieee.org/document/9417940/,2021 6th International Conference for Convergence in Technology (I2CT),2-4 April 2021,ieeexplore
10.1109/SBESC49506.2019.9046078,A Linked Data-Based Semantic Information Model for Smart Cities,IEEE,Conferences,"Smart cities typically involve a myriad of inter-connected systems intended to promote better management of urban and natural resources of cities, thereby contributing to the improve the quality of life of citizens. The heterogeneity of domains, systems, data, and relationships among them requires defining a data model able to express information in a flexible, extensible way while promoting interoperability between systems and applications. Furthermore, smart city systems can benefit from georeferenced information to allow for more effective actions over the real-world urban space. Aiming at tackling challenges related to data heterogeneity while considering georeferenced information, this work introduces LGeoSIM, a semantic-based information model for smart cities as means of fostering interoperability and powerful automated reasoning upon unambiguous information. LGeoSIM relies on the recent NGSI-LD Specification, thereby encompassing the principles of Linked Data to allow semantically defining information through ontologies and their interconnection. This paper also presents an implementation of LGeoSIM within Smart Geo Layers, a geographic-layered data middleware platform conceived to integrate data provided by heterogeneous sources in a smart city environment.",https://ieeexplore.ieee.org/document/9046078/,2019 IX Brazilian Symposium on Computing Systems Engineering (SBESC),19-22 Nov. 2019,ieeexplore
10.1109/ITEC.2019.8790564,A Machine Learning Approach for Adaptive Classification of Power MOSFET Failures,IEEE,Conferences,"In electric drives, semiconductor devices are susceptible to faults due to high switching stresses. An unsupervised principal component analysis (PCA) approach that utilizes the on-state median values of voltage and current signals as features is proposed; it is computationally efficient for realtime operation. A key innovation is handling variations of the operating point in the feature space using exponentially weighted samples to recursively compute the time evolution of means of on-state features used in a PCA-based anomaly detector. Data from accelerated tests on commercial SiC devices are used to validate the effectiveness of the proposed approach in predicting incipient semiconductor faults. An experimental setup is also used to demonstrate real-time implementation of the proposed method.",https://ieeexplore.ieee.org/document/8790564/,2019 IEEE Transportation Electrification Conference and Expo (ITEC),19-21 June 2019,ieeexplore
10.1109/ISAI.2016.0070,A Method to Guarantee Real-Time for Software-Defined Radio in User-Space,IEEE,Conferences,"With the rapid development of computer technology, Software-Defined Radio has become more and more popular. We can now transfer the communication system from hardware to software, which makes it more flexible and helps us virtualize the communication system. In this paper, we creatively introduce a method to guarantee hard real-time in user-space on Linux. Unlike the usual methods, Our method(real time with RDTSC) provides real-time feature by soft ware clock with RDTSC and isolating CPUs instead of interrupts from the hardware. Our method is highly lightweight and give the SDR systems more flexibility when compared to the usual methods with interrupts. The experiment and validation results show that real time with RDTSC has a significantperformance and can be used in physical communication systems.",https://ieeexplore.ieee.org/document/7816723/,2016 International Conference on Information System and Artificial Intelligence (ISAI),24-26 June 2016,ieeexplore
10.1109/CHASE.2016.40,A Mobile Cloud Computing Model Using the Cloudlet Scheme for Big Data Applications,IEEE,Conferences,"The wide spread of smart phones and their capabilities made them an important part of many people's life over the world. However, there are many challenges facing these devices such as: low computing power and fast energy drain from their batteries. One solution is to use mobile cloud computing services to run certain tasks at the cloud and returning back the results to the mobile device saving space and processing power. In this research, we introduce efficient Mobile Cloud Computing model based on the Cloudlet sheme. In our model, the mobile device don't need to communicate with the enterprise cloud server and instead contact the Cloudlet directly using cheaper technologies such as Wi-Fi, and no need for 3G/4G. Also, we propose a master-cloudlet management scheme to organize the communication between the cloudlets themselves. Our efficient mobile cloud computing model can be applied in many environments including universities and hospitals were big amounts of data is collected, stored and processed. The real implementation results show that our model out performs classical non-cloudlet mobile cloud computing models.",https://ieeexplore.ieee.org/document/7545816/,"2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)",27-29 June 2016,ieeexplore
10.1109/ICRA40945.2020.9196677,A Mobile Manipulation System for One-Shot Teaching of Complex Tasks in Homes,IEEE,Conferences,"We describe a mobile manipulation hardware and software system capable of autonomously performing complex human-level tasks in real homes, after being taught the task with a single demonstration from a person in virtual reality. This is enabled by a highly capable mobile manipulation robot, whole-body task space hybrid position/force control, teaching of parameterized primitives linked to a robust learned dense visual embeddings representation of the scene, and a task graph of the taught behaviors. We demonstrate the robustness of the approach by presenting results for performing a variety of tasks, under different environmental conditions, in multiple real homes. Our approach achieves 85% overall success rate on three tasks that consist of an average of 45 behaviors each. The video is available at: https://youtu.be/HSyAGMGikLk.",https://ieeexplore.ieee.org/document/9196677/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore
10.1109/CVPRW53098.2021.00227,A Monocular Pose Estimation Case Study: The Hayabusa2 Minerva-II2 Deployment,IEEE,Conferences,"In an environment of increasing orbital debris and remote operation, visual data acquisition methods are becoming a core competency of the next generation of spacecraft. However, deep space missions often generate limited data and noisy images, necessitating complex data analysis methods. Here, a state-of-the-art convolutional neural network (CNN) pose estimation pipeline is applied to the Hayabusa2 Minerva-II2 rover deployment; a challenging case with noisy images and a symmetric target. To enable training of this CNN, a custom dataset is created. The deployment velocity is estimated as 0.1908 m/s using a projective geometry approach and 0.1934 m/s using a CNN landmark detector approach, as compared to the official JAXA estimation of 0.1924 m/s (relative to the spacecraft). Additionally, the attitude estimation results from the real deployment images are shared and the associated tumble estimation is discussed.",https://ieeexplore.ieee.org/document/9523026/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),19-25 June 2021,ieeexplore
10.1109/ICICSP54369.2021.9611881,A Multi-agent Reinforcement Learning Routing Protocol in Mobile Robot Network,IEEE,Conferences,"Robots are now essential in unreachable, repeated, and dangerous real-world applications where they take place of human beings. One of the important capabilities of a multi-robot system is that it should be able to form an autonomous robot network to transmit information. However, due to the limited communication capability of a single robot, the highly variable environment where robots work, and the mobility of the robots, it is difficult for them to exchange information with each other in need. In this article, we propose a novel robot network routing protocol based on multi-agent reinforcement learning called MAQR. The robot nodes can deliver packets cooperatively. The mobility factor, buffer status, and packet delay of neighbor nodes are taken into consideration. We design a reliability model for a robot agent to make reliable routing decisions. We also design an adaptive exploration-exploitation method to balance the convergence speed, solution space as well as network fluctuation. The routing algorithm has been implemented and evaluated in simulation. Results show that MAQR can provide less packet delay, less average queue length and higher delivery ratio than other Q-learning based routing protocol.",https://ieeexplore.ieee.org/document/9611881/,2021 4th International Conference on Information Communication and Signal Processing (ICICSP),24-26 Sept. 2021,ieeexplore
10.1109/ICTAI.2010.81,A Nettree for Approximate Maximal Pattern Matching with Gaps and One-Off Constraint,IEEE,Conferences,"Recently, pattern matching with flexible gap constraints has attracted extensive attention especially in biological sequence analysis and mining patterns from sequences. An issue is to search Maximal Pattern Matching with Gaps and the One-Off Condition (MPMGOOC). Firstly, we introduce the concept of MPMGOOC. In order to solve the problem, we propose some special concepts of Nettree which is different from a tree in that a node may have more than one parent. Based on Nettree, an algorithm named Heuristic Search Occurrence (HSO) is proposed. The space and time complexities of the algorithm are O(W*m*n) and O(W*n*(n+m*m)) respectively, where m, n, and W are the length of pattern P, sequence S and the maximal gap respectively. The comparison results show that HSO achieves better performance than a state-of-the-art algorithm in most cases of the real-world biological data testing.",https://ieeexplore.ieee.org/document/5671434/,2010 22nd IEEE International Conference on Tools with Artificial Intelligence,27-29 Oct. 2010,ieeexplore
10.23919/ACC.1990.4791047,A Neural Net Approach to Space Vehicle Guidance,IEEE,Conferences,"The on-line implementation of numerical algorithms for solving the optimum trajectory/guidance problem for advanced space vehicles such as ALS, HLLV, AOTV, transatmospheric vehicles and interplanetary spacecraft is not possible due to their complexity. Hence, the current approach to the development of real-time guidance laws for these advanced space vehicles is to use approximation theory to obtain closed-loop guidance laws. Neural networks offer an alternative to the derivation and implementation of guidance laws. In this paper, we formulate the space vehicle guidance problem using a neural network approach and investigate the appropriate neural net architecture for modelling optimum guidance trajectories. In particular, we investigate the incorporation of a priori knowledge about the characteristics of the optimal guidance solution into the neural network architecture. The online classification performance of the developed network is demonstrated using a synthesized network trained with a data base of optimum guidance trajectories. Such a neural network based guidance approach can readily adapt to environment uncertainties such as those encountered by an AOTV during atmospheric maneuvers.",https://ieeexplore.ieee.org/document/4791047/,1990 American Control Conference,23-25 May 1990,ieeexplore
10.1109/IECON.2006.347341,A New Adaline Approach for Online Voltage Components Extraction from Unbalanced and Perturbed Power Systems,IEEE,Conferences,"This work presents theoretical studies and practical results obtained with voltage component extraction. The paper is centered on a new method for estimating the direct, inverse and homopolar voltage components from unbalanced and disturbed power systems. We introduce and develop a new decomposition of the voltages in the DQ-space that results in linear expressions explicitly separating AC from DC components. These expressions are learned by Adaline neural networks because of their simplicity and capabilities to approximate linear relationships and to learn online with respect to real-time applications. While learning, the Adalines estimate the amplitude and the phase of the direct, inverse and homopolar voltages of the electrical network and efficiently compensate for the disturbance events, i.e., time-varying nonlinear loads, parameters changes, noise perturbations, fluctuating distortion harmonics. The method was studied and successfully implemented, simulations and experiments under stationary and nonstationary conditions are reported. A demonstrative comparison with other methods is also addressed",https://ieeexplore.ieee.org/document/4153099/,IECON 2006 - 32nd Annual Conference on IEEE Industrial Electronics,6-10 Nov. 2006,ieeexplore
10.1109/CSICC52343.2021.9420614,A New Approach for Mapping of Soccer Robot Agents Position to Real Filed Based on Multi-Core Fuzzy Clustering,IEEE,Conferences,"Mapping the position of soccer robot agents to a real field, is one of the essential issues in the practical implementation of scientific contributions in this context. The lack of a proper assignment affects the scientific implementation of many subjects, such as routing, obstacle avoidance, and robot guidance. For this reason, the use of a clustering method is proposed in this article. Upon the entrance of a new agent, its position is mapped to the real field based on the clustering algorithm. After this mapping, the system begins to work according to the position of the agents, which is defined as the position of the centers of the clusters, as well as the rules defined in the knowledge-base. Considering the unknown and dynamic environment of the robot, some objects inherit common traits from multiple clusters. One reasonable solution for considering the cluster overlaps is to assign a set of membership degrees to each of them. Multiple membership degree assignments result from the fuzzy nature of the clusters. Due to the reduction of segmentations and the shrinkage of the search space, fuzzy clustering generally faces less computational overhead, while the identification and handling of vague, noisy, and outlier data also become much easier in them. The approach of the proposed method is based on the feasibility ideas and uses multi-core learning to identify clusters with complex data structures. The feasibility score of each data represents the percentages of the properties that data inherits from the clusters. Automatically adjusting the weights of the cores in an optimization framework, the proposed method avoids the damage caused by problems such as adopting inefficient cores, or irrelevant features.",https://ieeexplore.ieee.org/document/9420614/,"2021 26th International Computer Conference, Computer Society of Iran (CSICC)",3-4 March 2021,ieeexplore
10.1109/SNPD.2009.39,A New Method for Estimating the Number of Distinct Values over Data Streams,IEEE,Conferences,"Virtually all query optimization methods in data stream management system (DSMS) require a means of estimating the number of distinct values of an attribute in a data stream. Accurate assessment of the number of distinct values can be crucial for selecting a good query plan. Due to data streams' continuous, real-time and unbounded characteristics, data streams may not be stored in limited memory an effective method. Therefore, estimating the number of distinct values over data streams is a more difficult problem. In this paper, combining with data streams' properties and analyzing Bloom filter, we present a new estimation method based on circular Bloom filter using limited space. We store the distinct values in circular Bloom filter to solve effectively the problem that data streams could not be stored in limited memory. The theoretical analysis and the results of experiment indicate that the estimation method is more feasible and highly effective.",https://ieeexplore.ieee.org/document/5286690/,"2009 10th ACIS International Conference on Software Engineering, Artificial Intelligences, Networking and Parallel/Distributed Computing",27-29 May 2009,ieeexplore
10.1109/SVR.2016.26,A New User-Friendly Sketch-Based Modeling Method Using Convolution Surfaces,IEEE,Conferences,"3d modeling systems are essential tools for creating3D content for virtual reality systems. They are powerfuland sophisticated computer applications with a steep learningcurve. When considering simple shape prototyping it is moreappropriate to use a software with a simpler and intuitiveuser interface. This paper proposes a new sketch-based modelingapproach relying on convolution surfaces that enablesthe user to specify 3d shapes of arbitrary topology andresolution from 2d hand-drawn sketches that are embeddedand manipulated in 3d space. To make this approach feasiblewe propose two innovations: the first one is an interactionmechanism to create, manipulate and assemble independent2d sketches embedded in 3d space relying only on simplerotations, translations and a ray casting step; the second oneis a new approach to fit the level set surfaces generated bythe convolution of skeleton primitives to the silhouette curvesspecified by the user. The proposed fitting combines a methodthat automatically computes weights for the convolutionoperation with a Stolte's blending function that controls theinfluence of each implicit functional components to the finalshape. We show via experiments and preliminary user teststhat our method permits the user to easily express his 3dconcepts by sketching and assembling 2d simple drawings intridimensional space.",https://ieeexplore.ieee.org/document/7517260/,2016 XVIII Symposium on Virtual and Augmented Reality (SVR),21-24 June 2016,ieeexplore
10.1109/i-PACT52855.2021.9696457,A Novel Deep Recurrent Neural Network for Assessment of Role of Space Charge Effect in Partial Discharge Recognition and Diagnosis,IEEE,Conferences,"Partial Discharge (PD) has evolved into an inevitable tool for diagnosis of insulation of power equipment due to its inherent non-intrusive testing methodology. Since accurate recognition of complex overlapped PD sources is essential for effective diagnosis, recently the focus of research has shifted to challenges related to real-time PD measurement that involve complexities in discriminating multi-source pulse signatures, variations in applied voltage, dynamics of PD patterns related to space charge effect etc. Though research studies have successfully utilized a gamut of machine intelligence techniques such as neural networks, Hidden Markov Models, Support Vector Machines etc, effectiveness of recognition of complex overlapped PD sources based on space charge effect associated with PD has not been comprehensively established. This research focuses on implementation of Deep Recurrent Neural Network (DRNN) as a novel strategy for assessing the role of space charge and its associated memory propagation effect in PD signatures. Since Long Short-Term Memory (LSTM) based DRNN architecture augurs well for data involving sequence similarity recognition, the objective of this research is on establishing an indigenous approach of formulating identification markers to ascertain the role of space charge in PD patterns. The second objective is on establishing a unique approach to decipher state transitions of pulses using transition labels that describes the dynamics of PD signatures due to space charge effect. Detailed case studies based on laboratory benchmark models that replicate complex PD patterns clearly demonstrate the excellent capability of DRNN in deciphering the role of space charge in discriminating complex PD patterns.",https://ieeexplore.ieee.org/document/9696457/,2021 Innovations in Power and Advanced Computing Technologies (i-PACT),27-29 Nov. 2021,ieeexplore
10.1109/ICSS.2010.65,A Novel End-User Oriented Service Composition Model Based on Quotient Space Theory,IEEE,Conferences,"Nowadays, the services on the World Wide Web (WWW) are proliferating dramatically. The relationships of invoking among these services are becoming more and more complicated. The performances of traditional automated service composition methods suffer from the complexity of the service composition problem. Hence it is hardly to apply these automated methods in real cases. Semi-automated service composition is an alternative for coping with the complexity of service composition. A novel end-user oriented service composition model based on quotient space theory and service relation diagram is presented. This model proposes a hierarchical service composition from coarse-grain to fine-grain. The performance analysis and simulation showed that this model can reduce the complexity of service composition efficiently.",https://ieeexplore.ieee.org/document/5494284/,2010 International Conference on Service Sciences,13-14 May 2010,ieeexplore
10.1109/IWISA.2009.5072659,A Novel Feature Selection Approach Based on Swarm Intelligence,IEEE,Conferences,"The computational complexity of a texture classification algorithm is limited by the dimensionality of the feature space. A feature selection algorithm that can reduce the dimensionality of problem is often desirable, which has been studied by many authors because of its impact on the complexity of classifiers, Furthermore, feature selection in high dimension space is a NP hard problem. This paper presents a novel approach to solve feature subset selection based on improved ant colony optimization algorithm which hybrids heuristics information. The proposed approach has been implemented and tested on a real image texture classification problem. The results of proposed method are encouraging and outperform that of the presented ant colony optimization algorithm without heuristic information in this domain.",https://ieeexplore.ieee.org/document/5072659/,2009 International Workshop on Intelligent Systems and Applications,23-24 May 2009,ieeexplore
10.1109/ICCI-CC.2018.8482045,A Novel Feature Selection Based Classification Algorithm for Real-Time Medical Disease Prediction,IEEE,Conferences,"In the current medical databases, feature extraction and disease prediction are the essential requirements to Chronic Obstructive Pulmonary Disease (COPD) and Alzheimer's diseases. Most of the medical databases have heterogeneous features with different levels of severity patterns. Feature extraction and classification of high risk patterns may have potential benefits for decision making. In the medical applications, data classification algorithms are used to detect the disease severity that can help in early prediction of new type of disease patterns. Also, machine learning algorithms are more accurate, high true positive rate and reliable for heterogeneous features. Traditional classification models such as Nave Bayes, SVM, Feed forward neural networks, Regression models, etc are used to classify the homogeneous disease datasets with limited feature space. As the size of the Alzheimer's disease patterns and its categories are increasing, traditional data classification models are failed to process the disease patterns due to inconsistent, class imbalance, and sparsity issues, which may affect the disease prediction rate and error rate. Therefore, an efficient classification model for predicting the severity level of the heterogeneous feature types is essential with high true positivity and low error rate. In this paper, a novel feature selection based classification model is proposed to improve the disease classification rate and testing the new type of disease patterns for real-time patient disease prediction. In the proposed model, a novel probabilistic based feature selection measure for classification algorithm is designed and implemented for real-time patient disease prediction using the training datasets. Experimental results show that the proposed feature selection based classification algorithm is better than the traditional algorithms in terms of true positive rate, error rate and F-measure are concerned.",https://ieeexplore.ieee.org/document/8482045/,2018 IEEE 17th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),16-18 July 2018,ieeexplore
10.1109/AUTEEE.2018.8720798,A Novel Three-dimensional Indoor Localization Algorithm Based on Visual Visible Light Communication Using Single LED,IEEE,Conferences,"Visible light positioning (VLP) is widely believed to be a cost-effective answer to the growing demand for Indoor positioning. However, because of the nonlinear and highly complicated relationship between 3D world coordinate and 2D image coordinate, there is a need to develop effective VLP location algorithm to locate the positioning terminal using image sensor. Besides, due to the high computational cost of image processing, most existing VLP systems fail to deliver satisfactory performance in terms of real-time ability and positioning accuracy, both of which are crucial for the performance of indoor positioning system. The field of view (FOV) of image sensor affects the number of LEDs. Therefore, this paper proposes an image-sensor-based single-light positioning system and sets up relevant experiments to test the proposed system. What's more, the real-time ability is taken into consideration, which greatly improves the robustness and practicality of the system. As for the ID information of each LED, it utilizes the rolling shutter mechanism of the Complementary Metal Oxide Semiconductor (CMOS) image sensor and combines machine learning algorithm to identify. Different from the traditional LED-ID modulation and demodulation methods, the LED-ID detection and recognition problem was treated as a classification problem in machine learning filed. In this paper, two typical linear classifiers in the case of binary classification problem are introduced. As the features of different LED-ID is linear separable in the feature space, linear classifiers are used to identify the LED-ID. The scheme proposed could improve the speed of LED-ID identification and the robustness of the system by off-line training for the classifiers and online recognition of LED-ID. When there's only one LED in the camera image, it means the distance between the terminal and LED is so close that the diameter of the LED can be measured. Therefore, it is able to use the proposed single-light positioning algorithm to achieve a high precision positioning and to solve the problem of limited number of LEDs in FOV in the existing literature. The proposed single-light positioning algorithm also has lower calculation complexity. Experimental results show that the proposed single-light positioning algorithm provides an accuracy of 7.39 cm and the computational time is 43.05 ms, which reflects a good performance in both accuracy and real-time ability.",https://ieeexplore.ieee.org/document/8720798/,"2018 IEEE International Conference on Automation, Electronics and Electrical Engineering (AUTEEE)",16-18 Nov. 2018,ieeexplore
10.1109/DSAA.2016.42,A Parallel Framework for Grid-Based Bottom-Up Subspace Clustering,IEEE,Conferences,"Clustering is a popular data mining and machine learning technique which discovers interesting patterns from unlabeled data by grouping similar objects together. Clustering high-dimensional data is a challenging task as points in high dimensional space are nearly equidistant from each other, rendering commonly used similarity measures ineffective. Subspace clustering has emerged as a possible solution to the problem of clustering high-dimensional data. In subspace clustering, we try to find clusters in different subspaces within a dataset. Many subspace clustering algorithms have been proposed in the last two decades to find clusters in multiple overlapping subspaces of high-dimensional data. Subspace clustering algorithms iteratively find the best subset of dimensions for a cluster from 2d-1 possible combinations in d-dimensional data. Subspace clustering is extremely compute intensive because of exhaustive search of subspaces, especially in the bottom-up subspace clustering algorithms. To address this issue, an efficient parallel framework for grid-based bottom-up subspace clustering algorithms is developed, considering popular algorithms belonging to this category. The framework is implemented for shared memory, distributed memory, and hybrid systems and is tested for three grid-based bottom-up subspace clustering algorithms: CLIQUE, MAFIA, and ENCLUS. All parallel implementations exhibit impressive speedup and scalability on real datasets.",https://ieeexplore.ieee.org/document/7796919/,2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA),17-19 Oct. 2016,ieeexplore
10.1109/ICET52293.2021.9563174,A Physical-Cyber Dual Space Fusion Learning Assistant Method Based on Mixed Reality Implementation,IEEE,Conferences,"The learning assistant method of the Physical-Cyber dual space is an effective method to improve learning interaction and learning effect. In this paper, we proposed a learning assistant method based on mixed reality technology and the fusion of physical space and cyber space, including two classical learning scenes. Firstly, we integrated physical real classroom scene with online three-dimensional resource scene, which effectively expanded the imagination of students. It allows students to learn abstract knowledge of macro or micro scenes more intuitively. Secondly, we explored the online search functions to extend resources related to offline teaching knowledge. Finally, we constructed the instructing scene and application example of the dual space fusion learning companion system using HoloLens. We have come to the necessity, rationality and technical feasibility of the method, and carry out learning activities on the basis of HoloLens, which provides new ideas for promoting the innovation of teaching and learning forms in the future.",https://ieeexplore.ieee.org/document/9563174/,2021 IEEE International Conference on Educational Technology (ICET),18-20 June 2021,ieeexplore
10.1109/AERO50100.2021.9438232,A Pipeline for Vision-Based On-Orbit Proximity Operations Using Deep Learning and Synthetic Imagery,IEEE,Conferences,"Deep learning has become the gold standard for image processing over the past decade. Simultaneously, we have seen growing interest in orbital activities such as satellite servicing and debris removal that depend on proximity operations between spacecraft. However, two key challenges currently pose a major barrier to the use of deep learning for vision-based on-orbit proximity operations. Firstly, efficient implementation of these techniques relies on an effective system for model development that streamlines data curation, training, and evaluation. Secondly, a scarcity of labeled training data (images of a target spacecraft) hinders creation of robust deep learning models. This paper presents an open-source deep learning pipeline, developed specifically for on-orbit visual navigation applications, that addresses these challenges. The core of our work consists of two custom software tools built on top of a cloud architecture that interconnects all stages of the model development process. The first tool leverages Blender, an open-source 3D graphics toolset, to generate labeled synthetic training data with configurable model poses (positions and orientations), lighting conditions, backgrounds, and commonly observed in-space image aberrations. The second tool is a plugin-based framework for effective dataset curation and model training; it provides common functionality like metadata generation and remote storage access to all projects while giving complete independence to project-specific code. Time-consuming, graphics-intensive processes such as synthetic image generation and model training run on cloud-based computational resources which scale to any scope and budget and allow development of even the largest datasets and models from any machine. The presented system has been used in the Texas Spacecraft Laboratory with marked benefits in development speed and quality. Remote development, scalable compute, and automatic organization of data and artifacts have dramatically decreased iteration time while increasing reproducibility and system comprehension. Diverse, high-fidelity synthetic images that more closely replicate the real environment have improved model performance against real-world data. These results demonstrate that the presented pipeline offers tangible benefits to the application of deep learning for vision-based on-orbit proximity operations.",https://ieeexplore.ieee.org/document/9438232/,2021 IEEE Aerospace Conference (50100),6-13 March 2021,ieeexplore
10.1109/CSCWD.2007.4281598,A Q-Learning-based Decision Making Scheme for Application Reconfiguration in Sensor Networks,IEEE,Conferences,"Application reconfiguration is a principal method of improving the application flexibility and adaptability to dynamic environment in sensor networks. Motivated by this, we propose a novel Q-learning-based decision making scheme in sensor networks to support the environment adaptive reconfiguration decision. In this scheme, we introduce the rule-based reasoning to predict the trend of environmental change and make reconfiguration decision adoptively. In particular, we apply a real-time reinforcement learning algorithm, called Q-learning, to evaluate the certainty value of the rules in this decision mechanism. Moreover, we map the continuous state space of environment condition into a discrete and finite state space to design a certainty function. The simulation results show that the Q-learning-based decision mechanism can not only learn the change of environment, but also effectively make the reconfiguration decision in sensor networks.",https://ieeexplore.ieee.org/document/4281598/,2007 11th International Conference on Computer Supported Cooperative Work in Design,26-28 April 2007,ieeexplore
10.1109/Agro-Geoinformatics.2019.8820529,A Rapidly Diagnosis and Application System of Fusarium Head Blight Based on Smartphone,IEEE,Conferences,"Wheat (Triticum aestivum L.) is one of the three major cereals worldwide. The FusaHum graminearum Sehw., special fugus always damages the wheat ear, and produces vomitoxin,is difficult to control and prevent, and seriously threatens the health of humans, animals and China's food security. Currently, rapidly, accurately and non-destructively diagnostic devices or systems for this disease have not been disclosed. In this study, the infected ears with different severities were picked up in key growth stages. The diseased area of wheat ear was extracted using hypergreen characteristic, and a total of 30 features of infected ears were chosen including color (Lab, HSI, HSV, YCbCr color space), texture (LBP and LLE dimension reduction), and shape (squareness, shape complexity, and eccentricity). Then using the competitive adaptive re-weighted sampling (CARS) and rough set algorithm (RS) to screen the characteristics of the diseased ear, the four characteristics with the largest contribution were determined to establish the CARS-SVM and CARS-RS-SVM models respectively. The study found that the recognition rate of CARS-SVM model is 85.4%, while CARS-RS-SVM model is 92.7%. Thus the CARS-RS-SVM was thought of as the optimal model by two indicators of identification accuracy. On the basis, a wheat scab diagnosis system based on Android mobile phone was constructed. It consists of three parts - Clients, Service-Terminal and Database. The Client was designed by Android Studio and its functions mainly include image acquisition, image storage, GPS positioning, image uploading and diagnostic results display. The Service-Terminal was completed by the mixed programming of Myeclipse and Matlab software, and Tomcat was used as the Server. It mainly implements the functions of image receiving, image preprocessing, feature extraction and selection, and classifier modeling. The MySQL was used to establish two databases: the Disease Characteristics Database and the Disease Diagnosis Knowledge Base. Finally, through samples testing and validating, the Android-based mobile terminal can real-time collect the image of Fusarium head blight and upload the server. After the target image was processed and compared by the Disease Characteristics Database, the appropriate diagnostic knowledge was selected from the Disease Diagnosis Knowledge Base and feedbacked to the client. In summary, the results of this study showed that it was helpful for the rapid and non-destructive investigation of infected FHB in the field, and it would provide a reference for the study of other crop diseases, facilitate the application and development of new technologies such as artificial intelligence and big data in agriculture.",https://ieeexplore.ieee.org/document/8820529/,2019 8th International Conference on Agro-Geoinformatics (Agro-Geoinformatics),16-19 July 2019,ieeexplore
10.1109/HPCC/SmartCity/DSS.2018.00080,A Real Time Anomaly Detection Method Based on Variable N-Gram for Flight Data,IEEE,Conferences,"Anomaly detection of flight data is vital to aviation safety. A real time anomaly detection for flight data can identify safety issues as soon as anomalies occur and improve the safety of aviation system. However, existing anomaly detection methods for flight data either perform not very well on unknown issues or are limited in their capabilities of real time detection. This paper provides a method to detect real time flight data anomalies with comparable detection performance. Firstly it transforms discrete and continuous flight data into discrete sequences. And each sequence obtained from discrete and continuous data is segmented into several subsequences by sliding a window of the specified size. More importantly, the variable length n-grams are obtained from each subsequence. Then all of unique n-grams become the feature space of the training data set. In this way each subsequence is mapped to this feature space and becomes an input feature vector of one-class Support Vector Machine (SVM). Finally, anomalies in flight data can be detected in real time by the classifier learned on these feature vectors corresponding to subsequences. Experimental results show that it performs better to use linear kernel to train one-class SVM on our feature vectors than Gaussian kernel and the size of the sliding window takes a little effect on the performance of our detection method. Moreover, this method outperforms Multiple Kernel Anomaly Detection (MKAD) in real time anomaly detection.",https://ieeexplore.ieee.org/document/8622818/,2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS),28-30 June 2018,ieeexplore
10.1109/APCC47188.2019.9026506,A Real-time Vehicle Detection for Traffic Surveillance System Using a Neural Decision Tree,IEEE,Conferences,"Traffic surveillance system (TSS) is an essential tool to extract necessary information (count, type, speed, etc.) from cameras for traffic monitoring in many metro cities. In TSS, vehicle detection plays a pivotal role as it is a vital process for further analysis such as vehicle classification and vehicle tracking. So far there has been a considerable amount of research proposed with single-pipeline Convolution Neural Networks (CNN) to accommodate this subject. Although these studies achieved results with high accuracy, they required a large dataset and an implementation on dedicated hardware configuration. This paper presents a novel method with vision-based approach to detect moving vehicles from static surveillance cameras. Moving vehicles are detected and analysed by means of using a Neural Decision Tree accompanied with geometric features to classify vehicles and a Single Shot Detector to handle occlusion when inter-vehicle space between vehicles significantly decreases. Experiments have been conducted on the real-world data to evaluate the performance and accuracy of the proposed method. The results showed that our proposed method achieved a promising detection rate with real-time processing on regular hardware configuration.",https://ieeexplore.ieee.org/document/9026506/,2019 25th Asia-Pacific Conference on Communications (APCC),6-8 Nov. 2019,ieeexplore
10.1109/SMC42975.2020.9282929,A Reinforcement Learning Approach to Design Verification Strategies of Engineered Systems,IEEE,Conferences,"System verification is a critical process in the development of engineered systems. Engineers gain confidence in the correct functionality of the system before it is deployed into operation by executing verification activities. Choosing the right set of verification activities at the right system development stage, that is, designing a verification strategy (VS), is essential to balancing information discovery and verification cost. Only recently, quantitative methods have been proposed to support the design of verification strategies. However, their applicability in real-life scenarios is impractical due to their limited computational efficiency in the high dimensional solution space of the VS selection problem. This paper presents a reinforcement learning (RL) approach to search for a near-optimal VS. Specifically, the VS design problem is formulated as a Markov decision process (MDP) in which a value function is required. Then we combine tree search and a neural network (NN) to design a RL algorithm. In the RL algorithm, the value function is approximated as a NN that is trained in an iterative way. The near-optimal VS can be generated from the trained NN. A case study is presented to show the superiority of the proposed method.",https://ieeexplore.ieee.org/document/9282929/,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",11-14 Oct. 2020,ieeexplore
10.1109/ICARM52023.2021.9536056,A Review of Bilateral Teleoperation Control Strategies with Soft Environment,IEEE,Conferences,"In the past two decades, bilateral teleoperation with haptic feedback has attracted great research and application interests in both robotics and other areas. Initially triggered by the need to handle dangerous and remote distance tasks such as nuclear materials manipulation and space exploration, bilateral teleoperation has found its way into other applications as a result of development of control theory, robotic technology (both hardware and software) and latest breakthrough in artificial intelligence and machine learning. Consequently, bilateral teleoperation is found facing new challenges brought by these new applications. One major and obvious change is the working environment for the slave manipulator: different from rigid or solid contact environments which are reasonably assumed in early applications in industrial, nuclear and aerospace applications, the slave environment is now more complex and often the objects in contact are much softer in term of stiffness and can not be described by simple elastic model if good teleoperation performance (accurate and transparent) is expected. In this paper, the research of bilateral teleoperation system considering soft environment in recent 20 years has been surveyed for the first time in literature, to the knowledge of the authors. Following the difference in real applications, in this review the definition of soft environment covers linear elastic environment with much lower stiffness than conventional industrial environment and nonlinear complex soft environment with/out time-varying characteristics. Accordingly, the surveyed control strategies and structures in recent literature to improve the stability and accuracy of bilateral teleoperation with soft environment are classified and explained. Finally, the main applications, current challenges and future perspectives of bilateral teleoperation with soft environment are discussed.",https://ieeexplore.ieee.org/document/9536056/,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),3-5 July 2021,ieeexplore
10.1109/CISDA.2007.368137,A Review of Intelligent Systems Software for Autonomous Vehicles,IEEE,Conferences,"The need for intelligent unmanned vehicles has been steadily increasing. These vehicles could be air-, ground-, space-, or sea-based. This paper will review some of the most common software systems and methods that could be used for controlling such vehicles. Early attempts at mobile robots were confined to simple laboratory environments. For vehicles to operate in real-world noisy and uncertain environments, they need to include numerous sensors and they need to include both reactive and deliberative features. The most effective software systems have been hierarchical or multi-layered. Many of these systems mimic biological systems. This paper reviews several software approaches for autonomous vehicles. While there are similarities, there are differences as well. Most of these software systems are very difficult to use, and few of them have the ability to learn. Autonomous vehicles promise remarkable capabilities for both civilian and military applications, but much work remains to develop intelligent systems software which can be used for a wide range of applications. In particular there is a need for reliable open-source software that can be used on inexpensive autonomous vehicles",https://ieeexplore.ieee.org/document/4219084/,2007 IEEE Symposium on Computational Intelligence in Security and Defense Applications,1-5 April 2007,ieeexplore
10.1109/GCWkshps45667.2019.9024680,A Routing Optimization Method for Software-Defined SGIN Based on Deep Reinforcement Learning,IEEE,Conferences,"As space networks become more and more important, the Space-ground Integration network (SGIN) has received unprecedented attention. However, dynamic changes of topology and link status of satellite networks bring many challenges to routing optimization in the SGIN. Traditional routing optimization methods do not perform well, as they do not consider changes of topology and link status, as well as the association between flows. Since the Machine Learning (ML) technologies have shown significant advantages in dynamic routing optimization, we proposed a Machine Learning-based Space-ground Integration Networking (ML-SSGIN) framework that combines Software-Defined Networking (SDN) technologies to solve this challenge. To evaluate the feasibility of the proposed framework, the Deep Deterministic Policy Gradient (DDPG), a Deep Reinforcement Learning (DRL) algorithm, is deployed to perform routing optimization, which can make routing decisions based on real-time link status. In particular, we utilize a neural network that integrates Long Short-Term Memory Network (LSTM) and Dense layers for its actor and critic part to improve perceptual capabilities of contextual correlations between flows. We compared the proposed DDPG neural network with the one only having the Dense layers. The results show that the proposed architecture is feasible and effective. What's more, compared to Open Shortest Path First (OSPF) algorithm, our proposed routing optimization method can adapt to continuously change flows, and link status, which improves end-to-end throughput and latency.",https://ieeexplore.ieee.org/document/9024680/,2019 IEEE Globecom Workshops (GC Wkshps),9-13 Dec. 2019,ieeexplore
10.1109/CVPR42600.2020.00034,A Self-supervised Approach for Adversarial Robustness,IEEE,Conferences,"Adversarial examples can cause catastrophic mistakes in Deep Neural Network (DNNs) based vision systems e.g., for classification, segmentation and object detection. The vulnerability of DNNs against such attacks can prove a major roadblock towards their real-world deployment. Transferability of adversarial examples demand generalizable defenses that can provide cross-task protection. Adversarial training that enhances robustness by modifying target model's parameters lacks such generalizability. On the other hand, different input processing based defenses fall short in the face of continuously evolving attacks. In this paper, we take the first step to combine the benefits of both approaches and propose a self-supervised adversarial training mechanism in the input space. By design, our defense is a generalizable approach and provides significant robustness against the unseen adversarial attacks (e.g. by reducing the success rate of translation-invariant ensemble attack from 82.6% to 31.9% in comparison to previous stateof-the-art). It can be deployed as a plug-and-play solution to protect a variety of vision systems, as we demonstrate for the case of classification, segmentation and detection. Code is available at: https://github.com/ Muzammal-Naseer/NRP.",https://ieeexplore.ieee.org/document/9156294/,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 2020,ieeexplore
10.1109/CCDC52312.2021.9602180,A Semantic Trajectory Mining System Based on Deep Neural Network,IEEE,Conferences,"At the beginning of 2020, epidemic of COrona VIrus Disease 19 (COVID-19) broke out. During the epidemic prevention and control, artificial intelligence, big data and other technologies have become powerful weapons against the epidemic, and have been widely used in the fields of epidemic tracing, confirming virus transmission path, resource allocation and so on. In this study, BiLSTM-CRF model, Bootstrap and Tornado frameworks are used to implement a neural network-based semantic trajectory mining system for the COVID-19 patients. On the basis of collecting the data published by the health committees of various provinces and cities, the semantic trajectories of the patients are extracted to ensure the accuracy of the data and then establish mapping relationship between the real space and the text description of the trajectories of the patients, while taking the time and space factors into account and excavating the dynamic changes of the patients.",https://ieeexplore.ieee.org/document/9602180/,2021 33rd Chinese Control and Decision Conference (CCDC),22-24 May 2021,ieeexplore
10.1109/IMCET53404.2021.9665549,A Simple Neural Network for Efficient Real-time Generation of Dynamically-Feasible Quadrotor Trajectories,IEEE,Conferences,"In this work, we study the problem of efficiently generating dynamically-feasible trajectories for quadrotors in real-time. A supervised learning approach is used to train a simple neural network with two hidden layers. The training data is generated from a well-established trajectory generation method for quadrotors that minimizes jerk given a fixed time interval. More than a million dynamically-feasible trajectories between two random points in the three-dimensional (3D) space are generated and used as training data. The input of the neural network is a vector composed of initial and desired states, along with the final trajectory time. The output of the neural network generates the motion primitives of the trajectories, as well as the duration or final time of a segment. Simulation results show extremely fast generation of dynamically-feasible trajectories by the proposed learning algorithm, which makes it suitable for real-time implementation.",https://ieeexplore.ieee.org/document/9665549/,2021 IEEE 3rd International Multidisciplinary Conference on Engineering Technology (IMCET),8-10 Dec. 2021,ieeexplore
10.1109/EI250167.2020.9347305,A Simulation-Constraint Graph Reinforcement Learning Method for Line Flow Control,IEEE,Conferences,"Line flow control plays an essential role in maintaining the stability of power system. Considering the randomness and uncertainties in the grid, a simulation-constraint graph reinforcement learning method is proposed to provide a potential solution for the real-time dispatch via topology control. To improve the training efficiency and deployment stability of reinforcement learning agent in the power system with various constraints and large action space, an adaptive simulation regulation strategy and a step-wise control mechanism are utilized. Power system simulation is introduced to keep the agent from actions with invalidity or severe simulated outcomes. Then, graph neural network is applied to help the agent learn a better representation of power system state by dealing with the information hidden in topology. Finally, a numeric example on IEEE 14-bus system is employed to demonstrate the feasibility of the proposed method.",https://ieeexplore.ieee.org/document/9347305/,2020 IEEE 4th Conference on Energy Internet and Energy System Integration (EI2),30 Oct.-1 Nov. 2020,ieeexplore
10.1109/BigData52589.2021.9671964,A Synergetic Attack against Neural Network Classifiers combining Backdoor and Adversarial Examples,IEEE,Conferences,"The pervasiveness of neural networks (NNs) in critical computer vision and image processing applications makes them very attractive for adversarial manipulation. A large body of existing research thoroughly investigates two broad categories of attacks targeting the integrity of NN models. The first category of attacks, commonly called Adversarial Examples, perturbs the models inference by carefully adding noise into input examples. In the second category of attacks, adversaries try to manipulate the model during the training process by implanting Trojan backdoors. Researchers show that such attacks pose severe threats to the growing applications of NNs and propose several defenses against each attack type individually. However, such one-sided defense approaches leave potentially unknown risks in real-world scenarios when an adversary can unify different attacks to create new and more lethal ones bypassing existing defenses.In this work, we show how to jointly exploit adversarial perturbation and model poisoning vulnerabilities to practically launch a new stealthy attack, dubbed AdvTrojan. AdvTrojan is stealthy because it can be activated only when: 1) a carefully crafted adversarial perturbation is injected into the input examples during inference, and 2) a Trojan backdoor is implanted during the training process of the model. We leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, making it difficult to detect. The stealthiness behavior of AdvTrojan fools the users into accidentally trusting the infected model as a robust classifier against adversarial examples. AdvTrojan can be implemented by only poisoning the training data similar to conventional Trojan backdoor attacks. Our thorough analysis and extensive experiments on several benchmark datasets show that AdvTrojan can bypass existing defenses with a success rate close to 100% in most of our experimental scenarios and can be extended to attack federated learning as well as high-resolution images.",https://ieeexplore.ieee.org/document/9671964/,2021 IEEE International Conference on Big Data (Big Data),15-18 Dec. 2021,ieeexplore
10.1109/SSCI.2018.8628874,A Time Efficient Model for Region of Interest Extraction in Real Time Traffic Signs Recognition System,IEEE,Conferences,"Computation intelligence plays a major role in developing intelligent vehicles, which contains a Traffic Sign Recognition (TSR) system for increasing vehicle safety. Traffic sign recognition systems consist of an initial phase called Traffic Sign Detection (TSD), where images and colors are segmented and fed to the recognition phase. The most challenging process in TSR systems in terms of time consumption is the detection phase. The previous studies proposed different models for traffic sign detection, however, the computation time of these models still requires improvement for enabling real time systems. Therefore, this paper focuses on the computational time and proposes a novel time efficient color segmentation model based on logistic regression. This paper uses RGB color space as the domain to extract the features of our hypothesis; this has boosted the speed of the proposed model, since no color conversion is needed. The trained segmentation classifier is tested on 1000 traffic sign images taken in different lighting conditions. The experimental results show that the proposed model segmented 974 of these images correctly and in a time less than one-fifth of the time needed by any other robust segmentation methods.",https://ieeexplore.ieee.org/document/8628874/,2018 IEEE Symposium Series on Computational Intelligence (SSCI),18-21 Nov. 2018,ieeexplore
10.1109/EIECS53707.2021.9588055,A Traffic Accident Risk Prediction Model Based on Spatial Gated Memory Network,IEEE,Conferences,"The prediction of traffic accidents is becoming more and more important in the field of public safety, especially in travel route planning and road safety deployment. Due to the development of deep learning, traffic accident prediction can evolve from traditional machine learning algorithms to neural networks to predict the risk of accidents in a short period of time (hour level) in the future. However, the large urban area and sparse traffic accident samples will bring difficulties to deep learning prediction. If it is not solved, it will easily lead to zero-expansion problems, leading to inaccurate prediction results. In response to the above problems, this paper proposes a space-gated memory network (SGMN) based on deep learning. The model combines real-time accident risk, real-time traffic flow, and real-time weather with multiple inputs to predict sub-regions with high accident risk. In order to verify the performance of the model, two real data sets are used to evaluate the performance of the model. The results show that the performance of SGMN is better than that of commonly used memory neural networks such as RNN, LSTM, GRU, Convulsion and Hereto-ConvLSTM, which validates the model.",https://ieeexplore.ieee.org/document/9588055/,2021 International Conference on Electronic Information Engineering and Computer Science (EIECS),23-26 Sept. 2021,ieeexplore
10.1109/SNPD.2007.288,A Two-phase Flight Data Feature Selection Method Using both Filter and Wrapper,IEEE,Conferences,"Feature selection is an important issue in flight data mining. By selecting only relevant features of flight data, higher prediction accuracy can be expected and computational complexity can be reduced. In this paper we propose a novel two-phase flight data feature selection approach using both filter and wrapper. It begins by running artificial neural network weight analysis (ANNWA) as a filter approach to remove irrelevant features, then it runs genetic algorithm as a wrapper approach to remove redundant or useless features. We demonstrate the usefulness of the proposed approach on two real- world datasets based on flight data. Our algorithm reduces the size of flight data feature space significantly without compromising the classification or the prediction performance.",https://ieeexplore.ieee.org/document/4287549/,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",30 July-1 Aug. 2007,ieeexplore
10.1109/EH.2005.1,A case for using Minipop as the evolutionary engine in a CTRNN-EH control device: an analysis of area requirements and search efficacy,IEEE,Conferences,"Continuous time recurrent neural network-evolvable hardware (CTRNN-EH) control devices comprise of an analog continuous time recurrent neural network (CTRNN) with an on-board evolutionary algorithm (EA) engine to evolve the parameters of the neural network. These control devices were demonstrated to be effective for suppressing thermoacoustic (TA) instability in simulated jet engines. Currently, the construction of a VLSI CTRNN-EH device is underway for suppressing TA instability in a real combustion chamber while it is in operation. In this paper, we present a fully function digital EA engine for the CTRNN-EH control device. An ad-hoc hardware design is presented to realize space savings. The simulation and synthesis results of the hardware EA are presented. In addition to this, a demonstration of the efficacy of the EA across a noisy real world control problem is presented.",https://ieeexplore.ieee.org/document/1508504/,2005 NASA/DoD Conference on Evolvable Hardware (EH'05),29 June-1 July 2005,ieeexplore
10.1109/ICIT.2018.8352442,A centralized state of charge estimation technique for electric vehicles equipped with lithium-ion batteries in smart grid environment,IEEE,Conferences,"Electric vehicles (EVs) not only act as a load in grid-to-vehicle but also as a storage system in vehicle-to-grid concept. Further, both renewable energy sources (RESs) and EVs can be classified under multi-distributed generations. In this regard, fossil-based power generation units as traditional back-ups or distributed generations can be replaced by a combination of renewable energy sources (RESs) and EVs to alleviate suffering from the fluctuation of renewable power generation units. However, successful coordination of electric vehicles and renewable energy systems require accurate state estimations of EVs such as state of charge (SoC) due to intermittent renewable energy output. Hence this combination emphasizes the need for an efficient method of SoC estimation. Since battery management system is in the initial stage of development, none of the proposed intelligent state of charge estimation techniques has been capable of being implemented in battery management system. In this regard, the present article proposes an off-board real-time state of charge estimation technique, can be implemented in fog computing architecture, for lithium-ion battery based on the National Aeronautics and Space Administration (NASA) database enabling to compare the outcome of the present article with the recent studies. In this article, a non-dominated sorting genetic algorithm II (NSGA-II) is combined with an artificial neural network (ANN) technique to estimate SoC of four batteries simultaneously. The results of the proposed technique indicate high convergence rate and high accurate estimation.",https://ieeexplore.ieee.org/document/8352442/,2018 IEEE International Conference on Industrial Technology (ICIT),20-22 Feb. 2018,ieeexplore
10.1145/1985793.1985901,A comparison of model-based and judgment-based release planning in incremental software projects,IEEE,Conferences,"Numerous factors are involved when deciding when to implement which features in incremental software development. To facilitate a rational and efficient planning process, release planning models make such factors explicit and compute release plan alternatives according to optimization principles. However, experience suggests that industrial use of such models is limited. To investigate the feasibility of model and tool support, we compared input factors assumed by release planning models with factors considered by expert planners. The former factors were cataloged by systematically surveying release planning models, while the latter were elicited through repertory grid interviews in three software organizations. The findings indicate a substantial overlap between the two approaches. However, a detailed analysis reveals that models focus on only select parts of a possibly larger space of relevant planning factors. Three concrete areas of mismatch were identified: (1) continuously evolving requirements and specifications, (2) continuously changing prioritization criteria, and (3) authority-based decision processes. With these results in mind, models, tools and guidelines can be adjusted to address better real-life development processes.",https://ieeexplore.ieee.org/document/6032518/,2011 33rd International Conference on Software Engineering (ICSE),21-28 May 2011,ieeexplore
10.1109/ICSLP.1996.607836,A comparison of modified k-means (MKM) and NN based real time adaptive clustering algorithms for articulatory space codebook formation,IEEE,Conferences,"The paper proposes the use of a neural network based real time adaptive clustering algorithm for the formation of a codebook of limited set of acoustical representation of finite set of vocal tract shapes from an articulatory space. A modified k-means algorithm (MKM) used for clustering nearly 10000 vocal tract shapes into 1000 cluster centers to form a codebook of articulatory shapes is computationally intensive for the application. An investigative study on the use of the NN based algorithm over the MKM algorithm at the peripheral level, for an application on computer aided pronunciation education, suggests the former for less intensive computation, with the possibility of improving the performance of the system by implementing the algorithm using a dedicated neural computer. Preliminary results of this study are reported.",https://ieeexplore.ieee.org/document/607836/,Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96,3-6 Oct. 1996,ieeexplore
10.1109/IJCNN.1989.118711,A constraint satisfaction network for matching 3D objects,IEEE,Conferences,"A new approach is presented for matching visible surfaces of 3D objects using a constraint satisfaction network. This in turn provides the necessary basis for volumetric reconstruction from multiple views. By matching, the authors mean both to establish correspondence between individual faces and to compute 3D transform that would bring one in correspondence with the other. Toward this goal, constraints at three different levels of complexities are specified to produce stable and coherent assignments. The constraint satisfaction is implemented as a Hopfield network with an appropriate energy functional and minimized using simulated annealing. The system extracts objects faces by computing their bounding contours with adaptive scale space filtering. This process identifies important surface features such as jumps or occluding boundaries and creases. The pointwise feature descriptors are then linked, and an attributed graph is derived to represent the object. The nodes in the graph represent geometric surface features, and the links in the graph represent the relationship between adjacent surfaces. The authors present results on real images.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/118711/,International 1989 Joint Conference on Neural Networks, 1989,ieeexplore
10.1109/IFIC.2000.859890,A constructive Bayesian approach for vehicle monitoring,IEEE,Conferences,"A key component of a vehicle monitoring system is uncertainty management. Bayesian networks (BN) emerged as a normative and effective formalism for uncertain reasoning in many AI tasks. Since a priori modeling of the domain into a BN is impractical due to the vast interpretation space, the BN formalism has been considered inapplicable to this type of task. We propose a framework in which the BN formalism can be applied to vehicle monitoring. The framework explores domain decomposition, model separation, model approximation, model compilation and re-analysis. Experimental implementation demonstrated good performance at near-real time.",https://ieeexplore.ieee.org/document/859890/,Proceedings of the Third International Conference on Information Fusion,10-13 July 2000,ieeexplore
10.1109/ICRA.2012.6225245,A depth space approach to human-robot collision avoidance,IEEE,Conferences,"In this paper a real-time collision avoidance approach is presented for safe human-robot coexistence. The main contribution is a fast method to evaluate distances between the robot and possibly moving obstacles (including humans), based on the concept of depth space. The distances are used to generate repulsive vectors that are used to control the robot while executing a generic motion task. The repulsive vectors can also take advantage of an estimation of the obstacle velocity. In order to preserve the execution of a Cartesian task with a redundant manipulator, a simple collision avoidance algorithm has been implemented where different reaction behaviors are set up for the end-effector and for other control points along the robot structure. The complete collision avoidance framework, from perception of the environment to joint-level robot control, is presented for a 7-dof KUKA Light-Weight-Robot IV using the Microsoft Kinect sensor. Experimental results are reported for dynamic environments with obstacles and a human.",https://ieeexplore.ieee.org/document/6225245/,2012 IEEE International Conference on Robotics and Automation,14-18 May 2012,ieeexplore
10.1109/ACSSC.2014.7094554,A design and implementation framework for unsupervised high-resolution recursive filters in neuromotor prosthesis applications,IEEE,Conferences,"Neuromotor prostheses have the potential of restoring movement ability in patients with severe motor dysfuncion. In cortically-controlled neuromotor prostheses, the design of neural decoders for motor impaired patients requires initialization using a concurrently measured set of neural and motor imagery or observation data. In addition, the decoder implementation poses a scalability challenge with an increasing number of decoded neurons. Consequently, most neural decoder implementations resort to sub-sampling the neural firing rates, which results in noisy decoded outputs. In this work, we propose a new decoder design and implementation framework in which (i) the decoder initialization is unsupervised, (ii) the decoder is implemented using computationally-inexpensive recursive filters that can operate at high-resolution sampling of the neural data thereby minimizing the delay introduced in the system, and (iii) the decoder gives a smooth real-time control signal expressed by the span of neural data projections onto a low-dimensional latent space that possesses desirable features for the control task.",https://ieeexplore.ieee.org/document/7094554/,"2014 48th Asilomar Conference on Signals, Systems and Computers",2-5 Nov. 2014,ieeexplore
10.1109/ICSMC.2008.4811556,A digital circuit design of state-space recurrent neural networks,IEEE,Conferences,"This paper presents a digital circuit design of a state-space recurrent neural network (RNN). The proposed digital circuit design separates the datapath of the state-space RNN into a linear subcircuit and a nonlinear subcircuit. The linear subcircuit is realized by a matrix-vector multiplier while the nonlinear subcircuit by a customized nonlinear function computing unit. The throughput rate of the proposed RNN circuit is 36060.5 times faster than that of the software simulation using MATLAB<sup>reg</sup>. The proposed state-space RNN digital design methodology not only possesses the advantages including high computing speed, small area and portability, but also increases the possibility of using the digital RNN circuit in real-world dynamic problems.",https://ieeexplore.ieee.org/document/4811556/,"2008 IEEE International Conference on Systems, Man and Cybernetics",12-15 Oct. 2008,ieeexplore
10.1109/VRAIS.1995.512492,A distributed virtual environment for concurrent engineering,IEEE,Conferences,This paper presents a distributed virtual environment that supports collaboration among members of a geographically dispersed multidisciplinary team engaged in concurrent product development. The distributed virtual environment maintains a shared information space that contains product data in a standard ISO STEP compliant format. It supports a user configurable virtual environment and the integration of different CAE applications to support different engineering perspectives. The realistic manipulation of assembly models within the distributed virtual environment is supported by constraint-based 3D manipulation techniques developed at Leeds. The initial implementation of this architecture supports accurate assembly modelling and kinematic simulation for virtual prototypes and runs on a network of SGI Indy workstations over an ATM network.,https://ieeexplore.ieee.org/document/512492/,Proceedings Virtual Reality Annual International Symposium '95,11-15 March 1995,ieeexplore
10.1109/CISP.2012.6469706,A fast and highly accurate carrier acquisition for deep space applications,IEEE,Conferences,"On-board carrier acquisition in deep space telecommunications depends on the residual carrier, to overcome the detection difficulty of low-level received signal. Nowadays, it becomes reality for the receiver itself acquiring Doppler frequency offset. Frequency acquisition based on fast Fourier transform (FFT) has been applied extensively. In this paper, we introduce an algorithm of refined estimation after FFT frequency coarse estimation, which is theoretically analyzed to be maximum likelihood (ML) estimation. Its accuracy can achieve 10-4 when the signal-to-noise ratio (SNR) is 5dB. By the aid of data, the estimator performs well at low SNR ranges. In the design of carrier acquisition, the accurate estimation of frequency offset aids the phase-locked loop (PLL) into locked state. If the acquisition bandwidth of PLL is fixed, accurate estimation value helps to decrease the FFT size. Due to simple implementation and good performance, the algorithm can be applied in burst communications.",https://ieeexplore.ieee.org/document/6469706/,2012 5th International Congress on Image and Signal Processing,16-18 Oct. 2012,ieeexplore
10.1109/ICCIC.2013.6724210,A framework for text summarization in mobile web browsers,IEEE,Conferences,"In mobile devices, limited space and time for browsing contents of web pages is an important issue. Summarization solutions running on web browsers can help the user to view and access summarized content of web pages in real time. In this paper we propose a framework for summarizing text viewed through mobile web browsers. The framework consists of an application that runs as part the mobile browser and extracts raw text. The text is then preprocessed and input to the summarization algorithm. The algorithm can be implemented either on the device as part of the browser engine or on a remote cloud service. The cloud service uses appropriate techniques to summarize the text and returns the summarized content to the device, where it is then displayed. During summarization, an extraction algorithm is used that uses ontology and a weighing mechanism to determine suitability score for each sentence. The sentences with the highest suitability scores are selected for inclusion in the summary as per the compression required. We describe a number of user interface elements in the mobile web browser to trigger the summarization and display the results, and compare the performance of publicly available summarization services.",https://ieeexplore.ieee.org/document/6724210/,2013 IEEE International Conference on Computational Intelligence and Computing Research,26-28 Dec. 2013,ieeexplore
10.1109/IJCNN.2000.857872,A hippocampal model of visually guided navigation as implemented by a mobile agent,IEEE,Conferences,"Visually guided landmark navigation is based on space coding by hippocampal place cells (PC). A biologically realistic architecture of cooperative-competitive associative networks (implemented as a control system for mobile agents) emulates PC activity during local navigation in exploration and goal-retrieval. The system builds and stores panoramic views from landmarks and compares them with current inputs. Mismatch-induced low levels of recognition trigger a vigilance burst, which favors either the recognition of an alternative place category or the creation of a new category. The sole implementation of visual ""What"" and ""Where"" information does not restrain the generality of the model since several modalities could cooperate to give rise to more robust place field spatial categories. Providing the system with real visual inputs automatically extracted from a natural environment demonstrates that interspecies differences in PC coding result more from characteristics of the visual systems than from differences in processing. Conversely, differences in PC multiple codes within a system result from different levels of processing and/or different degrees of multimodality. Each code could be used within different navigational strategies. A control system derived from the model allows a mobile agent to learn a few places and the associated actions required to reach a goal. Generalisation property of the model provides the capacity to join the goal from any place in the learned environment.",https://ieeexplore.ieee.org/document/857872/,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,27-27 July 2000,ieeexplore
10.1109/IJCNN.2016.7727320,A hybrid neuro-evolutive algorithm for neural network optimization,IEEE,Conferences,"This paper proposes a hybrid neuro-evolutive algorithm (NEA) that uses a compact indirect encoding scheme (IES) for representing its genotypes, moreover has the ability to reuse the genotypes and automatically build modular, hierarchical and recurrent neural networks. A genetic algorithm (GA) evolves a Lindenmayer System that is used to design the neural network's architecture. This basic neural codification confers scalability and search space reduction in relation to other methods. Furthermore, the system uses a parallel genome scan engine that increases both the implicit parallelism and convergence of the GA. The fitness function of the NEA rewards economical artificial neural networks (ANNs) that are easily implemented. The NEA was tested on five real-world classification datasets and three well-known datasets for time series forecasting (TSF). The results are statistically compared against established state-of-the-art algorithms and various forecasting methods (ADANN, ARIMA, UCM, and Forecast Pro). In most cases, our NEA outperformed the other methods, delivering the most accurate classification and time series forecasting with the least computational effort. These superior results are attributed to the improved effectiveness and efficiency of NEA in the decision-making process.",https://ieeexplore.ieee.org/document/7727320/,2016 International Joint Conference on Neural Networks (IJCNN),24-29 July 2016,ieeexplore
10.1109/ICECS.2004.1399645,A learnable self-feedback ratio-memory cellular nonlinear network (SRMCNN) for associative memory applications,IEEE,Conferences,"A self-feedback ratio-memory cellular nonlinear network (SRMCNN) with the B template and modified Hebbian learning algorithm to learn and recognize image patterns is proposed and analyzed. In this SRMCNN, the coefficients of space-variant B templates are determined from the exemplar patterns during the learning period. The weights are the ratio of the absolute summation of its neighborhood weights in the B templates stored in the associative memory. The SRMCNN can recognize the learned patterns with distinct white-black noise and output the correct patterns. Matlab and HSPICE software have simulated the operation of the proposed SRMCNN. It is shown that the 18/spl times/18 SRMCNN can successfully learn and recognize 8 incompletely noisy patterns. As compared to other learnable CNN as associative memories, the proposed SRMCNN could improve pattern learning and recognition capability. The architecture can be implemented in nano-CMOS technology for a giga-scale learning system in real-time applications.",https://ieeexplore.ieee.org/document/1399645/,"Proceedings of the 2004 11th IEEE International Conference on Electronics, Circuits and Systems, 2004. ICECS 2004.",15-15 Dec. 2004,ieeexplore
10.1109/IROS.2014.6942859,A machine learning approach for real-time reachability analysis,IEEE,Conferences,"Assessing reachability for a dynamical system, that is deciding whether a certain state is reachable from a given initial state within a given cost threshold, is a central concept in controls, robotics, and optimization. Direct approaches to assess reachability involve the solution to a two-point boundary value problem (2PBVP) between a pair of states. Alternative, indirect approaches involve the characterization of reachable sets as level sets of the value function of an appropriate optimal control problem. Both methods solve the problem accurately, but are computationally intensive and do no appear amenable to real-time implementation for all but the simplest cases. In this work, we leverage machine learning techniques to devise query-based algorithms for the approximate, yet real-time solution of the reachability problem. Specifically, we show that with a training set of pre-solved 2PBVP problems, one can accurately classify the cost-reachable sets of a differentially-constrained system using either (1) locally-weighted linear regression or (2) support vector machines. This novel, query-based approach is demonstrated on two systems: the Dubins car and a deep-space spacecraft. Classification errors on the order of 10% (and often significantly less) are achieved with average execution times on the order of milliseconds, representing 4 orders-of-magnitude improvement over exact methods. The proposed algorithms could find application in a variety of time-critical robotic applications, where the driving factor is computation time rather than optimality.",https://ieeexplore.ieee.org/document/6942859/,2014 IEEE/RSJ International Conference on Intelligent Robots and Systems,14-18 Sept. 2014,ieeexplore
10.1109/PACIIA.2009.5406447,A method for constructing simplified kernel model based on kernel-MSE,IEEE,Conferences,"In this paper, we derive an efficient nonlinear feature extraction method from naive Kernel Minimum Squared Error (KMSE) method. The most contribution of the derived method is its feature extraction procedure that is much more computationally efficient than naive KMSE. Differing from naive KMSE that exploits some linear combination of the total training patterns to express the discriminant vector in feature space, the derived method attempts to select out a small number of patterns (referred to as significant nodes in this paper) from the training set and exploits some linear combination of significant nodes to approximate to the discriminant vector in feature space. According to the following two principles, an algorithm for producing significant nodes is designed. The significant node set should well represent the whole training patterns, and each significant node should contribute much for the feature extraction result. Experimental results on several benchmark datasets illustrate our method can efficiently classify the real-world data with the high recognition accuracy.",https://ieeexplore.ieee.org/document/5406447/,2009 Asia-Pacific Conference on Computational Intelligence and Industrial Applications (PACIIA),28-29 Nov. 2009,ieeexplore
10.1109/IROS.1996.571054,A method for expecting the features of objects and enabling real-time vision processing,IEEE,Conferences,"This paper presents a mathematical analysis of image processing, algorithms designed according to the results of this analysis, and their implementation. We prove that the search of objects features can be accelerated without loss of precision by using an inhomogeneous density of the sensitive cells the parameters space is composed of. In other words, the visual analysis should be concentrated in the region of the features space around the expected object position. The improvement relative to an uniform cell density is quantified using a cost function corresponding to time and precision optimisation. We show that a Kohonen neural network can be used for efficient image processing, and simulate this strategy. We introduce a simpler algorithm for the case that the object positions are Gauss-distributed around the expected position. This algorithm has been implemented it on a robot guided by a vision system. The robot learned to process images efficiently during the manoeuvres and after that was able to track objects moving in a fast and unpredictable manner.",https://ieeexplore.ieee.org/document/571054/,Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems. IROS '96,8-8 Nov. 1996,ieeexplore
10.1109/ICICIP.2012.6391527,A method of information source selection for domain ontology construction,IEEE,Conferences,"A method of information source selection is critical to building a domain ontology with regard to ontology quality and efficiency. A good method can, on the one hand, enhance quality and efficiency of building ontology, and, on the other hand, generalize and develop the field of ontology. Considerable progress has been achieved in this respect; yet, a traditional method only takes concepts into account, and falls short in solving practical problems as well. AVRN (Abstract method, VSM Vector Space Model, Relation distance method and neural network) method is proposed in this paper. Firstly, characteristics of information sources are addresses by abstract analysis method, such as conceptuality, relativity and document predictability. Then, in order to compute document weights, these characteristics' weights are determined by improved Vector Space Model, based on ontology relation distance method and neural network. Finally, the document weights are obtained from the neural network with training samples that is outputted by the OnMaker software. Combined with a real document data set of Wetland Protection, the method is tested and a good order effect on the document selection for ontology construction is attained.",https://ieeexplore.ieee.org/document/6391527/,2012 Third International Conference on Intelligent Control and Information Processing,15-17 July 2012,ieeexplore
10.1109/RAST.2003.1303948,A model of autonomous control system for deep space missions,IEEE,Conferences,"The particularities of autonomous control system for deep space missions are described. Some models are analyzed and compared. The team approach is analyzed in details. The general formal model is based on the theory of communicating sequential processes (CSP) and is set of communicating processes. Methods for reconfiguration, verification and trace control are described.",https://ieeexplore.ieee.org/document/1303948/,"International Conference on Recent Advances in Space Technologies, 2003. RAST '03. Proceedings of",20-22 Nov. 2003,ieeexplore
10.1109/CAIA.1993.366623,A monitoring system with tolerance for real-time data problems,IEEE,Conferences,"Summary form only given. To overcome the problems related to real-time data, the authors have developed a set of methods which allow a monitoring system to tolerate bad data, or to automatically recover if false conclusions are made based on bad data. DESSY (DEcision Support System) is a rule-based system which monitors Space Shuttle telemetry and makes intelligent evaluations of system hardware through assessments of telemetry data patterns and transitions. A set of methods is proposed to handle data problems that are based on strategies used by the human experts. They include rule disabling, use of context and expectations to make assessments that tolerate some bad data, and graceful recovery through system correction when reliable data return. Self correction is an important feature for the real-time expert system. DESSY has been implemented in a commercial real-time expert system development environment. The system currently monitors approximately 80 pieces of Shuttle telemetry. The robustness of DESSY makes it capable of lengthy periods of uninterrupted use in real-time operations.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/366623/,Proceedings of 9th IEEE Conference on Artificial Intelligence for Applications,1-5 March 1993,ieeexplore
10.1109/CNNA.2010.5430245,A multi-FPGA distributed embedded system for the emulation of Multi-Layer CNNs in real time video applications,IEEE,Conferences,"This paper describes the design and the implementation of an embedded system based on multiple FPGAs that can be used to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more CNN layers. The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates.",https://ieeexplore.ieee.org/document/5430245/,2010 12th International Workshop on Cellular Nanoscale Networks and their Applications (CNNA 2010),3-5 Feb. 2010,ieeexplore
10.1109/MLSP.2015.7324376,A multi-layer discriminative framework for parking space detection,IEEE,Conferences,"In this paper, we proposed a new multi-layer discriminative framework for vacant parking space detection. From bottom to top, the framework consists of an image feature extraction layer, a patch classification layer, a weighted combination layer, and a status inference layer. In the feature extraction layer, the framework extracts lighting-invariant features to relieve the effects from lighting and shadow. In the patch classification layer, image patches are selected. In order To overcome perspective distortion, each patch was normalized. For different patch, we trained classifiers to recognize the occlusion patterns, which are treated as the middle-level feature of the parking status. In the weighted combination layer, three spaces are grouped as a unit to easily handle inter-object occlusion. Based on the middle-level features, a boosted space classifier was trained to determine the local status of a 3-space unit. In the status inference layer, we regarded these local status decisions as high-level evidences and inferred the final status of the parking lot. The results in an outdoor parking lot show our system can well handle inter-object occlusion and achieve robust vacant space detection under many environmental variations. A real-time system was also implemented to demonstrate its computing efficiency.",https://ieeexplore.ieee.org/document/7324376/,2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP),17-20 Sept. 2015,ieeexplore
10.1109/ICPR.1996.547236,A neural 3-D object recognition architecture using optimized Gabor filters,IEEE,Conferences,"We present an object recognition architecture based on feature extraction by Gabor filter kernels and feature classification by an artificial neural network. The parameters of the Gabor filters are optimized to the specific problem by minimizing an energy function. Such Gabor filters extract features that can be more easily classified by the neural network. Moreover, the feature space is low-dimensional so feature extraction does not require much computational effort. The object recognition system is implemented on a Datacube and works in real-time.",https://ieeexplore.ieee.org/document/547236/,Proceedings of 13th International Conference on Pattern Recognition,25-29 Aug. 1996,ieeexplore
10.23919/ECC.2003.7085103,A neural approximation to the explicit solution of constrained linear MPC,IEEE,Conferences,"The solution to constrained linear model predictive control (MPC) problems can be pre-computed off-line in an explicit form as a piecewise affine (PWA) state feedback law defined on polyhedral regions of the state space. Even though real-time optimization is avoided, implementation of the PWA state-feedback law may still require a significant amount of computation due to the problem of determining which polyhedral region the state lies in. In this paper, a neural network approach to this problem is investigated.",https://ieeexplore.ieee.org/document/7085103/,2003 European Control Conference (ECC),1-4 Sept. 2003,ieeexplore
10.1109/IJCNN.2002.1007503,A neural network based automatic road signs recognizer,IEEE,Conferences,"Automatic road sign recognition systems are aimed at detection and recognition of one or more road signs from real-world color images. In this research, road signs are detected and extracted from real world scenes on the basis of their color and shape features. A dynamic region growing technique is adopted to enhance color segmentation results obtained in the HSV color space. The technique is based on a dynamic threshold that reduces the effect of hue instability in real scenes due to external brightness variation. Classification is then performed on extracted candidate regions using multilayer perceptron neural networks. The obtained results show good detection and recognition rates of the entire system with real outdoor scenes, using several light conditions. Finally, the implementation of the neural layer on the Georgia Institute of Technology SIMD Pixel Processor is outlined.",https://ieeexplore.ieee.org/document/1007503/,Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290),12-17 May 2002,ieeexplore
10.1109/ICAIE.2010.5641447,A new K-NN query algorithm based on the symmetric virtual grid and dynamic circle,IEEE,Conferences,"The effective implementation of the spatial K-NN query is a particularly concerned problem in geographic information system. In the previous K-NN query algorithms, the tree index will use measurement distance and pruning strategies to reduce the required search space, the grid index will divide the space region into the grid cells with a certain size, search the neighbor region of the queried object through the location relationship among the grid cells, based on the previous K-NN algorithm study, a new K-NN query algorithm based on the symmetric virtual grid and dynamic circle is proposed in this paper, the algorithm will mainly union the data objects whose grid cells are symmetrical with the grid cell which the queried object is in and the queried object, and form multiple virtual grids, query in accordance with the order form inside to outside. In the searching process, use the queried object as the center, the largest K-NN distance in the virtual grid as the radius to make the dynamic circle so as to find the data objects to be searched first in the next virtual grid, the experiments show that the improved K-NN query algorithm can accurately and effectively find the neighbor objects meeting the conditions, and has a wide range of applications in practice.",https://ieeexplore.ieee.org/document/5641447/,2010 International Conference on Artificial Intelligence and Education (ICAIE),29-30 Oct. 2010,ieeexplore
10.1109/ICMLC.2003.1264549,A new algorithm for real-time ellipse detection,IEEE,Conferences,"We present a robust ellipse detector based on robust edge detection and Hough transform for real-time applications. The algorithm consists of three steps: we first perform edge detection, we than use a rule-based approach to compute edge convexity, and finally we decompose the parameter space of the Hough transform to achieve computational efficiency. We present our experiments, which show indeed that the proposed detector is able to detect elliptic features with an excellent accuracy under various image conditions and achieve a high speed at about 10 frames per second on a Pentium-III 500 MHz PC.",https://ieeexplore.ieee.org/document/1264549/,Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),5-5 Nov. 2003,ieeexplore
10.1109/ICDIM.2010.5664217,A new combined KSVM and KFD model for classification and recognition,IEEE,Conferences,"The Kernel Support Vector Machine (KSVM) is a powerful nonlinear classification methodology where, the Support Vectors (SVs) fully describe the decision surface by incorporating local information in the Kernel space. On the other hand, the Kernel Fisher Discriminant(KFD) is a non-linear classifier which has proven to be powerful and competitive to several state-of-the-art classifiers. This paper proposes a novel KSVM + KFD model which combines these two methods. This model can be viewed as an extension to the KSVM by incorporating 'global' characteristics of the data to estimate the decision boundary in the Kernel space. On the other hand, this new model could also be considered as an improvement to the KFD by incorporating the Support Vectors (local margin concept) into the KFD formulation. The KSVM + KFD model can be reduced to the classical KSVM model so that existing KSVM software can be used for easy implementation. An extensive comparison of the KSVM + KFD to the KFD, KSVM, Linear Discriminant Analysis (LDA), Linear Support Vector Machine (LSVM) and the combined LSVM and LDA, performed on real data sets, has shown the advantages of our proposed model. In particular, the experiments on face recognition have clearly shown the superiority of the KSVM + KFD over other methods.",https://ieeexplore.ieee.org/document/5664217/,2010 Fifth International Conference on Digital Information Management (ICDIM),5-8 July 2010,ieeexplore
10.1109/ICONIP.2002.1201950,A new kernel clustering algorithm,IEEE,Conferences,We propose a new kernel clustering algorithm. It estimates an in advance fixed number of vectors and margins in a feature space. Each pair of vector and margin defines a hyperplane in feature space and thus separates the data in two clusters. All the clusters together carry important information about the data set. The estimation in feature space is done implicitly by the use of a kernel. Therefore nonlinear clusters in the space of the data can be obtained. The clusters are estimated by optimizing a homogeneous quadratic program. We show how our algorithm can be efficiently implemented and we demonstrate the usefulness with a real world example.,https://ieeexplore.ieee.org/document/1201950/,"Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.",18-22 Nov. 2002,ieeexplore
10.1109/ICACI.2012.6463292,A novel Bayesian network structure learning algorithm based on Maximal Information Coefficient,IEEE,Conferences,"Greedy Equivalent Search (GES) is an effective algorithm for Bayesian network problem, which searches in the space of graph equivalence classes. However, original GES may easily fall into local optimization trap because of empty initial structure. In this paper, An improved GES method is prosposed. It firstly makes a draft of the real network, based on Maximum Information Coefficient (MIC) and conditional independence tests. After this step, many independent relations can be found. To ensure correctness, then this draft is used to be a seed structure of original GES algorithm. Numerical experiment on four standard networks shows that NEtoGS (the number of graph structure, which is equivalent to the God Standard network) has big improvement. Also, the total of learning time are greatly reduced. Therefore, our improved method can relatively quickly determine the structure graph with highest degree of data matching.",https://ieeexplore.ieee.org/document/6463292/,2012 IEEE Fifth International Conference on Advanced Computational Intelligence (ICACI),18-20 Oct. 2012,ieeexplore
10.1109/ICSMC.2003.1244605,A novel approach for person authentication and content-based tracking in videos using kernel methods and active appearance models,IEEE,Conferences,"A novel integration of methods for person authentication and tracking is proposed for real time security systems. The implementation of the idea for this real time implementation follows a three step procedure-face detection, recognition and content-based tracking. Instead of analyzing continuous videos we sample the frame based on a method derived from Shannon's information theory model. The Face-detector detects multi-viewed faces in a video using feature-based kernel methods in a reduced feature space obtained using ICA. The identified ""face regions"" are then passed on to the face recognition system which is based on Active Appearance Models (AAM). Once the subject is recognized, it can be tracked in the video using kernel based object tracking method. Several space reduction techniques have been used like ICA, PCA and skin-color segmentation.",https://ieeexplore.ieee.org/document/1244605/,"SMC'03 Conference Proceedings. 2003 IEEE International Conference on Systems, Man and Cybernetics. Conference Theme - System Security and Assurance (Cat. No.03CH37483)",8-8 Oct. 2003,ieeexplore
10.1109/GHTCE.2012.6490115,A novel hybrid human detection system,IEEE,Conferences,"This paper describes a novel system for hybrid human detection. The detection is based on combined human models, such as head-shoulder model, face model, skin color model, hair model, and histogram of oriented gradients (HOG) model. Adaboost technique is adapted to build a strong classifier to integrate multiple human models as listed above. Our system consists of three parts. First, moving regions are detected and segmented from the input video sequence using improved mixture of Gaussian models (GMM) [1]. We improve the conventional GMM method by eliminating smearing effect, caused by slowly moving objects, and accelerating the process of GMM method. Second, shadows are removed by adopting shadow pattern in HSV color space. Third, our hybrid human models are used to detect humans in the pre-segmented moving regions. A real-time system has been implemented to segment and classify objects. Examples of object classification (humans, running dogs, vehicles) are provided in this paper. Experimental results show that our hybrid models can effectively detect humans in video sequences from fixed monocular color or grayscale camera, and robust to observation noise, lighting changing, etc.",https://ieeexplore.ieee.org/document/6490115/,2012 IEEE Global High Tech Congress on Electronics,18-20 Nov. 2012,ieeexplore
10.1109/ICIEA.2010.5514756,A novel image representation and learning method using SVM for region-based image retrieval,IEEE,Conferences,"Support vector machines (SVM) is gaining a considerable attention as an approach to improvement performance of the content-based image retrieval (CBIR). Most SVM for CBIR rely on global feature, which length of the feature representation is fixed. However, region-based image retrieval (RBIR) use variable length representation, and common kernel utilize the inner product or l<sub>p</sub> norm in input space, they are infeasible for RBIR. In this paper, we present a SVM-Based relevance feedback techniques for region-based image retrieval including: (1) introducing an image segmentation algorithm and devising a compact and computational effective representation for the color content of a region of an image; (2) using earth mover's distance and hybrid feature including color, texture and shape as feature vector to match image; (3) developing a generalized SVM as a learning machines kernel for region-based image retrieval. Experimental results on a database of 1000 real images demonstrate the efficacy and robustness of the proposed method.",https://ieeexplore.ieee.org/document/5514756/,2010 5th IEEE Conference on Industrial Electronics and Applications,15-17 June 2010,ieeexplore
10.1109/IUCC-CIT-DSCI-SmartCNS55181.2021.00058,A novel random walk strategy for network embedding using community aware and node influence biasing,IEEE,Conferences,"Network embedding (NE), a central issue for graph deep learning research, intends to represent nodes, edges, sub-graphs, and other information into a low-dimensional vector space. A highly accurate NE algorithm benefits significantly in developing AI applications related to social platforms such as node classification and link prediction. Existing studies mainly focus on preserving the local and global information of nodes. The propagation of bias information is not fully considered according to the desires of the nodes. Incorporating community aware and node influence into the random walk process for NE issues are not addressed. This work proposes a highly accurate NE algorithm, namely Community Aware and Node Influence Biased Embedding (CNBE), to deal with the NE problem of social networks. CNBE recognizes the community information and the random walk with node influence bias to preserve the global structure of nodes and local neighborhood information of reserved nodes. By random walk sampling of predefined rules in CNBE, we can get the sequence of nodes that reserved the local and global neighborhood node information. Then the SkipGram model is applied to learn the low-dimensional vector representation of nodes. Experiments on three real-world networks indicate that our proposed algorithm method outperforms three state-of-the-art network embedding approaches.",https://ieeexplore.ieee.org/document/9719612/,2021 20th International Conference on Ubiquitous Computing and Communications (IUCC/CIT/DSCI/SmartCNS),20-22 Dec. 2021,ieeexplore
10.1109/PLASMA.2016.7534202,A parallel electrostatic solver for XOOPIC code,IEEE,Conferences,"Summary form only given. We report on the implementation of a parallel electrostatic field solver in the plasma particle simulation code XOOPIC1. Poisson's equation is solved by finite differences in real space, using iterative methods to solve a large sparse system of linear equations. Domain decomposition is extended to two dimensions, including particle passing and electrostatic field solution. We utilize Trilinos solvers for our implementation. In particular AZTEC, a set of iterative solvers, preconditioners and matrix-vector multiplication routines, and ML, a multigrid preconditioning package, from Sandia National Lab are used2. These libraries are optimized for a range of highly parallel platforms and are expected to be highly efficient and scalable to more than 1000 processors. Accuracy of the solver is compared to analytic results, and performance scaling is measured.",https://ieeexplore.ieee.org/document/7534202/,2016 IEEE International Conference on Plasma Science (ICOPS),19-23 June 2016,ieeexplore
10.1109/EMPDP.2001.905053,A parallel neurochip for neural networks implementing the reactive tabu search algorithm: application case studies,IEEE,Conferences,"In this work we present two different applications implemented on the neurocomputer Totem Nc3001 from Neuricam Inc. The goal of the experimentation is to test, on real problems, the performance of this powerful parallel unit consisting of 32 Digital Signal Processors (DSPs) and to evaluate its suitability to neural network applications. The first problem implemented is a typical classification algorithm in which the network recognises which points belong to different regions inside a 2D space. The second problem is more computationally heavy and consists of a network able to reproduce the eye movements, if properly stimulated. A comparison is reported between Matlab implementations or handwritten code run on workstations and the performance obtained from the Totem chip.",https://ieeexplore.ieee.org/document/905053/,Proceedings Ninth Euromicro Workshop on Parallel and Distributed Processing,7-9 Feb. 2001,ieeexplore
10.1109/EMPDP.1999.746650,A parallel processor for neural networks,IEEE,Conferences,"We present two different algorithms implemented through neural networks on a multiprocessor device. The parallel single-chip TI TMS32C80 Multimedia Video Processor (MVP). The goal of this experimentation is to test, on real problems, the performance of this powerful unit made up by one Master Risc Processor and by four Slave Digital Signal Processors (DSPs) and to evaluate its suitability to neural network applications. The first problem implemented is a typical classification algorithm in which the network recognises which points belong to different regions inside a 2D space. The second problem is more computationally heavy and consists of a network able to recognise 'handwritten' digits. The parallel version of the first algorithm, was also tested on a commercially available supercomputer.",https://ieeexplore.ieee.org/document/746650/,Proceedings of the Seventh Euromicro Workshop on Parallel and Distributed Processing. PDP'99,3-5 Feb. 1999,ieeexplore
10.1109/CCAAW.2017.8001611,A pattern matching approach to map cognitive domain ontologies to the IBM TrueNorth Neurosynaptic System,IEEE,Conferences,"Cognitive agents are typically utilized in autonomous systems for automated decision making. These systems interact in real time with their environments and are generally heavily power constrained. Thus there is a strong need for a real time agent running on a low power computing platform for these systems. This paper examines how some of the components of a cognitive agent can be mapped onto the IBM TrueNorth neurosynaptic system to achieve real time performance at low power. The agent examined is the Cognitively Enhanced Complex Event Processing (CECEP) architecture. Although it is geared towards autonomous decision making, CECEP also has applications in operations research, intelligence analysis, and data mining. One of the key components of CECEP is the Cognitive Domain Ontology (CDO), used for mining decisions from a large knowledge repository based on situational inputs and constraints. As CDOs are the most time and power consuming part of CECEP, we implemented them on the TrueNorth processor by mapping the solution space of CDOs into a pattern matching form.",https://ieeexplore.ieee.org/document/8001611/,2017 Cognitive Communications for Aerospace Applications Workshop (CCAA),27-28 June 2017,ieeexplore
10.1109/IJCNN.2013.6706752,A perception evolution network for unsupervised fast incremental learning,IEEE,Conferences,"A perception evolution network (PEN) is proposed for unsupervised fast incremental (or on-line) learning in this paper. The network has two layers: the perception layer receives the external data from the reality environment and the knowledge layer learns and records the knowledge contained in the data from the perception layer in incremental and self-organizing way. In PEN model, new input channels can be added to the perception layer freely during learning. When the perception layer gets some new input channels, the network will create corresponding data transmission channels to the knowledge layer. The prior learned knowledge stored in the knowledge layer will be expanded to a higher-dimensional space which contains the attributes of the new input channels. For incremental (or on-line) learning, PEN can automatically obtain suitable prototypes and find the topology structure of the learning data without any priori knowledge. The noise processing mechanism guarantees PEN can work in the complex real-world (noisy) environment. The experiments for both artificial data set and real-world data set show that the proposed method is effective.",https://ieeexplore.ieee.org/document/6706752/,The 2013 International Joint Conference on Neural Networks (IJCNN),4-9 Aug. 2013,ieeexplore
10.1109/ICCAS.2015.7364829,A precise position control of robot manipulator with eight joints,IEEE,Conferences,"We describe a new approach to the design and real-time implementation of an adaptive controller for robotic manipulator based on digital signal processors in this paper. The Texas Instruments DSPs(TMS320C80) chips are used in implementing real-time adaptive control algorithms to provide enhanced motion control performance for dual-arm robotic manipulators. In the proposed scheme, adaptation laws are derived from model reference adaptive control principle based on the improved direct Lyapunov method. The proposed adaptive controller consists of an adaptive feed-forward and feedback controller and time-varying auxiliary controller elements. The proposed control scheme is simple in structure, fast in computation, and suitable for real-time control. Moreover, this scheme does not require any accurate dynamic modeling, nor values of manipulator parameters and payload. Performance of the proposed adaptive controller is illustrated by simulation and experimental results for robot manipulator consisting of dual arm with four degrees of freedom at the joint space and cartesian space.",https://ieeexplore.ieee.org/document/7364829/,"2015 15th International Conference on Control, Automation and Systems (ICCAS)",13-16 Oct. 2015,ieeexplore
10.1109/IECON.2005.1569052,A real-time neuro-computing three-dimensional space vector algorithm for three-phase four-leg converters,IEEE,Conferences,"Four-leg voltage source converters have successfully been used to nullify the zero-sequence current generated by unbalanced or nonlinear loads. This paper introduces an on-line, simple, intelligent, and computationally efficient neuro-computing classification algorithm for the implementation of three-dimensional space vector modulation (SVM) on four-leg voltage-source inverters. The proposed technique uses the concepts of counter propagation neural networks (CPN) for prism identification, and employs a nonlinear classifier network for tetrahedron identification. Nonlinear function approximations and bulky look up tables are successfully avoided, and exact positioning of the switching instants is obtained. Analytical analysis and simulations on a four-leg voltage-source converter validate the proposed scheme",https://ieeexplore.ieee.org/document/1569052/,"31st Annual Conference of IEEE Industrial Electronics Society, 2005. IECON 2005.",6-10 Nov. 2005,ieeexplore
10.1109/IROS.1991.174683,A realtime trajectory planning method for manipulators based on kinetic energy,IEEE,Conferences,"Most conventional trajectory planners are based on the trapezoidal pattern of the end-effector velocity on the desired path in a task space. This method can be implemented in real time. However, the motion planned by this method usually leads to a very slow motion, because it is difficult to take into consideration a constraint on the magnitude of joint velocities or joint torques. The authors propose a new trajectory planner for three DOF manipulators based on the kinetic energy of manipulators. The supplied electric power for the manipulators mainly depends on the differentiation of the kinetic energy. Therefore, the proposed method can take into consideration the constraints on the magnitude of the supplied electric power and joint velocities, i.e. one can realize a 'high speed motion' under the constraints on the supplied electric power and joint velocities. Also, the method proposed does not require much calculation and can be implemented in real time.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/174683/,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,3-5 Nov. 1991,ieeexplore
10.1109/SICE.2002.1195611,A reinforcement learning using adaptive state space construction strategy for real autonomous mobile robots,IEEE,Conferences,"In the recent robotics, much attention has been focused on utilizing reinforcement learning for designing robot controllers. However, there still exists difficulties, one of them is well known as state space explosion problem. As the state space for a learning system becomes continuous and high dimensional, its combinational state space exponentially explodes and the learning process is time consuming. In this paper, we propose an adaptive state space recruitment strategy for reinforcement learning, which enables the system to divide state space gradually according to task complexity and progress of learning. Some simulation results and real robot implementation show the validity of the method.",https://ieeexplore.ieee.org/document/1195611/,Proceedings of the 41st SICE Annual Conference. SICE 2002.,5-7 Aug. 2002,ieeexplore
10.1109/IRDS.2002.1041504,A reinforcement learning with adaptive state space recruitment strategy for real autonomous mobile robots,IEEE,Conferences,"In the recent robotics, much attention has been focused on utilizing reinforcement learning for designing robot controllers. However, there still exists difficulties, one of them is well known as state space explosion problem. As the state space for learning system becomes continuous and high dimensional, the learning process results in time-consuming since its combinational states explodes exponentially. In order to adopt reinforcement learning for such complicated systems, it should be taken not only ""adaptability"" but ""computational efficiencies"" into account. In the paper, we propose an adaptive state space recruitment strategy for reinforcement learning, which enables the system to divide state space gradually according to task complexity and progress of learning. Some simulation results and real robot implementation show the validity of the method.",https://ieeexplore.ieee.org/document/1041504/,IEEE/RSJ International Conference on Intelligent Robots and Systems,30 Sept.-4 Oct. 2002,ieeexplore
10.1109/ICMLC.2008.4621002,A rendering algorithm based on ray-casting for medical images,IEEE,Conferences,"Ray-casting algorithm is usually used in medical volume visualization, but there are a lot of limitations inherent to this algorithm, like high computational demand, low quality images, or a fixed classification. In contrast, real-time high quality volume rendering algorithms are still a challenge nowadays. We introduce here an efficient and accurate volume rendering algorithm for medical images, and this algorithm can achieve up to 2.5 fps on PCs for 5125124822 Byte volume. The algorithm classifies volume data into the foreground voxels and background voxels, and the former voxels are resampled using the similar LOD (Layer of Detail) technique, then optical attribute of the resampling points is determined by a new method. This method is reasonably associated with the distance between the object and the viewpoint, as well as the object and the light source. In addition, the background voxels are displayed by the accelerated method based on space leaping resampling. The results show that interactive volume rendering can be implemented for most medical volume data on PCs. Meanwhile, tissues or organs can be displayed clearly, which is more coincident with the human vision.",https://ieeexplore.ieee.org/document/4621002/,2008 International Conference on Machine Learning and Cybernetics,12-15 July 2008,ieeexplore
10.1109/ICISCAE48440.2019.221702,A research on face cognition method with deep ensemble learning and feedback mechanism,IEEE,Conferences,"Face recognition technology is an important research field for deep learning. In order to overcome the shortcomings of traditional open-loop face cognition mode and deep neural network structure, and to imitate human cognition model with real-time evaluation of cognitive results and self-optimized regulation of feature space and classification criteria, drawing on the theory of closed-loop control theory, this paper explores a face intelligent cognition method with deep ensemble learning and feedback mechanism. Firstly, based on the DEEPID neural network, a multi-layer feature space of face images from the global to the local is established. Secondly, based on feature separability evaluation and variable precision rough set theory, a face cognition decision information system model with dynamic feature representation is established from the perspective of information theory, to compact the multilayer feature space. Thirdly, the ensemble random vector functional-link net is used to construct the classification criterion for the compacted multi-layer feature space. Finally, the entropy measure index of face recognition results is constructed to provide a quantitative basis for the self-optimization adjustment mechanism of face feature space and classification criteria. The experimental results show that the proposed method can effectively improve the recognition rate of face images compared with the existing methods.",https://ieeexplore.ieee.org/document/9075735/,2019 2nd International Conference on Information Systems and Computer Aided Education (ICISCAE),28-30 Sept. 2019,ieeexplore
10.1109/ICPP.2000.876164,A scalable parallel subspace clustering algorithm for massive data sets,IEEE,Conferences,"Clustering is a data mining problem which finds dense regions in a sparse multi-dimensional data set. The attribute values and ranges of these regions characterize the clusters. Clustering algorithms need to scale with the data base size and also with the large dimensionality of the data set. Further, these algorithms need to explore the embedded clusters in a subspace of a high dimensional space. However the time complexity of the algorithm to explore clusters in subspaces is exponential in the dimensionality of the data and is thus extremely compute intensive. Thus, parallelization is the choice for discovering clusters for large data sets. In this paper we present a scalable parallel subspace clustering algorithm which has both data and task parallelism embedded in it. We also formulate the technique of adaptive grids and present a truly unsupervised clustering algorithm requiring no user inputs. Our implementation shows near linear speedups with negligible communication overheads. The use of adaptive grids results in two orders of magnitude improvement in the computation time of our serial algorithm over current methods with much better quality of clustering. Performance results on both real and synthetic data sets with very large number of dimensions on a 16 node IBM SP2 demonstrate our algorithm to be a practical and scalable clustering technique.",https://ieeexplore.ieee.org/document/876164/,Proceedings 2000 International Conference on Parallel Processing,21-24 Aug. 2000,ieeexplore
10.1109/ROMAN.1995.531969,A shape knowledge representation scheme and its application on a multi-modal interface for a virtual space teleconferencing system,IEEE,Conferences,"This paper introduces recent advances in research on 3D shape ontologies. The purpose of shape ontologies is to formalize the characteristics of 3D geometric primitives so as to bridge the gap between knowledge-level representation of shapes in the human mind and their low-level computer graphics representations. In the formalizations described in this paper, an intermediate symbolic representation in the form of implicit, parametric functions is used for this purpose. With this intermediate level representation of shape knowledge it becomes possible to interactively change the ""meaning"" of a primitive by transforming its parametric space according to information extracted from natural language and/or hand gestures. Furthermore, ""meaningful"" hierarchical combinations of shapes into complex shapes also becomes possible because they can also be parameterized and thus formalized.",https://ieeexplore.ieee.org/document/531969/,Proceedings 4th IEEE International Workshop on Robot and Human Communication,5-7 July 1995,ieeexplore
10.1109/CEC.2005.1555005,A space saving digital VLSI evolutionary engine for CTRNN-EH devices,IEEE,Conferences,"Continuous time recurrent neural network - evolvable hardware (CTRNN-EH) control devices are composed of an analog continuous time recurrent neural network (CTRNN) with an onboard evolutionary algorithm (EA) engine that evolves the parameters of the neural network. These control devices have been demonstrated to be useful in a variety of real time control applications and are amenable to mixed-signal VLSI implementation for the control applications under stringent size and power constraints. Unlike the CTRNNs, which are analog in nature, the EA engine has to be implemented using digital VLSI techniques. Because these techniques do not offer the advantages of small area and power directly, the task of adhering to size and power constraints is challenging and must be accomplished at the algorithmic level. In this paper, the authors discussed the aforementioned issues in detail and also propose a space-saving digital EA engine for the CTRNN-EH device. The EA engine has been modeled in Verilog HDL. The synthesis results are presented and the functionality of the EA is demonstrated on a small test problem.",https://ieeexplore.ieee.org/document/1555005/,2005 IEEE Congress on Evolutionary Computation,2-5 Sept. 2005,ieeexplore
10.1109/FPGA.1993.279462,A stochastic neural architecture that exploits dynamically reconfigurable FPGAs,IEEE,Conferences,"The authors present an expandable digital architecture that provides an efficient real time implementation platform for large neural networks. The architecture makes heavy use of the techniques of bit serial stochastic computing to carry out the large number of required parallel synaptic calculations. In this design all real valued quantities are encoded on to stochastic bit streams in which the '1' density is proportional to the given quantity. The actual digital circuitry is simple and highly regular thus allowing very efficient space usage of fine grained FPGAs. Another feature of the design is that the large number of weights required by a neural network are generated by circuitry tailored to each of their specific values, thus saving valuable cells. Whenever one of these values is required to change, the appropriate circuitry must be dynamically reconfigured. This may always be achieved in a fixed and minimum number of cells for a given bit stream resolution.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/279462/,[1993] Proceedings IEEE Workshop on FPGAs for Custom Computing Machines,5-7 April 1993,ieeexplore
10.1109/ROMAN.1995.531967,A study of real time facial expression detection for virtual space teleconferencing,IEEE,Conferences,"A new method for real-time detection of facial expressions from time-sequential images is proposed. The proposed method does not need the tape marks that were pasted to the face for detecting expressions in real-time in the current implementation for the virtual space teleconferencing. In the proposed method, four windows are applied to the four areas in the face image: the left and right eyes, mouth and forehead. Each window is divided into blocks that consist of 8 by 8 pixels. The discrete cosine transform (DCT) is applied to each block, and the feature vector of each window is obtained from taking the summations of the DCT energies in the horizontal, vertical and diagonal directions. By a conversion table, the feature vectors are related to real 3D movements in the face. Experiment show some promising results for accurate expression detection and for the realization of real-time hardware implementation of the proposed method.",https://ieeexplore.ieee.org/document/531967/,Proceedings 4th IEEE International Workshop on Robot and Human Communication,5-7 July 1995,ieeexplore
10.1109/EMBC.2013.6610156,A tensorial approach to access cognitive workload related to mental arithmetic from EEG functional connectivity estimates,IEEE,Conferences,"The association of functional connectivity patterns with particular cognitive tasks has long been a topic of interest in neuroscience, e.g., studies of functional connectivity have demonstrated its potential use for decoding various brain states. However, the high-dimensionality of the pairwise functional connectivity limits its usefulness in some real-time applications. In the present study, the methodology of tensor subspace analysis (TSA) is used to reduce the initial high-dimensionality of the pairwise coupling in the original functional connectivity network to a space of condensed descriptive power, which would significantly decrease the computational cost and facilitate the differentiation of brain states. We assess the feasibility of the proposed method on EEG recordings when the subject was performing mental arithmetic task which differ only in the difficulty level (easy: 1-digit addition v.s. 3-digit additions). Two different cortical connective networks were detected, and by comparing the functional connectivity networks in different work states, it was found that the task-difficulty is best reflected in the connectivity structure of sub-graphs extending over parietooccipital sites. Incorporating this data-driven information within original TSA methodology, we succeeded in predicting the difficulty level from connectivity patterns in an efficient way that can be implemented so as to work in real-time.",https://ieeexplore.ieee.org/document/6610156/,2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),3-7 July 2013,ieeexplore
10.1109/ICONIP.1999.843962,A visualised software library: a software self-organising map,IEEE,Conferences,"This paper presents an approach to self-structuring software libraries. The authors developed a representation scheme to construct a feature space over a collection of software assets. The feature space is self-organised by an unsupervised neural network. The self-organising map (SOM). The high-dimensional feature space is then protected onto the two-dimensional SOM output layer that makes the most important distance relationships among the software assets geometrically explicit. This approach has been applied to a real case, where a visualised library containing a set of UNIX commands is constructed. The results obtained from a retrieval experiment based on the library demonstrated great potential.",https://ieeexplore.ieee.org/document/843962/,ICONIP'99. ANZIIS'99 & ANNES'99 & ACNN'99. 6th International Conference on Neural Information Processing. Proceedings (Cat. No.99EX378),16-20 Nov. 1999,ieeexplore
10.1109/ICDM51629.2021.00021,ACE-HGNN: Adaptive Curvature Exploration Hyperbolic Graph Neural Network,IEEE,Conferences,"Graph Neural Networks (GNNs) have been widely studied in various graph data mining tasks. Most existing GNNs embed graph data into Euclidean space and thus are less effective to capture the ubiquitous hierarchical structures in real-world networks. Hyperbolic Graph Neural Networks (HGNNs) extend GNNs to hyperbolic space and thus are more effective to capture the hierarchical structures of graphs in node representation learning. In hyperbolic geometry, the graph hierarchical structure can be reflected by the curvatures of the hyperbolic space, and different curvatures can model different hierarchical structures of a graph. However, most existing HGNNs manually set the curvature to a fixed value for simplicity, which achieves a suboptimal performance of graph learning due to the complex and diverse hierarchical structures of the graphs. To resolve this problem, we propose an Adaptive Curvature Exploration Hyperbolic Graph Neural Network named ACE-HGNN to adaptively learn the optimal curvature according to the input graph and downstream tasks. Specifically, ACE-HGNN exploits a multi-agent reinforcement learning framework and contains two agents, ACE-Agent and HGNN-Agent for learning the curvature and node representations, respectively. The two agents are updated by a Nash Q-leaning algorithm collaboratively, seeking the optimal hyperbolic space indexed by the curvature. Extensive experiments on multiple real-world graph datasets demonstrate a significant and consistent performance improvement in model quality with competitive performance and good generalization ability.",https://ieeexplore.ieee.org/document/9679192/,2021 IEEE International Conference on Data Mining (ICDM),7-10 Dec. 2021,ieeexplore
10.1109/ICECS.1999.813244,AEDS: a novel technique for detecting DNA bands in autoradiograph images,IEEE,Conferences,"The discipline of image processing is haunted by many problems, such as poor edge detection in low contrast images, speed of recognition, high computational cost and impracticality. Thus, new measures are required to solve these problems. Scale space analysis is an efficient solution to the edge detection of objects in low to high contrast images. However, this approach is time consuming and computationally expensive. The parallel processing properties of a neural network provide an ideal solution to managing the large amounts of data processed in image analysis, however their application to multiscale analysis is still in its infancy. This paper reports on a new approach to solving the aforementioned problems. The novel idea is based on combining neural network arbitration and scale space analysis to automatically select one optimum scale for the entire image at which scale space edge detection can be applied. This new approach to edge detection is formalised in the automatic edge detection scheme (AEDS). The AEDS is implemented on a real-life problem namely, the detection of bands within DNA autoradiograph images.",https://ieeexplore.ieee.org/document/813244/,"ICECS'99. Proceedings of ICECS '99. 6th IEEE International Conference on Electronics, Circuits and Systems (Cat. No.99EX357)",5-8 Sept. 1999,ieeexplore
10.1109/ISPASS51385.2021.00014,AIBench Training: Balanced Industry-Standard AI Training Benchmarking,IEEE,Conferences,"Earlier-stage evaluations of a new AI architecture/system need affordable AI benchmarks. Only using a few AI component benchmarks like MLPerf alone in the other stages may lead to misleading conclusions. Moreover, the learning dynamics are not well understood, and the benchmarks' shelf-life is short. This paper proposes a balanced benchmarking methodology. We use real-world benchmarks to cover the factors space that impacts the learning dynamics to the most considerable extent. After performing an exhaustive survey on Internet service AI domains, we identify and implement nineteen representative AI tasks with state-of-the-art models. For repeatable performance ranking (RPR subset) and workload characterization (WC subset), we keep two subsets to a minimum for affordability. We contribute by far the most comprehensive AI training benchmark suite. The evaluations show: (1) AIBench Training (v1.1) outperforms MLPerf Training (v0.7) in terms of diversity and representativeness of model complexity, computational cost, convergent rate, computation, and memory access patterns, and hotspot functions; (2) Against the AIBench full benchmarks, its RPR subset shortens the benchmarking cost by 64%, while maintaining the primary workload characteristics; (3) The performance ranking shows the single-purpose AI accelerator like TPU with the optimized TensorFlow framework performs better than that of GPUs while losing the latter's general support for various AI models. The specification, source code, and performance numbers are available from the AIBench homepage https://www.benchcouncil.org/aibench-training/index.html.",https://ieeexplore.ieee.org/document/9408170/,2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),28-30 March 2021,ieeexplore
10.1109/CloudCom.2019.00037,APEX: Adaptive Ext4 File System for Enhanced Data Recoverability in Edge Devices,IEEE,Conferences,"Recently Edge Computing paradigm has gained significant popularity both in industry and academia. With its increased usage in real-life scenarios, security, privacy and integrity of data in such environments have become critical. Malicious deletion of mission-critical data due to ransomware, trojans and viruses has been a huge menace and recovering such lost data is an active field of research. As most of Edge computing devices have compute and storage limitations, difficult constraints arise in providing an optimal scheme for data protection. These devices mostly use Linux/Unix based operating systems. Hence, this work focuses on extending the Ext4 file system to APEX (Adaptive Ext4): a file system based on novel on-the-fly learning model that provides an Adaptive Recover-ability Aware file allocation platform for efficient post-deletion data recovery and therefore maintaining data integrity. Our recovery model and its lightweight implementation allow significant improvement in recover-ability of lost data with lower compute, space, time, and cost overheads compared to other methods. We demonstrate the effectiveness of APEX through a case study of overwriting surveillance videos by CryPy malware on Raspberry-Pi based Edge deployment and show 678% and 32% higher recovery than Ext4 and current state-of-the-art File Systems. We also evaluate the overhead characteristics and experimentally show that they are lower than other related works.",https://ieeexplore.ieee.org/document/8968863/,2019 IEEE International Conference on Cloud Computing Technology and Science (CloudCom),11-13 Dec. 2019,ieeexplore
10.1109/IICAIET49801.2020.9257862,ASIC Layout Design-Space Exploration of Pan-and-Tompkins Pre-Processing Algorithm for High Efficiency Electrocardiogram Monitor,IEEE,Conferences,"Cardiovascular diseases (CVDs) is the leading cause of the death globally. Ambulatory Electrocardiogram (ECG) and mobile monitoring is very important for early heart disease detection and prevention, but its measurement normally contains various types of noise which affect the analysis accuracy. Moreover, long hour ECG monitoring requires an efficient architecture to support real-time processing and low power consumption. This paper presents an application specific integrated circuit (ASIC) design of Pan-and-Tompkins ECG pre-processing algorithm which aims to remove several unwanted noise to increase analysis accuracy. The complete design flow covers high-level algorithm modelling in Matlab, followed by synthesizable design at Register Transfer Level (RTL) until logic synthesis, physical synthesis and static timing analysis to produce VLSI layout. Several power optimization techniques as well as different ASIC process technology libraries in terms of SilTerra's 180nm CMOS Logic Generic Library (CL180G) and Synopsys 32nm Generic Library (SAED32) are deployed for design-space exploration to study the design trade-off in terms of power consumption, timing performance, and the logic area usage. Results show that the clock gating technique is able to reduce 32.4% of dynamic power in design using CL180G generic library, whereas the integration of several power optimization techniques using SAED32 generic library is able to reduce 43.82% of dynamic power, 91.21% of leakage power and 91.25% of total power.",https://ieeexplore.ieee.org/document/9257862/,2020 IEEE 2nd International Conference on Artificial Intelligence in Engineering and Technology (IICAIET),26-27 Sept. 2020,ieeexplore
10.1109/ICDSC.2009.5289359,Abnormal motion detection in a real-time smart camera system,IEEE,Conferences,"This paper discusses a method for abnormal motion detection and its real-time implementation on a smart camera. Abnormal motion detection is a surveillance technique that only allows unfamiliar motion patterns to result in alarms. Our approach has two phases. First, normal motion is detected and the motion paths are trained, building up a model of normal behaviour. Feed-forward neural networks are here used for learning. Second, abnormal motion is detected by comparing the current observed motion to the stored model. A complete demonstration system is implemented to detect abnormal paths of persons moving in an indoor space. As platform we used a wireless smart camera system containing an SIMD (single-instruction multiple-data) processor for real-time detection of moving persons and an 8051 microcontroller for implementing the neural network. The 8051 also functions as camera host to broadcast abnormal events using ZigBee to a main network system.",https://ieeexplore.ieee.org/document/5289359/,2009 Third ACM/IEEE International Conference on Distributed Smart Cameras (ICDSC),30 Aug.-2 Sept. 2009,ieeexplore
10.1109/RECONFIG.2017.8279771,Accelerating low rank matrix completion on FPGA,IEEE,Conferences,"Low Rank Matrix Completion (LRMC) is widely used in the analysis of incomplete datasets. In this paper, we propose a novel FPGA-based accelerator to speedup a matrix-factorization-based LRMC algorithm that uses stochastic gradient descent. The accelerator is a multi-pipelined architecture with parallel pipelines processing distinct data from a shared on-chip buffer. We propose two distinct on-chip buffer architectures based on a design-space exploration of the performance tradeoffs offered by two competing design methodologies: memory-efficiency versus concurrent conflict-free accesses. Our first design (i.e., memory-efficient design) organizes the buffer into banks and maximally utilizes available on-chip memory for matrix chunk processing without requiring complex address translation tables for on-chip addressing; however, it could incur bank conflicts when concurrent accesses to the same bank occur. The second design (i.e., bank-conflict-free design) exploits parallel multiport memory access and completely eliminates bank conflicts by duplicating the stored data; however, it has much higher on-chip RAM consumption. Intuitively, design one enables (slower) acceleration of (larger) chunks of the input matrix whereas design two enables (faster) processing of (smaller) matrix chunks but requires more iterations for processing the complete matrix. We propose a simple but efficient partitioning approach for supporting large input matrices that do not fit in the on-chip memory of FPGA. We also develop algorithmic optimizations based on matching to reduce data dependencies for parallel pipeline execution. We implement our designs on a state-of-the-art UltraScale+ FPGA device. We use real-life datasets for the evaluation and compare these two designs by varying the number of pipelines. The data dependency optimization results in at least 21.6 x data dependency reduction and improves the execution time by up to 66.3 x compared with non-optimized baseline designs. The memory-efficient design is also shown to be more scalable than the bank-conflict-free design. Compared with the state-of-the-art multi-core implementation and GPU implementation, the bank-conflict-free design achieves 5.4 x and 5.2 x speedup, respectively; the memory-efficient design achieves 16.7 x and 16.2 x speedup, respectively.",https://ieeexplore.ieee.org/document/8279771/,2017 International Conference on ReConFigurable Computing and FPGAs (ReConFig),4-6 Dec. 2017,ieeexplore
10.1109/VAST.2007.4388990,Activity Analysis Using Spatio-Temporal Trajectory Volumes in Surveillance Applications,IEEE,Conferences,"In this paper, we present a system to analyze activities and detect anomalies in a surveillance application, which exploits the intuition and experience of security and surveillance experts through an easy- to-use visual feedback loop. The multi-scale and location specific nature of behavior patterns in space and time is captured using a wavelet-based feature descriptor. The system learns the fundamental descriptions of the behavior patterns in a semi-supervised fashion by the higher order singular value decomposition of the space described by the training data. This training process is guided and refined by the users in an intuitive fashion. Anomalies are detected by projecting the test data into this multi-linear space and are visualized by the system to direct the attention of the user to potential problem spots. We tested our system on real-world surveillance data, and it satisfied the security concerns of the environment.",https://ieeexplore.ieee.org/document/4388990/,2007 IEEE Symposium on Visual Analytics Science and Technology,30 Oct.-1 Nov. 2007,ieeexplore
10.1109/AFGR.2004.1301573,AdaBoost with totally corrective updates for fast face detection,IEEE,Conferences,"An extension of the AdaBoost learning algorithm is proposed and brought to bear on the face detection problem. In each weak classifier selection cycle, the novel totally corrective algorithm reduces aggressively the upper bound on the training error by correcting coefficients of all weak classifiers. The correction steps are proven to lower the upper bound on the error without increasing computational complexity of the resulting detector. We show experimentally that for the face detection problem, where large training sets are available, the technique does not overfit. A cascaded face detector of the Viola-Jones type is built using AdaBoost with the totally corrective update. The same detection and false positive rates are achieved with a detector that is 20% faster and consists of only a quarter of the weak classifiers needed for a classifier trained by standard AdaBoost. The latter property facilitates hardware implementation, the former opens scope for the increease in the search space, e.g the range of scales at which faces are sought.",https://ieeexplore.ieee.org/document/1301573/,"Sixth IEEE International Conference on Automatic Face and Gesture Recognition, 2004. Proceedings.",19-19 May 2004,ieeexplore
10.1109/ICSESS.2018.8663793,Adaptation of Alpha-Beta Search Algorithm for Real-Time Strategy Game,IEEE,Conferences,"Real-time strategy(RTS)[1] game is known for its large action space, rapid response speed, and subtle game scenarios, and it has proven to be a very challenging application area in artificial intelligence research. In the RTS game, the search time increases exponentially as the number of units increases. So it is not possible to achieve a complete search for the game tree in the strict circumstances of time constraints. In this paper, we propose a new tree model, and named T-AlphaBeta search algorithm. It can reduce the interval of Alpha and Beta appropriately by use the dynamic T value in the AlphaBeta[2] [3] algorithm. It can return to the better results at the same time of fast search, so that there is a balance between search depth and search time. We use SparCraft as our test platform, which shows that our approach can achieve better results.",https://ieeexplore.ieee.org/document/8663793/,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),23-25 Nov. 2018,ieeexplore
10.1109/CoDIT.2018.8394934,Adaptive Dynamic Programming Based Motion Control of Autonomous Underwater Vehicles,IEEE,Conferences,"In this paper, Adaptive Dynamic Programming (ADP) technique is utilized to achieve optimal motion control of Autonomous Underwater Vehicle (AUV) System. The paper proposes a model-free based method that takes into consideration the actuator input and obstacle position while tracing an optimal path. The concept of machine learning enables to develop a path-planner which aims to avoid collisions with static obstacles. The ADP approach is realized to approximate the solution of the cost functional for optimization purpose by which the positions of the locally situated obstacles need not be priori-known until they are within a designed approximation safety envelope. The methodology is implemented to achieve the path-planning objective using dynamic programming technique. The Least-squares policy method serves as a recursive algorithm to approximate the value function for the domain, providing an approach for the finite space discrete control system. The concept behind the design of an obstacle-free path finder is to generate an optimal action that minimizes the local cost, defined by a functional, under constrained optimization. The most advantageous value function is described by the Hamilton Jacobi Bellman (HJB) equation, that is impractical to solve using analytical methods. To overcome the complex calculations subject to HJB, a method based on Reinforcement Learning (RL), called ADP is implemented. This paper outlines the concept of machine learning to realize a real time obstacle avoidance system.",https://ieeexplore.ieee.org/document/8394934/,"2018 5th International Conference on Control, Decision and Information Technologies (CoDIT)",10-13 April 2018,ieeexplore
10.1109/MICAI.2007.26,Adaptive Hierarchical Fuzzy CMAC Controller with Stable Learning Algorithm for Unknown Nonlinear Systems,IEEE,Conferences,"In this paper, adaptive hierarchical fuzzy CMAC neural network controller (HFCMAC), for a certain class of nonlinear dynamical system is presented. The main advantages of adaptive HFCMAC control are: Better performance of the controller because adaptive HFCMAC can adjust itself to the changing enviroment and can be implemented in real time applications. The proposed method provides a simple control architecture that merges hierarchical structure, CMAC neural network and fuzzy logic. The input space dimension in CMAC is a time-consuming task especially when the number of inputs is huge this would be overload the memory and make the neuro-fuzzy system very hard to implement. This is can be simplified using a number of low-dimensional fuzzy CMAC in a hierarchical form. A new adaptation law is obtained for the method proposed, the overall adaptive scheme guarantees the global stability of the resulting closed-loop system in the sense that all signals involved are uniformly bounded. Simulation results for its applications to one example is presented to demonstrate the performance of the proposed methodology.",https://ieeexplore.ieee.org/document/4659319/,"2007 Sixth Mexican International Conference on Artificial Intelligence, Special Session (MICAI)",4-10 Nov. 2007,ieeexplore
10.1109/CIMCA.2008.66,Adaptive Local Learning Soft Sensor for Inferential Control Support,IEEE,Conferences,"In this work we focused on the development of an adaptive Soft Sensor which may be deployed in a real-life environment, for example as inferential control support. To be able to do this, the Soft Sensor must fulfil certain constraints like being able to deal with data impurities or to adapt itself with changing data. The task is approached by training a set of models with limited validity in the data space and by proposing a statistically-based technique for the combination of the local models. The combination weights are related to the estimated performance of the local models in the neighbourhood of the processed data sample. The performance and other benefits of the proposed Soft Sensor are demonstrated in terms of a case study where the model deals with raw industrial data.",https://ieeexplore.ieee.org/document/5172632/,2008 International Conference on Computational Intelligence for Modelling Control & Automation,10-12 Dec. 2008,ieeexplore
10.1109/IPDPSW52791.2021.00012,Adaptive Stochastic Gradient Descent for Deep Learning on Heterogeneous CPU+GPU Architectures,IEEE,Conferences,"The widely-adopted practice is to train deep learning models with specialized hardware accelerators, e.g., GPUs or TPUs, due to their superior performance on linear algebra operations. However, this strategy does not employ effectively the extensive CPU and memory resources - which are used only for preprocessing, data transfer, and scheduling - available by default on the accelerated servers. In this paper, we study training algorithms for deep learning on heterogeneous CPU+GPU architectures. Our two-fold objective - maximize convergence rate and resource utilization simultaneously - makes the problem challenging. In order to allow for a principled exploration of the design space, we first introduce a generic deep learning framework that exploits the difference in computational power and memory hierarchy between CPU and GPU through asynchronous message passing. Based on insights gained through experimentation with the framework, we design two heterogeneous asynchronous stochastic gradient descent (SGD) algorithms. The first algorithm - CPU+GPU Hogbatch - combines small batches on CPU with large batches on GPU in order to maximize the utilization of both resources. However, this generates an unbalanced model update distribution which hinders the statistical convergence. The second algorithm - Adaptive Hogbatch - assigns batches with continuously evolving size based on the relative speed of CPU and GPU. This balances the model updates ratio at the expense of a customizable decrease in utilization. We show that the implementation of these algorithms in the proposed CPU+GPU framework achieves both faster convergence and higher resource utilization than TensorFlow on several real datasets.",https://ieeexplore.ieee.org/document/9460628/,2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),17-21 June 2021,ieeexplore
10.1109/CDC.1991.261440,Adaptive classification and control-rule optimisation via a learning algorithm for controlling a dynamic system,IEEE,Conferences,"The authors present a control-rule optimizing algorithm. They describe a learning algorithm, for controlling a dynamic system, in which an incremental version of the genetic algorithm is used to learn classification of the state-space of process control while a batch version of the genetic algorithm is used to optimize a set of control actions. The dynamic system chosen was a motor-driven cart on which a pole was mounted. The learning algorithm for controlling a cart-pole balancing system has been implemented by using a real-time parallel computation architecture.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/261440/,[1991] Proceedings of the 30th IEEE Conference on Decision and Control,11-13 Dec. 1991,ieeexplore
10.1109/CGNCC.2014.7007211,Adaptive terrain classification in field environment based on self-supervised learning,IEEE,Conferences,"This paper focuses on terrain classification in field environment and proposes a self-supervised terrain classification method which is based on 3D laser sensor and monocular vision sensor to adapt to changes in terrain environment and external conditions. First of all, extract typical traversable areas and typical obstacle areas by analyzing range data from 3D laser sensor and project these two kinds of areas into image space to label the image data. Then extract visual feature from the corresponding image to train classifier to classify the terrain. The experiment results demonstrate that the proposed method in this paper can obtain high classification accuracy and good real-time performance.",https://ieeexplore.ieee.org/document/7007211/,"Proceedings of 2014 IEEE Chinese Guidance, Navigation and Control Conference",8-10 Aug. 2014,ieeexplore
10.1109/ICRA40945.2020.9196582,Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video,IEEE,Conferences,"Key challenges for the deployment of reinforcement learning (RL) agents in the real world are the discovery, representation and reuse of skills in the absence of a reward function. To this end, we propose a novel approach to learn a task-agnostic skill embedding space from unlabeled multi-view videos. Our method learns a general skill embedding independently from the task context by using an adversarial loss. We combine a metric learning loss, which utilizes temporal video coherence to learn a state representation, with an entropy-regularized adversarial skill-transfer loss. The metric learning loss learns a disentangled representation by attracting simultaneous viewpoints of the same observations and repelling visually similar frames from temporal neighbors. The adversarial skill-transfer loss enhances re-usability of learned skill embeddings over multiple task domains. We show that the learned embedding enables training of continuous control policies to solve novel tasks that require the interpolation of previously seen skills. Our extensive evaluation with both simulation and real world data demonstrates the effectiveness of our method in learning transferable skills from unlabeled interaction videos and composing them for new tasks. Code, pretrained models and dataset are available at http://robotskills.cs.uni-freiburg.de.",https://ieeexplore.ieee.org/document/9196582/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore
10.1109/ICPR48806.2021.9412167,Adversarial Training for Aspect-Based Sentiment Analysis with BERT,IEEE,Conferences,"Aspect-Based Sentiment Analysis (ABSA) studies the extraction of sentiments and their targets. Collecting labeled data for this task in order to help neural networks generalize better can be laborious and time-consuming. As an alternative, similar data to the real-world examples can be produced artificially through an adversarial process which is carried out in the embedding space. Although these examples are not real sentences, they have been shown to act as a regularization method which can make neural networks more robust. In this work, we fine- tune the general purpose BERT and domain specific post-trained BERT (BERT-PT) using adversarial training. After improving the results of post-trained BERT with different hyperparameters, we propose a novel architecture called BERT Adversarial Training (BAT) to utilize adversarial training for the two major tasks of Aspect Extraction and Aspect Sentiment Classification in sentiment analysis. The proposed model outperforms the general BERT as well as the in-domain post-trained BERT in both tasks. To the best of our knowledge, this is the first study on the application of adversarial training in ABSA. The code is publicly available on a GitHub repository at https://github.com/IMPLabUniPr/Adversarial-Training-for-ABSA.",https://ieeexplore.ieee.org/document/9412167/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore
10.1109/SysTol.2013.6693835,Aerodynamic model inversion for virtual sensing of longitudinal flight parameters,IEEE,Conferences,"Introduction of Fly-By-Wire and increasing levels of automation improve the safety of civil aircraft significantly, and result in advanced capabilities for detecting, protecting and optimizing A/C guidance and control. However, this higher complexity requires the availability of some key flight parameters to be extended, to keep for a nominal behaviour of the flight control systems. Hence, the monitoring and the consolidation of those signals is a significant issue, usually achieved via many functionally redundant sensors to enlarge the way those parameters are measured. This solution penalizes the overall system performance in terms of weight, power consumption, space requirements, and extra maintenance needs. Other alternatives rely on signal processing or model-based techniques that make a global use of all or part of the sensor data available, supplemented by a model-based simulation of the flight mechanics (analytical redundancy). That processing achieves a real-time estimation of the critical parameters and yields dissimilar signals. Filtered and consolidated information are delivered in unfaulty conditions by estimating an extended state vector including wind components, and can replace failed signals in degraded conditions (virtual probes). Accordingly, this paper describes a new model-based approach allowing the longitudinal flight parameters of a civil A/C to be estimated on-line, through an Aerodynamic Model Inversion. To facilitate onboard implementation, the main aerodynamic coefficients are approximated by a set of surrogate models. Results are displayed to evaluate the performances of that approach in different flight conditions, including external disturbances and modeling errors. They correspond to different simulations and real flight tests.",https://ieeexplore.ieee.org/document/6693835/,2013 Conference on Control and Fault-Tolerant Systems (SysTol),9-11 Oct. 2013,ieeexplore
10.1109/COMGEO.2013.23,Agent Based Modeling of Moving Point Objects in Geospatial Data,IEEE,Conferences,We describe a framework for agent based modeling of moving point objects. Spatial movements are generated based on two overlapping ontologies. The first ontology includes the landmarks and descriptive outdoor behavior attributes. The second ontology includes indoor space and descriptive indoor behavior attributes. The modeling is based on ontology that includes the landmarks and descriptive behavior attributes of moving objects. The goal of our research in this area is to generate various spatial movements of point objects that can be classified into different known patterns. The agent behaviors can be modified semi-automatically based on changes in ontologies. This modeling is accomplished over a representation of a real-world data like road networks.,https://ieeexplore.ieee.org/document/6602054/,2013 Fourth International Conference on Computing for Geospatial Research and Application,22-24 July 2013,ieeexplore
10.1109/ICST46399.2020.00051,Agent-based Testing of Extended Reality Systems,IEEE,Conferences,"Testing for quality assurance (QA) is a crucial step in the development of Extended Reality (XR) systems that typically follow iterative design and development cycles. Bringing automation to these testing procedures will increase the productivity of XR developers. However, given the complexity of the XR environments and the User Experience (UX) demands, achieving this is highly challenging. We propose to address this issue through the creation of autonomous cognitive test agents that will have the ability to cope with the complexity of the interaction space by intelligently explore the most prominent interactions given a test goal and support the assessment of affective properties of the UX by playing the role of users.",https://ieeexplore.ieee.org/document/9159086/,"2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)",24-28 Oct. 2020,ieeexplore
10.1109/ICBASE53849.2021.00107,Aggregation Transfer Learning for Multi-Agent Reinforcement learning,IEEE,Conferences,"Multi-agent reinforcement learning is currently mainly used in many real-time strategy games. For example, StarCraft, UAV combat. Multi-agent reinforcement learning algorithms have attracted widespread attention. In large-scale multi-agent environment, there is still the problem of state space explosion. Especially in transfer training, since the network input size is fixed, the existing network structure is difficult to adapt to large-scale scenario transfer training. In this paper, we use aggregation transfer training for multi-agent combat problems from aerial unmanned aerial vehicle (UAV) combat scenarios to extend the small-scale learning to large-scale and complex scenarios. We combine the graph neural network (GNN) with the MADDPG algorithm to process the agent observation with aggregation function and take it as input. It starts training from a small-scale multi-UAV combat scenario, gradually increases the number of UAV. The experimental results indicate that MADDPG methods for multi-agent UAV combat problems trained via aggregation transfer learning are able to reach the target performance more quickly, provide superior performance, compared with ones trained without aggregation transfer learning. The versatility of the confrontation model has also been improved.",https://ieeexplore.ieee.org/document/9696029/,2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),24-26 Sept. 2021,ieeexplore
10.1109/IGARSS.2011.6048931,Aggregation of parallel computing and hardware/software co-design techniques for high-performance remote sensing applications,IEEE,Conferences,"Developing computationally efficient processing techniques for massive volumes of hyperspectral data is critical for space-based Earth science and planetary exploration. In particular, many remote sensing imaging applications require a response in real time in areas such as environmental modeling and assessment, target detection for military and homeland defense/security purposes, and risk prevention and response. This paper propose the aggregation of parallel computing and HW/SW co-design techniques using processor arrays (PAs) units as specialized hardware architectures for the real time enhancement of remote sensing imagery. An extended descriptive experiment design regularization (DEDR) method that incorporates projections onto convex solution sets (POCS) for spatial spectrum pattern (SSP) reconstruction is used to be efficiently implemented (i.e., HW-level) via the new proposition of the aggregation techniques. Finally, it is reported and discussed the Xilinx Virtex-5 FPGA implementation and high-performance issues related to real time enhancement of large-scale real-world RS imagery.",https://ieeexplore.ieee.org/document/6048931/,2011 IEEE International Geoscience and Remote Sensing Symposium,24-29 July 2011,ieeexplore
10.1109/ICCCNT45670.2019.8944690,AirNote  Pen it Down!,IEEE,Conferences,"Writing is a mode of coherent communication which can effectively convey our thoughts. Today, typing and writing are the usual modes of recording information. Another technique that is rapidly gaining popularity is air-writing. It refers to writing characters or words in free space using an air-pen or a finger. It differs from conventional writing methods as there is no pen-up and pen-down motion. With the evolution of smart wearables, the digital world can now be controlled with human gestures. These wearables are capable of perceiving and comprehending our actions. Our project capitalizes on this need gap, by focusing on creating a motion-to-text converter that would potentially act as software for the smart wearables for air-writing. This project is a point gesture detector-cum-identifier. We will use computer vision to trace the trajectory of finger and machine learning to recognize the word (out of the image that is formed through the action of motions). This will make air-writing possible. The generated text can be further be used for various purposes such as sending messages, mail, etc. It will prove to be a powerful communication tool for those with hearing difficulties. It will be an efficient way to communicate and will reduce the usage of mobile phones as well as notebooks, thereby making the actions of writing and texting redundant.",https://ieeexplore.ieee.org/document/8944690/,"2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",6-8 July 2019,ieeexplore
10.1109/CBD.2016.066,Allocation of Resources after Disaster Based on Big Data from SNS and Spatial Scan,IEEE,Conferences,"After a disaster such as earthquakes, debris flows, forest fires, or landslides, etc., a lot of people have to be away from their home and gather in a shelter. In addition, the refugees suffer from the shortage of necessary resources due to impaired life infrastructure, such as damaged roads and communication networks. The degree of reducing damage depends on the amount of food, water, daily necessities and communication resources required by each shelter. How to effectively and efficiently allocate resources according to grasp the exact need of a disaster situation will be an important issue. We estimate the degree of the disaster by collecting and analyzing big data from the SNS, and building a platform for the communication resources to be efficiently and effectively allocated. In order to achieve this goal, we are challenging the following issues A) Understanding situations (user requirements) after disaster occur The SNS streams large scale semantic information about real time situation in society, especially during and after disaster. It is both domain-specific and computational challenge in processing the heterogeneous large data set to extract the exact situational content with reduced semantic uncertainty. The machine learning (ML) and natural language processing (NPL) tool kits are useful in semantic analysis, but still needs domain-specific implementation and computational improvement for the situation understanding from the SNS big data. B) Understanding distribution patterns of situations/users' requirements The disaster related situation is spatiotemporally correlated, and varies dynamically in space and time. It is also domain-specific and computational challenge in estimating the spatiotemporal distribution patterns of the disaster affect based on the spatial big data from SNS. The scan statistics such as the spatial scan have provided well tested mathematical tools and software for spatial data mining. However, new methodologies are necessary since the assumptions have to be different when it meets the spatial big data in SNS. And the computational complexity in spatial big data is also a bottleneck for real-time processing. C) Solving uncertainty of big crowd data One of the major features in big crowd data, e.g., SNS data, is uncertainty behind the data. Especially in a disaster scenario, the collecting time period cannot be long enough to smooth the data automatically. How to efficiently solve uncertainty problem in the big crowd data in a disaster scenario becomes a new and big challenge for disaster management.",https://ieeexplore.ieee.org/document/7815232/,2016 International Conference on Advanced Cloud and Big Data (CBD),13-16 Aug. 2016,ieeexplore
10.1109/RSP.2018.8631989,Ambient Intelligence for the Internet of Things Through Context-Awareness,IEEE,Conferences,"With the advent of the Internet of Things, new devices feature advanced capabilities that are used in homes, rooms, and offices for better comfort and to save user's time and energy. A complete smart space control system should automatically adjust settings to the user preferences based on data previously collected from such smart things. This paper describes a system that uses environmental data collected from sensors in smart devices to define contexts and user preferences to accomplish this requirement. Contextual data is fed to a context-aware decision engine, composed by a combination of machine learning and data mining techniques. The context-aware decision engine is capable of identifying important data relationships. This allows the use the environment's contextual information to make intelligent control decisions and adjust the environment's settings to user's preferences and reduce the overall power consumption, yielding better services to the user and improving human-technology interaction. We implemented and evaluated the system through a real case study. Results confirm the system's ability to automatically control a smart room with little overhead and latency, while promoting user comfort and energy savings.",https://ieeexplore.ieee.org/document/8631989/,2018 International Symposium on Rapid System Prototyping (RSP),4-5 Oct. 2018,ieeexplore
10.1109/WIIAT.2008.368,An Adaptive Multi-agent System for Continuous Learning of Streaming Data,IEEE,Conferences,"The task of continuous online unsupervised learning of streaming data in complex dynamic environments under conditions of uncertainty requires the maximizing (or minimizing) of a certain similarity-based objective function defining an optimal segmentation of the input data set into clusters, which is an NP-hard optimization problem in a general metric space and is computationally intractable for real-world problems of practical interest. This paper describes the developed adaptive multi-agent approach to continuous online clustering of streaming data, which is originally sensitive to environmental variations and provides a fast dynamic response with event-driven incremental improvement of optimization results, trading-off operating time and result quality. Our two main contributions include a computationally efficient market-based algorithm of continuous agglomerative hierarchical clustering of streaming data and a knowledge-based self-organizing multi-agent system for implementing it. Experimental results demonstrate the strong performance of the implemented multi-agent learning system for continuous online clustering of both synthetic datasets and datasets from the RoboCup Soccer and Rescue domains.",https://ieeexplore.ieee.org/document/4740615/,2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,9-12 Dec. 2008,ieeexplore
10.1109/CyberC.2010.83,An Ambient Service Model for Providing Structured Web Information Based on User-Contexts,IEEE,Conferences,"Users often visit many stores while they compare merchandise in order to purchase the merchandise or merchandise related to it. Given a service providing information such as location of these stores, they can reduce their time and effort spent by wandering around them because they can go straight to these stores. And they can obtain new purchase opportunities because they can know what kinds of related stores are around them. Thus, they can do efficient purchasing activities. In this paper, we discuss service providing information of related stores efficiently in order to support users' efficient purchasing activities. And we propose an ambient service model that consists of three layers: structured (purchase-related) information space, real-life space, and ambient information space. In this model, a lot of store information collected from the web is grouped and structured automatically by relation in terms of purchasing. And users search information of related stores by using an ambient query that is automatically created by their context in real-life space. Finally, they obtain information of related stores in the form of hierarchy structure through map interface. Then, they can search other kinds of information of related stores additionally by using the hierarchy structure. We implemented a system based on this model by using Radio-Frequency Identification (RFID), map-based, location-based and ontology technology. The implemented system was performed in specific shopping region (Ilsan Lapesta shopping mall, Goyang-city Gyeonggi-do, Korea). And we confirmed that users can efficiently obtain information of related stores through the system. We expect that this model can be used for developing services that provide information of objects related to various objects besides stores.",https://ieeexplore.ieee.org/document/5617020/,2010 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery,10-12 Oct. 2010,ieeexplore
10.1109/ICCC.2018.00010,An Edge Based Smart Parking Solution Using Camera Networks and Deep Learning,IEEE,Conferences,"The smart parking industry continues to evolve as an increasing number of cities struggle with traffic congestion and inadequate parking availability. For urban dwellers, few things are more irritating than anxiously searching for a parking space. Research results show that as much as 30% of traffic is caused by drivers driving around looking for parking spaces in congested city areas. There has been considerable activity among researchers to develop smart technologies that can help drivers find a parking spot with greater ease, not only reducing traffic congestion but also the subsequent air pollution. Many existing solutions deploy sensors in every parking spot to address the automatic parking spot detection problems. However, the device and deployment costs are very high, especially for some large and old parking structures. A wide variety of other technological innovations are beginning to enable more adaptable systems-including license plate number detection, smart parking meter, and vision-based parking spot detection. In this paper, we propose to design a more adaptable and affordable smart parking system via distributed cameras, edge computing, data analytics, and advanced deep learning algorithms. Specifically, we deploy cameras with zoom-lens and motorized head to capture license plate numbers by tracking the vehicles when they enter or leave the parking lot; cameras with wide angle fish-eye lens will monitor the large parking lot via our custom designed deep neural network. We further optimize the algorithm and enable the real-time deep learning inference in an edge device. Through the intelligent algorithm, we can significantly reduce the cost of existing systems, while achieving a more adaptable solution. For example, our system can automatically detect when a car enters the parking space, the location of the parking spot, and precisely charge the parking fee and associate this with the license plate number.",https://ieeexplore.ieee.org/document/8457691/,2018 IEEE International Conference on Cognitive Computing (ICCC),2-7 July 2018,ieeexplore
10.1109/ATC52653.2021.9598291,An Edge-AI Heterogeneous Solution for Real-time Parking Occupancy Detection,IEEE,Conferences,"In the digital era, building smart cities is a highly desired goal that every country strives to achieve. With the advancement of technology, many smart city systems have been developed at a rapid rate of which Smart Parking is emerging as one of the core components. Smart Parking promises to automate the parking process, thereby saving time, resources and effort for searching an optimal parking space as well as reducing traffic congestion and population. As one of the newly emerging and disrupting technology, Artificial Intelligence, Machine Learning and Deep Learning (AI/ML/DL) are being utilized in many aspects of developing a Smart Parking system. In this paper, we propose an solution for accelerating AI/ML/DL algorithms deployed on low-cost System-on-Chip platforms (SoCs), which are often used as edge devices in Smart Parking system. In particular, we leverage Binary Neural Network (BNN), one of the most advanced deep learning models, to build a heterogeneous algorithm for real-time identifying parking occupancy based on the integration of SoCs and existing surveillance systems. The proposed solution is implemented and evaluated in Zynq UltraScale+ MPSoC with high accuracy (approx. 87%), low latency (avg. 16ms) and high frame per second (FPS) rate.",https://ieeexplore.ieee.org/document/9598291/,2021 International Conference on Advanced Technologies for Communications (ATC),14-16 Oct. 2021,ieeexplore
10.1109/ICOSEC51865.2021.9591700,An Efficient implementation of Network Malicious Traffic Screening based on Big Data Analytics,IEEE,Conferences,"Internet plays an increasingly important role in politics, military, economy, culture and social life. Cyberspace with the Internet as the core has become the fifth largest strategic space after land, sea, air and space. Network security has brought great harm to personal, social and national security. From the perspective of technology development, traffic identification can be divided into three categories: Traffic Identification Based on port, traffic identification based on depth detection package and traffic identification based on machine learning. This research paper studies about the potential distribution of real data through network confrontation training, and then uses the trained model to generate a small number of samples.",https://ieeexplore.ieee.org/document/9591700/,2021 2nd International Conference on Smart Electronics and Communication (ICOSEC),7-9 Oct. 2021,ieeexplore
10.1109/GreenCom-CPSCom.2010.25,An Embedded Software Power Model Based on Algorithm Complexity Using Back-Propagation Neural Networks,IEEE,Conferences,"Nowadays as low carbon economy is greatly advocated worldwide, the electricity consumption caused by a huge number of embedded computer systems is gaining more and more attention. Different instruction set, software algorithm and high-level software architecture can significantly affect the system energy consumption. In this paper, we first analyze the relations between software power consumption and some software characteristics on algorithm level. Through measuring three algorithm complexity characteristics, i.e., time complexity, space complexity and input scale, we propose an embedded software power model based on algorithm complexity. Then, we design and train a back propagation neural network to fit the power model accurately based on a sample training function set and more than 400 software power data. Simulation results show that the error between the estimation values of this model and the real measured values is below 10 percent, and this model can effectively estimate the power consumption of software in an early stage of software design.",https://ieeexplore.ieee.org/document/5724868/,"2010 IEEE/ACM Int'l Conference on Green Computing and Communications & Int'l Conference on Cyber, Physical and Social Computing",18-20 Dec. 2010,ieeexplore
10.1109/ISCID.2008.170,An Emotion Generation Model for Interactive Virtual Robots,IEEE,Conferences,"Making a computer generate its own emotion is an important part of the affective computing, and this would have wide applications in human-computer interaction and artificial intelligence. In this paper, we will describe an emotion generation model for a multimodal virtual human. The relationship among the emotion, mood and personality are discussed firstly, and the PAD (pleasure-arousal-dominance) emotion space is used to define the emotion and the mood. Then, we use a random graphical model to generate emotion based on the evaluation of the overall influence of mood, previous emotion and the outside stimulations. Finally, a 3D virtual human head with facial expressions is designed to show the emotion generation outputs. Experimental results demonstrate that the emotion generation model based on random graphs works effectively and meets the basic principle of human emotion generation.",https://ieeexplore.ieee.org/document/4725498/,2008 International Symposium on Computational Intelligence and Design,17-18 Oct. 2008,ieeexplore
10.1109/HPCS.2018.00034,An Ensemble-Based P2P Framework for the Detection of Deviant Business Process Instances,IEEE,Conferences,"The problem of discriminating ""deviant"" traces (i.e. traces diverging from normal/desired outcomes, such as frauds, faults) in the execution log of a business process can be faced by extracting a classification model for the traces, after mapping them onto some suitable feature space. An ensemble-learning approach was recently proposed that trains multiple base learners on different vector-space views of the given log, and a probabilistic meta-model that combines the predictions of the discovered base classifiers. However, the sequential centralised implementation of this learning approach makes it unsuitable for real applications, where large volumes of traces are produced continuously, while both deviant and normal behaviours tend to change over the time. We here propose an online deviance detection framework that leverages a novel incremental learning scheme, which extracts different base models from different chunks of a trace stream, and dynamically combines them in an ensemble model. Notably, the system is based on a p2p architecture that allows it to distribute the entire learning procedure among multiple nodes and to exploit the power of HPC resources (e.g. cloud computing environments). Preliminary tests on a real-life log confirmed the validity of the approach, in terms of both effectiveness and efficiency.",https://ieeexplore.ieee.org/document/8514339/,2018 International Conference on High Performance Computing & Simulation (HPCS),16-20 July 2018,ieeexplore
10.1109/CEC45853.2021.9504837,An Entropy Driven Multiobjective Particle Swarm Optimization Algorithm for Feature Selection,IEEE,Conferences,"Feature selection is an important research field in machine learning since high-dimensionality is a common characteristic of real-world data. It has two main objectives, which are to maximize the classification accuracy while minimizing the number of selected features. As the two objectives are usually in conflict with each other, it makes feature selection a multi-objective problem. However, the large search space and discrete Pareto front makes it not easy for existing evolutionary multi-objective algorithms. In order to deal with the above mentioned difficulties in feature selection, an entropy driven multiobjective particle swarm optimization algorithm is proposed to remove redundant feature and decrease computational complexity. First, its basic idea is to model feature selection as a multiobjective optimization problem by optimizing the number of features and the classification accuracy in supervised condition simultaneously. Second, a particle initialization strategy based on information entropy is designed to improve the quality of initial solutions, and an adaptive velocity update rule is used to swap between local search and global search. Besides, a specified discrete nondominated sorting is designed. These strategies enable the proposed algorithm to gain better performance on both the quality and size of feature subset. The experimental results show that the proposed algorithm can maintain or improve the quality of Pareto fronts evolved by the state-of-the-art algorithms for feature selection.",https://ieeexplore.ieee.org/document/9504837/,2021 IEEE Congress on Evolutionary Computation (CEC),28 June-1 July 2021,ieeexplore
10.1109/CEC.2019.8790094,An Evolutionary Machine Learning Approach Towards Less Conservative Robust Optimization,IEEE,Conferences,"In the recent era, multi-criteria decision making under uncertainty is gaining importance due to its wide range of applicability. Among several types of uncertainty handling techniques, Robust Optimization (RO) is considered as an efficient and tractable approach provided one has accessibility to data in uncertain regions. However, solutions of RO may actually deviate from actual results in real scenarios, due to conservative sampling. This paper proposes a methodology to amalgamate unsupervised machine learning algorithms with RO which thereby makes it data-driven. A novel evolutionary fuzzy clustering mechanism is implemented to transcript the uncertain space such that the exact regions of uncertainty are identified. Subsequently, density based boundary point detection and Delaunay triangulation based boundary construction enables intelligent Sobol based sampling in these regions for use in RO. Results of two test cases with varying dimensions are presented along with a comprehensive comparison between conventional RO approach using box uncertainty set and proposed methodology. Considered case studies include highly nonlinear real life model for continuous casting from steelmaking industries, where a time expensive multi-objective optimization problem under uncertainty is formulated to resolve the conflict in productivity and energy consumption. Optimal Artificial Neural Network (ANN) surrogate assisted optimization under uncertainty for casting model is performed to obtain solutions in realistic time. The resulting RO problem being multi-objective in nature, the Pareto solutions are obtained by NSGA II.",https://ieeexplore.ieee.org/document/8790094/,2019 IEEE Congress on Evolutionary Computation (CEC),10-13 June 2019,ieeexplore
10.1109/ICOSEC49089.2020.9215321,An Extensive Study and Comparison of the Various Approaches to Object Detection using Deep Learning,IEEE,Conferences,"Smart spaces are specialized environments developed to enable the automatic monitoring of events in a monitored setting. Smart surveillance uses deep learning for object detection, to detect any hazards or predict potential threats in the designated smart space. Deep learning improves the accuracy of the dataset and even humans in tasks like image classification, speech recognition, and predictive tasks. In smart spaces, deep learning can be used for actions like voice recognition, to identify trends in collected data and smart surveillance. Deep learning algorithms are capable of locating a region of interest in a frame and predicting a label for the object in the region of interest. There are a wide variety of architectures available, each with its advantages and limitations. This paper aims to provide a study of deep learning architecture performance tuning. After an extensive comparison, considering the given evaluation metrics and time constraints of a real-time smart surveillance system, the YOLO architecture and its variants are found to be the most efficient. This architecture has been implemented on a smart space dataset and the results have been documented.",https://ieeexplore.ieee.org/document/9215321/,2020 International Conference on Smart Electronics and Communication (ICOSEC),10-12 Sept. 2020,ieeexplore
10.1109/ICSSIT48917.2020.9214185,An Extensive Study and Comparison of the Various Approaches to Object Detection using Deep Learning,IEEE,Conferences,"Smart spaces are specialized environments developed to enable the automatic monitoring of events in a monitored setting. Smart surveillance uses deep learning for object detection, to detect any hazards or predict potential threats in the designated smart space. Deep learning improves the accuracy of the dataset and even humans in tasks like image classification, speech recognition, and predictive tasks. In smart spaces, deep learning can be used for actions like voice recognition, to identify trends in collected data and smart surveillance. Deep learning algorithms are capable of locating a region of interest in a frame and predicting a label for the object in the region of interest. There are a wide variety of architectures available, each with its advantages and limitations. This paper aims to provide a study of deep learning architecture performance tuning. After an extensive comparison, considering the given evaluation metrics and time constraints of a real-time smart surveillance system, the YOLO architecture and its variants are found to be the most efficient. This architecture has been implemented on a smart space dataset and the results have been documented.",https://ieeexplore.ieee.org/document/9214185/,2020 Third International Conference on Smart Systems and Inventive Technology (ICSSIT),20-22 Aug. 2020,ieeexplore
10.23919/ChiCC.2018.8483465,An Improved Ensemble Adaptive Kernel PLS Soft Sensor Model and its Application,IEEE,Conferences,"To avoid the disadvantage of traditional PLS model in dealing with nonlinear data, Kernel PLS (KPLS) algorithm has been proposed. By mapping nonlinear data into high-dimensional space with kernel function, the original data set can be processed using linear models in the new space. However, when facing diverse complicated nonlinear features, the simple kernel method also exhibits some limitations. To tackle this problem, an improved k-means based Ensemble Adaptive Kernel Partial Least Squares (EAKPLS) is proposed. Its whole processes are implemented as follows. In the modeling phase, the data is first divided into k sub datasets by k-means clustering algorithm. Then for each subset, different kernels and corresponding kernel parameters are chosen adaptively by introducing PSO algorithm. In the prediction phase, ensemble learning is introduced to obtain the final predictable value where Bayes' theorem is applied, where an improved weights assignment strategy is also presented. Ultimately, numerical and real industrial test cases are both given to demonstrate its feasibility and effectiveness.",https://ieeexplore.ieee.org/document/8483465/,2018 37th Chinese Control Conference (CCC),25-27 July 2018,ieeexplore
10.1109/WorldS4.2019.8904020,"An Integrated Framework for Autonomous Driving: Object Detection, Lane Detection, and Free Space Detection",IEEE,Conferences,"In this paper, we present a deep neural network based real-time integrated framework to detect objects, lane markings, and drivable space using a monocular camera for advanced driver assistance systems. The object detection framework detects and tracks objects on the road such as cars, trucks, pedestrians, bicycles, motorcycles, and traffic signs. The lane detection framework identifies the different lane markings on the road and also distinguishes between the ego lane and adjacent lane boundaries. The free space detection framework estimates the drivable space in front of the vehicle. In our integrated framework, we propose a pipeline combining the three deep neural networks into a single framework, for object detection, lane detection, and free space detection simultaneously. The integrated framework is implemented in C++ and runs real-time on the Nvidia's Drive PX 2 platform.",https://ieeexplore.ieee.org/document/8904020/,2019 Third World Conference on Smart Trends in Systems Security and Sustainablity (WorldS4),30-31 July 2019,ieeexplore
10.1109/ICDMW.2013.135,An Online Clustering Algorithm That Ignores Outliers: Application to Hierarchical Feature Learning from Sensory Data,IEEE,Conferences,"Surveillance sensors are a major source of unstructured Big Data. Discovering and recognizing spatiotemporal objects (e.g., events) in such data is of paramount importance to the security and safety of facilities and individuals. Hierarchical feature learning is at the crux to the problems of discovery and recognition. We present a multilayered convergent neural architecture for storing repeating spatially and temporally coincident patterns in data at multiple levels of abstraction. The bottom-up weights in each layer are learned to encode a hierarchy of over complete and sparse feature dictionaries from space- and time-varying sensory data by recursive layer-by-layer spherical clustering. This density-based clustering algorithm ignores outliers by the use of a unique adaptive threshold in each neuron's transfer function. The model scales to full-sized high-dimensional input data and also to an arbitrary number of layers, thereby possessing the capability to capture features at any level of abstraction. It is fully-learnable with only two manually tunable parameters. The model was deployed to learn meaningful feature hierarchies from audio, images and videos which can then be used for recognition and reconstruction. Besides being online, operations in each layer of the model can be implemented in parallelized hardware, making it very efficient for real world Big Data applications.",https://ieeexplore.ieee.org/document/6753963/,2013 IEEE 13th International Conference on Data Mining Workshops,7-10 Dec. 2013,ieeexplore
10.23919/OCEANS44145.2021.9705666,An Underwater Simulation Server Oriented to Cooperative Robotic Interventions: The Educational Approach,IEEE,Conferences,"Experiments that require the use of Supervised Autonomous Underwater Vehicles for Intervention (I-AUV) are not easy to be performed, specially when deployed in the sea or in scenarios where the robot might face lack of space and communication (e.g. interior of pipes). Also, there are some applications where the robots need to cooperate in a closed manner, for example when transporting and assembling big pipes. In fact, these two scenarios are being studied in the context of the H2020-ElPeacetolero and TWINBOT (TWIN roBOTs for cooperative underwater intervention mission) [1] projects, being necessary to have a simulation tool that offer more realistic rendering and being compatible with the real robot Application Programming Interface.This paper presents a new underwater simulation server, implemented using video game and robotic techniques, which operates by enabling the researchers control the robots in the scene in a simple and efficient manner, while using HTTP commands that have demonstrated a huge facility in the project integration process. Moreover, this simplicity has allowed the application of the simulation server in the educational context. The use of this tool has resulted to be very adequate for the students, who have used it to learn computer science and artificial intelligence algorithms to solve problems like a cooperative transportation robotic task. As case study, four educational experiments are presented, performed by masters degree students, focusing on user interfaces, image compression for underwater channels, autonomous cooperative grasping and robot arm movement in a AUV.",https://ieeexplore.ieee.org/document/9705666/,OCEANS 2021: San Diego  Porto,20-23 Sept. 2021,ieeexplore
10.1109/IMVIP.2007.37,An Unsupervised Approach for Segmentation and Clustering of Soccer Players,IEEE,Conferences,"In this work we consider the problem of soccer team discrimination. The approach we propose starts from the monocular images acquired by a still camera. The first step is the soccer player detection, performed by means of background subtraction. An algorithm based on pixels energy content has been implemented in order to detect moving objects. The use of energy information, combined with a temporal sliding window procedure, allows to be substantially independent from motion hypothesis. Colour histograms in RGB space are extracted from each player, and provided to the unsupervised classification phase. This is composed by two distinct modules: firstly, a modified version of the BSAS clustering algorithm builds the clusters for each class of objects. Then, at runtime, each player is classified by evaluating its distance, in the features space, from the classes previously detected. Algorithms have been tested on different real soccer matches of the Italian Serie A.",https://ieeexplore.ieee.org/document/4318147/,International Machine Vision and Image Processing Conference (IMVIP 2007),5-7 Sept. 2007,ieeexplore
10.1109/EMPD.1995.500705,An application of ANN in scheduling pumped-storage,IEEE,Conferences,"An artificial neural network (ANN) based optimization method in scheduling pumped-storage is proposed in the paper. Short-term scheduling as well as real-time dispatch of a pumped-storage station is a constrained optimization problem. It becomes more complicated when coordinated with other generation resources. The computation time is often long and the operation conditions may change unpredictably. A fast and practical way is expected. The ANN is used as a signal processing device, which represents mapping functions from input space to output space. Through a training process, multi-layered feedforward and neural networks can be used to approximate the continuous functions with a given accuracy and real-time solution can be achieved. In this paper three layer feedforward ANN and improved BP algorithm are adopted to solve the problem of pumped-storage scheduling. A set of ANN training data are obtained by running an optimization software. The paper describes how to select and organize the input data and how to train the ANN. A work example is presented and a comparison with traditional method is made. It shows that a fast and accurate solution for pumped-storage scheduling can be achieved with ANN.",https://ieeexplore.ieee.org/document/500705/,Proceedings 1995 International Conference on Energy Management and Power Delivery EMPD '95,21-23 Nov. 1995,ieeexplore
10.1109/IROS.1991.174539,An approach to on-line obstacle avoidance for robot arms,IEEE,Conferences,"Presents an approach to on-line obstacle avoidance for fixed-base robot manipulators. It guarantees a collision-free path for the robot during real-time operations. This approach is based on analytic geometry and is suitable for continuous path control. Considering the potential collision with obstacles, the next trajectory point to move to is corrected. This strategy is direct formulated in the operational space in which the tasks are described and applicable for two-dimensional as well as for three-dimensional space. Because this algorithm requires no access to joint control, it can be also used for commercial robots given the desired path. One can assign it for various robots, here the implementation for the PUMA 560 is presented as an example.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/174539/,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,3-5 Nov. 1991,ieeexplore
10.1109/ComManTel.2013.6482400,An efficient I/O based clustering HTN in Web Service Composition,IEEE,Conferences,"AI-planning has been evidently proved to be effective as a successful Web Service Composition (WSC) technique. However, with a numerous number of Web services in real world, planning time and search space become influencing factors of performance. In order to provide better plans in terms of both efficiency and quality, it is desired to increase the automation level of the applied composition approach. In this paper, we propose a novel model of AI-planning for WSC. The model adopts an I/O-based clustering technique to generate a Hierarchical Task Network (HTN) clustered planning domain. Experiments show that the proposed approach is effective and efficient in service composition.",https://ieeexplore.ieee.org/document/6482400/,"2013 International Conference on Computing, Management and Telecommunications (ComManTel)",21-24 Jan. 2013,ieeexplore
10.1109/FSKD.2011.6019741,An efficient clustering approach using ant colony algorithm in mutidimensional search space,IEEE,Conferences,"Clustering is an important data analysis technique and it widely used in many field such as data mining, machine learning and pattern recognition. Ant colony optimization clustering is one of the popular partition algorithm. However, in mutidimensional search space, its results is usually ordinary as the disturbing of redundant information. To address the problem, this paper presents MD-ACO clustering algorithm which improves the ant structure to implement attribute reduction. Four real data sets from UCI machine learning repository are used to evaluate MD-ACO with ACO. The results show that MD-ACO is more competitive.",https://ieeexplore.ieee.org/document/6019741/,2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD),26-28 July 2011,ieeexplore
10.1109/ICCSS.2017.8091440,An efficient density-based clustering for multi-dimensional database,IEEE,Conferences,"Cluster analysis aims at classifying data elements into different categories according to their similarity. It is a common task in data mining and useful in various field including pattern recognition, machine learning, information retrieval and so on. As an extensive studied area, many clustering methods are proposed in literature. Among them, some methods are focused on mining clusters with arbitrary shapes. However, when dealing with large-scale and multi-dimensional data, there is still a need for an efficient and versatile clustering method to identify these arbitrary shapes that may be embedded in these multi-dimensional space. In this paper, we propose a density-based clustering algorithm that adopts a divide-and-conquer strategy. To handle large-scale and multi-dimensional data, we first divide the data by grid cells. It is very efficient in large-scale cases where other algorithms often fail. Moreover, rather than tuning the grid cell width, we present a way to automatically determine the grid cell width. Then, we propose a flood-filling like algorithm to identify the clusters with arbitrary shapes over these grid cells. Finally, extensive experiments are conducted in both synthetic databases and real-world databases, showing that the proposed algorithm efficiently finds accurate clusters in both low-dimensional and multi-dimensional databases.",https://ieeexplore.ieee.org/document/8091440/,"2017 4th International Conference on Information, Cybernetics and Computational Social Systems (ICCSS)",24-26 July 2017,ieeexplore
10.1109/GLSV.1994.289971,An efficient multiprocessor implementation scheme for real-time DSP algorithms,IEEE,Conferences,"An algorithm to derive minimum-processor implementation for real-time DSP algorithms is proposed. In order to make the number of possible schedules finite and to assure the optimal schedule within the search space, the authors define a novel notion of cutoff time. All the possible schedules can find an equivalent schedule that finishes before cutoff time. Next, they apply all efficient heuristic periodic scheduling and fully static allocation algorithms derived from two generic problem solving heuristics developed in a branch of artificial intelligence research called planning. Extensive benchmarks have been tested and the results are most encouraging.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/289971/,Proceedings of 4th Great Lakes Symposium on VLSI,4-5 March 1994,ieeexplore
10.1109/SIU.2017.7960263,An energy efficient additive neural network,IEEE,Conferences,"In this paper, we propose a new energy efficient neural network with the universal approximation property over space of Lebesgue integrable functions. This network, called additive neural network, is very suitable for mobile computing. The neural structure is based on a novel vector product definition, called ef-operator, that permits a multiplier-free implementation. In ef-operation, the ""product"" of two real numbers is defined as the sum of their absolute values, with the sign determined by the sign of the product of the numbers. This ""product"" is used to construct a vector product in n-dimensional Euclidean space. The vector product induces the lasso norm. The proposed additive neural network successfully solves the XOR problem. The experiments on MNIST dataset show that the classification performances of the proposed additive neural networks are very similar to the corresponding multi-layer perceptron.",https://ieeexplore.ieee.org/document/7960263/,2017 25th Signal Processing and Communications Applications Conference (SIU),15-18 May 2017,ieeexplore
10.1109/AIIA.1988.13342,An expert PID controller,IEEE,Conferences,"The authors report on the development of an expert PID (proportional-integral-differential) controller for industrial processes. It uses the optimal property of a certain region in the P, I, and D parameter space providing minimum integrated (average) error when the required damping is ensured otherwise. The expert system uses an algorithm which basically modifies the settings of a conventional real-time PID controller continually until the parameters enter the optimum region mentioned above. The optimum region for a given process depends on the characterization of that process which is carried out in a simple indentification procedure prior to convergence. The authors describe the theory, the algorithm used, the hardware, and the software aspects of the implementation of the expert system as well as the test results obtained on a simulated process control system.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/13342/,Proceedings of the International Workshop on Artificial Intelligence for Industrial Applications,25-27 May 1988,ieeexplore
10.1109/ICDAR.2001.953749,An hybrid MLP-SVM handwritten digit recognizer,IEEE,Conferences,"This paper presents an original hybrid MLP-SVM method for unconstrained handwritten digits recognition. Specialized support vector machines (SVMs) are introduced to improve significantly the multilayer perceptron (MLP) performances in local areas around the separation surfaces between each pair of digit classes, in the input pattern space. This hybrid architecture is based on the idea that the correct digit class almost systematically belongs to the two maximum MLP outputs and that some pairs of digit classes constitute the majority of MLP substitutions (errors). Specialized local SVMs are introduced to detect the correct class among these two classification hypotheses. The hybrid MLP-SVM recognizer achieves a recognition rate of 98.01%, for real mail zip code digits recognition task, a performance better than several classifiers reported in recent researches.",https://ieeexplore.ieee.org/document/953749/,Proceedings of Sixth International Conference on Document Analysis and Recognition,13-13 Sept. 2001,ieeexplore
10.1109/CDC.2012.6426325,An improved Predictive Optimal Controller with elastic search space for steam temperature control of large-scale supercritical power unit,IEEE,Conferences,"Predictive optimal control (POC) combined with artificial neural networks (ANNs) modeling and advanced heuristic optimization is a powerful technique for intelligent control. But actual implementation of the POC in complex industrial processes is limited by its known drawbacks, including the oscillation resulting from random search direction, difficulty in meeting the real-time requirement, and unresolved adaptability and generalization ability of the ANN predictive model. In resolving these problems, an improved Intelligent Predictive Optimal Controller (IPOC) with elastic search space is proposed in this paper. A new simpler and high-efficiency Particle Swarm Optimization (PSO) algorithm is adopted to find the optimal solution in fewer epochs to meet the real-time control requirements. The system output error in each control step is fed back to adjust the search space dynamically to prevent control oscillation and also make it easier to find the optimal solution. An improved recurrent neural network with external delayed inputs and outputs is constructed to model the dynamic response of the highly nonlinear system. The proposed IPOC is used to superheater steam temperature control of a 600MW supercritical power unit. Extensive control simulation tests are made to verify the validity of the new control scheme in a full-scope simulator.",https://ieeexplore.ieee.org/document/6426325/,2012 IEEE 51st IEEE Conference on Decision and Control (CDC),10-13 Dec. 2012,ieeexplore
10.1109/ICAIBD.2018.8396176,An improved deep neural network model for job matching,IEEE,Conferences,"Job matching which benefit job seekers, employees and employers is very important today. In this work, a deep neural network model is proposed to predict an employee's future career details, which includes position name, salary and company scale based on the online resume data. Like most NLP tasks, the input features are multi-field, non-sparse, discrete and categorical, while their dependencies are mostly unknown. Previous works were mostly focused on engineering, which resulted in a large feature space and heavy computation. To solve this task, we use embedding layers to explore feature interactions and merge two automatically learned features extracted from the resumes. Experimental results on over 70,000 real-word online resumes show that our model outperforms shallow models, like SVM and Random Forests, in effectiveness and accuracy.",https://ieeexplore.ieee.org/document/8396176/,2018 International Conference on Artificial Intelligence and Big Data (ICAIBD),26-28 May 2018,ieeexplore
10.1109/ICPCA.2010.5704136,An intelligent interaction system architecture of the internet of things based on context,IEEE,Conferences,"The intelligent interaction is an important supporting technique for achieving the merging of the information space and physical space in the internet of things. For now there isn 't a general architecture of the intelligent interaction in the environment of the internet of things. In this paper, we propose an intelligent interaction architecture based on the context merging in the internet of things. The components of the architecture are described in detail, and the intelligent interactions among the things or between the user and the physical world are achieved, which breaks through the boundary between the virtualization and reality in the pervasive computing and provides more intelligent computations and services. Finally, we present an intelligent electrical power grid system by using the proposed intelligent interaction architecture. An illustrative example shows the validity of the architecture.",https://ieeexplore.ieee.org/document/5704136/,5th International Conference on Pervasive Computing and Applications,1-3 Dec. 2010,ieeexplore
10.1109/ICMLC.2014.7009153,An intelligent space location identification system based on passive RFID tags,IEEE,Conferences,"The development of context-aware control for smart space applications becomes popular recently, in which radio frequency identification (RFID) plays an important role. For an indoor context-aware smart space system, RFID tags and readers are utilized to locate object coordinates, cf. GPS utilizes GNSS for capture outdoor location information. Previous researches proposed to deploy regularly spaced reference tags, based on which the space location of target object tag can be estimated. Due to tag signal collision and received signal errors, the estimation accuracy is limited. We propose to utilize more than one RFID readers and control the reading power to eliminate estimation error. Passive tags that are cost effective are deployed for dense tag grid, e.g., for high accuracy location estimation. Experiments show that the proposed method can significantly reduce the estimation error from 69 cm to be under 15 cm. This RFID-based object location system can be applied to help people to locate personal belongings. In addition, a video surveillance system can be integrated with the object location method to provide context-awareness service.",https://ieeexplore.ieee.org/document/7009153/,2014 International Conference on Machine Learning and Cybernetics,13-16 July 2014,ieeexplore
10.1109/ROBOT.1992.220085,An optimal scheduling of pick place operations of a robot-vision-tracking system by using back-propagation and Hamming networks,IEEE,Conferences,"The authors present a neural network approach to solve the dynamic scheduling problem for pick-place operations of a robot-vision-tracking system. An optimal scheduling problem is formulated to minimize robot processing time without constraint violations. This is a real-time optimization problem which must be repeated for each group of objects. A scheme which uses neural networks to learn the mapping from object pattern space to optimal order space offline and to recall online what has been learned is presented. The idea was implemented in a real system to solve a problem in large commercial dishwashing operations. Experimental results have been shown that with four different objects, time savings of up to 21% are possible over first-come, first-served schemes currently used in industry.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/220085/,Proceedings 1992 IEEE International Conference on Robotics and Automation,12-14 May 1992,ieeexplore
10.1109/EnT47717.2019.9030591,Analysis of approaches to the universal approximation of a continuous function using Kolmogorovs superposition,IEEE,Conferences,"Kolmogorov and Arnold proved that any real continuous bounded function of many variables can be represented as a superposition of functions of one variable and addition. In subsequent works by Neht-Nielsen, it was shown that such a specific type of superposition can be interpreted as a two-layer forward neural network. Such a superposition can also be used as a universal approximation of the function of many variables. In the work, one of the variants of numerical implementation of Kolmogorov's superposition, proposed by Sprecher and modified by Koppen, was investigated. In addition, the functions of the first layer were considered as a generator of space-filling curves (Peano curves). Numerical experiments were conducted to study the accuracy of the approximation of the function of many variables for the numerical implementation of Kolmogorov's superposition, proposed by Sprecher and modified by Koppen, for a simplified version of this superposition and for an approach using filling curves. A comparative analysis showed that the best results are obtained using space-filling curves.",https://ieeexplore.ieee.org/document/9030591/,2019 International Conference on Engineering and Telecommunication (EnT),20-21 Nov. 2019,ieeexplore
10.1109/ICMLA.2016.0041,Android Malware Detection: Building Useful Representations,IEEE,Conferences,"The problem of proactively detecting Android Malware has proven to be a challenging one. The challenges stem from a variety of issues, but recent literature has shown that this task is hard to solve with high accuracy when only a restricted set of features, like permissions or similar fixed sets of features, are used. The opposite approach of including all available features is also problematic, as it causes the features space to grow beyond reasonable size. In this paper we focus on finding an efficient way to select a representative feature space, preserving its discriminative power on unseen data. We go beyond traditional approaches like Principal Component Analysis, which is too heavy for large-scale problems with millions of features. In particular we show that many feature groups that can be extracted from Android application packages, like features extracted from the manifest file or strings extracted from the Dalvik Executable (DEX), should be filtered and used in classification separately. Our proposed dimensionality reduction scheme is applied to each group separately and consists of raw string preprocessing, feature selection via log-odds and finally applying random projections. With the size of the feature space growing exponentially as a function of the training set's size, our approach drastically decreases the size of the feature space of several orders of magnitude, this in turn allows accurate classification to become possible in a real world scenario. After reducing the dimensionality we use the feature groups in a light-weight ensemble of logistic classifiers. We evaluated the proposed classification scheme on real malware data provided by the antivirus vendor and achieved state-of-the-art 88.24% true positive and reasonably low 0.04% false positive rates with a significantly compressed feature space on a balanced test set of 10,000 samples.",https://ieeexplore.ieee.org/document/7838145/,2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA),18-20 Dec. 2016,ieeexplore
10.1109/ICITEED.2016.7863311,Android based real-time static Indonesian sign language recognition system prototype,IEEE,Conferences,"Sign language uses gestures instead of speech sounds to communicate. But in general, normal people rarely trying to learn sign language to interact with the deaf community. Recently, there are many sign language recognition system that had been developed. But most of them were implemented using desktop and laptop computer, which is impractical due to its weight and size. This paper presents a prototype of real time static Indonesian sign language recognition using the Android Smart Phone, so it can be used anywhere and anytime. YCrCb color space combined with skin color detection is used to remove the background image and form a segmented image. Detection contour with convex hull algorithms are used to localize and save an area of the hand. Convexity defect algorithms then are used to extract the hand gesture's features using a radiant line from the center of the palm to the fingertips. The classification of hand gesture that performs sign alphabets is accomplished using back propagation neural network algorithm in order to determine a suitable alphabet. The performance test of the system is done by recognizing some variation hand gesture poses for Indonesian sign language alphabet. The results show that the system can detect the position of the user's hand. Furthermore, the system can recognize the alphabet sign from user hand gesture input, reaching 91.66% success rate in testing using Android devices in real time.",https://ieeexplore.ieee.org/document/7863311/,2016 8th International Conference on Information Technology and Electrical Engineering (ICITEE),5-6 Oct. 2016,ieeexplore
10.1109/BigData.2016.7841008,Android malware detection with weak ground truth data,IEEE,Conferences,"For Android malware detection, precise ground truth is a rare commodity. As security knowledge evolves, what may be considered ground truth at one moment in time may change, and apps once considered benign turn out to be malicious. The inevitable noise in data labels poses a challenge to creating effective machine learning models. Our work is focused on approaches for learning classifiers for Android malware detection in a manner that is methodologically sound with regard to the uncertain and ever-changing ground truth in the problem space. We leverage the fact that although data labels are unavoidably noisy, a malware label is much more precise than a benign label. While you can be confident that an app is malicious, you can never be certain that a benign app is really benign or just an undetected malware. Based on this insight, we leverage a modified Logistic Regression classifier that allows us to learn from only positive and unlabeled data, without making any assumptions about benign labels. We find Label Regularized Logistic Regression to perform well for noisy app datasets, as well as datasets where there is a limited amount of positive labeled data, both of which are representative of real-world situations.",https://ieeexplore.ieee.org/document/7841008/,2016 IEEE International Conference on Big Data (Big Data),5-8 Dec. 2016,ieeexplore
10.1109/DSC50466.2020.00029,Anobeat: Anomaly Detection for Electrocardiography Beat Signals,IEEE,Conferences,"Electrocardiography signals are composed of variform heartbeats which could indicate the condition of the heart and reveal the risk of heart attacks. Many existing classification-based works for abnormal beats detection are limited by the class-imbalanced data or labor-intensive manual annotation bias. A promising trend to address the issue is to identify the abnormal data that differs from the normal data by utilizing normal (oneclass) data to learn the manifold and detect the anomaly to the unseen and unlabeled data in an/aunsupervised/semi-supervised manner. In this paper, we propose Anobeat, a semi-supervised approach, to perform the abnormal beat detection by facilitating adversarial regularized autoencoders constrained with multifeature and reconstruction error. In order to obtain a robust and reasonable latent coding, we deploy two discriminators in the latent space and visual space to distinguish real and fake features and minimize the distance between two features to train the visual discriminator in alternate steps. Meanwhile, we minimize the reconstruction error and maximum distance between input and noise features to improve the decoder. The adversarial multi-feature constraints enable the generator to learn the latent representations of the target normal data and reconstruct the beats properly. Experiments showed that Anobeat achieved ROC-AUC of 0.960 and 0.894 in the MIT-BIH intrapatient and inter-patient dataset respectively, which outperforms the most competitive baseline by 1.61% and 0.62% respectively. Anobeat also performs comparative robustness and shows good interpretability in the European ST-T and MIT-BIH Arrhythmia Database.",https://ieeexplore.ieee.org/document/9172828/,2020 IEEE Fifth International Conference on Data Science in Cyberspace (DSC),27-30 July 2020,ieeexplore
10.1109/ITSC45102.2020.9294394,Anomalous State Recognition of Lane-changing Behavior using a Hybrid Autoencoder Architecture,IEEE,Conferences,"This paper presents a hybrid unsupervised architecture for anomalous lane-changing behavior recognition. Anomaly detection aims to identify unusual driving behavior caused by either environmental or phycological stimuli, and is of great important in road safety. First, a Recurrent Convolutional Autoencoder (RC-AE) is built to explore the spatial-temporal features derived from the high-dimensional behavior data. Second, Reconstruct Error analysis of the autoencoder and one-class support vector machine method are both applied to identify anomalous lane-changing behavior in the learned feature space by autoencoder. Last, we employ T-Distributed Stochastic Neighbor Embedding (T-SNE) for data visualization in the anomaly detection. Based on the kernel density estimation analysis, anomalous and normal lane-changing sample groups display distinct difference over probability distributions. The findings contribute to a better understanding on drivers' natural lane-changing behavior, and can provide important insight into real-time personalized unusual lane-changing behavior monitoring system development.",https://ieeexplore.ieee.org/document/9294394/,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),20-23 Sept. 2020,ieeexplore
10.1109/IAW.2004.1437798,Anomalous packet identification for network intrusion detection,IEEE,Conferences,"A packet-level anomaly detection system for network intrusion detection in high-bandwidth network environments is described. The approach is intended for hardware implementation and could be included in the network interface, switch or firewall. Efficient implementation in software on a network host is also possible. Network traffic is characterized using a novel technique that maps packet-level payloads onto a set of counters using bit-pattern hash functions, which were chosen for their implementation efficiency in both hardware and software. Machine learning is accomplished by mapping unlabelled training data onto a set of two-dimensional grids and forming a set of bitmaps that identify anomalous and normal regions. These bitmaps are used as the classifiers for real-time detection. The proposed method is extremely efficient in both the offline machine learning and real-time detection components and has the potential to provide accurate detection performance due to the ability of the bitmaps to capture nearly arbitrary shaped regions in the feature space. Results of a preliminary study are presented that demonstrate the effectiveness of the technique.",https://ieeexplore.ieee.org/document/1437798/,"Proceedings from the Fifth Annual IEEE SMC Information Assurance Workshop, 2004.",10-11 June 2004,ieeexplore
10.1109/CEC.2004.1330898,Anomaly detection based on unsupervised niche clustering with application to network intrusion detection,IEEE,Conferences,"We present a new approach to anomaly detection based on unsupervised niche clustering (UNC). The UNC is a genetic niching technique for clustering that can handle noise, and is able to determine the number of clusters automatically. The UNC uses the normal samples for generating a profile of the normal space (clusters). Each cluster can later be characterized by a fuzzy membership function that follows a Gaussian shape defined by the evolved cluster centers and radii. The set of memberships are aggregated using a max-or fuzzy operator in order to determine the normalcy level of a data sample. Experiments on synthetic and real data sets, including a network intrusion detection data set, are performed and some results are analyzed and reported.",https://ieeexplore.ieee.org/document/1330898/,Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat. No.04TH8753),19-23 June 2004,ieeexplore
10.1109/GECOST52368.2021.9538768,Anticipation of Parking Vacancy During Peak/Non-peak Hours using Convolutional Neural Network  YOLOv3 in University Campus,IEEE,Conferences,"Searching for a publicly available parking space has become a nightmare to many drivers. With the constant development of global urbanization, human population has increased drastically in the past decades. Searching for a publicly available parking space in a highly populated area can be daunting and time consuming. No matter how much time is spent to find a vacant parking space, it always causes traffic congestion in the area. To alleviate these problems, it is of utmost importance to have a system that can detect and display the vacant parking spaces in real-time. This paper has conducted a study of anticipation of parking vacancy using convolutional neural network called YOLOv3 in a university campus. Image data is gathered from the video capture of the universitys campus open space parking lot. The YOLOv3 algorithm is used to train and predict whether the space is vacant or occupied. Results showed that YOLOv3 has been able to correctly predict the vacant space. The result of the rendering video will then be transformed into an image and is sent to the students via a Telegram group.",https://ieeexplore.ieee.org/document/9538768/,"2021 International Conference on Green Energy, Computing and Sustainable Technology (GECOST)",7-9 July 2021,ieeexplore
10.1109/ICSESS47205.2019.9040712,Application of GA-Optimized ANN on Modeling the Performance of Coiled Adiabatic Capillary Tubes,IEEE,Conferences,"An Artificial Neural Network (ANN) model optimized with Genetic Algorithms (GA) is applied to predict the mass flow rate in coiled adiabatic capillary tubes. Capillary tubes are the key flow control devices in small refrigeration and air conditioning units, which are usually coiled to save space. The flashing flow through coiled capillary tubes is much complex and the physical process is typically non-linear, which needs complicated mathematical model (conservative equations) for precise prediction. A GA-optimized ANN model is thus employed to address this challenging problem, which is valuable for the design of coiled capillary tubes in real applications. The training samples are from the experimental data on a one-pass-through test facility, which provides accurate source datasets. The results show that the predicted mass flow rates with GA-optimized ANN model agree well with the test data with an average error of 2.43%.",https://ieeexplore.ieee.org/document/9040712/,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),18-20 Oct. 2019,ieeexplore
10.1109/ICSME.2019.00057,Application of Philosophical Principles in Linux Kernel Customization,IEEE,Conferences,"Philosophical principles are very useful in customization of Linux kernel, e.g., the answer for the question: ""For the pointer to the start address of page table, is it a physical address or a virtual address?"" can be derived by one simple philosophical principle: the depth of recursion is limited. This is because if the pointer were a virtual address, there would be another new page table to store the translation information of this virtual address, but who was responsible for storing the translation information of the start address of this new page table? This would result an infinite recursion. So the pointer definitely is a physical address. In fact, the usefulness of philosophical principles comes from the reduction of searching space. And this reduction is very important in customization of Linux kernel, for it could cut down the size of the new code needed to be read. This is especially valuable when considering that Linux kernel is continuously updating and huge now. Another example to further demonstrate the reduction of searching space in customization is showed in the following: in customization of file system in kernel version 3.10, the question: ""Does the Linux kernel itself maintain the consistency between the buffer cache and the page cache?"". This is a hard problem in practice, for without any guidance of philosophical principle, a developer has to read all of the code in Linux kernel to get a precise answer. The tricky part of this question is that if the developer only read a part of the codes and doesn't find any mechanisms for maintenance of cache consistency, the conclusion of non-existence of such mechanisms still can not be drawn, for there's still a possibility that such mechanisms exist in the codes not explored. Besides, if the developer search internet to find the answer, assume that the developer is lucky enough, he/she finally finds one program example on a web page shows that the inconsistency may raise between buffer cache and page cache. He/she still can not get the conclusion that Linux kernel does not maintain such consistency, because that program example maybe is only valid in a specific scenario, e.g. in kernel version 2.26, not 3.10. But we can get a satisfied answer by using the philosophical principle: the cost of management process should be far less than the value created by being managed process. By this principle, it can be drawn that Linux kernel doesn't maintain the consistency between the buffer cache and page cache in kernel 3.10. This is because that the data in buffer cache and page cache is highly dependent on application logic, so if Linux kernel wanted to maintain such consistency, it would have to track all these applications, which cost was much higher than the benefits that these applications could produce. However, the successful application of philosophical principles depends on two factors: firstly, establishment of a mapping between concepts in Linux system and well-known concepts in human society. This is not a new idea, e.g. the word of ""cost"" is a concept first appeared in human society, not in computer science, but nowadays, developers establish a mapping between this concept and concepts in computer science. Although the idea is very old, it is still very effective. Since well-known concepts in human society are familiar to most developers and are what they have in common, the cost of applying philosophical principles is reduced. Besides this, already existing cause-effect relations among concepts in human society can be highly possible to be reused in philosophical deduction in Linux kernel. E.g, in the mapping we established, process is treated as a human and since in religion of human society, God creates humankind, it is natural to derive that there's one process that creates all other processes in Linux system with high probability. Secondly, a concrete model with many qualitative and quantitative details should be the basis of philosophical deduction. We build such model according to our past experiences and the construction of the model follows the philosophical principle: unfold the complexity only when it is necessary. E.g., in this model, for a specific detail, it is covered only when it is required in practice. This is to lower down the cost of modelling huge and continuously evolving Linux kernel. This model is very important, without it, philosophical deduction is impossible. But it is really a hard work, according to our experiences, it needs at least 6-years of work on Linux kernel for one developer to build it. Although philosophical principles are very useful in practice, there's a big gap on the recognition of philosophical principles between academic researchers and industry practioners. E.g., some academic researcher seriously doubts whether the mapping above, which mentioned God, is helpful. In fact, it is, for by this mapping, a developer will know that the existence of the process, which is the origin of all other processes, is highly possible and also that process maybe is not easily observed. This is true, for that process is the process which PID is zero and that process can not be observed by Linux command: ""ps -e"". That process is a very valuable point of customization, e.g., by modifying that process, all processes in the Linux will be affected. Why does this big gap exist? We believe there're at least three reasons: i. The bias on philosophical principles. This usually comes from the observation that some developers establish wrong mapping between the philosophical principles and the objects in real world. But is that true for those that has been verified many times in practice? ii. Wrong expectations. E.g., hope to get the precise answer when applying philosophical principles, instead of reducing the searching space. iii. Some academic researchers do not realize that a good philosophical principle usually is the result of a deep learning process of many years by human brain. Finally, we suggest that more efforts should be put on the studying of philosophical principles in program understanding and we believe that in the near future, the philosophical principles plus AI will be a trend in program understanding.",https://ieeexplore.ieee.org/document/8919057/,2019 IEEE International Conference on Software Maintenance and Evolution (ICSME),29 Sept.-4 Oct. 2019,ieeexplore
10.1109/ROBOT.2000.844768,Application of automatic action planning for several work cells to the German ETS-VII space robotics experiments,IEEE,Conferences,"Experiences in space robotics show, that the user normally has to cope with a huge amount of data. So, only robot and mission specialists are able to control the robot arm directly in teleoperation mode. By means of an intelligent robot control in cooperation with virtual reality methods, it is possible for non-robot specialists to generate tasks for a robot or an automation component intuitively. Furthermore, the intelligent robot control improves the safety of the entire system. The on-ground robot control and command station for the robot arm ERA onboard the satellite ETS-VII builds on a new resource-based action planning approach to manage robot manipulators and other automation components. In the case of ERA, the action planning system also takes care of the ""real"" robot onboard the satellite and the ""virtual"" robot in the simulation system. By means of the simulation system, the user can plan tasks ahead as well as analyze and visualize different strategies. The paper describes the mechanism of resource-based action planning, its application to different work cells, the practical experiences gained from the implementation for the on-ground robot control and command station for the robot arm ERA developed in the GETEX project as well as the services it provides to support VR-based man machine interfaces.",https://ieeexplore.ieee.org/document/844768/,Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),24-28 April 2000,ieeexplore
10.1109/CDC.1999.833361,Application of reinforcement learning control to a nonlinear dexterous robot,IEEE,Conferences,"In this paper, the effects of basic parameters in reinforcement learning control such as eligibility, action and critic network weights, system nonlinearities, gradient information, state-space partitioning, variance of exploration were studied in detail. We attempt to increase feasibility for practical applications, implementation, learning efficiency, and performance. Reinforcement learning is then applied for control of a nonlinear dexterous robot. This control problem dictates that the learning is performed online, based on binary and real valued reinforcement signal from a critic network, without knowing the system model nonlinearity. The learning algorithm consists of an action and critic networks that learn to keep the multifinger hand of the dexterous robot within desired limits.",https://ieeexplore.ieee.org/document/833361/,Proceedings of the 38th IEEE Conference on Decision and Control (Cat. No.99CH36304),7-10 Dec. 1999,ieeexplore
10.1109/AEECA52519.2021.9574150,Application of virtual panorama technology based on multi-source information fusion in high quality development of the Yellow River,IEEE,Conferences,"Virtual panorama technology based on multi-source information fusion is a virtual reality technology based on image technology to generate realistic graphics. Panorama production software is used to generate panoramic images with a strong sense of scene and perspective effect that can be fully displayed at 720 degrees. Various kinds of voice, video, image and other hot spots are added in the virtual panorama image, and the step and front-end customized display are freely added, and multiple types of artificial intelligence graphic algorithms are applied to realize the virtual panorama platform of multi-source information fusion. The technology has been widely used in some businesses of the Yellow River, with strong practicability, large application and development space, and obvious advantages in investment benefit ratio.",https://ieeexplore.ieee.org/document/9574150/,2021 IEEE International Conference on Advances in Electrical Engineering and Computer Applications (AEECA),27-28 Aug. 2021,ieeexplore
10.1109/HUMANOIDS.2012.6651500,Applying statistical generalization to determine search direction for reinforcement learning of movement primitives,IEEE,Conferences,"In this paper we present a new methodology for robot learning that combines ideas from statistical generalization and reinforcement learning. First we apply statistical generalization to compute an approximation for the optimal control policy as defined by training movements that solve the given task in a number of specific situations. This way we obtain a manifold of movements, which dimensionality is usually much smaller than the dimensionality of a full space of movement primitives. Next we refine the policy by means of reinforcement learning on the approximating manifold, which results in a learning problem constrained to the low dimensional manifold. We show that in some situations, learning on the low dimensional manifold can be implemented as an error learning algorithm. We apply golden section search to refine the control policy. Furthermore, we propose a reinforcement learning algorithm with an extended parameter set, which combines learning in constrained domain with learning in full space of parametric movement primitives, which makes it possible to explore actions outside of the initial approximating manifold. The proposed approach was tested for learning of pouring action both in simulation and on a real robot.",https://ieeexplore.ieee.org/document/6651500/,2012 12th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2012),29 Nov.-1 Dec. 2012,ieeexplore
10.1109/CCCS.2018.8586801,Approving Psycho-Neuro-Computer Systems to prevent (Systemic Vs Individualistic Perspective) Cybercrimes in Information Highway,IEEE,Conferences,"In this paper the authors described the importance of Schizophrenia in Medical Systems versus Computer Systems. Dissimilar Schizophrenias were identified. The schizophrenia is thoughtful, consequently it could be discovered in future. But there is no Intelligence Quotient (IQ). The author contrasting this perception with present research work i.e., Cybercrimes in Higher Educations. This encouraging Psycho-Neuro-Computer Systems are very essential things to control and prevent the Cybercrimes in Cyber space. On each successive day the thoughts, feelings, behavior of humans are going in destructive manner. Especially in internet by using this Communication Technologies and facilities the Cybercrimes are ever-increasing very quickly. The Online Networking Systems are anguish, the people who have Schizophrenia is a serious psychological disorder in which people understand reality abnormally. Schizophrenia may result in some combination of illusions and tremendously disordered thinking and behavior that damages daily functioning, and can be disabling. The author developing positive psychology between the higher education students and faculties to reduce the Online Cybercrimes. In our research Cognitive Systems are showing main role which is interrelated with computer science and psychology. It provides us with a systematic foundation in the principles, ethics, morals, values and techniques used by intelligent systems (both natural and artificial) to interact with the web world. My research required Emotional Intelligence (or) Knowledge (or) Emotional Quotient (EQ) use emotions to enhance positive thoughts. People with high emotions can control, evaluate towards negative thoughts and perceive others emotions and thoughts, uniform estimation calculated of cleverness.",https://ieeexplore.ieee.org/document/8586801/,"2018 IEEE 3rd International Conference on Computing, Communication and Security (ICCCS)",25-27 Oct. 2018,ieeexplore
10.1109/ICUEMS50872.2020.00074,Apriori-based Spatial Pattern Mining Algorithm for Big Data,IEEE,Conferences,"With the establishment of recent information system networks, maritime vessel trajectory data are becoming increasingly available. Spatial pattern mining in these data can measure the behaviour of ships in space and time and can be used in traffic monitoring or other security purpose. The difficulty of this topic is that it needs to deal with both highdimensional data and huge amount of data.In this paper, a cooccurrence pattern mining algorithm based on Mapreduce architecture is presented. The algorithm is focused on the mining on spatial co-occurrence pattern, and implemented in the parallel partitioning architecture. The experiments on real data sets show that the large-scale ship trajectory data can be processed effectively.",https://ieeexplore.ieee.org/document/9151537/,2020 International Conference on Urban Engineering and Management Science (ICUEMS),24-26 April 2020,ieeexplore
10.1109/CCWC.2019.8666495,Architecturally Compressed CNN: An Embedded Realtime Classifier (NXP Bluebox2.0 with RTMaps),IEEE,Conferences,"The convolution neural networks have revolutionized the computer vision domain. It has proven to be a dominant technology to carry out tasks such as image classification, semantic segmentation, and object detection. The convolution neural networks surpass the performance of the existing algorithms such as SIFT, HOG, etcetera. Where, instead of manually engineering the features, supervised learning help to learn the essential low-level and high-level features necessary for classifications. The convolution neural networks have become a popular tool to counter computer vision problems. However, it is computationally, and memory intensive to train and deploy the network because of the model size of a deep convolution neural networks. However, the research in the field of design space exploration (DSE) of neural networks and compression techniques to develop compact architectures, have made convolution neural networks memory and computationally efficient. These techniques have also improved the feasibility of convolution neural network for deployment on embedded targets. The paper explores the concept of compact convolution filters to reduce the number of parameters in a convolution neural network. The intuition behind the approach is that replacing convolution filters with a stack of compact convolution filters helps in developing a compact architecture with competitive accuracy. This paper explores the fire module a compact convolution filter and proposes a method of recreating a state-of-the-art architecture VGG-16 using the fire modules to develop a compact architecture, which is further trained on the CIFAR-10 dataset and deployed on a real-time embedded platform known as Bluebox 2.0 by NXP using RTMaps software framework.",https://ieeexplore.ieee.org/document/8666495/,2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC),7-9 Jan. 2019,ieeexplore
10.1109/ITSC45102.2020.9294435,Architecture Design and Development of an On-board Stereo Vision System for Cooperative Automated Vehicles,IEEE,Conferences,"In a cooperative automated driving scenario like platooning, the ego vehicle needs reliable and accurate perception capabilities to autonomously follow the lead vehicle. This paper presents the architecture design and development of an on-board stereo vision system for cooperative automated vehicles. The input to the proposed system is stereo image pairs. It uses three deep neural networks to detect and classify objects, lane markings, and free space boundary simultaneously in front of the ego vehicle. The rectified left and right image frames of the stereo camera are used to compute a disparity map to estimate the detected object's depth and radial distance. It also estimates the object's relative velocity, azimuth, and elevation angle with respect to the ego vehicle. It sends the perceived information to the vehicle control system and displays the perceived information in a meaningful way on the human-machine interface. The system runs on both PC (x86_64 architecture) with Nvidia GPU, and the Nvidia Drive PX 2 (aarch64 architecture) automotive-grade compute platform. It is deployed and evaluated on Renault Twizy cooperative automated driving research platform. The presented results show that the stereo vision system works in real-time and is useful for cooperative automated vehicles.",https://ieeexplore.ieee.org/document/9294435/,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),20-23 Sept. 2020,ieeexplore
10.1109/ICMLA.2016.0118,Area-Specific Crime Prediction Models,IEEE,Conferences,"The convergence of public data and statistical modeling has created opportunities for public safety officials to prioritize the deployment of scarce resources on the basis of predicted crime patterns. Current crime prediction methods are trained using observed crime and information describing various criminogenic factors. Researchers have favored global models (e.g., of entire cities) due to a lack of observations at finer resolutions (e.g., ZIP codes). These global models and their assumptions are at odds with evidence that the relationship between crime and criminogenic factors is not homogeneous across space. In response to this gap, we present area-specific crime prediction models based on hierarchical and multi-task statistical learning. Our models mitigate sparseness by sharing information across ZIP codes, yet they retain the advantages of localized models in addressing non-homogeneous crime patterns. Out-of-sample testing on real crime data indicates predictive advantages over multiple state-of-the-art global models.",https://ieeexplore.ieee.org/document/7838222/,2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA),18-20 Dec. 2016,ieeexplore
10.1109/EMBC46164.2021.9630252,Arousal-Valence Classification from Peripheral Physiological Signals Using Long Short-Term Memory Networks,IEEE,Conferences,"The automated recognition of human emotions plays an important role in developing machines with emotional intelligence. However, most of the affective computing models are based on images, audio, videos and brain signals. There is a lack of prior studies that focus on utilizing only peripheral physiological signals for emotion recognition, which can ideally be implemented in daily life settings using wearables, e.g., smartwatches. Here, an emotion classification method using peripheral physiological signals, obtained by wearable devices that enable continuous monitoring of emotional states, is presented. A Long Short-Term Memory neural network-based classification model is proposed to accurately predict emotions in real-time into binary levels and quadrants of the arousal-valence space. The peripheral sensored data used here were collected from 20 participants, who engaged in a naturalistic debate. Different annotation schemes were adopted and their impact on the classification performance was explored. Evaluation results demonstrate the capability of our method with a measured accuracy of &gt;93% and &gt;89% for binary levels and quad classes, respectively. This paves the way for enhancing the role of wearable devices in emotional state recognition in everyday life.",https://ieeexplore.ieee.org/document/9630252/,2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),1-5 Nov. 2021,ieeexplore
10.1109/CIEEC50170.2021.9510594,Artificial Intelligence Assistant Decision-Making Method for Main &amp; Distribution Power Grid Integration Based on Deep Deterministic Network,IEEE,Conferences,"This paper studies the technology of generating DDPG (deep deterministic policy gradient) by using the deep dual network and experience pool network structure, and puts forward the sampling strategy gradient algorithm to randomly select actions according to the learned strategies (action distribution) in the continuous action space, based on the dispatching control system of the power dispatching control center of a super city power grid, According to the actual characteristics and operation needs of urban power grid, The developed refined artificial intelligence on-line security analysis and emergency response plan intelligent generation function realize the emergency response auxiliary decision-making intelligent generation function. According to the hidden danger of overload and overload found in the online safety analysis, the relevant load lines of the equipment are searched automatically. Through the topology automatic analysis, the load transfer mode is searched to eliminate or reduce the overload or overload of the equipment. For a variety of load transfer modes, the evaluation index of the scheme is established, and the optimal load transfer mode is intelligently selected. Based on the D5000 system of Metropolitan power grid, a multi-objective and multi resource coordinated security risk decision-making assistant system is implemented, which provides integrated security early warning and decision support for the main network and distribution network of city power grid. The intelligent level of power grid dispatching management and dispatching operation is improved. The state reality network can analyze the joint state observations from the action reality network, and the state estimation network uses the actor action as the input. In the continuous action space task, DDPG is better than dqn and its convergence speed is faster.",https://ieeexplore.ieee.org/document/9510594/,2021 IEEE 4th International Electrical and Energy Conference (CIEEC),28-30 May 2021,ieeexplore
10.1109/RAST.2019.8767447,Artificial Intelligence Implementation on Voice Command and Sensor Anomaly Detection for Enhancing Human Habitation in Space Mission,IEEE,Conferences,"The work in this paper describes implementation of Artificial Neural Network (ANN) on space processor LEON3. The ANN has been tested for training voice signal and for detecting anomaly signal on multiple analog sensors. The build-in radiation hardened UART 115200 interface of standard Space Hardware was utilized to receive compressed data from Artificial Intelligence (AI) Kit. The AI Kit was built to acquire human voice (as voice command) or to process input signals from multiple sensors concurrently. The Kit enables voice intuitively by pressing a training button and selecting proper command type from the keypad. The experiment results show that voice commands were detected successfully with accuracy of more than 95%. The second experiment was carried out by using analog sensor signals mixer to allow AI to learn and determine type of sensor data anomalies when some failures occur. The anomalies types were generated by adding unexpected stimulus signals to AI Kit analog input terminals. The result shows that the anomalies can be detected with accuracy of 80%. The size of AI Kit is relatively small and it was built with commercial components that enable replacement with space radiation hardened components. The space computer platform is based on LEON3 processor core and synthesized on Xilinx Virtex-5QV Field Programmable Logic Arrays (FPGA). The core runs at 100 MHz. The feed forward artificial neural networks Algorithm was implemented on Real-Time Executive for Multiprocessor Systems (RTEMS) operating system. The AI Kit consists of audio signal pre-amplifier, automatic gain control circuit, analog signal buffer circuit, high pass filter circuit (HPF), analog to digital converter ADC which is integrated in the 8 bit microcontroller. The modified FFT algorithm that runs on microcontroller is used for data compression and for increasing uniqueness of the data acquired. A DC/DC converter for battery usage is included, when 5V voltage supply is not available.",https://ieeexplore.ieee.org/document/8767447/,2019 9th International Conference on Recent Advances in Space Technologies (RAST),11-14 June 2019,ieeexplore
10.1109/IPAPS49326.2019.9069391,Attributes of Big Data Analytics for Data-Driven Decision Making in Cyber-Physical Power Systems,IEEE,Conferences,"Big data analytics is a virtually new term in power system terminology. This concept delves into the way a massive volume of data is acquired, processed, analyzed to extract insight from available data. In particular, big data analytics alludes to applications of artificial intelligence, machine learning techniques, data mining techniques, time-series forecasting methods. Decision-makers in power systems have been long plagued by incapability and weakness of classical methods in dealing with large-scale real practical cases due to the existence of thousands or millions of variables, being time-consuming, the requirement of a high computation burden, divergence of results, unjustifiable errors, and poor accuracy of the model. Big data analytics is an ongoing topic, which pinpoints how to extract insights from these large data sets. The extant article has enumerated the applications of big data analytics in future power systems through several layers from grid-scale to local-scale. Big data analytics has many applications in the areas of smart grid implementation, electricity markets, execution of collaborative operation schemes, enhancement of microgrid operation autonomy, management of electric vehicle operations in smart grids, active distribution network control, district hub system management, multi-agent energy systems, electricity theft detection, stability and security assessment by PMUs, and better exploitation of renewable energy sources. The employment of big data analytics entails some prerequisites, such as the proliferation of IoT-enabled devices, easily-accessible cloud space, blockchain, etc. This paper has comprehensively conducted an extensive review of the applications of big data analytics along with the prevailing challenges and solutions.",https://ieeexplore.ieee.org/document/9069391/,2020 14th International Conference on Protection and Automation of Power Systems (IPAPS),31 Dec.-1 Jan. 2020,ieeexplore
10.1109/IC3.2019.8844913,Augmentation of Images through DCGANs,IEEE,Conferences,"Now-a-days, extending images has become a challenging task to implement. Many of algorithms like convolution neural networks (CNN), Generative Adversarial Network (GAN) are used to fill out the image spaces. But the challenge arrives to guess or make appropriate assumption of image extended borders. We used Deep Convolution Generative Adversarial Network(DCGAN) over GAN and CNN to implement it.[1], [2] GAN's are implemented to guess the space in image by generating fake examples using generator and decides using discriminator which determines how much the image is real. DCGAN is improved version of GAN which generates spatial correlations [5].",https://ieeexplore.ieee.org/document/8844913/,2019 Twelfth International Conference on Contemporary Computing (IC3),8-10 Aug. 2019,ieeexplore
10.1109/EMBC46164.2021.9629804,Augmented Reality Assisted Surgical Navigation System for Epidural Needle Intervention,IEEE,Conferences,"An augmented reality (AR)-assisted surgical navigation system was developed for epidural needle intervention. The system includes three components: a virtual reality-based surgical planning software, a patient and tool tracking system, and an AR-based surgical navigation system. A three-dimensional (3D) path plan for the epidural needle was established on the preoperative computed tomography (CT) image. The plan is then registered to the intraoperative space by 3D models of the target vertebrae using skin markers and real-time tracking information. In the procedure, the plan and tracking information are transmitted to the head-mounted display (HMD) through a wireless network such that the device directly visualizes the plan onto the back surface of the patient. The physician determines the entry point and inserts the needle into the target based on the direct visual guidance of the system. An experiment was conducted to validate the system using two torso phantoms that mimic human respiration. The experimental results demonstrated that the time and the number of X-rays required for needle insertion were significantly decreased by the proposed method (43.620.55sec, 2.91.3times) compared to those of the conventional fluoroscopy-guided approach (124.5  46.7s, 9.32.4times), whereas the average targeting errors were similar in both cases. The proposed system may potentially decrease ionizing radiation exposure not only to the patient but also to the medical team.",https://ieeexplore.ieee.org/document/9629804/,2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),1-5 Nov. 2021,ieeexplore
,Augmented reality approach for paper map visualization,IEEE,Conferences,"Paper map visualization is good value proposition, even in the `everything is digital' world, because of the huge information density and ease of use at any remote or mobile location, especially in the military context. Completely doing away with the paper map brings in a discomfort factor for the military user. In this paper, we present a paper map data visualization solution that augments the information from the digital world onto a spread of a paper map. Augmented Reality techniques are used to enhance paper maps digitally resulting in a 3D visualization space with some interactions. The dynamic information of the terrain, placing virtual objects and interaction are features of the digital world that are superimposed onto a paper map. We have used ARToolkit marker tracking based approach and a see through Head Mounted Device for implementation. The digital elevation data in VRML format is split into smaller bits before rendering. The rendering is done after culling the relevant part of the terrain data and the same is popped up in stereo for enhanced visualization. The user can place virtual objects using markers. The distance between markers (user and reference) is calculated and is used for interactive features. The interaction options are load/select, slice/split and latch/freeze. We have achieved better performance by loading only the region of interest and not the complete terrain. The paper also illustrates the basic hardware set-up required for implementation.",https://ieeexplore.ieee.org/document/5738756/,2010 International Conference on Communication and Computational Intelligence (INCOCCI),27-29 Dec. 2010,ieeexplore
10.1109/ICC.2019.8761821,Authentication Scheme Based on Hashchain for Space-Air-Ground Integrated Network,IEEE,Conferences,"With the development of artificial intelligence and self-driving, vehicular ad-hoc network (VANET) has become an irreplaceable part of the Intelligent Transportation Systems (ITSs). However, the traditional network of the ground cannot meet the requirements of transmission, processing, and storage among vehicles. Under this circumstance, integrating space and air nodes into the whole network can provide comprehensive traffic information and reduce the transmission delay. The high mobility and low latency in the Space-Air-Ground Integrated Network (SAGIN) put forward higher requirements for security issues such as identity authentication, privacy protection and data security. This paper simplifies the Blockchain and proposes an identity authentication and privacy protection scheme based on the Hashchain in the SAGIN. The scheme focuses on the characteristics of the wireless signal to identify and authenticate the nodes. The verification and backup of the records on the block are implemented with the distributed streaming platform, Kafka algorithm, instead of the consensus. Furthermore, this paper analyzes the security of this scheme. Afterward, the experimental results reveal the delay brought by the scheme using the simulation of SUMO, OMNeT++, and Veins.",https://ieeexplore.ieee.org/document/8761821/,ICC 2019 - 2019 IEEE International Conference on Communications (ICC),20-24 May 2019,ieeexplore
10.1145/3238147.3238175,AutoConfig: Automatic Configuration Tuning for Distributed Message Systems,IEEE,Conferences,"Distributed message systems (DMSs) serve as the communication backbone for many real-time streaming data processing applications. To support the vast diversity of such applications, DMSs provide a large number of parameters to configure. However, It overwhelms for most users to configure these parameters well for better performance. Although many automatic configuration approaches have been proposed to address this issue, critical challenges still remain: 1) to train a better and robust performance prediction model using a limited number of samples, and 2) to search for a high-dimensional parameter space efficiently within a time constraint. In this paper, we propose AutoConfig - an automatic configuration system that can optimize producer-side throughput on DMSs. AutoConfig constructs a novel comparison-based model (CBM) that is more robust that the prediction-based model (PBM) used by previous learning-based approaches. Furthermore, AutoConfig uses a weighted Latin hypercube sampling (wLHS) approach to select a set of samples that can provide a better coverage over the high-dimensional parameter space. wLHS allows AutoConfig to search for more promising configurations using the trained CBM. We have implemented AutoConfig on the Kafka platform, and evaluated it using eight different testing scenarios deployed on a public cloud. Experimental results show that our CBM can obtain better results than that of PBM under the same random forests based model. Furthermore, AutoConfig outperforms default configurations by 215.40% on average, and five state-of-the-art configuration algorithms by 7.21%-64.56%.",https://ieeexplore.ieee.org/document/9000079/,2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE),3-7 Sept. 2018,ieeexplore
10.1109/RCAR49640.2020.9303043,Autobot for Effective Design Space Exploration and Agile Generation of RBFNN Hardware Accelerator in Embedded Real-time Computing,IEEE,Conferences,"This paper presents a method of employing Auto-bot to replace humans in the task of efficient hardware design for radial basis function neural network (RBFNN) in real-time computing applications. Autobot applies quick iterations using hardware generation and supports various number systems such as floating-point, half-floating point, and mixed-precision and hardware architectures to perform possible design space exploration, enabling an agile analysis for those requests. We have implemented and employed Autobot to successfully test with the applications of RBFNN-based Mackey-Glass chaotic time series prediction, servo motor control, and data classification. Analysis of these results shows that Autobot is able to deliver the hardware accelerator with less execution time than previous works, which also shortens the design time from days to minutes. Therefore, the proposed methodology is a useful alternative for agile real-time hardware development on FPGA.",https://ieeexplore.ieee.org/document/9303043/,2020 IEEE International Conference on Real-time Computing and Robotics (RCAR),28-29 Sept. 2020,ieeexplore
10.1109/BigMM52142.2021.00015,Automated Detection of Corona Cavities from SDO Images with YOLO,IEEE,Conferences,"Solar eruptions, such as filament eruptions and Corona Mass Ejections (CMEs), are primary drivers of hazardous space weather that impacts the critical infrastructure on the Earth. Researchers now believe that corona cavities serve as launch pads for CMEs. In this study, we proposed, implemented, and evaluated a method for automated corona cavity detection using YOLOv5 framework. We applied our method to the data observed by the Solar Dynamics Observatory (SDO) during the period from 2010 to 2013. With 18, 512 x 512 images, we were able to build a model that achieved a precision rate of 0.964, a recall rate of 0.883, and a mAP@0.5 of 0.898. With GPU freely available on the Google Colaboratory, training the model takes about 40 minutes and the inference time per image is about 9 milliseconds. Experiments showed the proposed method is promising in real-time corona cavity detection, and potentially tracking CMEs and predicting space weather.",https://ieeexplore.ieee.org/document/9643198/,2021 IEEE Seventh International Conference on Multimedia Big Data (BigMM),15-17 Nov. 2021,ieeexplore
10.1109/DICTA.2012.6411741,Automated Detection of Root Crowns Using Gaussian Mixture Model and Bayes Classification,IEEE,Conferences,"In this paper a method for automatic detection of root crowns in root images, are designed, implemented and quantitatively compared. The approach is based on the theory of statistical learning. The root images are preprocessed with algorithms for intensity normalization, segmentation, edge detection and scale space corner detection. The features used in the experiments are the Zernike moments of the bi-level image patch centered around high curvature detections. Zernike moments are orthogonal and thus can be rightly assumed to be independent. The densities of the feature vectors for different classes are modelled with Gaussian mixture model (GMM), with a diagonal covariance matrix. The parameters for the feature's distribution densities for different classes are learnt by expectation maximization. Bayes rule and Neymann-Pearson criteria is used to design the classification method. We experiment with different orders of Zernike moments and different number of Gaussians in the GMM. The experiments are done on a real dataset with images of rice, corn, and grass roots. Pattern classification results are quantitatively analyzed using Receiver Operating Characteristic (ROC) curves and area under the ROC curves. We quantitatively compare the results of the proposed method with that of support vector machine (SVM) which is another very popular statistical learning method for pattern classification.",https://ieeexplore.ieee.org/document/6411741/,2012 International Conference on Digital Image Computing Techniques and Applications (DICTA),3-5 Dec. 2012,ieeexplore
10.23919/OCEANS44145.2021.9705933,Automated Synthetic Aperture Sonar Image Segmentation using Spatially Coherent Clustering,IEEE,Conferences,"Seabed image segmentation is an important product for a variety of fields, including, habitat mapping, geological surveys, mine counter measures, and naval route planning. Developing a clustering algorithm that can both accurately segment high resolution imagery and generalize well over large areas is challenging. In this paper we will evaluate the performance of a new unsupervised image segmentation algorithm. The method utilizes imagery derived features (intensity and texture) to identify clusters (different seabed types) in feature space while also encouraging local homogeneity. In this paper we will demonstrate how our spatially coherent k-means clustering algorithm can efficiently and accurately segment Synthetic Aperture Sonar (SAS) images. Our experiments show that our spatially coherent clustering algorithm can increase segmentation accuracy up to 15 and 20 percent relative to OpenCV k-means and ArcGIS Pro ISO clustering, respectively.",https://ieeexplore.ieee.org/document/9705933/,OCEANS 2021: San Diego  Porto,20-23 Sept. 2021,ieeexplore
10.1109/SMC.2018.00655,Automated Training Plan Generation for Athletes,IEEE,Conferences,"In sports, athletes need detailed and individualised training plans for maintaining and improving their skills in order to achieve their best performance in competitions. This presents a considerable workload for coaches, who besides setting objectives have to formulate extremely detailed training plans. Automated Planning, which has already been successfully deployed in many real-world applications such as space exploration, robotics, and manufacturing processes, embodies a useful mechanism that can be exploited for generating training plans for athletes. In this paper, we propose the use of Automated Planning techniques for generating individual training plans, which consist of exercises the athlete has to perform during training, given the athlete's current performance, period of time, and target performance that should be achieved. Our experimental analysis, which considers general training of kickboxers, shows that apart of considerable less planning time, training plans automatically generated by the proposed approach are more detailed and individualised than plans prepared manually by an expert coach.",https://ieeexplore.ieee.org/document/8616652/,"2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",7-10 Oct. 2018,ieeexplore
10.1109/EMBC.2016.7592132,Automated classification of pathological gait after stroke using ubiquitous sensing technology,IEEE,Conferences,"This study uses machine learning methods to distinguish between healthy and pathological gait. Examples of multi-dimensional pathological and normal gait sequences were collected from post-stroke and healthy individuals in a real clinical setting and with two Kinect sensors. The trajectories of rotational angle and global velocity of selected body joints (hips, spine, shoulders, neck, knees and ankles) over time formed the gait sequences. The combination of k nearest neighbor (kNN) and dynamic time warping (DTW) was used for classification. Leave one subject out cross validation was implemented to evaluate the performance of the binary classifier in terms of F1-score in the original feature space, and also in a reduced dimensional feature space using PCA. The pair of k = 1 in kNN and the warping window size 25% of gait sequences in DTW achieved maximum F1-score. Using PCA, pathological gait sequences were discriminated from healthy sequences with the F1-score = 96%.",https://ieeexplore.ieee.org/document/7592132/,2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),16-20 Aug. 2016,ieeexplore
10.1145/3061639.3062207,Automated systolic array architecture synthesis for high throughput CNN inference on FPGAs,IEEE,Conferences,"Convolutional neural networks (CNNs) have been widely applied in many deep learning applications. In recent years, the FPGA implementation for CNNs has attracted much attention because of its high performance and energy efficiency. However, existing implementations have difficulty to fully leverage the computation power of the latest FPGAs. In this paper we implement CNN on an FPGA using a systolic array architecture, which can achieve high clock frequency under high resource utilization. We provide an analytical model for performance and resource utilization and develop an automatic design space exploration framework, as well as source-to-source code transformation from a C program to a CNN implementation using systolic array. The experimental results show that our framework is able to generate the accelerator for real-life CNN models, achieving up to 461 GFlops for floating point data type and 1.2 Tops for 816 bit fixed point.",https://ieeexplore.ieee.org/document/8060313/,2017 54th ACM/EDAC/IEEE Design Automation Conference (DAC),18-22 June 2017,ieeexplore
10.1109/ISSREW.2019.00102,Automatic Cause Detection of Performance Problems in Web Applications,IEEE,Conferences,"The execution of similar units can be compared by their internal behaviors to determine the causes of their potential performance issues. For instance, by examining the internal behaviors of different fast or slow web requests more closely, and by clustering and comparing their internal executions, one can determine what causes some requests to run slowly or behave in unexpected ways. In this paper, we propose a method of extracting the internal behavior of web requests as well as introduce a pipeline that detects performance issues in web requests and provides insights into their root causes. First, low-level and fine-grained information regarding each request is gathered by tracing both the user space and the kernel space. Second, further information is extracted and fed into an outlier detector. Finally, these outliers are then clustered by their behavior, and each group is analyzed separately. Experiments revealed that this pipeline is indeed able to detect slow web requests and provide additional insights into their true root causes. Notably, we were able to identify a real PHP cache contention issue using the proposed approach.",https://ieeexplore.ieee.org/document/8990337/,2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),27-30 Oct. 2019,ieeexplore
10.1109/CYBER.2017.8446080,Automatic Safety Helmet Wearing Detection,IEEE,Conferences,"Surveillance is very essential for the safety of power substation. The detection of whether wearing safety helmets or not for perambulatory workers is the key component of overall intelligent surveillance system in power substation. In this paper, a novel and practical safety helmet detection framework based on computer vision, machine learning and image processing is proposed. In order to ascertain motion objects in power substation, the ViBe background modelling algorithm is employed. Moreover, based on the result of motion objects segmentation, real-time human classification framework C4 is applied to locate pedestrian in power substation accurately and quickly. Finally, according to the result of pedestrian detection, the safety helmet wearing detection is implemented using the head location, the color space transformation and the color feature discrimination. Extensive compelling experimental results in power substation illustrate the efficiency and effectiveness of the proposed framework.",https://ieeexplore.ieee.org/document/8446080/,"2017 IEEE 7th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",31 July-4 Aug. 2017,ieeexplore
10.1109/ICSE43902.2021.00048,Automatic Web Testing Using Curiosity-Driven Reinforcement Learning,IEEE,Conferences,"Web testing has long been recognized as a notoriously difficult task. Even nowadays, web testing still mainly relies on manual efforts in many cases while automated web testing is still far from achieving human-level performance. Key challenges include dynamic content update and deep bugs hiding under complicated user interactions and specific input values, which can only be triggered by certain action sequences in the huge space of all possible sequences. In this paper, we propose WebExplor, an automatic end-to-end web testing framework, to achieve an adaptive exploration of web applications. WebExplor adopts a curiosity-driven reinforcement learning to generate high-quality action sequences (test cases) with temporal logical relations. Besides, WebExplor incrementally builds an automaton during the online testing process, which acts as the high-level guidance to further improve the testing efficiency. We have conducted comprehensive evaluations on six real-world projects, a commercial SaaS web application, and performed an in-the-wild study of the top 50 web applications in the world. The results demonstrate that in most cases WebExplor can achieve significantly higher failure detection rate, code coverage and efficiency than existing state-of-the-art web testing techniques. WebExplor also detected 12 previously unknown failures in the commercial web application, which have been confirmed and fixed by the developers. Furthermore, our in-the-wild study further uncovered 3,466 exceptions and errors.",https://ieeexplore.ieee.org/document/9402046/,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),22-30 May 2021,ieeexplore
10.1109/ICCChina.2014.7008377,Automatic collecting of indoor localization fingerprints: An crowd-based approach,IEEE,Conferences,"For typical indoor positioning systems employing a training/positioning model based on Wi-Fi fingerprints, significant training costs extremely restrict this kind of indoor localization system to be widely deployed and implemented with real location based applications. In this paper, we present a crowd-based approach to solve this problem, which automatically collects and constructs fingerprints database for anonymous buildings through common crowd customers with their smart-phones. However, such a crowd-based approach also introduces an accuracy degradation problem as crowd customers are not professional trained and equipped. So in this approach we employ fixed and hint landmarks to do error resetting. In our practical system, common corridor crossing points will serve as fixed landmarks and cross points between different crowd paths serve as hint landmarks. Machine-learning techniques are utilized for short range approximation around fixed landmarks and fuzzy logic decision technology is applied for searching hint landmarks in crowd traces space. We test this crowd-based automatic collecting approach on a dataset of about 5.09km walking in four corridors and the rooms besides. Experimental results indicate that this automatic collecting approach successfully construct indoor fingerprint radio map with rather high accuracy.",https://ieeexplore.ieee.org/document/7008377/,2014 IEEE/CIC International Conference on Communications in China (ICCC),13-15 Oct. 2014,ieeexplore
10.1109/ISIE.1999.796864,Automatic edge detection of DNA bands in autoradiograph images,IEEE,Conferences,"Scale space analysis is an efficient solution to the edge detection of objects in low to high contrast images. However, this approach is time consuming and computationally expensive. The parallel processing properties of a neural network provide an ideal solution to managing the large amounts of data processed in image analysis, however their application to multiscale analysis is still in its infancy. This paper reports on a new approach to detecting 2-dimensional and 3-dimensional objects in low to high contrast images. The novel idea is based on combining neural network arbitration and scale space analysis to automatically select one optimum scale for the entire image at which scale space edge detection can be applied. Thus, introducing new measures to solve many of the problems existing in the discipline of image processing, such as poor edge detection in low contrast images, speed of recognition and high computational cost. This new approach to edge detection is formalised in the Automatic Edge Detection Scheme (AEDS). The AEDS is implemented on a real-life application namely, the detection of bands within low contrast DNA autoradiograph images. Results are presented to show the AEDS overcoming the aforementioned common problems with image processing techniques.",https://ieeexplore.ieee.org/document/796864/,ISIE '99. Proceedings of the IEEE International Symposium on Industrial Electronics (Cat. No.99TH8465),12-16 July 1999,ieeexplore
10.1109/TAI.1996.560799,Automatic scale selection as a pre-processing stage to interpreting real-world data,IEEE,Conferences,"Summary form only given. We perceive objects in the world as meaningful entities only over certain ranges of scale. This fact that objects in the world appear in different ways depending on the scale of observation has important implications if one aims at describing them. It shows that the notion of scale is of utmost importance when processing unknown measurement data by automatic methods. In their seminal works, Witkin (1983) and Koenderink (1984) proposed to approach this problem by representing image structures at different scales in a so-called scale-space representation. Traditional scale-space theory building on this work, however, does not address the problem of how to select local appropriate scales for further analysis. After a brief review of the main ideas behind a scale-space representation, I describe a systematic methodology for generating hypotheses about interesting scale levels in image data based on a general principle stating that local extrema over scales of different combinations of normalized derivatives are likely candidates to correspond to interesting image structures. Specifically, I show how this idea can be used for formulating feature detectors which automatically adapt their local scales of processing to the local image structure. I show how the scale selection approach applies to various types of feature detection problems in early vision. In many computer vision applications, the poor performance of the low-level vision modules constitutes a major bottleneck.",https://ieeexplore.ieee.org/document/560799/,Proceedings Eighth IEEE International Conference on Tools with Artificial Intelligence,16-19 Nov. 1996,ieeexplore
10.1109/IV47402.2020.9304624,Autonomous Driving: Framework for Pedestrian Intention Estimation in a Real World Scenario,IEEE,Conferences,"Rapid advancements in driver assistance technology will lead to the integration of fully autonomous vehicles on our roads that will interact with other road users. To address the problem that driverless vehicles make interaction through eye contact impossible, we describe a framework for estimating the crossing intentions of pedestrians in order to reduce the uncertainty that the lack of eye contact between road users creates. The framework was deployed in a real vehicle and tested with three experimental cases that showed a variety of communication messages to pedestrians in a shared space scenario. Results from the performed field tests showed the feasibility of the presented approach.",https://ieeexplore.ieee.org/document/9304624/,2020 IEEE Intelligent Vehicles Symposium (IV),19 Oct.-13 Nov. 2020,ieeexplore
10.1109/IROS45743.2020.9341657,Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning on Graphs,IEEE,Conferences,"We consider an autonomous exploration problem in which a range-sensing mobile robot is tasked with accurately mapping the landmarks in an a priori unknown environment efficiently in real-time; it must choose sensing actions that both curb localization uncertainty and achieve information gain. For this problem, belief space planning methods that forward- simulate robot sensing and estimation may often fail in real-time implementation, scaling poorly with increasing size of the state, belief and action spaces. We propose a novel approach that uses graph neural networks (GNNs) in conjunction with deep reinforcement learning (DRL), enabling decision-making over graphs containing exploration information to predict a robot's optimal sensing action in belief space. The policy, which is trained in different random environments without human intervention, offers a real-time, scalable decision-making process whose high-performance exploratory sensing actions yield accurate maps and high rates of information gain.",https://ieeexplore.ieee.org/document/9341657/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore
10.1109/ICDM50108.2020.00084,Autonomous Graph Mining Algorithm Search with Best Speed/Accuracy Trade-off,IEEE,Conferences,"Graph data is ubiquitous in academia and industry, from social networks to bioinformatics. The pervasiveness of graphs today has raised the demand for algorithms that can answer various questions: Which products would a user like to purchase given her order list? Which users are buying fake followers to increase their public reputation? Myriads of new graph mining algorithms are proposed every year to answer such questions - each with a distinct problem formulation, computational time, and memory footprint. This lack of unity makes it difficult for a practitioner to compare different algorithms and pick the most suitable one for a specific application. These challenges - even more severe for non-experts - create a gap in which state-of-the-art techniques developed in academic settings fail to be optimally deployed in real-world applications. To bridge this gap, we propose AutoGM, an automated system for graph mining algorithm development. We first define a unified framework UnifiedGM that integrates various message-passing based graph algorithms, ranging from conventional algorithms like PageRank to graph neural networks. Then UnifiedGM defines a search space in which five parameters are required to determine a graph algorithm. Under this search space, AutoGM explicitly optimizes for the optimal parameter set of UnifiedGM using Bayesian Optimization. AutoGM defines a novel budget-aware objective function for the optimization to incorporate a practical issue - finding the best speed-accuracy trade-off under a computation budget - into the graph algorithm generation problem. Experiments on real-world benchmark datasets demonstrate that AutoGM generates novel graph mining algorithms with the best speed/accuracy trade-off compared to existing models with heuristic parameters.",https://ieeexplore.ieee.org/document/9338289/,2020 IEEE International Conference on Data Mining (ICDM),17-20 Nov. 2020,ieeexplore
10.1109/SIMPAR.2016.7862403,Autonomous exploration by expected information gain from probabilistic occupancy grid mapping,IEEE,Conferences,"Occupancy grid maps are spatial representations of environments, where the space of interest is decomposed into a number of cells that are considered either occupied or free. This paper focuses on exploring occupancy grid maps by predicting the uncertainty of the map. Based on recent improvements in computing occupancy probability, this paper presents a novel approach for selecting robot poses designed to maximize expected map information gain represented by the change in entropy. This result is simplified with several approximations to develop an algorithm suitable for real-time implementation. The predicted information gain proposed in this paper governs an effective autonomous exploration strategy when applied in conjunction with an existing motion planner to avoid obstacles, which is illustrated by numerical examples.",https://ieeexplore.ieee.org/document/7862403/,"2016 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR)",13-16 Dec. 2016,ieeexplore
10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00201,BDMF: A Biased Deep Matrix Factorization Model for Recommendation,IEEE,Conferences,"As a representative collaborative filtering method, matrix factorization has been widely used in personalized recommendation. Recently, deep matrix factorization model, which utilizes deep neural networks to project users and items into a latent structured space, has received increased attention. In this paper, inspired by the idea of BiasedSVD that introduces bias to both users and items, we propose a novel matrix factorization model with neural network architecture, named BDMF, short for Biased Deep Matrix Factorization. Specifically, we first construct a user-item interaction matrix with explicit ratings and implicit feedback, and randomly sample users and items as the input. Next, we feed this input to the proposed BDMF model to learn latent factors of both users and items, and then use them to predict the ratings for personalized ranking. We also formally show that BDMF works on the same principle as BiasedSVD, which means that BDMF can be viewed as a deep neural network implementation of BiasedSVD. Finally, extensive experiments on real-world datasets are conducted and the results verify the superiority of our model over other state-of-the-art.",https://ieeexplore.ieee.org/document/9060377/,"2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",19-23 Aug. 2019,ieeexplore
10.1109/IES53407.2021.9594013,Ball Position Transformation with Artificial Intelligence Based on Tensorflow Libraries,IEEE,Conferences,Research on wheeled soccer robots has been carried out by several researchers. This is due to the existence of national and international competitions. Previous research was to create a ball position transformation system with a modified method of neural network architecture. This research was developed by building an intelligent transformation system with the Tensorflow library. This transformation system aims to be able to directly measure the distance of objects in real terms without first changing the environmental image from an omni field to a flat plane with conventional camera calibration techniques. This process can replace manual calibration with a variety of field size changes The system can transform with mean error 0.0000026 on epoch 10000 using conda-tensorflowneural network libraries. It can transform the position of the ball from the omni space to the cartesian space. This system was implemented on wheeled soccer robot as keeper.,https://ieeexplore.ieee.org/document/9594013/,2021 International Electronics Symposium (IES),29-30 Sept. 2021,ieeexplore
10.1109/ICDL-EpiRob48136.2020.9278071,Bayesian Optimization for Developmental Robotics with Meta-Learning by Parameters Bounds Reduction,IEEE,Conferences,"In robotics, methods and softwares usually require optimizations of hyperparameters in order to be efficient for specific tasks, for instance industrial bin-picking from homogeneous heaps of different objects. We present a developmental framework based on long-term memory and reasoning modules (Bayesian Optimisation, visual similarity and parameters bounds reduction) allowing a robot to use meta-learning mechanism increasing the efficiency of such continuous and constrained parameters optimizations. The new optimization, viewed as a learning for the robot, can take advantage of past experiences (stored in the episodic and procedural memories) to shrink the search space by using reduced parameters bounds computed from the best optimizations realized by the robot with similar tasks of the new one (e.g. bin-picking from an homogenous heap of a similar object, based on visual similarity of objects stored in the semantic memory). As example, we have confronted the system to the constrained optimizations of 9 continuous hyperparameters for a professional software (Kamido) in industrial robotic arm bin-picking tasks, a step that is needed each time to handle correctly new object. We used a simulator to create bin-picking tasks for 8 different objects (7 in simulation and one with real setup, without and with meta-learning with experiences coming from other similar objects) achieving goods results despite a very small optimization budget, with a better performance reached when meta-learning is used (84.3 % vs 78.9 % of success overall, with a small budget of 30 iterations for each optimization) for every object tested (p-value=0.036).",https://ieeexplore.ieee.org/document/9278071/,2020 Joint IEEE 10th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob),26-30 Oct. 2020,ieeexplore
10.1109/DSC.2016.48,Behavior Analysis Based SMS Spammer Detection in Mobile Communication Networks,IEEE,Conferences,"In a communication network, automatic short message service (SMS) spammer detection is a big challenge for a telecommunication operator nowadays, especially with the development of the rich communication services (RCS). Three main problems exist in the areas of research and real practice. They are (1) the whole-volume content based SMS spam detection techniques cannot be easily used on the side of network due to the issue of user privacy, (2) traditional ways to filter the spam according to the combination of key words and sending frequency can be easily bypassed by adding the interference words, (3) Most of them result in a great deal of manual review after the automatic filtering due to a low precision rate. To make up the aforementioned gaps, we study the user behavior characteristics. A two-dimensional visualized result indicates that any combination of two user behavior attributes cannot distinguish the abnormal users from the whole set by splitting the 2-dimensional space. Thus, the integration of multiple user behavior attributes is exploited to train the classifier in a labeled set by machine learning algorithms, respectively, including decision tree, random forest, supported vector machine (SVM), logistic regression, and self-organized feature mapping (SOM). The performance comparison indicates that random forest is a good choice to balance the tradeoff of the precision rate and the recall rate, and in an acceptable time. The experimental result shows the proposed method without the knowledge of SMS content has a significant improvement in terms of precision rate and recall rate compared with the traditional method using the combination of key words and sending frequency used in most of existing networks.",https://ieeexplore.ieee.org/document/7866183/,2016 IEEE First International Conference on Data Science in Cyberspace (DSC),13-16 June 2016,ieeexplore
10.1109/CONECCT52877.2021.9622586,Benchmarking Transformer-Based Transcription on Embedded GPUs for Space Applications,IEEE,Conferences,"Speech transcription is a necessary tool for backend applications commonly found in voice assistants. Transcription is typically performed using cloud-based servers or custom hardware, but those resources are not always amenable to space environments due to size, weight, power, and cost constraints. Therefore, it is important to determine the performance of and optimal conditions for running transcription on hardware that is feasible for deployment in a space application. This research investigates and evaluates the performance of the wav2vec2 speech transcription engine, the current state-of-the-art model for this domain with and without optimizations. The target hardware, the NVIDIA Xavier NX Jetson embedded GPU, was chosen for its modern GPU architecture and small form factor. In addition to examining the input scaling behavior, we evaluate the hyperparameters of the clustered attention optimization, and average power and energy for inference relative to the operating power mode of the device. The clustered attention model outperformed the improved-clustered model for large input sizes, but the wav2vec2 model without clustering performed better for small input sizes. The clustered model energy per inference (13.90 J) was less than energy per inference of the improved-cluster model (15.03 J) and the vanilla softmax model (15.85 J). All models meet real-time speech processing requirements necessary to perform onboard inference entirely on a space system.",https://ieeexplore.ieee.org/document/9622586/,"2021 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",9-11 July 2021,ieeexplore
10.1109/DEST.2007.372004,Benefits of Ontologies in Real Time Data Access,IEEE,Conferences,"Next generation Business Intelligence systems requires flexible and on-time access to enterprise's data. This paper highlights the importance of a data integration layer in a Business Intelligence system and the benefits that the use of an ontology as data description formalism and query interface, can bring to the system. In particular we focus on the aspects of data mapping and ontology enrichment, giving a general overview of the problem. We introduce also BT Data Foundry platform that has been implemented by Intelligent Systems Research Centre of BT Exact (British Telecom) as a method to provide the Business Intelligence system with a common data access layer. We provide also an outline of the solution to data mapping and ontology enrichment problems, based on semantic bridges and formal concept analysis (FCA). We use a normalization layer based on heuristic to reduce the terms space.",https://ieeexplore.ieee.org/document/4233738/,2007 Inaugural IEEE-IES Digital EcoSystems and Technologies Conference,21-23 Feb. 2007,ieeexplore
10.1109/CVPR46437.2021.00937,Binary Graph Neural Networks,IEEE,Conferences,"Graph Neural Networks (GNNs) have emerged as a powerful and flexible framework for representation learning on irregular data. As they generalize the operations of classical CNNs on grids to arbitrary topologies, GNNs also bring much of the implementation challenges of their Euclidean counterparts. Model size, memory footprint, and energy consumption are common concerns for many real-world applications. Network binarization allocates a single bit to parameters and activations, thus dramatically reducing the memory requirements (up to 32x compared to single-precision floating-point numbers) and maximizing the benefits of fast SIMD instructions on modern hardware for measurable speedups. However, in spite of the large body of work on binarization for classical CNNs, this area remains largely unexplored in geometric deep learning. In this paper, we present and evaluate different strategies for the binarization of graph neural networks. We show that through careful design of the models, and control of the training process, binary graph neural networks can be trained at only a moderate cost in accuracy on challenging benchmarks. In particular, we present the first dynamic graph neural network in Hamming space, able to leverage efficient k-NN search on binary vectors to speed-up the construction of the dynamic graph. We further verify that the binary models offer significant savings on embedded devices. Our code is publicly available on Github<sup>1</sup>.",https://ieeexplore.ieee.org/document/9578443/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore
10.1109/ICSIDP47821.2019.9173414,Binary Separable Convolutional: An Efficient Fast Image Classification Method,IEEE,Conferences,"The training and running of neural network require large computational space and memory space, which makes it difficult to deploy on a resource-constrained embedded sys-tems. To address this limitation, we introduce a two-stage pipeline: Depth separable, local binary. Our method is divided into two steps. Firstly, a deep separable convolution is applied to each input channel. Then, point-by-point convolution is applied to the feature map obtained by the filter. In the second step, we use local binarization method to initialize the filter corresponding to the input channel into sparse binary. In network training, the sparse binary filter remains fixed and only needs to train convolution of 11 size. Our experimental results show that, on the basis of approximate accuracy with the original network, we have reduced the number of convolution parameters by 9x to 10x, and reduced the training time and testing time to by 2x. Our compression method helps to deploy complex neural networks on resource-constrained embedded platform.",https://ieeexplore.ieee.org/document/9173414/,"2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)",11-13 Dec. 2019,ieeexplore
10.1109/IDAACS53288.2021.9660834,Biometric Identification via Oculomotor System Based on the Volterra Model,IEEE,Conferences,"In recent years, there has been an increase in interest in biometrics research involving the use of brain characteristics commonly known as behavioral traits. Human eyes contain a rich source of idiosyncratic information which may be used for the recognition of an individual's identity. This article implements an innovative experiment and a new approach to processing human eye movements, ultimately aimed at biometric identification of individuals. In our experiment, the subjects observe special test visual stimuli, which are generated on the computer monitor screen. The eye movements are tracked in dynamics providing information for constructing a nonparametric nonlinear dynamic model (Volterra model) of a human's oculomotor system (OMS) in the form of multivariate transient functions. The implemented method treats eye trajectories as 2-D distributions of points on the Coordinate-Time plane. The efficiency of dynamic characteristics for personality identification is confirmed by examples of models built on the basis of data from real experiments. The resulting OMS models are a source of information for the selection of informative features, in the space of which the decisive rule of optimal identification of individuals is determined using machine learning methods. Promising results at the task of identification according to behavioral characteristics of an individual have been obtained - recognition accuracy is higher than 97%.",https://ieeexplore.ieee.org/document/9660834/,2021 11th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS),22-25 Sept. 2021,ieeexplore
10.1109/KESE.2009.47,Bleeding Simulation Based Particle System for Surgical Simulator,IEEE,Conferences,"Bleeding simulation is important components of a surgical simulator. However, realistic simulation of bleeding is a challenging problem. There are several fluid flow models described in the literature, but they are computationally slow and do not satisfy real-time requirement of a surgical simulator. The paper introduce a bleeding simulation algorithm based on the particle system and implement arterial squirting blood, blood splashes in free space, blood drops running down a surface. This algorithm calculates how many blood particles are needed to model a particular instance of bleeding, and assigns a velocity to each particle in a gravitational field so that particles flow out of the injured region at the proper rate. The key characteristics of our algorithm are a visually realistic display and real-time computational performance. Experiment results show that proposed method is feasible and effective. In addition, this algorithm can also be used to simulate water irrigation and smoke effect in virtual surgery simulator.",https://ieeexplore.ieee.org/document/5383596/,2009 Pacific-Asia Conference on Knowledge Engineering and Software Engineering,19-20 Dec. 2009,ieeexplore
10.1109/CSCS52396.2021.00073,Bluetooth Communications in Educational Robotics,IEEE,Conferences,"In a world in a continuous and rapid change, it is absolutely necessary for our students to keep up with the rapid progress of new technologies: Internet of Things (IoT), Robotics, Artificial Intelligence (AI), Virtual Reality (VR), Augmented Reality (AR) etc. The rapid evolution and diversification of these emerging technologies has recently led to their introduction into the educational offer of the school curriculum for the gymnasium. The discipline of Information and Communication Technology (ICT) has already been implemented, a discipline that involves both the formation of skills to use new technologies and the formation of computational thinking necessary for the efficient and intelligent use of these technologies. In order to teach and learn Physics from a STEM (Science, Technology, Engineering and Mathematics) educational perspective, we initiated optional school courses of IoT, Robotics and AI (approached through Machine Learning). These courses stimulate, at the level of students, computational thinking, creativity and innovation and lead, from an interdisciplinary perspective, to the development of emerging specializations such as Mathematics-Physics-Automation, Mathematics-Physics-Electronics, Mathematics-Physics-Informatics-Robotics etc. In this paper we presented a method of approaching, in the school educational space, the study of wireless communication technologies between smart devices, through an Educational Robotics project. The project consisted of creating a wireless controlled mobile robotic platform (robot car) via a Bluetooth module connected to an Arduino Uno board.",https://ieeexplore.ieee.org/document/9481012/,2021 23rd International Conference on Control Systems and Computer Science (CSCS),26-28 May 2021,ieeexplore
10.1109/IJCNN52387.2021.9534340,Born-Again Decision Boundary: Unsupervised Concept Drift Detection by Inspector Neural Network,IEEE,Conferences,"In the actual operation of machine learning models, differences between training and operational data distributions are a problem referred to as concept drifts. Most existing detection approaches assume that labels can be made readily available, but it is unrealistic to acquire all labels instantaneously. As concept drift is often unpredictable, the deployed machine learning model is not always amenable to concept drift detection. In this paper, we propose the Born-Again Decision Boundary algorithm, which is a novel unsupervised concept drift detection method for inspecting a deployed black box model without labels. The proposed method builds an inspector model that recreates the decision boundaries from the deployed model using black-box rule extraction techniques. This model calculates the distance from the decision boundary to the data point and monitors a region of uncertainty in the input space. Our method removes the dependency on machine learning algorithms, which has hindered the existing methods in practical applications and can be widely applied regardless of the classification algorithm. Experimental results on synthetic and real-world data show that the proposed method is capable of detecting concept drift on unlabeled data.",https://ieeexplore.ieee.org/document/9534340/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore
10.1109/ICNNB.2005.1614779,Bp Neural Network Implementation On Real-time Reconfigurable FPGA System For A Soft-sensing Process,IEEE,Conferences,"In this paper, the algorithm and structure of BP NN (Backpropagation neural network) and its training process used for a soft-sensing process are described and its implementation on real-time reconfigurable FPGA (Field Programmable Field Array) system is introduced. The whole system is totally controlled by a microprocessor chip which can completely manage to in system reconfigure FPGA between the two models: BP neural network model and its training model. That is, only one single FPGA is configured with multifunction. This technique can be widely applied into the field such as measurement of human-body internal state, space traffic equipment, deep-sea exploration, where small size of measurement equipment is required and only one microcomputer system used for multifunction is allowed to be installed.",https://ieeexplore.ieee.org/document/1614779/,2005 International Conference on Neural Networks and Brain,13-15 Oct. 2005,ieeexplore
10.1109/AIVR50618.2020.00052,Breast3D: An Augmented Reality System for Breast CT and MRI,IEEE,Conferences,"Adoption of Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) - known collectively as Extended Reality (XR) devices has been rapidly increasing over recent years. However, the focus of XR research has shown a lack of diversity in solutions to the problems within medicine, with it being predominantly focused in augmenting surgical procedures. Whilst important, XR applied to aiding medical diagnosis and surgical planning is relatively unexplored. In this paper we present a fully functional mammographic image analysis system, Breast3D, that can reconstruct MRI and CT scan data in XR. With breast cancer Breast Imaging-Reporting and Data System (BI-RADS) risk lexicon, early detection and clinical workflow such as Multi-disciplinary team (MDT) meetings for cancer in mind, our new mammography visualization system reconstructs CT and MRI volumes in a real 3D space. Breast3D is built upon the past literature and inspired from research for diagnosis and surgical planning. In addition to visualising the models in MR using the Microsoft HoloLens, Breast3D is versatile and portable to different XR head-mounted displays such as HTC Vive. Breast3D demonstrates the early potential for XR within diagnostics of 3D mammographic modalities, an application that has been proposed but until now has not been implemented.",https://ieeexplore.ieee.org/document/9319058/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore
10.1109/IJCNN.2017.7966229,Brewing the first ever automatic memory management utility for SpiNNaker: Real-time garbage collection for STDP simulations,IEEE,Conferences,"First generation SpiNNaker chip uses ARM968, with highly limited internal memory space, as its core element. In simulations of learning algorithms, many biologically plausible learning rules require history traces of each neuron's activity to be stored. As a result, the history traces of neurons rapidly fill the internal memory space eventually reaching the limits of ARM968. To lower the possibility of memory overflow, we propose to introduce a memory management routine working in the background, which must respect the biological timing constraints of the SpiNNaker simulations. Real-time garbage collection is an automatic memory management technique that can satisfy these requirements. This study presents the first ever implementation of real-time garbage collector for SpiNNaker architecture and evaluates the performance, carefully considering the biological real-time constraints of the system.",https://ieeexplore.ieee.org/document/7966229/,2017 International Joint Conference on Neural Networks (IJCNN),14-19 May 2017,ieeexplore
10.1109/SiPS.2015.7345005,Bridge deep learning to the physical world: An efficient method to quantize network,IEEE,Conferences,"As better performance is achieved by deep convolutional network with more and more layers, the increasing number of weighting and bias parameters makes it only possible to be implemented on servers in cyber space but infeasible to be deployed in physical-world embedded systems because of huge storage and memory bandwidth requirements. In this paper, we proposed an efficient method to quantize the model parameters. Instead of taking the quantization process as a negative effect on precision, we regarded it as a regularize problem to prevent overfitting, and a two-stage quantization technique including soft- and hard-quantization is developed. With the help of our quantization method, not only 93.75% of the parameter memory size can be reduced by replacing the word length from 32-bit to 2-bit, but the testing accuracy after quantization is also better than previous approaches in some dataset, and the additional training overhead is only 3% of the ordinary one.",https://ieeexplore.ieee.org/document/7345005/,2015 IEEE Workshop on Signal Processing Systems (SiPS),14-16 Oct. 2015,ieeexplore
10.1109/ISNCC.2018.8530988,Building an Intelligent and Efficient Smart Space to Detect Human Behavior in Common Areas,IEEE,Conferences,"Smart spaces have become an integral part of our daily routines to improve quality of life for many different groups of people. The use of embedded systems to build these smart spaces, in combination with data analytics, can provide real-time information about the environment and how it interacts with the people in it. In this paper, we demonstrate how one embedded system that acquires data based on a 2-dimensional positional-grid, movement, temperature and vibration is used to build a smart and pervasive space. Data collected from these sensors is used for real time localization in conjunction with machine learning mechanisms to analyze human activities. We evaluate five machine learning algorithms, namely Logistic Regression, Support Vector Machine, Decision Tree, Random Forest, Naive Bayes and Artificial Neural Network applied on a dataset collected in our lab. Results show high classification performance for all methods giving up-to 99.95% classification accuracy. These patterns provide useful information about occupancy patterns, movement patterns, etc., which will be later used to allocate computational resources in the smart space accordingly. Furthermore, our implementation does not use any camera or microphone deployment, hence addressing potential privacy issues.",https://ieeexplore.ieee.org/document/8530988/,"2018 International Symposium on Networks, Computers and Communications (ISNCC)",19-21 June 2018,ieeexplore
10.1109/MLSP52302.2021.9596414,Caesynth: Real-Time Timbre Interpolation and Pitch Control with Conditional Autoencoders,IEEE,Conferences,"In this paper, we present a novel audio synthesizer, CAESynth, based on a conditional autoencoder. CAESynth synthesizes timbre in real-time by interpolating the reference sounds in their shared latent feature space, while controlling a pitch independently. We show that training a conditional autoen-coder based on accuracy in timbre classification together with adversarial regularization of pitch content allows timbre distribution in latent space to be more effective and stable for timbre interpolation and pitch conditioning. The proposed method is applicable not only to creation of musical cues but also to exploration of audio affordance in mixed reality based on novel timbre mixtures with environmental sounds. We demonstrate by experiments that CAESynth achieves smooth and high-fidelity audio synthesis in real-time through timbre interpolation and independent yet accurate pitch control for musical cues as well as for audio affordance with environmental sound. A Python implementation along with some generated samples are shared online.",https://ieeexplore.ieee.org/document/9596414/,2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP),25-28 Oct. 2021,ieeexplore
10.1109/ICCE-Berlin50680.2020.9352201,Camera-LIDAR Object Detection and Distance Estimation with Application in Collision Avoidance System,IEEE,Conferences,"Nowadays we are aware of accelerated development of automotive software. Numerous of ADAS (Advanced Driver Assistance Systems) systems are being developed these days. One such system is the forward CAS (Collision Avoidance System). In order to implement such a system, this paper presents one solution for detecting an object located directly in front of the vehicle and estimating its distance. The solution is based on the use of camera and LIDAR (Light Detection and Ranging) sensor fusion. The camera was used for object detection and classification, while 3D data obtained from LIDAR sensor were used for distance estimation. In order to map the 3D data from the LIDAR to the 2D image space, a spatial calibration was used. The solution was developed as a prototype using the ROS (Robot Operating System) based Autoware open source platform. This platform is essentially a framework intended for the development and testing of automotive software. ROS as the framework on which the Autoware platform is based, provides a library for the Python and C++ programming languages, intended for creating new applications. For the reason that this is a prototype project, and it is popular for application in machine learning, we decided to use the Python programming language. The solution was tested inside the CARLA simulator, where the estimation of the obstacle distance obtained at the output of our algorithm was compared with the ground truth values obtained from the simulator itself. Measurements were performed under different weather conditions, where this algorithm showed satisfactory results, with real-time processing.",https://ieeexplore.ieee.org/document/9352201/,2020 IEEE 10th International Conference on Consumer Electronics (ICCE-Berlin),9-11 Nov. 2020,ieeexplore
10.1109/INFOCOM42981.2021.9488865,Can You Fix My Neural Network? Real-Time Adaptive Waveform Synthesis for Resilient Wireless Signal Classification,IEEE,Conferences,"Due to the sheer scale of the Internet of Things (IoT) and 5G, the wireless spectrum is becoming severely congested. For this reason, wireless devices will need to continuously adapt to current spectrum conditions by changing their communication parameters in real-time. Therefore, wireless signal classification (WSC) will become a compelling necessity to decode fast-changing signals from dynamic transmitters. Thanks to its capability of classifying complex phenomena without explicit mathematical modeling, deep learning (DL) has been demonstrated to be a key enabler of WSC. Although DL can achieve a very high accuracy under certain conditions, recent research has unveiled that the wireless channel can disrupt the features learned by the DL model during training, thus drastically reducing the classification performance in real-world live settings. Since retraining classifiers is cumbersome after deployment, existing work has leveraged the usage of carefully-tailored Finite Impulse Response (FIR) filters that, when applied at the transmitter's side, can restore the features that are lost because of the the channel actions, i.e., waveform synthesis. However, these approaches compute FIRs using offline optimization strategies, which limits their efficacy in highly-dynamic channel settings. In this paper, we improve the state of the art by proposing Chares, a Deep Reinforcement Learning (DRL)-based framework for channel-resilient adaptive waveform synthesis. Chares adapts to new and unseen channel conditions by optimally computing through DRL the FIRs in real time. Chares is a DRL agent whose architecture is based upon the Twin Delayed Deep Deterministic Policy Gradients (TD3), which requires minimal feedback from the receiver and explores a continuous action space for best performance. Chares has been extensively evaluated on two well-known datasets with an extensive number of channels. We have also evaluated the real-time latency of Chares with an implementation on field-programmable gate array (FPGA). Results show that Chares increases the accuracy up to 4.1x when no waveform synthesis is performed, by 1.9x with respect to existing work, and can compute new actions within 41 s.",https://ieeexplore.ieee.org/document/9488865/,IEEE INFOCOM 2021 - IEEE Conference on Computer Communications,10-13 May 2021,ieeexplore
10.1109/CAIA.1990.89198,Case study of a knowledge-based system which plans molecular genetic experiments,IEEE,Conferences,"A logic-based approach to planning recombinant DNA experiments is presented. Plans are constructed by attempting to match lists of constraints with existing molecules, with failure resulting in rule-directed decomposition of these constraints to create subproblems. Several AI techniques which are not traditionally associated with planning but which find application for this problem domain are used. These include an object-oriented paradigm to help reduce the size of the search space and to afford an approach to efficient machine learning and the use of simulation to solve aspects of the frame problem and other real-world difficulties idiosyncratic to molecular genetics. The resulting system is capable of solving tasks which could not be addressed by previous attempts at planning in this problem domain.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/89198/,Sixth Conference on Artificial Intelligence for Applications,5-9 May 1990,ieeexplore
10.1109/URS.2007.371826,Cellular automata urban growth model calibration with genetic algorithms,IEEE,Conferences,"Last few decades witness a dramatic increase in city population worldwide associated with excessive urbanization rates. This raises the necessity to understand the dynamics of urban growth process for sustainable distribution of available resources. Cellular automata, an artificial intelligence technique composed of pixels, states, neighborhood and transition rules, is being widely implemented to model the urban growth process due to its ability to fit such complex spatial nature using simple and effective rules. The main objective of our work is to use genetic algorithms to effectively calibrate, i.e., identify transition rule values, a cellular automata urban growth model that is designed as a function of multitemporal satellite imagery and population density. Transition rules in our model identify the required neighborhood urbanization level for a test pixel to develop. Calibration is performed spatially to find best rule values per township. Genetic algorithms calibration model, through proper design of their parameters, including objective function, initial population, selection, crossover and mutation, is prepared to fit the cellular automata model. Genetic algorithms start processing the initial solution space, through sequential implementation of the parameters, to identify the best rule values using a predefined criterion over the maximum number of iterations. Minimum objective function, representing the total modeling errors, is used to identify the optimal rule values. Each rule set is evaluated in term of urban level and pattern match with reality. Calibration with genetic algorithms proves to be effective in producing the optimal rule values in a time effective manner at an early generation. Proposed calibration algorithm is implemented to model the historical urban growth of Indianapolis-IN, USA. Urban growth results show a close match for both urban count and pattern with reality.",https://ieeexplore.ieee.org/document/4234425/,2007 Urban Remote Sensing Joint Event,11-13 April 2007,ieeexplore
10.1109/ROBOT.1993.292250,Cellular robotics: simulation and HW implementation,IEEE,Conferences,"Aspects of self-organization are presented in this paper. Computer simulations as well as a real prototypical implementation are used to illustrate the proposed approach. Results of simulations are presented to compare different strategies of self-organization enabling a system of autonomous robots to form a chain between two landmarks in a completely unknown environment. This chain implicitly represents a path between any two points of the environment without an explicit representation of free space (no single robot has a global map of the environment). The experimental part, even if restricted to a few robots, demonstrates that the set of stimuli-action processes used in the simulations are indeed feasible on real systems.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/292250/,[1993] Proceedings IEEE International Conference on Robotics and Automation,2-6 May 1993,ieeexplore
10.1109/ENC.2005.14,Cervical cancer detection using colposcopic images: a temporal approach,IEEE,Conferences,"In the present work, we propose a methodology analysis of the colposcopic images to help the expert to make a more robust diagnosis of precursor lesions of cervical cancer. Although some others approaches have been used to assess cervical lesion, a complete methodology to evaluate temporal changes of tissue color is still missing. The different processes involved in the analysis are described. The image registration was implemented using the phase correlation method followed by a locally applied algorithm based on the normalized cross-correlation. During the parameterization process, each time series obtained from the image sequences was represented as a parabola in a parameter space. A supervised Bayesian learning approach is proposed to classify the features in the parameter space according to the classification made by the colposcopist. Then those labels are used as a criterion to categorize the tissue and perform the image segmentation. Some preliminary results are shown using unsupervised learning with real data.",https://ieeexplore.ieee.org/document/1592214/,Sixth Mexican International Conference on Computer Science (ENC'05),26-30 Sept. 2005,ieeexplore
10.1109/C2I454156.2021.9689446,Challenges in Design and Development of Electron Beam Weld Predictor Software,IEEE,Conferences,"Electron beam welding (EBW) is high power density welding process used predominantly for fabrication of launch vehicles and spacecrafts made of aerospace grade materials. EBW process at LPSC-ISRO is a space qualified process and the cost and time involved in imbibing quality into the EBW process amounts to 65% of the total fabrication costs and 80 % of the total welding time. The weld quality is ensured by evaluating a pre-weld WTS (Weld Test Specimen), each time a weld is done.. The LPSC-ISRO-Bangalore, houses many variants of EBW machines and in order to tap, utilise the huge potential of empirical data available in-house, this software has been designed and developed to predict the EBW machine parameters for a given material and geometry details. The software developed is based on machine learning of the empirical data. The accuracy of prediction results have been validated using EBW experiments and presently the first version of the software is available for deployment in ISRO and continual improvement of prediction accuracies is ensured by enhancing the database with additional reference points. The software aids in reduction of total manufacturing cost and lead time as the WTS (Weld Test Specimen) requirements have been substantially reduced with aid of this software. Unlike software development for electrical and electronic systems, the challenges in design and development of software for mechanical systems (EBW machines in this design) commences with limited data availability in assessable form; data collection and assimilation are humongous as it is based on almost 20 nos of parameters related to EBW machine and weld quality and not all parameters are in measurable or assessable format. Despite this limitation, the software has been developed in two phases including feature extraction studies in phase2 thus enhancing the prediction accuracies achieved in phase1(a primitive and draft software design). Also the other challenges are input and output parameters for the software are in analog format &amp; knowledge based (interpretation needs skill); the empirical data is predominantly dependent on physics of EBW such as geometry, material properties and software accuracy to be validated by real-time physical EBW experiments. The challenges have been overcome by ensuring precision in empirical data collection, assimilation and cleansing and also feature extraction studies indicated the relevance in selection of input parameters. The selection of machine learning algorithm is also based on the applicability of the selected algorithm to address all the above challenges endured. Also the experimental results validated the prediction accuracies achieved via the Phase2 developed software. Despite the challenges endured, the indigenously designed, developed and validated EBW predictor software can predict EBW machine parameters (beam current and focus current) without any need to weld a WTS and hence, the software developed aids in reduction of the total manufacturing cost and also the lead time involved in realisation of launch vehicles and spacecrafts for various ISRO missions have been substantially reduced with induction of this EBW Predictor software.",https://ieeexplore.ieee.org/document/9689446/,"2021 2nd International Conference on Communication, Computing and Industry 4.0 (C2I4)",16-17 Dec. 2021,ieeexplore
10.1109/FiCloudW.2017.79,Changes of Cyber-Attacks Techniques and Patterns after the Fourth Industrial Revolution,IEEE,Conferences,"In this paper, we predicted the changes of cyber-attacks techniques and patterns after the fourth industrial revolution with the epochal shift of information and communication technology and innovation of science and technology. Cyber space will be hyper-connection, cross-domain, and super intelligence space as connecting everything in the world due to a fusion of information and communication technologies such as artificial intelligence, internet of things, and cyber-physical systems. Cyber-attacks will use all electronic devices including wireless or wire networks, hardware, software, and cyber-physical systems as a route. The hacking tool's functions will evolve into a variety of forms reflecting human thought and behavioral procedures. The attack target will not be limited to a specific object. The purpose of the cyber-attack is to focus on secondary effects and indirect attacks as well as direct attacks.",https://ieeexplore.ieee.org/document/8113773/,2017 5th International Conference on Future Internet of Things and Cloud Workshops (FiCloudW),21-23 Aug. 2017,ieeexplore
10.1109/IEMBS.2007.4353509,Channel and Feature Selection in Multifunction Myoelectric Control,IEEE,Conferences,"Real time controlling devices based on myoelectric singles (MES) is one of the challenging research problems. This paper presents a new approach to reduce the computational cost of real time systems driven by myoelectric signals (MES) (a.k.a Electromyography-EMG). The new approach evaluates the significance of feature/channel selection on MES pattern recognition. Particle Swarm Optimization (PSO), an evolutionary computational technique, is employed to search the feature/channel space for important subsets. These important subsets will be evaluated using a multilayer perceptron trained with back propagation neural network (BPNN). Practical results acquired from tests done on six subject's datasets of MES signals measured in a noninvasive manner using surface electrodes are presented. It is proved that minimum error rates can be achieved by considering the correct combination of features/channels, thus providing a feasible system for practical implementation purpose for rehabilitation of patients.",https://ieeexplore.ieee.org/document/4353509/,2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,22-26 Aug. 2007,ieeexplore
10.1109/ICNP.2016.7784407,Characterizing industrial control system devices on the Internet,IEEE,Conferences,"Industrial control system (ICS) devices with IP addresses are accessible on the Internet and play a crucial role for critical infrastructures like power grid. However, there is a lack of deep understanding of these devices' characteristics in the cyberspace. In this paper, we take a first step in this direction by investigating these accessible industrial devices on the Internet. Because of critical nature of industrial control systems, the detection of online ICS devices should be done in a real-time and non-intrusive manner. Thus, we first analyze 17 industrial protocols widely used in industrial control systems, and train a probability model through the learning algorithm to improve detection accuracy. Then, we discover online ICS devices in the IPv4 space while reducing the noise of industrial honeypots. To observe the dynamics of ICS devices in a relatively long run, we have deployed our discovery system on Amazon EC2 and detected online ICS devices in the whole IPv4 space for eight times from August 2015 to March 2016. Based on the ICS device data collection, we conduct a comprehensive data analysis to characterize the usage of ICS devices, especially in the answer to the following three questions: (1) what are the distribution features of ICS devices, (2) who use these ICS devices, and (3) what are the functions of these ICS devices.",https://ieeexplore.ieee.org/document/7784407/,2016 IEEE 24th International Conference on Network Protocols (ICNP),8-11 Nov. 2016,ieeexplore
10.1109/ICCSS53909.2021.9721962,Chebyshev Polynomial Broad Learning System,IEEE,Conferences,"The broad learning system (BLS) has been attracting more and more attention due to its excellent property in the field of machine learning. A great deal of variants and hybrid structures of BLS have also been designed and developed for better performance in some specialized tasks. In this paper, the Chebyshev polynomials are introduced into the BLS to take advantage of their powerful approximation capability, where the feature windows are replaced by a set of Chebyshev polynomials. This new variant, named Chebyshev polynomial BLS (CPBLS), has a light structure with a reduction in computational complexity since the sparse autoencoder is removed. Instead, the dimension of each input sample is expended by n + 1 Chebyshev polynomials, mapping the original feature into a new feature space with higher dimension, which helps to classify the patterns in training. The proposed CPBLS is evaluated by some popular datasets from UCI and KEEL repositories, and it outperforms some representative neural networks and neuro-fuzzy models in terms of classification accuracy. The CPBLS also show some advantages over the recent developed compact fuzzy BLS (CFBLS) which indicates its great potential in future research and real-world applications.",https://ieeexplore.ieee.org/document/9721962/,"2021 8th International Conference on Information, Cybernetics, and Computational Social Systems (ICCSS)",10-12 Dec. 2021,ieeexplore
10.1109/CNNA.1998.685324,Classification systems based on neural networks,IEEE,Conferences,"Classification is a problem that appears in many real life applications. We describe the general case of multi-class classification, where the task of the classification system is to map an input vector x to one of K&gt;2 given classes. This problem is split in many two-class classification problems, each of them describing a part of the whole problem. These are solved by neural networks, producing an intermediate output in a reference space, which is then decoded to the solution of the original problem. The methods described here are then applied to the handwritten character recognition problem to produce the results described later in the article. It is suspected that they also may be applied successfully in the context of the CNN paradigm and be implemented on a CNN-Universal Machine.",https://ieeexplore.ieee.org/document/685324/,1998 Fifth IEEE International Workshop on Cellular Neural Networks and their Applications. Proceedings (Cat. No.98TH8359),14-17 April 1998,ieeexplore
10.1109/INES.2013.6632823,Cloud security monitoring and vulnerability management,IEEE,Conferences,"Cloud infrastructure becomes the primary business environment for all types of enterprises during recent years. In cloud computing security is a fundamental concern, loss of control and potential lack of trust prevent large set of potential customers to immerse in the cloud world. One of the major key problem is how one can test, monitor or measure the underlying Cloud infrastructure from user/customer space. We have developed a solution which is able to examine the infrastructure, from security point-of-views. We offer a clear, adaptable, concise and easy-to-extend framework to assess the underlying cloud infrastructure. Our developed solution is generic and multipurpose it can act as a vulnerability scanner, and performance benchmarking tool at the same time. It is virtualized, it is agent based and collects assessment information by the decentralized Security Monitor and it archives the results received from the components and visualize them via a web interface for the tester/administrators. In this paper we present our virtualized cloud security monitor and assessment solution, we describe its functionalities and provide some examples of its results captured in real systems.",https://ieeexplore.ieee.org/document/6632823/,2013 IEEE 17th International Conference on Intelligent Engineering Systems (INES),19-21 June 2013,ieeexplore
10.1109/EMBC.2018.8513597,Clustering Based Kernel Reinforcement Learning for Neural Adaptation in Brain-Machine Interfaces,IEEE,Conferences,"Reinforcement learning (RL) interprets subject's movement intention in Brain Machine Interfaces (BMIs) through trial-and-error with the advantage that it does not need the real limb movements. When the subjects try to control the external devices purely using brain signals without actual movements (brain control), they adjust the neural firing patterns to adapt to device control, which expands the state-action space for the RL decoder to explore. The challenge is to quickly explore the new knowledge in the sizeable state-action space and maintain good performance. Recently quantized attention-gated kernel reinforcement learning (QAGKRL) was proposed to quickly explore the global optimum in Reproducing Kernel Hilbert Space (RKHS). However, its network size will grow large when the new input comes, which makes it computationally inefficient. In addition, the output is generated using the whole input structure without being sensitive to the new knowledge. In this paper, we propose a new kernel based reinforcement learning algorithm that utilizes the clustering technique in the input domain. The similar neural inputs are grouped, and a new input only activates its nearest cluster, which either utilizes an existing sub-network or forms a new one. In this way, we can build the sub-feature space instead of the global mapping to calculate the output, which transfers the old knowledge effectively and also consequently reduces the computational complexity. To evaluate our algorithm, we test on the synthetic spike data, where the subject's task mode switches between manual control and brain control. Compared with QAGKRL, the simulation results show that our algorithm can achieve a faster learning curve, less computational time, and more accuracy. This indicates our algorithm to be a promising method for the online implementation of BMIs.",https://ieeexplore.ieee.org/document/8513597/,2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),18-21 July 2018,ieeexplore
10.1109/ASP-DAC47756.2020.9045595,Co-Exploring Neural Architecture and Network-on-Chip Design for Real-Time Artificial Intelligence,IEEE,Conferences,"Hardware-aware Neural Architecture Search (NAS), which automatically finds an architecture that works best on a given hardware design, has prevailed in response to the ever-growing demand for real-time Artificial Intelligence (AI). However, in many situations, the underlying hardware is not pre-determined. We argue that simply assuming an arbitrary yet fixed hardware design will lead to inferior solutions, and it is best to co-explore neural architecture space and hardware design space for the best pair of neural architecture and hardware design. To demonstrate this, we employ Network-on-Chip (NoC) as the infrastructure and propose a novel framework, namely NANDS, to co-explore NAS space and NoC Design Search (NDS) space with the objective to maximize accuracy and throughput. Since two metrics are tightly coupled, we develop a multi-phase manager to guide NANDS to gradually converge to solutions with the best accuracy-throughput tradeoff. On top of it, we propose techniques to detect and alleviate timing performance bottleneck, which allows better and more efficient exploration of NDS space. Experimental results on common datasets, CIFAR10, CIFAR-100 and STL-10, show that compared with state-of-the-art hardware-aware NAS, NANDS can achieve 42.99% higher throughput along with 1.58% accuracy improvement. There are cases where hardware-aware NAS cannot find any feasible solutions while NANDS can.",https://ieeexplore.ieee.org/document/9045595/,2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC),13-16 Jan. 2020,ieeexplore
10.1109/C5.2005.8,Co-creation system and human-computer interaction,IEEE,Conferences,"The purpose of this research is to realize a ""co-creation system"". Co-creation means co-emergence of real-time coordination by sharing subjective space and time between different persons. Human communication with emergent reality like this is essential to improve communicability in social systems, and we assume such communication needs two kinds of information process at the same time. One is explicit communication such as the transmission of messages and the other is implicit embodied interaction such as the sympathy and direct experience. The conventional IT system mainly covers the former process but we have been pointing out the importance of the latter process. Especially, this implicit process is related with rhythmic interaction between humans, such as entrainment of body motion. From this background, using these implicit and explicit processing complementarily, we are developing co-creative man-machine interfaces and communication media. We think this dual-processing based new technology will be effective for recovering human linkage and mutual-reliability that has been weakened in modern IT society.",https://ieeexplore.ieee.org/document/1419802/,"Third International Conference on Creating, Connecting and Collaborating through Computing (C5'05)",28-29 Jan. 2005,ieeexplore
10.1109/ECC.2014.6862630,Co-design of hardware and algorithms for real-time optimization,IEEE,Conferences,"It is difficult or impossible to separate the performance of an optimization solver from the architecture of the computing system on which the algorithm is implemented. This is particularly true if measurements from a physical system are used to update and solve a sequence of mathematical optimization problems in real-time, such as in control, automation, signal processing and machine learning. In these real-time optimization applications the designer has to trade off computing time, space and energy against each other, while satisfying constraints on the performance and robustness of the resulting cyber-physical system. This paper is an informal introduction to the issues involved when designing the computing hardware and a real-time optimization algorithm at the same time, which can result in systems with efficiencies and performances that are unachievable when designing the sub-systems independently. The co-design process can, in principle, be formulated as a sequence of uncertain and non-smooth optimization problems. In other words, optimizers might be used to design optimizers. Before this can become a reality, new systems theory and numerical methods will have to be developed to solve these co-design problems effectively and reliably.",https://ieeexplore.ieee.org/document/6862630/,2014 European Control Conference (ECC),24-27 June 2014,ieeexplore
10.1109/ICECCME52200.2021.9591113,Cobots for FinTech,IEEE,Conferences,"Embedded devices enabling payments transaction processing in Financial Services industry cannot have any margin for error. These devices need to be tested &amp; validated by replicating production like environment to the extent possible. This means literally handling payments related events like swiping a credit card, tapping a mobile phone or pressing buttons amongst many other things like in real world. Embedded Software development is time consuming as it involves multiple man-machine interactions and dependencies such as managing and handling embedded devices, operating devices (Push buttons, interpret display panels, read receipt printouts etc.) and sharing devices for collaboration within team. During the current pandemic, it was impossible for software teams to travel to office, share devices or even procure necessary devices on time for project related tasks. This caused delay to project delivery and increased Time to market. The paper describes how the team used Capgemini's flexible Robotics as a Service (RaaS) platform that helped during pandemic to automate feasible man-machine interactions using Robotic arms. The paper provides details of the work done by the team that involves internet of things (IoT), Artificial Intelligence (AI) to remotely handle and operate hardware and devices thereby completing embedded software development life cycles faster and well within budget while ensuring superior product quality and importantly ensuring team's health and safety. This is novel in Financial Services space.",https://ieeexplore.ieee.org/document/9591113/,"2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",7-8 Oct. 2021,ieeexplore
10.1109/CIMCA.2005.1631533,Cognitive Perception in RAFALE-SP Methodology,IEEE,Conferences,"Several methodologies based on multi-agent systems (MAS) already exist. They help designers to describe software or to create MAS which aim at solving complex problems by simulations. Due to used formalisms, a methodology may be more or less generic. In this context, we have created a mobility oriented methodology called RAFALE-SP based on multi-agent systems. It helps us to describe mobiles which move on a space. This environment can be a virtual representation of a real space like a town where unpredictable events arise. We apply our methodology to solve problems which come from different research areas. We use it to find answers to geographical problems. The presented methodology begins by a conceptual representation of each mobile type and finishes by a mobility simulator. It uses several formalisms: UML and Ploom-unity. They allow us to define mobiles, their interactions and their environment. According to their knowledge, their behaviour rules, mobiles moves on a space by following few motion types. They get an individual perception of the world. In this paper we focus on mobile perception description",https://ieeexplore.ieee.org/document/1631533/,"International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)",28-30 Nov. 2005,ieeexplore
10.1109/ITSC.2017.8317627,Cognitive map-based model: Toward a developmental framework for self-driving cars,IEEE,Conferences,"End-to-end learning and multi-sensor fusion-based methods are two major frameworks used for self-driving cars. To enable these intelligence vehicles to acquire driving skills at a level comparable to that of human drivers, long short-term memory of previous self-driving processes is necessary, but is difficult to introduce into the above-mentioned frameworks. In this paper, we propose a model for self-driving cars called the cognitive map-based neural network (CMNN). Our framework consists of three parts: a convolutional neural network that can perceive the environment in the manner that the human visual cortex does, a cognitive map to describe the locations of objects in a complex traffic scene and the relationships among them, and a recurrent neural network to process long short-term memory from the cognitive map, which is updated in real time. The proposed model is built to simultaneously handle three tasks: i) detecting free space and lane boundaries, ii) estimating vehicle pose and obstacle distance, and iii) learning to plan and control based on the behaviors of a human driver. More significantly, our approach introduces external instructions during an end-to-end driving process. To test it, we created a large-scale road vehicle dataset (RVD) containing more than 50,000 labeled road images captured by three cameras. We implemented the proposed model on an embedded system.",https://ieeexplore.ieee.org/document/8317627/,2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC),16-19 Oct. 2017,ieeexplore
10.1109/ICECE48499.2019.9058541,Collaborative Filtering Based on Factorization and Distance Metric Learning,IEEE,Conferences,"Collaborative filtering predicts user preferences by modeling the user's historical behavior. Matrix factorization plays an important role in collaborative filtering. Matrix factorization use dot product to predicts rating. Nevertheless, the dot product does not satisfy the triangle inequality, which may limit their expressiveness and lead to sub-optimal solutions. This paper combines metric learning and distance factorization. Firstly, convert the scoring matrix to a distance matrix. Then the distance matrix is factorization by metric learning to obtain the position of users and items in low-dimensional Euclidean space. User preferences and distance are negatively correlated, the user is closer to the favorite item and farther away from the unloved item, that is, the user's preference is reflected by the distance. Experiments conducted on real-world data sets have shown that compared with classical algorithm, our method significantly improves the accuracy of recommendations.",https://ieeexplore.ieee.org/document/9058541/,2019 IEEE 2nd International Conference on Electronics and Communication Engineering (ICECE),9-11 Dec. 2019,ieeexplore
10.1109/IRI-05.2005.1506442,Collaborative knowledge management by integrating knowledge modeling and workflow modeling,IEEE,Conferences,"Recently both industrial and academic researches show great interest in knowledge management. However, users still find it hard to obtain a suitable knowledge management tool that fits for their needs. Although business requirements always change, current software systems still cannot be adapted to these changes quickly. In this paper, a framework is put forward to facilitate the design of an adaptive knowledge management system. In this framework, the structural knowledge modeling is combined with processes, which are used for ensuring the quality of knowledge acquisition in the framework. Two kinds of spaces, knowledge space and process space, and a knowledge state model are introduced. Finally, application systems based on this framework, which are being used in three real business enterprises for controlling life cycle of knowledge management in China, are discussed.",https://ieeexplore.ieee.org/document/1506442/,"IRI -2005 IEEE International Conference on Information Reuse and Integration, Conf, 2005.",15-17 Aug. 2005,ieeexplore
10.1109/EEEI.2010.5662183,Collision-based computing and Event Calculus to represent and control complex engineering systems,IEEE,Conferences,"Software is always more a strategic asset to sustain complexity of human societies. The requirement to control an increasing number of continuously changing processes is the cause of a transition from centralized to distributed and cooperative organizations. The consequence is a computer systems design integrated in the process of define/improve system models with a progressive approach too. To reach the described research objective we have developed a new approach to design a computer system. In our design approach we mapped the physical structure of a system to a multidimensional Cellular Automata Space (CAS). CAS collects data from sensors that are at the specific coordinates of cells and it is the substrata for energy, materials and information flow representation. Events and Actions in a CAS are described using Event Calculus and the flow of fluents is interpreted on the basis of Collision-based computing paradigm. Trajectories approaching a collision site represent input values, and trajectories of localizations traveling away from a collision site represent output values. At this point we have a model of a real system as dynamic networks of fluents. This is represented using the language of flow pattern, inspired by F. Niscida's work, a semi-symbolic representation of the patterns in CAS states space. The flow patterns are used in a qualitative analysis to monitor and interpret the result of CAS computation.",https://ieeexplore.ieee.org/document/5662183/,2010 IEEE 26-th Convention of Electrical and Electronics Engineers in Israel,17-20 Nov. 2010,ieeexplore
10.1109/SNPD.2018.8441141,Color Classification based on Pixel Intensity Values,IEEE,Conferences,"What is this color? This is an easy question but sometimes difficult to answer. Because there are millions of colors but a few color names available. In this paper, we propose a method for naming colors based on pixel intensity values. Color naming is important and can be applied to many real world applications such as colorblind assistance, image segmentation, and image retrieval. Our goal here is to map color pixel intensity values to the twelve basic color names: pink, purple, red, orange, yellow, green, cyan, blue, brown, white, grey, and black. We consider only twelve categories, because basic color names were found to be shared between languages. To do so, our method firstly uses the hue value of HSV color space to categorize colors into a predefined number of ranges. Then, we apply a nearest neighbor classification on RGB color space to get color names, where only training colors containing in the same range as testing colors will be considered. Through experiments, we show that the first step will help filter out some incorrect answers, so that color names obtained from the nearest neighbor classification are more accurately. In addition, the evaluation results demonstrate that the proposed method is promising.",https://ieeexplore.ieee.org/document/8441141/,"2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",27-29 June 2018,ieeexplore
10.1109/SIS.2007.368020,Combined Training of Recurrent Neural Networks with Particle Swarm Optimization and Backpropagation Algorithms for Impedance Identification,IEEE,Conferences,"A recurrent neural network (RNN) trained with a combination of particle swarm optimization (PSO) and backpropagation (BP) algorithms is proposed in this paper. The network is used as a dynamic system modeling tool to identify the frequency-dependent impedances of power electronic systems such as rectifiers, inverters, and DC-DC converters. As a category of supervised learning methods, the various backpropagation training algorithms developed for recurrent neural networks use gradient descent information to guide their search for optimal weights solutions that minimize the output errors. While they prove to be very robust and effective in training many types of network structures, they suffer from some serious drawbacks such as slow convergence and being trapped at local minima. In this paper, a modified particle swarm optimization technique is used in combination with the backpropagation algorithm to traverse in a much larger search space for the optimal solution. The combined method preserves the advantages of both techniques and avoids their drawbacks. The method is implemented to train a RNN that successfully identifies the impedance characteristics of a three-phase inverter system. The performance of the proposed method is compared to those of both BP and PSO when used separately to solve the problem, demonstrating its superiority",https://ieeexplore.ieee.org/document/4223149/,2007 IEEE Swarm Intelligence Symposium,1-5 April 2007,ieeexplore
10.1109/ICCA.2016.7505247,Communication of spatial expressions on multi-agent systems using the qualitative Ego-Sphere,IEEE,Conferences,"The need for spatial representations and spatial reasoning is omnipresent in various real world applications of autonomous systems. The task of the qualitative spatial reasoning sub-field of Artificial Intelligence is to provide formalisms allowing a machine to represent and make inferences about spatial entities. In this work we make use of one such formalism for representing qualitative location, named Qualitative Ego-Sphere (QES), that discretises the world around a visual agent into sectors, as well as with respect to the relative distance of objects from the observer's point of view. QES was used in this paper as a means for communicating spatial expressions between pairs of agents. Four situations were proposed and implemented in order to address interactions between pairs of artificial agents and between an artificial agent and a human. Tests with human volunteers suggested that the human description of space in sectors agrees with the qualitative discretisation provided by QES. However, no similar agreement rates were obtained regarding the description of space related to the distance between objects. This was arguably due to the fact that qualitative distance judgements imply the existence of some relative external reference frame (not taken into account in the Qualitative Ego-Sphere formalism).",https://ieeexplore.ieee.org/document/7505247/,2016 12th IEEE International Conference on Control and Automation (ICCA),1-3 June 2016,ieeexplore
10.1109/IJCNN.2007.4371111,Compact hardware for real-time speech recognition using a Liquid State Machine,IEEE,Conferences,"Hardware implementations of Spiking Neural Networks are numerous because they are well suited for implementation in digital and analog hardware, and outperform classic neural networks. This work presents an application driven digital hardware exploration where we implement realtime, isolated digit speech recognition using a Liquid State Machine (a recurrent neural network of spiking neurons where only the output layer is trained). First we test two existing hardware architectures, but they appear to be too fast and thus area consuming for this application. Then we present a scalable, serialised architecture that allows a very compact implementation of spiking neural networks that is still fast enough for real-time processing. This work shows that there is actually a large hardware design space of Spiking Neural Network hardware that can be explored. Existing architectures only spanned part of it.",https://ieeexplore.ieee.org/document/4371111/,2007 International Joint Conference on Neural Networks,12-17 Aug. 2007,ieeexplore
10.1109/PIC.2018.8706141,Comparison and Analysis on Typical Network Representation Learning Algorithms,IEEE,Conferences,"Large-scale complex networks show complex nonlinear relationships among objects, such as the social relationships in the real world, the citation relationship among papers and the interactions among proteins in biology. The analysis of complex network systems make it possible to reveal network structures, information disseminating laws, and communication patterns. Network representation learning (NRL) algorithms focus on mapping the original network structure information to a low-dimensional vector space through a series of operations under the premise of maximally retaining the network structure. In order to analyze current representative NRL algorithms effectively to provide valuable references for other researchers, we built an experimental platform to perform and test the NRL algorithms based on matrix factorization, the NRL algorithms based on shallow neural network and the NRL algorithms based on deep neural network, with datasets on Collaboration Network, Social Network and Citation Network. We implemented a series of comprehensive experiments, based on metrics include precision@k, micro-F1 and macro-F1. Our experiments include network reconstruction, vertex classification, and link prediction, and show readers principles, performances and applications of typical NRL algorithms.",https://ieeexplore.ieee.org/document/8706141/,2018 IEEE International Conference on Progress in Informatics and Computing (PIC),14-16 Dec. 2018,ieeexplore
10.1109/ITNT52450.2021.9649145,Comparison of Reinforcement Learning Algorithms for Motion Control of an Autonomous Robot in Gazebo Simulator,IEEE,Conferences,"This article compares various implementations of deep Q learning as it is one of the most efficient reinforcement learning algorithms for discrete action space systems. The efficiency of the implementations for the classical Cartpole problem ported to the Gazebo environment is investigated. Then, these algorithms are compared for a self-created bipedal robot problem. Since the creation and configuration of a real robotic system is a laborious process, the initial debugging of the robot can be performed using the appropriate software that simulates the real environment. In our case, the Gazebo simulator was used. Using the simulator allows you to conduct research without having a real robotic system. In this case, it is possible to transfer the results from the simulator to the real system. The result of the study is the conclusion about the greatest efficiency of deep Q-learning with the experience reproduction mechanism. Also, the conclusion is that even for a robot with two degrees of freedom, Q-learning algorithms are not effective enough, and a comparative study with other families of reinforcement learning algorithms is needed.",https://ieeexplore.ieee.org/document/9649145/,2021 International Conference on Information Technology and Nanotechnology (ITNT),20-24 Sept. 2021,ieeexplore
10.1109/RCAR52367.2021.9517443,Complementary Multi-Branch CNNs Towards Real-World 3D Point Classification,IEEE,Conferences,"Autonomous driving requires precise and efficient point clouds processing techniques, where deep learning has shown great potential. Nonetheless, most of the existing works achieve high accuracy in synthetic data while performing unsatisfactorily in real-world datasets. On the other hand, though existing multi-view-based, volumetric-based, and point-based methods have achieved promising results, they still suffer from high computational complexity or low memory utilization efficiency. For example, the computational time and memory consumption of multi-view-based and volumetric-based methods increase at most cubically with the increasing of the input resolution, while half of the point-based methods's runtime is wasted due to their irregular memory access. To address these issues, we propose a novel method to fuse complementary CNNs, dubbed as Multi-Branch Convolutional Neural Networks (MBCNN), which achieves the current state-of-the-art performance while maintaining high efficiency. The underlying design of MBCNN is to utilize the point-based convolution as a local-feature branch and spherical convolution as a global-feature branch, to capture patterns in space and exploit the merit of the pointwise MLP, which is robust to the perturbation. Besides, we design an efficient voxel-based indexing technique to prevent the feature-extracting phase from being blocked, thus significantly accelerating the procedure of finding a point's neighbor. Experimental results show that MBCNN achieves state-of-the-art performance on the real-world 3D point clouds dataset while reducing 30% of runtime in neighbor indexing.",https://ieeexplore.ieee.org/document/9517443/,2021 IEEE International Conference on Real-time Computing and Robotics (RCAR),15-19 July 2021,ieeexplore
10.1109/AMFG.2003.1240833,Component-based LDA method for face recognition with one training sample,IEEE,Conferences,"Many face recognition algorithms/systems have been developed in the last decade and excellent performances are also reported when there is sufficient number of representative training samples. In many real-life applications, only one training sample is available. Under this situation, the performance of existing algorithms will be degraded dramatically or the formulation is incorrect, which in turn, the algorithm cannot be implemented. We propose a component-based linear discriminant analysis (LDA) method to solve the one training sample problem. The basic idea of the proposed method is to construct local facial feature component bunches by moving each local feature region in four directions. In this way, we not only generate more samples, but also consider the face detection localization error while training. After that, we employ a sub-space LDA method, which is tailor-made for small number of training samples, for the local feature projection to maximize the discrimination power. Finally, combining the contributions of each local feature draws the recognition decision. FERET database is used for evaluating the proposed method and results are encouraging.",https://ieeexplore.ieee.org/document/1240833/,2003 IEEE International SOI Conference. Proceedings (Cat. No.03CH37443),17-17 Oct. 2003,ieeexplore
10.1109/DCC.2009.75,Compressed Kernel Perceptrons,IEEE,Conferences,"Kernel machines are a popular class of machine learning algorithms that achieve state of the art accuracies on many real-life classification problems. Kernel perceptrons are among the most popular online kernel machines that are known to achieve high-quality classification despite their simplicity. They are represented by a set of B prototype examples, called support vectors, and their associated weights. To obtain a classification, a new example is compared to the support vectors. Both space to store a prediction model and time to provide a single classification scale as O(B). A problem with kernel perceptrons is that on noisy data the number of support vectors tends to grow without bounds with the number of training examples. To reduce the strain at computational resources, budget kernel perceptrons have been developed by upper bounding the number of support vectors. In this work, we propose a new budget algorithm that upper bounds the number of bits needed to store kernel perceptron. Setting the bitlength constraint could facilitate development of hardware and software implementations of kernel perceptrons on resource-limited devices such as microcontrollers. The proposed compressed kernel perceptron algorithm decides on the optimal tradeoff between number of support vectors and their bit precision. The algorithm was evaluated on several benchmark data sets and the results indicate that it can train highly accurate classifiers even when the available memory budget is below 1Kbit. This promising result points to a possibility of implementing powerful learning algorithms even on the most resource-constrained computational devices.",https://ieeexplore.ieee.org/document/4976459/,2009 Data Compression Conference,16-18 March 2009,ieeexplore
10.1109/TENSYMP46218.2019.8971326,Computational Color Naming for Human-Machine Interaction,IEEE,Conferences,"In this paper, we present two simple methods for automatic color naming, the first one learns the pixel-wise color name annotations from the color-chips and later method learns the image-wise color names from the real-world weakly labelled images. Color information is an important feature for many computer vision applications. Color name as a descriptor finds it's applications in many real-world tasks such as Color Blind assistance, image retrieval and scene understanding. Color naming in images is a challenging problem due to shadows, view angles, illumination conditions and surface reflections. Manual labelling of color names for real-world images in applications like search engines, fashion parsing and tracking is a tedious task and time consuming. The proposed systems for color naming automates the process and avoids human labelling. These methods are based on superpixels in the CIELAB color space. We trained Random Forests classifier with color-chip dataset for pixel-wise color naming and for image-wise color naming it is trained on weakly color labelled image dataset. Both models are tested on real-world images for color name judgments. Experimental results shows that color names learned through these proposed systems have advantages in terms of implementation costs, speed of execution and can be used in real-time applications with lowcost hardware.",https://ieeexplore.ieee.org/document/8971326/,2019 IEEE Region 10 Symposium (TENSYMP),7-9 June 2019,ieeexplore
10.1109/FUZZY.1995.410037,"Computational intelligence and multimedia in education: NASA/RMS arm, control in diabetes, intelligent workstation, and management decision aide",IEEE,Conferences,"The use of computational intelligence and multimedia in the formal classroom and in continuing education is a powerful, many faceted tool. We summarize the system development principles important in educating engineers and scientists in the practical application of these tools. Intelligent systems, human interfaces, neural networks, genetic algorithms, evolutionary systems and virtual reality form a toolbox of opportunities. Examples are discussed in the light of turning fuzzy computing concepts into applications. Simulation skills needed for success in industry are emphasized. A prime example of such an educational tool is the system developed at the NASA Goddard Space Flight Center for full scale simulation in actual hardware of the Remote Manipulator System (RMS) arm used on the Shuttle. Other examples include the new Teledoc software expert system at the Diabetes Research Institute (DRI). The computer ""talks"" to callers to elicit information about glucose levels, stress, exercise, diet and various symptoms before adjusting insulin dosages, alerting patients to potential problems add their solutions-including calling the doctor. An intelligent workstation designed to filter information for busy executives, and a decision aide for managers are also discussed.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/410037/,Proceedings of 1995 IEEE International Conference on Fuzzy Systems.,20-24 March 1995,ieeexplore
10.1109/MSR.2017.20,Concept-Based Classification of Software Defect Reports,IEEE,Conferences,"Automatic identification of the defect type from the textual description of a software defect can significantly speed-up as well as improve the software defect management life-cycle. This has been recognized in the research community and multiple solutions based on supervised learning approach have been proposed in the recent literature. However, these approaches need significant amount of labeled training data for use in real-life projects. In this paper, we propose to use Explicit Semantic Analysis (ESA) to carry out concept-based classification of software defect reports. We compute the ""semantic similarity"" between the defect type labels and the defect report in a concept space spanned by Wikipedia articles and then, assign the defect type which has the highest similarity with the defect report. This approach helps us to circumvent the problem of dependence on labeled training data. Experimental results show that using concept-based classification is a promising approach for software defect classification to avoid the expensive process of creating labeled training data and yet get accuracy comparable to the traditional supervised learning approaches. To the best of our knowledge, this is the first use of Wikipedia and ESA for software defect classification problem.",https://ieeexplore.ieee.org/document/7962367/,2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20-21 May 2017,ieeexplore
10.1109/ICNN.1988.23841,Conceptual graph knowledge systems as problem context for neural networks,IEEE,Conferences,"For a connectionist network to be able to learn to generalize well, there must be some correspondence between the structure/constraints of the net's architecture and those of the given problem space. Therefore, recourse to experiments with real-world problems will always be required in connectionist research. The author gives an outline of a problem area for which connectionist nets hold great promise: knowledge systems, where the knowledge is encoded/represented using conceptual graphs. Certain aspects of this problem context are already known, and these are probed for possible implementation by connection is nets. The approach used is to present some basic properties of conceptual graphs, indicate operations important in their application, and point out those that might be candidates for implementation with neural nets. A special representation schema for conceptual graphs is used for their implementation by neural nets.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/23841/,IEEE 1988 International Conference on Neural Networks,24-27 July 1988,ieeexplore
10.1109/RTSS52674.2021.00026,Concurrent Order Dispatch for Instant Delivery with Time-Constrained Actor-Critic Reinforcement Learning,IEEE,Conferences,"Instant delivery has developed rapidly in recent years and significantly changed the lifestyle of people due to its timeliness and convenience. In instant delivery, the order dispatch process is concurrent. Couriers take new orders continuously and deliver multiple orders in a delivery trip (i.e., a batch). The delivery time of orders in a batch is often overlapped and interlinked with each other. The pickup and delivery sequence of the existing orders in a batch changes dynamically due to time constraints and real-time overdue possibility (i.e., the rate of deliveries that are not finished in promised time). Most of existing order dispatch mechanisms are designed for independent order dispatch or concurrent delivery without strict time constraints, hence are incapable of handling real-time concurrent dispatch with strict time constraints in on-demand instant delivery. To address the challenge, we propose a Time-Constrained Actor- Critic Reinforcement learning based concurrent dispatch system called TCAC-Dispatch to enhance the long-term overall revenue and reduce the overdue rate. Specifically, we design a deep matching network (DMN) with a variable action space, which integrates the state embedding (including route behaviors encoding) and actions embedding features into a long-term matching value. Then the Actor-Critic model tackles the concurrent order dispatch problem considering strict time constraints and stochastic demand-supply in instant delivery. An estimated time-based action pruning module is designed to ensure time constraints guarantee and accelerate the training as well as dispatching processes. We evaluate the TCAC-Dispatch with one-month data involved with 36.48 million orders and 42,000 couriers collected from one of the largest instant delivery companies in China, i.e., Eleme. Empirical experiments are conducted on a data-driven emulator deployed on the development environment of Eleme and results show that our method achieves 22% of the increase in total revenue and reduces the overdue rate by 21.6%.",https://ieeexplore.ieee.org/document/9622337/,2021 IEEE Real-Time Systems Symposium (RTSS),7-10 Dec. 2021,ieeexplore
10.1109/IROS.2009.5354270,Consideration on robotic giant-swing motion generated by reinforcement learning,IEEE,Conferences,"This study attempts to make a compact humanoid robot acquire a giant-swing motion without any robotic models by using reinforcement learning; only the interaction with environment is available. Generally, it is widely said that this type of learning method is not appropriated to obtain dynamic motions because Markov property is not necessarily guaranteed during the dynamic task. However, in this study, we try to avoid this problem by embedding the dynamic information in the robotic state space; the applicability of the proposed method is considered using both the real robot and dynamic simulator. This paper, in particular, discusses how the robot with 5-DOF, in which the Q-Learning algorithm is implemented, acquires a giant-swing motion. Further, we describe the reward effects on the Q-Learning. Finally, this paper demonstrates that the application of the Q-Learning enable the robot to perform a very attractive giant-swing motion.",https://ieeexplore.ieee.org/document/5354270/,2009 IEEE/RSJ International Conference on Intelligent Robots and Systems,10-15 Oct. 2009,ieeexplore
10.1109/AERO50100.2021.9438171,Considerations in the Deployment of Machine Learning Algorithms on Spaceflight Hardware,IEEE,Conferences,"Recent advances in artificial intelligence (AI) and machine learning (ML) have revolutionized many fields. ML has many potential applications in the space domain. Next generation space instruments are producing data at rates that exceed the capabilities of current spacecraft to store or transmit to ground stations. Deployment of ML algorithms onboard future spacecraft could perform processing of sensor data as it is gathered, reducing data volume and providing a dramatic increase in throughput of meaningful data. ML techniques may also be used to enhance the autonomy of space missions. However ML techniques have not yet been widely deployed in space environments, primarily due to limitations on the computational capabilities of spaceflight hardware. The need to verify that high-performance computational hardware can reliably operate in this environment delays the adoption of these technologies. Nevertheless, the availability of advanced processing capabilities onboard spacecraft is increasing. These platforms may not provide the processing power of terrestrial equivalents, but they do provide the resources necessary for deploying real-time execution of ML algorithms. In this paper, we present results exploring the implementation of ML techniques on computationally-constrained, high-reliability spacecraft hardware. We show two ML algorithms utilizing deep learning techniques which illustrate the utility of these approaches for space applications. We describe implementation considerations when tailoring these algorithms for execution on computationally-constrained hardware and present a workflow for performing these optimizations. We also present initial results on characterizing the trade space between algorithm accuracy, throughput, and reliability on a variety of hardware platforms with current and anticipated paths to spaceflight.",https://ieeexplore.ieee.org/document/9438171/,2021 IEEE Aerospace Conference (50100),6-13 March 2021,ieeexplore
10.1109/IECON.1994.398074,Constrained Hopfield neural network for real-time predictive control,IEEE,Conferences,The hardware implementation of an optimization network with restrictions to perform real-time generalized predictive control (GPC) is described. The use of a space-efficient stochastic architecture allows a realization on a programmable logic device. As a result a programmable neural chip coprocessor that solves optimization problems subject to restrictions has been developed. Expressions for network parameters are provided to implement GPC. An adaptive controller is achieved using RAM memories to store the network parameters. Experimental results from a simple implementation of the controller are included.&lt;<ETX>&gt;</ETX>,https://ieeexplore.ieee.org/document/398074/,Proceedings of IECON'94 - 20th Annual Conference of IEEE Industrial Electronics,5-9 Sept. 1994,ieeexplore
10.1109/DSAA.2019.00051,Constrained Multi-Objective Optimization for Automated Machine Learning,IEEE,Conferences,"Automated machine learning has gained a lot of attention recently. Building and selecting the right machine learning models is often a multi-objective optimization problem. General purpose machine learning software that simultaneously supports multiple objectives and constraints is scant, though the potential benefits are great. In this work, we present a framework called Autotune that effectively handles multiple objectives and constraints that arise in machine learning problems. Autotune is built on a suite of derivative-free optimization methods, and utilizes multi-level parallelism in a distributed computing environment for automatically training, scoring, and selecting good models. Incorporation of multiple objectives and constraints in the model exploration and selection process provides the flexibility needed to satisfy trade-offs necessary in practical machine learning applications. Experimental results from standard multi-objective optimization benchmark problems show that Autotune is very efficient in capturing Pareto fronts. These benchmark results also show how adding constraints can guide the search to more promising regions of the solution space, ultimately producing more desirable Pareto fronts. Results from two real-world case studies demonstrate the effectiveness of the constrained multi-objective optimization capability offered by Autotune.",https://ieeexplore.ieee.org/document/8964219/,2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA),5-8 Oct. 2019,ieeexplore
10.1109/ICDM.2010.15,Constraint Based Dimension Correlation and Distance Divergence for Clustering High-Dimensional Data,IEEE,Conferences,"Clusters are hidden in subspaces of high dimensional data, i.e., only a subset of features is relevant for each cluster. Subspace clustering is challenging since the search for the relevant features of each cluster and the detection of the final clusters are circular dependent and should be solved simultaneously. In this paper, we point out that feature correlation and distance divergence are important to subspace clustering, but both have not been considered in previous works. Feature correlation groups correlated features independently thus helps to reduce the search space for the relevant features search problem. Distance divergence distinguishes distances on different dimensions and helps to find the final clusters accurately. We tackle the two problems with the aid of a small amount domain knowledge in the form of must-links and cannot-links. We then devise a semi-supervised subspace clustering algorithm CDCDD. CDCDD integrates our solutions of the feature correlation and distance divergence problems, and uses an adaptive dimension voting scheme, which is derived from a previous unsupervised subspace clustering algorithm FINDIT. Experimental results on both synthetic data sets and real data sets show that the proposed CDCDD algorithm outperforms FINDIT in terms of accuracy, and outperforms the other constraint based algorithm SCMINER in terms of both accuracy and efficiency.",https://ieeexplore.ieee.org/document/5694017/,2010 IEEE International Conference on Data Mining,13-17 Dec. 2010,ieeexplore
10.1109/AITest.2019.00015,Constraint-Based Testing of An Industrial Multi-Robot Navigation System,IEEE,Conferences,"Intelligent multi-robot systems get more and more deployed in industrial settings to solve complex and repetitive tasks. Due to safety and economic reasons they need to operate dependably. To ensure a high degree of dependability, testing the deployed system has to be done in a rigorous way. Advanced multi-robot systems show a rich set of complex behaviors. Thus, these systems are difficult to test manually. Moreover, the space of potential environments and tasks for such systems is enormous. Therefore, methods that are able to explore this space in a structured way are needed. One way to address these issues is through model-based testing. In this paper we present an approach for testing the navigation system of a fleet of industrial transport robots. We show how all potential environments and navigation behaviors as well as requirements and restrictions can be represented in a formal constraint-based model. Moreover, we present the concept of coverage criteria in order to handle the potentially infinite space of test cases. Finally, we show how test cases can be derived from this model in an efficient way. In order to show the feasibility of the proposed approach we present an empirical evaluation of a prototype implementation using a real industrial use case.",https://ieeexplore.ieee.org/document/8718216/,2019 IEEE International Conference On Artificial Intelligence Testing (AITest),4-9 April 2019,ieeexplore
10.1109/IJCNN52387.2021.9533996,Constraint-Guided Reinforcement Learning: Augmenting the Agent-Environment-Interaction,IEEE,Conferences,"Reinforcement Learning (RL) agents have great successes in solving tasks with large observation and action spaces from limited feedback. Still, training the agents is data-intensive and there are no guarantees that the learned behavior is safe and does not violate rules of the environment, which has limitations for the practical deployment in real-world scenarios. This paper discusses the engineering of reliable agents via the integration of deep RL with constraint-based augmentation models to guide the RL agent towards safe behavior. Within the constraints set, the RL agent is free to adapt and explore, such that its effectiveness to solve the given problem is not hindered. However, once the RL agent leaves the space defined by the constraints, the outside models can provide guidance to still work reliably. We discuss integration points for constraint guidance within the RL process and perform experiments on two case studies: a strictly constrained card game and a grid world environment with additional combinatorial subgoals. Our results show that constraint-guidance does both provide reliability improvements and safer behavior, as well as accelerated training.",https://ieeexplore.ieee.org/document/9533996/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore
10.1109/ISIC.2007.4450916,Construction of Expanded Input Space for Modeling Hysteretic Systems,IEEE,Conferences,"A neural model for hysteresis based on expanded input space is proposed in this article. In this method, the behavior of hysteresis is considered as a dynamic system that can be described by a nonlinear state space equation containing hysteretic state. In order to transfer the multi-valued mapping of hysteresis into a one-to-one mapping, an expanded input space involving the original input variable and a so-called Duhem operator is constructed. Thus, the neural networks can be employed to approximate the relation between the hysteretic state and the output of the system that is also the output of hysteresis. The proposed model has a simple architecture that can be easily implemented for on-line adaptation for the model in case of the unexpected change of operating environment. Furthermore, the dynamic performance of the model is improved because of the existence of Duhem operator. Finally, the method is used to the modeling of hysteresis in a piezoelectric actuator.",https://ieeexplore.ieee.org/document/4450916/,2007 IEEE 22nd International Symposium on Intelligent Control,1-3 Oct. 2007,ieeexplore
10.1109/SMC42975.2020.9282816,Container Terminal Liner Berthing Time Prediction with Computational Logistics and Deep Learning,IEEE,Conferences,"The quayside running conditions play a key role in container terminal logistics systems, and the terminal liner berthing time (LBT) is the central index of quayside service efficiency that is also the important evidence and guidance to task scheduling and resource allocation at container terminals. The computational logistics and deep learning are combined to discuss the prediction of LBT by the generalization, unification and integration of the essence and connotation of computation. It is supposed to integrate the deep neural networks learning computation and logistics generalized computation for container terminals (LGC-CT) cross the boundaries between information space and physical world. A deep learning model is designed and executed to predict and evaluate LBT at a typical container terminal in China based on its LBT data for the past four years, which is also intended to lay a good foundation for the configuration, deployment and execution of LGC-CT. The deep neural networks are designed and implemented by the fusion of long short-term memory network, gated recurrent unit one, Gaussian noise one and dense one with TensorFlow 2.3, which demonstrates the feasibility and credibility of the proposed compound computing architecture and paradigms preliminarily.",https://ieeexplore.ieee.org/document/9282816/,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",11-14 Oct. 2020,ieeexplore
10.1109/RO-MAN47096.2020.9223341,Context Dependent Trajectory Generation using Sequence-to-Sequence Models for Robotic Toilet Cleaning,IEEE,Conferences,"A robust, easy-to-deploy robot for service tasks in a real environment is difficult to construct. Record-and-playback (R&amp;P) is a method used to teach motor-skills to robots for performing service tasks. However, R&amp;P methods do not scale to challenging tasks where even slight changes in the environment, such as localization errors, would either require trajectory modification or a new demonstration. In this paper, we propose a Sequence-to-Sequence (Seq2Seq) based neural network model to generate robot trajectories in configuration space given a context variable based on real-world measurements in Cartesian space. We use the offset between a target pose and the actual pose after localization as the context variable. The model is trained using a few expert demonstrations collected using teleoperation. We apply our proposed method to the task of toilet cleaning where the robot has to clean the surface of a toilet bowl using a compliant end-effector in a constrained toilet setting. In the experiments, the model is given a novel offset context and it generates a modified robot trajectory for that context. We demonstrate that our proposed model is able to generate trajectories for unseen setups and the executed trajectory results in cleaning of the toilet bowl.",https://ieeexplore.ieee.org/document/9223341/,2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),31 Aug.-4 Sept. 2020,ieeexplore
10.1109/ICDCS47774.2020.00101,Context-Aware Deep Model Compression for Edge Cloud Computing,IEEE,Conferences,"While deep neural networks (DNNs) have led to a paradigm shift, its exorbitant computational requirement has always been a roadblock in its deployment to the edge, such as wearable devices and smartphones. Hence a hybrid edge-cloud computational framework is proposed to transfer part of the computation to the cloud, by naively partitioning the DNN operations under the constant network condition assumption. However, real-world network state varies greatly depending on the context, and DNN partitioning only has limited strategy space. In this paper, we explore the structural flexibility of DNN to fit the edge model to varying network contexts and different deployment platforms. Specifically, we designed a reinforcement learning-based decision engine to search for model transformation strategies in response to a combined objective of model accuracy and computation latency. The engine generates a context-aware model tree so that the DNN can decide the model branch to switch to at runtime. By the emulation and field experimental results, our approach enjoys a 30%  50% latency reduction while retaining the model accuracy.",https://ieeexplore.ieee.org/document/9355722/,2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS),29 Nov.-1 Dec. 2020,ieeexplore
10.1109/DICTA.2009.81,Context-Based Appearance Descriptor for 3D Human Pose Estimation from Monocular Images,IEEE,Conferences,"In this paper we propose a novel appearance descriptor for 3D human pose estimation from monocular images using a learning-based technique. Our image-descriptor is based on the intermediate local appearance descriptors that we design to encapsulate local appearance context and to be resilient to noise. We encode the image by the histogram of such local appearance context descriptors computed in an image to obtain the final image-descriptor for pose estimation. We name the final image-descriptor the histogram of local appearance context (HLAC). We then use relevance vector machine (RVM) regression to learn the direct mapping between the proposed HLAC image-descriptor space and the 3D pose space. Given a test image, we first compute the HLAC descriptor and then input it to the trained regressor to obtain the final output pose in real time. We compared our approach with other methods using a synchronized video and 3D motion dataset. We compared our proposed HLAC image-descriptor with the Histogram of shape context and histogram of SIFT like descriptors. The evaluation results show that HLAC descriptor outperforms both of them in the context of 3D Human pose estimation.",https://ieeexplore.ieee.org/document/5384910/,2009 Digital Image Computing: Techniques and Applications,1-3 Dec. 2009,ieeexplore
10.1109/QRS.2015.17,Cross-Project Aging Related Bug Prediction,IEEE,Conferences,"In a long running system, software tends to encounter performance degradation and increasing failure rate during execution, which is called software aging. The bugs contributing to the phenomenon of software aging are defined as Aging Related Bugs (ARBs). Lots of manpower and economic costs will be saved if ARBs can be found in the testing phase. However, due to the low presence probability and reproducing difficulty of ARBs, it is usually hard to predict ARBs within a project. In this paper, we study whether and how ARBs can be located through cross-project prediction. We propose a transfer learning based aging related bug prediction approach (TLAP), which takes advantage of transfer learning to reduce the distribution difference between training sets and testing sets while preserving their data variance. Furthermore, in order to mitigate the severe class imbalance, class imbalance learning is conducted on the transferred latent space. Finally, we employ machine learning methods to handle the bug prediction tasks. The effectiveness of our approach is validated and evaluated by experiments on two real software systems. It indicates that after the processing of TLAP, the performance of ARB bug prediction can be dramatically improved.",https://ieeexplore.ieee.org/document/7272913/,"2015 IEEE International Conference on Software Quality, Reliability and Security",3-5 Aug. 2015,ieeexplore
10.1109/CEC45853.2021.9504779,Cultural Weight-Based Fish School Search: A Flexible Optimization Algorithm For Engineering,IEEE,Conferences,"Many real-life engineering applications are optimization problems. To find the best configuration of variables to minimize costs and maximize efficiency are typically used engineering software as CAD, CAE and CAM. In this context, Machine Learning can be used to automate and improve this type of application. This despite, those searches not seldomly evoke unreliable areas and suggest risky solutions. Because of inaccuracies, volatility, unfeasibility, and specificities of real environments, the easy incorporation of cultural practices (i.e. normative, situational, domain and historic knowledge) and as well as the production of multiple acceptable solutions for a problem is always welcome, especially in Engineering. The present article put forward a hybridization of a multi-modal algorithm (Weight-Based Fish School Search - wFSS) with Cultural Algorithms' belief space. New cwFSS is able to guide the optimization also considering normative knowledge from experts, technical literature and problem domain readily available knowledge to prevent the incorporation of constraints into the fitness function. We also evaluated the use of temporal knowledge to guide the simulation. The proposed method was tested in a thermal power plant efficiency optimization and compared with standard wFSS and the Niching Migratory Multi-Swarm Optimizer (NMMSO), winner of CEC'2015 niching competition. As results, cwFSS has outperformed at times NMMSO about time, fitness and variability, as well as traditional wFSS about time, stability, safeness and variability of the multimodal solutions. By avoiding penalties, the appropriation of a priori search directly into the search can effectively and elegantly help better support for engineering decisions.",https://ieeexplore.ieee.org/document/9504779/,2021 IEEE Congress on Evolutionary Computation (CEC),28 June-1 July 2021,ieeexplore
10.1109/ADFSP.1998.685683,D-FANNS (dynamical functional artificial neural networks)-a new avenue for intelligent analog signal processing,IEEE,Conferences,"Summary form only given. Intelligent signal processing may be defined as the process of mapping a signal x into a binary vector or matrix y, so that y enables the detection, classification, or interpretation of an event present in x. (In the case of an interpretation in an appropriate language, y would represent a digitally coded relational structure.) We denote by f the input-output map of such an intelligent signal processing filter. In a number of applications, it is possible to naturally implement the nonlinear filter map f by an artificial neural network (ANN). We consider the case in which x is an analog signal (waveform) belonging to L2(I), where I is an appropriate interval of the real line R1 (i.e., L2(I) is the space of square integrable functions on I), and propose the realization of f by an artificial neural network in which the synaptic weight actions of the first layer are implemented by a filter bank. We call such a network a dynamical functional artificial neural network (D-FANN) to distinguish it from a conventional functional artificial neural network (FANN), where a synaptic weight action is implemented by a scalar product (integration) in L2(I), between the incoming waveform x and a ""distributed"" functional weight. Compared with conventional FANNs, D-FANNs permit simple and meaningful causal realizations of intelligent analog signal processors. A novel element in the present paper is the introduction of a ""D-FANN gain equation"", in a way analogous to that in Kalman filtering. Applications of D-FANNs to real and simulated data are now in progress and these results are discussed.",https://ieeexplore.ieee.org/document/685683/,1998 IEEE Symposium on Advances in Digital Filtering and Signal Processing. Symposium Proceedings (Cat. No.98EX185),5-6 June 1998,ieeexplore
10.23919/SpliTech.2019.8783027,DANN: Digital Audio Neural Network,IEEE,Conferences,"Neural networks have been widely used in computer audio to deal with many synthesis and processing parameters. Experiments have also led to direct synthesis of audio using dynamic neural networks that exhibit oscillating behaviour, as they are clocked at audio rates. There is clearly great potential in convolutional networks as they directly implement complex resonances enabling acoustic interaction. It thus transpires that many different neural network types were used for direct audio synthesis and processing. The proposed innovation extends a fully-connected network of computational cells to facilitate long feedback delays (needed for waveguides) intertwined with zero delays (needed for polynomials). Another improvement is the network computing algorithm that allows for randomisation of delays by adjusting the order of calculations. The DANN model is a specification for network of cells and links that allows direct translation from digital audio synthesis and processing schemes. Taking basic examples such as exponential decay, logistic chaos and biquad filter it transpires that there are exact DANN equivalents to these basic building blocks. It is therefore suggested that complex DANN schemes can reproduce myriad synthesis and processing techniques in a transparent fashion. As a matter of proving the concept a simple snare drum synthesizer is translated to a DANN scheme in order to explore the parameter space in this domain. Thanks to the large amount of training schemes there is great potential be found in training networks for velocity sensitive synthesis to include complex physical interaction sensors. Audio signal processing applications may include modelling of analogue equipment and symbolic acoustic instrument processing. The simplicity of the network grants easy implementation on fast architectures to facilitate near real-time processing and higher sample rates. Beyond the practical uses proposed it is further suggested that network simplification procedures may lead to yet unknown compact schemes for complex signal processing problems in the audio domain, potentially reverberation and pitch-shifting.",https://ieeexplore.ieee.org/document/8783027/,2019 4th International Conference on Smart and Sustainable Technologies (SpliTech),18-21 June 2019,ieeexplore
10.1145/3240765.3240801,DNNBuilder: an Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs,IEEE,Conferences,"Building a high-performance FPGA accelerator for Deep Neural Networks (DNNs) often requires RTL programming, hardware verification, and precise resource allocation, all of which can be time-consuming and challenging to perform even for seasoned FPGA developers. To bridge the gap between fast DNN construction in software (e.g., Caffe, TensorFlow) and slow hardware implementation, we propose DNNBuilder for building high-performance DNN hardware accelerators on FPGAs automatically. Novel techniques are developed to meet the throughput and latency requirements for both cloud- and edge-devices. A number of novel techniques including high-quality RTL neural network components, a fine-grained layer-based pipeline architecture, and a column-based cache scheme are developed to boost throughput, reduce latency, and save FPGA on-chip memory. To address the limited resource challenge, we design an automatic design space exploration tool to generate optimized parallelism guidelines by considering external memory access bandwidth, data reuse behaviors, FPGA resource availability, and DNN complexity. DNNBuilder is demonstrated on four DNNs (Alexnet, ZF, VGG16, and YOLO) on two FPGAs (XC7Z045 and KU115) corresponding to the edge- and cloud-computing, respectively. The fine-grained layer-based pipeline architecture and the column-based cache scheme contribute to 7.7x and 43x reduction of the latency and BRAM utilization compared to conventional designs. We achieve the best performance (up to 5.15x faster) and efficiency (up to 5.88x more efficient) compared to published FPGA-based classification-oriented DNN accelerators for both edge and cloud computing cases. We reach 4218 GOPS for running object detection DNN which is the highest throughput reported to the best of our knowledge. DNNBuilder can provide millisecond-scale real-time performance for processing HD video input and deliver higher efficiency (up to 4.35x) than the GPU-based solutions.",https://ieeexplore.ieee.org/document/8587697/,2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),5-8 Nov. 2018,ieeexplore
10.1109/DAC18074.2021.9586295,Dancing along Battery: Enabling Transformer with Run-time Reconfigurability on Mobile Devices,IEEE,Conferences,"A pruning-based AutoML framework for run-time reconfigurability, namely RT<sup>3</sup>, is proposed in this work. This enables Transformer-based large Natural Language Processing (NLP) models to be efficiently executed on resource-constrained mobile devices and reconfigured (i.e., switching models for dynamic hardware conditions) at run-time. Such reconfigurability is the key to save energy for battery-powered mobile devices, which widely use dynamic voltage and frequency scaling (DVFS) technique for hardware reconfiguration to prolong battery life. In this work, we creatively explore a hybrid block-structured pruning (BP) and pattern pruning (PP) for Transformer-based models and first attempt to combine hardware and software reconfiguration to maximally save energy for battery-powered mobile devices. Specifically, RT<sup>3</sup> integrates two-level optimizations: First, it utilizes an efficient BP as the first-step compression for resource-constrained mobile devices; then, RT<sup>3</sup> heuristically generates a shrunken search space based on the first level optimization and searches multiple pattern sets with diverse sparsity for PP via reinforcement learning to support lightweight software reconfiguration, which corresponds to available frequency levels of DVFS (i.e., hardware reconfiguration). At run-time, RT<sup>3</sup> can switch the lightweight pattern sets within 45ms to guarantee the required real-time constraint at different frequency levels. Results further show that RT<sup>3</sup> can prolong battery life over $ 4\times$ improvement with less than 1% accuracy loss for Transformer and 1.5% score decrease for DistilBERT.",https://ieeexplore.ieee.org/document/9586295/,2021 58th ACM/IEEE Design Automation Conference (DAC),5-9 Dec. 2021,ieeexplore
10.23919/IRS.2019.8768102,Data Analytics and Machine Learning in Wide Area Surveillance Systems,IEEE,Conferences,"Modern surveillance networks are able to provide trajectories of all kind of vessels and aircrafts within worldwide or at least extended environment. Most widely used are Automatic Dependent Surveillance - Broadcast (ADS-B) and (Satellite-) Automatic Identification System (AIS) used within air and maritime surveillance. Both of them are cooperative systems. Besides these systems, sensor networks based on ground installations or mounted on airborne and space-based platforms deliver object trajectories independent of any cooperation. Examples include GMTI radar-based systems operating on UAV platforms and coastal or air traffic control sensor network installations. These surveillance systems provide mid- and long-term trajectories. The challenging part is the related situational awareness and the estimation of the intent of the tracked objects. New technologies include activity-based intelligence and the determination of patterns of life. An approach for these technologies can be found in the advanced analysis of those trajectories, which are extracted by the mentioned surveillance systems. Trajectories are partitioned into specific segments of interest using cluster algorithms. This helps to decode their pattern of life based on unsupervised machine learning. Trajectories are aggregated into different routes with dedicated representatives. Calculated probabilities indicate the frequentation of these routes. This allows predictive analytics and the identification of anomalous behaviour. Finally, these new data analytic techniques have to be integrated in existing near real time surveillance systems. This requires specific system architectures as well as a completely new software and hardware landscape. So, trajectory-based Machine Learning is embedded in local or global clouds and uses dedicated mechanisms for distributed and parallel processing.",https://ieeexplore.ieee.org/document/8768102/,2019 20th International Radar Symposium (IRS),26-28 June 2019,ieeexplore
10.23919/IRS48640.2020.9253816,"Data Analytics, Machine Learning and Risk Assessment for Surveillance and Situation Awareness",IEEE,Conferences,"Modern surveillance networks are able to provide trajectories of all kinds for aircrafts and vessels worldwide or at least in extended areas of the airspace or earth surface. Best known are Automatic Dependent Surveillance - Broadcast (ADS-B) and (Satellite-) Automatic Identification System (AIS) used in air and maritime surveillance. Both of them are cooperative systems. Besides these sources, sensors based on ground installations or mounted on airborne and space-based platforms deliver object trajectories independently of any transponders. This is done by advanced tracking and fusion algorithms generating trajectories out of sensor measurements. Examples include GMTI radar-based systems operating on UAV platforms or imaging systems based on high altitude pseudo satellites (HAPS) and satellites. These surveillance systems enable a continuous extraction of mid- and long-term trajectories of objects. Besides the trajectory generation, the challenge will be to place them into the right context and to provide situational awareness. This includes the estimation of the intents of the tracked objects, activity-based intelligence, and the determination of patterns of life. Otherwise, even modern surveillance systems are not able to take a real advantage of the gathered data. Therefore, trajectories are further processed by data analytics and machine learning. Unsupervised machine learning offers techniques to cluster and to partition trajectories, extract highly frequented routes and points of interest, predict object movement and identify anomalous behaviour. On the other hand, transponder and broadcast systems provide additional attributes of the tracked trajectories. These labels pave the way for numerous supervised machine learning methods. The derived predictors realise the determination of object types and activities. Finally, these new data analytic techniques have to be integrated in existing near real time surveillance systems. This requires specific system architectures as well as a completely new software and hardware landscape. In summary, trajectory-based data analytics, machine learning and risk assessment are embedded on local or global clouds and use dedicated mechanisms for distributed and parallel processing.",https://ieeexplore.ieee.org/document/9253816/,2020 21st International Radar Symposium (IRS),5-8 Oct. 2020,ieeexplore
10.1109/ICCCNT51525.2021.9579976,"Data Processing in IoT, Sensor to Cloud: Survey",IEEE,Conferences,"IoT is connecting Things over the Internet and the realization of the environment through smart things to create a responsive space. Many surveys predicted the growth of IoT devices is going to be around 50 billion and an average of 7 devices per person. IoT has shown promising future with its applications like smart city, connected factories, buildings, roadways, smart health and many more. To make the promise a reality IoT has to overcome many hurdles like scalability, connectivity, architectural, big data, analysis, security, and privacy. In this literature survey, an attempt has been made to identify current challenges faced by IoT implementation and possible solutions, future opportunities, and research openings. Further, the processing of sensed data at IoT device, edge/fog layer, and the cloud is discussed in detail. Keywords- IoT, IoT architecture, Machine learning, Deep",https://ieeexplore.ieee.org/document/9579976/,2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT),6-8 July 2021,ieeexplore
10.1109/TAI.2002.1180785,Data mining using cultural algorithms and regional schemata,IEEE,Conferences,"In the paper we demonstrate how evolutionary search for functional optima can be used as a vehicle for data mining, that is, in the process of searching for optima in a multi-dimensional space we can keep track of the constraints that must be placed on related variables in order to move towards the optima. Thus, a side effect of evolutionary search can be the mining of constraints for related variables. We use a cultural algorithm framework to embed the search and store the results in regional schemata. An application to a large-scale real world archaeological data set is presented.",https://ieeexplore.ieee.org/document/1180785/,"14th IEEE International Conference on Tools with Artificial Intelligence, 2002. (ICTAI 2002). Proceedings.",4-6 Nov. 2002,ieeexplore
10.1109/ICAS49788.2021.9551175,Data-Driven Kalman-Based Velocity Estimation for Autonomous Racing,IEEE,Conferences,"Real-time velocity estimation is a core task in autonomous driving, which is carried out based on available raw sensors such as wheel odometry and motor currents. When the system dynamics and observations can be modeled together as a fully known linear Gaussian state space (SS) model, the celebrated Kalman filter (KF) is a low complexity optimal solution. However, both linearity of the underlying SS model and accurate knowledge of it are often not encountered in practice. This work proposes to estimate the velocity using a hybrid data-driven (DD) implementation of the KF for non-linear systems, coined KalmanNet. KalmanNet integrates a compact recurrent neural network in the flow of the classical KF, retaining low computational complexity, high data efficiency, and interpretability, while enabling operation in non-linear SS models with partial information. We apply KalmanNet on an autonomous racing car as part of the Formula Student (FS) Driverless competition. Our results demonstrate the ability of KalmanNet to outperform a state-of-the-art implementation of the KF that uses a postulated SS model, while being applicable on the vehicle control unit used by the car.",https://ieeexplore.ieee.org/document/9551175/,2021 IEEE International Conference on Autonomous Systems (ICAS),11-13 Aug. 2021,ieeexplore
10.1109/ICBBE.2008.696,Decoding Hand Kinematics and Neural States Using Gaussian Process Model,IEEE,Conferences,"Probabilistic modeling of correlated neural population firing activity is central to understanding the neural code and building practical decoding algorithms, the accurate reconstruction of a continuous motion signal is necessary for the control of devices such as computer cursors, robots, or a patient's own paralyzed limbs. For such applications we developed a realtime system that uses Gaussian process techniques to estimate hand motion from the firing rates of multiple neurons. Gaussian Processes for Machine Learning presents one of the most important Bayesian machine learning approaches based on a particularly effective method for placing a prior distribution over the space of functions. Decoding was performed using Gaussian processes model which gives an efficient method for Bayesian inference when the firing rates and hand kinematics are nonlinear. Gaussian processes provide a principled, practical, probabilistic approach to learning in noisy measurements. In off-line experiments, the Gaussian processes model reconstructions of hand trajectory were more accurate than previously reported results. The resulting decoding algorithm provides a principled probabilistic model of motor-cortical coding, decodes hand motion in real time, provides an estimate of uncertainty, and is straight to implement. Additionally the formulation unifies and extends previous models of neural coding while providing insights into the motor-cortical code.",https://ieeexplore.ieee.org/document/4535576/,2008 2nd International Conference on Bioinformatics and Biomedical Engineering,16-18 May 2008,ieeexplore
10.1109/IJCNN52387.2021.9534268,Deep Embedded Clustering of Urban Communities Using Federated Learning,IEEE,Conferences,"Deep clustering utilizes representation learning to learn features in an unsupervised setting. Although successful, the current models rely on the assumption of the centralized dataset, which due to the privacy concerns is becoming less realistic. To address this challenge, we propose a federated deep convolutional embedded clustering framework. Our framework relies on a federated server to orchestrate the training between workers where each participant individually trains the model with the objective of decreasing clustering loss using KullbackLeibler divergence. To avoid feature space being distorted by the clustering loss, each worker maintains their own local decoder which for privacy reasons is not shared with the federated server. Empirical results with both IID and non-IID client data on benchmark datasets demonstrates the feasibility of our federated training when compared to the centralized counterpart. We also evaluate our model on a real world application of community detection using GPS traces and measure the computational complexity and energy consumption on a smartphone.",https://ieeexplore.ieee.org/document/9534268/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore
10.1109/CCDC49329.2020.9164231,Deep Generative Model for Malware Detection,IEEE,Conferences,"Malware detection play different roles in a computer system and exhibit high degrees of importance with respect to system security. Malware detection is the process of attempting to infer the reputation score of the files via the applications. However, malware detection approaches are challenged by the large, dynamic, and heterogeneous space of benign binaries that they must track. In this research, we use deep generative models to develop two a semi-supervised Bayesian models for malware detection, in which we model the data generating process to be dependent on a Gaussian mixture. Furthermore, we propose the efficient stochastic gradient optimization technique used in deep generative models makes our model suitable for large data sets. Extensive experimental results on one real-world dataset demonstrate that our model is the effectiveness. Moreover, the semi-supervised deep generative scheme achieves comparable or even better results in malware detection when compared with classic and alternative machine learning models. This demonstrates the feasibility of deep generative model and presents a promising new approach to malware detection.",https://ieeexplore.ieee.org/document/9164231/,2020 Chinese Control And Decision Conference (CCDC),22-24 Aug. 2020,ieeexplore
10.1109/TENSYMP52854.2021.9550904,Deep Learning based Smart Parking for a Metropolitan Area,IEEE,Conferences,"In this study, we have introduced a method for utilizing the maximum parking space available for a metropolitan city. This will result in much lesser traffic congestion due to street-side parking. Furthermore, it will also decrease the hassle drivers face when they have to leave their vehicles on the side of the road to do other activities. The method introduces a Deep Learning based system where parking spaces are detected using Data Capturing Units (DCU). These DCUs feed data into our database which can be accessed by the users from our mobile application. The users can book parking spaces accordingly. All these data are saved in real-time and can be accessed through the mobile application. A vehicle classification system has also been designed that achieves an accuracy of 77% from multiple vehicle classes. Furthermore, a number plate recognition system has been used for the identification and safety protocols of the vehicles in parking sites. The number plate identification system is very precise and achieves an accuracy of over 90% for each digit. To the best of our knowledge, no other system of this kind has been implemented for the city of Dhaka before this. On top of that, successful implementation in a hectic city like Dhaka implies that it can be applied anywhere in the world. We believe this system can have a huge impact in reducing traffic congestions and can save an endless measure of time and money for citizens in a metropolitan area.",https://ieeexplore.ieee.org/document/9550904/,2021 IEEE Region 10 Symposium (TENSYMP),23-25 Aug. 2021,ieeexplore
10.1109/ICDMW.2015.121,Deep Learning for Image Retrieval: What Works and What Doesn't,IEEE,Conferences,"To build an industrial content-based image retrieval system (CBIRs), it is highly recommended that feature extraction, feature processing and feature indexing need to be fully considered. Although research that bloomed in the past years suggest that the convolutional neural network (CNN) be in a leading position on feature extraction &amp; representation for CBIRs, there are less instructions on the deep analysis of feature related topics, for example the kind of feature representation that has the best performance among the candidates provided by CNN, the extracted features generalization ability, the relationship between the dimensional reduction and the accuracy loss in CBIRs, the best distance measure technique in CBIRs and the benefit of the coding techniques in improving the efficiency of CBIRs, etc. Therefore, several practicing studies were conducted and a thorough analysis was made in this research attempting to answer the above questions. The results in the study on both ImageNet-2012 and an industrial dataset provided by Sogou demonstrate that fc4096a and fc4096b perform the best on the datasets from unseen categories. Several interesting and practicing conclusions are drawn, for instance, fc4096a and fc4096b are found to have a better generalization ability than other features of CNN and could be considered as the first choice for industrial CBIRs. Furthermore, a novel feature binarization approach is presented in this paper for better efficiency of CBIRs. More specifically, the binarization is capable of reducing 31/32 space usage of original data. To sum up, the conclusions seem to provide practical instructions on real industrial CBIRs.",https://ieeexplore.ieee.org/document/7395863/,2015 IEEE International Conference on Data Mining Workshop (ICDMW),14-17 Nov. 2015,ieeexplore
10.1109/ICMIM48759.2020.9299052,Deep Open Space Segmentation using Automotive Radar,IEEE,Conferences,"In this work, we propose the use of radar with advanced deep segmentation models to identify open space in parking scenarios. A publically available dataset of radar observations called SCORP was collected. Deep models are evaluated with various radar input representations. Our proposed approach achieves low memory usage and real-time processing speeds, and is thus very well suited for embedded deployment.",https://ieeexplore.ieee.org/document/9299052/,2020 IEEE MTT-S International Conference on Microwaves for Intelligent Mobility (ICMIM),23-23 Nov. 2020,ieeexplore
10.1109/ICRA48506.2021.9561145,Deep Reinforcement Learning Framework for Underwater Locomotion of Soft Robot,IEEE,Conferences,"Soft robotics is an emerging technology with excellent application prospects. However, due to the inherent compliance of the materials used to build soft robots, it is extremely complicated to control soft robots accurately. In this paper, we introduce a data-based control framework for solving the soft robot underwater locomotion problem using deep reinforcement learning (DRL). We first built a soft robot that can swim based on the dielectric elastomer actuator (DEA). We then modeled it in a simulation for the purpose of training the neural network and tested the performance of the control framework through real experiments on the robot. The framework includes the following: a simulation method for the soft robot that can be used to collect data for training the neural network, the neural network controller of the swimming robot trained in the simulation environment, and the computer vision method to collect the observation space from the real robot using a camera. We confirmed the effectiveness of the learning method for the soft swimming robot in the simulation environment by allowing the robot to learn how to move from a random initial state to a specific direction. After obtaining the trained neural network through the simulation, we deployed it on the real robot and tested the performance of the control framework. The soft robot successfully achieved the goal of moving in a straight line in disturbed water. The experimental results suggest the potential of using deep reinforcement learning to improve the locomotion ability of mobile soft robots.",https://ieeexplore.ieee.org/document/9561145/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore
10.23919/SoftCOM50211.2020.9238313,Deep Semantic Image Segmentation for UAV-UGV Cooperative Path Planning: A Car Park Use Case,IEEE,Conferences,"Navigation of Unmanned Ground Vehicles (UGV) in unknown environments is an active area of research for mobile robotics. A main hindering factor for UGV navigation is the limited range of the on-board sensors that process only restricted areas of the environment at a time. In addition, most existing approaches process sensor information under the assumption of a static environment. This restrains the exploration capability of the UGV especially in time-critical applications such as search and rescue. The cooperation with an Unmanned Aerial Vehicle (UAV) can provide the UGV with an extended perspective of the environment which enables a better-suited path planning solution that can be adjusted on demand. In this work, we propose a UAV-UGV cooperative path planning approach for dynamic environments by performing semantic segmentation on images acquired from the UAVs view via a deep neural network. The approach is evaluated in a car park scenario, with the goal of providing a path plan to an empty parking space for a ground-based vehicle. The experiments were performed on a created dataset of real-world car park images located in Croatia and Germany, in addition to images from a simulated environment. The segmentation results demonstrate the viability of the proposed approach in producing maps of the dynamic environment on demand and accordingly generating path plans for ground-based vehicles.",https://ieeexplore.ieee.org/document/9238313/,"2020 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",17-19 Sept. 2020,ieeexplore
10.1109/ICASSP.2017.7952155,Deep attractor network for single-microphone speaker separation,IEEE,Conferences,"Despite the overwhelming success of deep learning in various speech processing tasks, the problem of separating simultaneous speakers in a mixture remains challenging. Two major difficulties in such systems are the arbitrary source permutation and unknown number of sources in the mixture. We propose a novel deep learning framework for single channel speech separation by creating attractor points in high dimensional embedding space of the acoustic signals which pull together the time-frequency bins corresponding to each source. Attractor points in this study are created by finding the centroids of the sources in the embedding space, which are subsequently used to determine the similarity of each bin in the mixture to each source. The network is then trained to minimize the reconstruction error of each source by optimizing the embeddings. The proposed model is different from prior works in that it implements an end-to-end training, and it does not depend on the number of sources in the mixture. Two strategies are explored in the test time, K-means and fixed attractor points, where the latter requires no post-processing and can be implemented in real-time. We evaluated our system on Wall Street Journal dataset and show 5.49% improvement over the previous state-of-the-art methods.",https://ieeexplore.ieee.org/document/7952155/,"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",5-9 March 2017,ieeexplore
10.1109/SAI.2017.8252172,Deep learning teachology for the prediction of solar flares from GOES data,IEEE,Conferences,"Predicting solar storms from real-time satellites data is extremely important for the protection of variousaviation, power and communication infrastructures. In this paper deep learning technologyis applied, for the first time, tothe real-time prediction of solar flares by analyzing the x-ray flux (1-minute cadence) time-series data from the satellite GOES (Geostationary Operational Environmental Satellite). The prediction system introduced here consists of 2units. The first converts GOES data to Markov Transition Field (MTF) images. An unsupervised feature learning algorithm and the prediction, flare or no-flare, are implemented by a Convolutional Neural Network in the second unit. Severalevaluation metrics, as required by space weather specialists, are applied to evaluate the performance of the system.",https://ieeexplore.ieee.org/document/8252172/,2017 Computing Conference,18-20 July 2017,ieeexplore
10.1109/CNS48642.2020.9162219,DeepBLOC: A Framework for Securing CPS through Deep Reinforcement Learning on Stochastic Games,IEEE,Conferences,"One important aspect in protecting Cyber Physical System (CPS) is ensuring that the proper control and measurement signals are propagated within the control loop. The CPS research community has been developing a large set of check blocks that can be integrated within the control loop to check signals against various types of attacks (e.g., false data injection attacks). Unfortunately, it is not possible to integrate all these checks within the control loop as the overhead introduced when checking signals may violate the delay constraints of the control loop. Moreover, these blocks do not completely operate in isolation of each other as dependencies exist among them in terms of their effectiveness against detecting a subset of attacks. Thus, it becomes a challenging and complex problem to assign the proper checks, especially with the presence of a rational adversary who can observe the check blocks assigned and optimizes her own attack strategies accordingly. This paper tackles the inherent state-action space explosion that arises in securing CPS through developing DeepBLOC (DB)-a framework in which Deep Reinforcement Learning algorithms are utilized to provide optimal/sub-optimal assignments of check blocks to signals. The framework models stochastic games between the adversary and the CPS defender and derives mixed strategies for assigning check blocks to ensure the integrity of the propagated signals while abiding to the real-time constraints dictated by the control loop. Through extensive simulation experiments and a real implementation on a water purification system, we show that DB achieves assignment strategies that outperform other strategies and heuristics.",https://ieeexplore.ieee.org/document/9162219/,2020 IEEE Conference on Communications and Network Security (CNS),29 June-1 July 2020,ieeexplore
10.1109/SANER.2019.8668044,DeepCT: Tomographic Combinatorial Testing for Deep Learning Systems,IEEE,Conferences,"Deep learning (DL) has achieved remarkable progress over the past decade and has been widely applied to many industry domains. However, the robustness of DL systems recently becomes great concerns, where minor perturbation on the input might cause the DL malfunction. These robustness issues could potentially result in severe consequences when a DL system is deployed to safety-critical applications and hinder the real-world deployment of DL systems. Testing techniques enable the robustness evaluation and vulnerable issue detection of a DL system at an early stage. The main challenge of testing a DL system attributes to the high dimensionality of its inputs and large internal latent feature space, which makes testing each state almost impossible. For traditional software, combinatorial testing (CT) is an effective testing technique to balance the testing exploration effort and defect detection capabilities. In this paper, we perform an exploratory study of CT on DL systems. We propose a set of combinatorial testing criteria specialized for DL systems, as well as a CT coverage guided test generation technique. Our evaluation demonstrates that CT provides a promising avenue for testing DL systems.",https://ieeexplore.ieee.org/document/8668044/,"2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)",24-27 Feb. 2019,ieeexplore
10.1109/BigData.2018.8622372,DeepFP: A Deep Learning Framework For User Fingerprinting via Mobile Motion Sensors,IEEE,Conferences,"In this paper, we propose a deep learning framework for user fingerprinting via mobile motion sensors, DeepFP, which can identify and track users based on their behavioral patterns while interacting with the smartphone. Existing machine learning techniques for user identification are classification-oriented and thus are not amenable easily to large-scale, real world deployment. They need to be trained on all the users whom they want to identify. DeepFP exploits metric learning techniques and deep neural networks to address the challenges of current user identification techniques. We leverage feature embedding to directly extract informative features and map input samples to a discriminative lower-dimensional space, where recurrent neural networks are used to model the temporal information of data. DeepFP does not need to re-train to identify new users which makes it feasible to be used in real world scenarios with a huge number of users, without needing a large number of training samples. Experiments on a publicly available mobile sensors dataset and comparison with other embedding methods depict the effectiveness of DeepFP.",https://ieeexplore.ieee.org/document/8622372/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore
10.1145/3240765.3240791,DeepFense: Online Accelerated Defense Against Adversarial Deep Learning,IEEE,Conferences,"Recent advances in adversarial Deep Learning (DL) have opened up a largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems. With the wide-spread usage of DL in critical and time-sensitive applications, including unmanned vehicles, drones, and video surveillance systems, online detection of malicious inputs is of utmost importance. We propose DeepFense, the first end-to-end automated framework that simultaneously enables efficient and safe execution of DL models. DeepFense formalizes the goal of thwarting adversarial attacks as an optimization problem that minimizes the rarely observed regions in the latent feature space spanned by a DL network. To solve the aforementioned minimization problem, a set of complementary but disjoint modular redundancies are trained to validate the legitimacy of the input samples in parallel with the victim DL model. DeepFense leverages hardware/software/algorithm co-design and customized acceleration to achieve just-in-time performance in resource-constrained settings. The proposed countermeasure is unsupervised, meaning that no adversarial sample is leveraged to train modular redundancies. We further provide an accompanying API to reduce the non-recurring engineering cost and ensure automated adaptation to various platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders of magnitude performance improvement while enabling online adversarial sample detection.",https://ieeexplore.ieee.org/document/8587702/,2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),5-8 Nov. 2018,ieeexplore
10.1109/DASC-PICom-DataCom-CyberSciTec.2017.200,DeepSim: Cluster Level Behavioral Simulation Model for Deep Learning,IEEE,Conferences,"We are witnessing an explosion of AI based use cases driving the computer industry, and especially datacenter and server architectures. As Intel faces fierce competition in this emerging technology space, it is important that architecture definitions and directions are driven with data from proper tools and methodologies, and insights are drawn from end-to-end holistic analysis at the datacenter levels. In this paper, we introduce DeepSim, a cluster-level behavioral simulation model for deep learning. DeepSim, which is based on the Intel CoFluent simulation framework, uses timed behavioral models to simulate complex interworking between compute nodes, networking, and storage at the datacenter level, providing a realistic performance model of a real-world image recognition applications based on the popular Deep Learning Framework Caffe. The end-to-end simulation data from DeepSim provides insight which can be used for architecture analysis driving future datacenter architecture directions. DeepSim enables scalable system design, deployment, and capacity planning through accurate performance insights. Results from preliminary scaling studies (e.g. node scaling and network scaling) and what-if analyses (e.g., Xeon with HBM and Xeon Phi with dual OPA) are presented in this paper. The simulation results are correlated well with empirical measurements, achieving an accuracy of 95%.",https://ieeexplore.ieee.org/document/8328544/,"2017 IEEE 15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)",6-10 Nov. 2017,ieeexplore
10.1109/ICMLC.2008.4620589,Defending against tcp syn flooding with a new kind of syn-agent,IEEE,Conferences,"TCP-based flooding attack is a common form of Denial-of-Service (DoS) attacks which abuses network resources and may bring serious threats to the network. The SYN flood attack is a DoS method affecting hosts to retain the half-open state and exhaust its memory resources. This attack is hard to be filtered by the routers in case that the source IP address is always spoofed. There are some common ways to defend against this attach, but all of them either requires a high-performance firewall or trade time for space. In this paper, we proposed a method to build a new kind of syn-agent which uses the TCP header reserved flag bits to notify the server a complete three-way TCP handshake. First the syn-agent instead of the real server answer the client with ACK after received a SYN packet from the client. Then if it is a syn-attack, there should be no further ACKs after this. After a given short period, the half-open TCP sock should be deleted from the agent. If it is a really connection request, after the third time handshake packet arrived, the agent set the reserved bit in the TCP header to be 1 and route the packet to the real server. When the server received a packet with the reserved bits set to be 1, it directly allocates memory for the connection and begins to communicate.",https://ieeexplore.ieee.org/document/4620589/,2008 International Conference on Machine Learning and Cybernetics,12-15 July 2008,ieeexplore
10.1109/AHS.2018.8541460,Delay Tolerant Network Routing as a Machine Learning Classification Problem,IEEE,Conferences,"This paper discusses a machine learning-based approach to routing for delay tolerant networks (DTNs) [1]. DTNs are networks which experience frequent disconnections between nodes, uncertainty of an end-to-end path, long one-way trip times, and may have high error rates and asymmetric links. Such networks exist in deep space satellite networks, very rural environments, disaster areas and underwater environments. In this work, we use machine learning classifiers to predict a set of neighboring nodes which are the most likely to deliver a message to a desired location based on message history delivery information. We use the Common Open Research Emulator (CORE) [2] to emulate the DTN environment based on real-world location traces and collect network traffic statistics from the Bundle Protocol implementation IBR-DTN [3]. The software architecture for classification-based routing, analysis and preparation of the network history data and prediction results are discussed.",https://ieeexplore.ieee.org/document/8541460/,2018 NASA/ESA Conference on Adaptive Hardware and Systems (AHS),6-9 Aug. 2018,ieeexplore
10.1109/CAIA.1990.89189,Demonstrating artificial intelligence for space systems-integration and project management issues,IEEE,Conferences,"As part of its systems autonomy demonstration project (SADP), the National Aeronautics and Space Administration (NASA) has recently demonstrated the Thermal Expert System (TEXSYS). Advanced real-time expert system and human interface technology was successfully developed and integrated with conventional controllers of prototype space hardware to provide intelligent fault detection, isolation and recovery capability. Many specialized skills were required, and responsibility for the various phases of the project therefore spanned multiple NASA centers, internal departments and contractor organizations. The test environment required communication among many types of hardware and software as well as between many people. The integration, testing, and configuration management tools and methodologies which were applied to the TEXSYS project to assure its safe and successful completion are detailed. The project demonstrated that artificial intelligence technology, including model-based reasoning, is capable of the monitoring and control of a large, complex system in real time.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/89189/,Sixth Conference on Artificial Intelligence for Applications,5-9 May 1990,ieeexplore
10.1109/ICCAD51958.2021.9643473,Demystifying the Characteristics of High Bandwidth Memory for Real-Time Systems,IEEE,Conferences,"The number of functionalities controlled by software on every critical real-time product is on the rise in domains like automotive, avionics and space. To implement these advanced functionalities, software applications increasingly adopt artificial intelligence algorithms that manage massive amounts of data transmitted from various sensors. This translates into unprecedented memory performance requirements in critical systems that the commonly used DRAM memories struggle to provide. High-Bandwidth Memory (HBM) can satisfy these requirements offering high bandwidth, low power and high-integration capacity features. However, it remains unclear whether the predictability and isolation properties of HBM are compatible with the requirements of critical embedded systems. In this work, we perform to our knowledge the first timing analysis of HBM. We show the unique structural and timing characteristics of HBM with respect to DRAM memories and how they can be exploited for better time predictability, with emphasis on increased isolation among tasks and reduced worst-case memory latency.",https://ieeexplore.ieee.org/document/9643473/,2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD),1-4 Nov. 2021,ieeexplore
10.1109/ICRA.2016.7487691,Denoising auto-encoders for learning of objects and tools affordances in continuous space,IEEE,Conferences,"The concept of affordances facilitates the encoding of relations between actions and effects in an environment centered around the agent. Such an interpretation has important impacts on several cognitive capabilities and manifestations of intelligence, such as prediction and planning. In this paper, a new framework based on denoising Auto-encoders (dA) is proposed which allows an agent to explore its environment and actively learn the affordances of objects and tools by observing the consequences of acting on them. The dA serves as a unified framework to fuse multi-modal data and retrieve an entire missing modality or a feature within a modality given information about other modalities. This work has two major contributions. First, since training the dA is done in continuous space, there will be no need to discretize the dataset and higher accuracies in inference can be achieved with respect to approaches in which data discretization is required (e.g. Bayesian networks). Second, by fixing the structure of the dA, knowledge can be added incrementally making the architecture particularly useful in online learning scenarios. Evaluation scores of real and simulated robotic experiments show improvements over previous approaches while the new model can be applied in a wider range of domains.",https://ieeexplore.ieee.org/document/7487691/,2016 IEEE International Conference on Robotics and Automation (ICRA),16-21 May 2016,ieeexplore
10.1109/CEC48606.2020.9185611,DenseDisp: Resource-Aware Disparity Map Estimation by Compressing Siamese Neural Architecture,IEEE,Conferences,"Stereo vision cameras are flexible sensors due to providing heterogeneous information such as color, luminance, disparity map (depth), and shape of the objects. Today, Convolutional Neural Networks (CNNs) present the highest accuracy for the disparity map estimation [1]. However, CNNs require considerable computing capacity to process billions of floating-point operations in a real-time fashion. Besides, commercial stereo cameras produce huge size images (e.g., 10 Megapixels [2]), which impose a new computational cost to the system. The problem will be pronounced if we target resource-limited hardware for the implementation. In this paper, we propose DenseDisp, an automatic framework that designs a Siamese neural architecture for disparity map estimation in a reasonable time. DenseDisp leverages a meta-heuristic multi-objective exploration to discover hardware-friendly architectures by considering accuracy and network FLOPS as the optimization objectives. We explore the design space with four different fitness functions to improve the accuracy-FLOPS trade-off and convergency time of the DenseDisp. According to the experimental results, DenseDisp provides up to 39. 1x compression rate while losing around 5% accuracy compared to the state-of-the-art results.",https://ieeexplore.ieee.org/document/9185611/,2020 IEEE Congress on Evolutionary Computation (CEC),19-24 July 2020,ieeexplore
10.1109/IES53407.2021.9594053,Design And Development of Human Anatomy Learning Platform for Medical Students Based On Augmented Intelligence Technology,IEEE,Conferences,"Augmented Intelligence technology was introduced for the task of helping improve human work in various fields, one of which is education. Several problems in the learning process, which are currently completely virtual, raise new problems, especially related to practicums which require teaching modules as guidelines for practicum implementation but still maintain the impression of interactive learning. For this reason, Augmented Reality technology is applied as a solution to build a practical human anatomy module, then called AIVE Platform embedded in smartphones to provide informative and immersive learning that can be run indoors or outdoors so that it is not limited by space and time. This platform can run on Android and iOS which is built on the AR Foundation framework to work across platforms. This module has followed the rules of the anatomical atlas that include labels on each part, there is also a login system to store student usage history, as well as the choice of learning mode. This module has been licensed to operate from a teaching doctor in anatomy to be used as a teaching module. The PIECES framework used to analyze the importance and satisfaction level of the platform gives score 4.085 out of 5 on and 4.081 out of 5 respectively.",https://ieeexplore.ieee.org/document/9594053/,2021 International Electronics Symposium (IES),29-30 Sept. 2021,ieeexplore
10.1109/CAC53003.2021.9727460,Design and Implementation of Automatic Transport Control System for Warehouse with High Spatiotemporal Coupling Constraints,IEEE,Conferences,"In a logistics warehouse with high storage density, conflicts between items often occur during warehouse management due to the coupling of space between transportations and storages. This paper designs and implements a warehouse automatic transport control system (ATCS). Besides, a special path planning strategy based on Reverse Escort Algorithm (REA) is applied in the target warehouse. Based on the realistic warehouse model, the algorithm is improved to be more convenient for practical implementation. Simulation results prove the effectiveness and optimization effect of the proposed path planning algorithm.",https://ieeexplore.ieee.org/document/9727460/,2021 China Automation Congress (CAC),22-24 Oct. 2021,ieeexplore
10.1109/ISKE47853.2019.9170453,Design and Implementation of On-orbit Valuable Image Extraction for the TianZhi-1 Satellite,IEEE,Conferences,"Recently, software-defined satellite has become a research hotspot in the aerospace. Based on an advanced computing platform with open system architecture, researchers can upload software for specific tasks even the satellite has been launched into space. This paper we have designed an on-orbit application for China's first software-defined satellite TianZhi-1, which use Android smartphone as a system platform. Two main tasks are focused on our work, one is to reduce data redundancy and the other is to compress the size of the software. First, a light-weight and extensible framework is designed to support different image processing algorithms. Following this, we propose a three-step approach for on-orbit valuable image extraction, include image denoising, stitching, and salient object extraction. Experiments on the real satellite achieve outstanding results.",https://ieeexplore.ieee.org/document/9170453/,2019 IEEE 14th International Conference on Intelligent Systems and Knowledge Engineering (ISKE),14-16 Nov. 2019,ieeexplore
10.1109/AINIT54228.2021.00134,Design and Implementation of Virtual Animation Based on Unity3D,IEEE,Conferences,"This project uses Unity3D engine to make VR virtual reality experiment, uses UG NX to model the experiment model, uses Shadow SDK plug-in to make the glasses show binocular effects, particle system can add special effects to the experiment, it is the second rendering of the three-dimensional space Three-dimensional images are mainly used to render different experimental requirements for effects such as smoke, lire, water droplets, fallen leaves, etc., through the use of dotween plug-in in unity3D to make the experiment, the experiment is presented in VR glasses, which greatly promotes VR Teaching development.",https://ieeexplore.ieee.org/document/9725101/,"2021 2nd International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)",15-17 Oct. 2021,ieeexplore
10.1109/INDIANCC.2019.8715622,Design and Real Time Implementation of Sliding Mode and Adaptive Fuzzy Control on Quanser Gyroscope,IEEE,Conferences,"3-DoF Gyroscope is one of the fundamental building blocks of any guidance, navigation and control system. The physical properties exhibited by the gyroscope are relevant in the field of sea, air and space vehicles. Attitude control, momentum wheel control and satellite orientation are some of the applications of the 3-DoF gyroscope. While sliding mode control is one of the promiment variable structure control that has been used to solve a lot of nonlinear control problems, adaptive fuzzy belongs to the family of intelligent control that can solve some of the very complex nonlinear control problems where there is partial to no knowledge of the system or where there is a substantial dynamic variation in the plant. It can adapt itself over the domain of compact input set. Both the control schemes are able to drive the error to zero and thus stabilize the plant about a given fixed point ensuring stability and boundedness of all the signals. This paper deals with practical implementation of sliding mode and direct adaptive fuzzy control on quanser gyroscope. Practical results are shown to be in comformity with the simulation results.",https://ieeexplore.ieee.org/document/8715622/,2019 Fifth Indian Control Conference (ICC),9-11 Jan. 2019,ieeexplore
10.1109/FUZZY.1994.343884,Design and analysis of an anti-slip fuzzy controller for heavy duty off road vehicles,IEEE,Conferences,This paper illustrates the design and analysis of a distributed fuzzy control system to prevent wheel slippage in heavy-duty off-road vehicles with hydrostatic power transmission using the vector space analytical approach. The controller described have been fully implemented in a commercially available city tractor and tested on a test track as well as under real working conditions.&lt;<ETX>&gt;</ETX>,https://ieeexplore.ieee.org/document/343884/,Proceedings of 1994 IEEE 3rd International Fuzzy Systems Conference,26-29 June 1994,ieeexplore
10.1109/ETFA.2015.7301549,Design and implementation for multiple-robot deployment in intelligent space,IEEE,Conferences,"This paper presents the problem of robot deployment for a number of scattered tasks. We aim to minimize the duration it takes for all robots to reach their assigned task locations. In previous work, we have proposed a team composed of one carrier robot (CR) and several servant robots to accomplish the mission. Then we have suggested an algorithm that determines a path of the CR for an efficient deployment under a few constraints, which is verified by simulations. Assuming that the servant robots are unmanned aerial vehicles (UAVs), the present paper extends the discussion to a real robot experiment. We design and implement a deployment system in intelligent space. The feasibility of the study is demonstrated through an experiment.",https://ieeexplore.ieee.org/document/7301549/,2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA),8-11 Sept. 2015,ieeexplore
10.1109/ICBAIE52039.2021.9389950,Design of Cloud Computing Platform Based Accurate Measurement for Structure Monitoring Using Fiber Bragg Grating Sensors,IEEE,Conferences,"The efficient integration of distributed fiber Bragg grating (FBG) sensors and cloud computing platform is used to achieve accurate measurement and evaluation of physical quantities, which solves the problems of traditional fiber Bragg grating sensing technology for health structure monitoring system, such as the cost and space constraints, it is difficult to deploy enough servers to deal with data collection, transmission and storage in real time. The cloud platform using fiber Bragg grating sensors adopts the structure of erbium-doped fiber cascaded Bragg grating, reasonably configures the FBG demodulator acquisition and analysis software, deploys the health monitoring system in the cloud, constructs the cloud platform of high-efficiency health monitoring optical fiber sensor network, improves the scalability of the system, flexibly deploys applications and services, and ensures the security and reliability of various real-time monitoring data and professional data. It can meet the needs of some specific or wide application fields for the automation technology, structural mechanics, computer technology, Internet architecture, cloud deployment and interdisciplinary practical comprehensive valuable application research.",https://ieeexplore.ieee.org/document/9389950/,"2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",26-28 March 2021,ieeexplore
10.1109/AIEA53260.2021.00013,Design of a Real-time Robot Control System oriented for Human-Robot Cooperation,IEEE,Conferences,"An open real-time control system based on the EtherCAT fieldbus communication technology is proposed to fulfill the high real-time requirement of the human-robot cooperation controller in this paper. An open source real-time kernel of Xenomai is employed as the real-time software platform of the robot control system. Based on this, four-layer interfaces architecture are accomplished, which are human-machine cooperation control layer, motion control layer, robot axis control layer and hardware abstraction layer, through the corresponding four real-time tasks to meet the demand of human-robot cooperation operations. In addition, the scheduling task is developed to manage the 4 real-time tasks. The dual buffer communication mechanisms and priority-based scheduling strategy between layers was exploited to synchronize these real-time tasks. The underlying hardware abstract interface and the human-robot collaborative control algorithm interface are opened in the control system as the quadric exploitation interfaces to meet the need of developing application tasks in real-time space. Experiment results which are conducted on a self-developed 6-DOF collaborative robot show that the proposed control system is effective in real-time control applications of human-robot cooperative control at the control cycle of 5 milliseconds.",https://ieeexplore.ieee.org/document/9525600/,2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA),14-16 May 2021,ieeexplore
10.1109/NAFIPS.2001.943712,Design of neuro-fuzzy controller on DSP for real-time control of induction motors,IEEE,Conferences,"This paper deals with the DSP implementation of the high performance induction motor drive that presented on the viewpoint of the design and experiment. The speed control system for the induction motor drive is based on the ANFIS (adaptive network-based fuzzy inference system) controller, that is, a sophisticated neuro-fuzzy controller. This ANFIS controller acts as a feed forward controller that provides the plant with the proper control input and accomplish error backpropagation algorithm through the network. In this paper, the DSP (TMS320F240) has been used to perform the high-speed calculation of the space vector PWM and to build the ANFIS control algorithm. It is confirmed that proposed algorithm provides the more improved control performance for the conventional V/F controller and vector controller. The proposed ANFIS algorithm and DSP technique can be applied to the precise speed control of the induction motor drive system or the field of power electronics.",https://ieeexplore.ieee.org/document/943712/,Proceedings Joint 9th IFSA World Congress and 20th NAFIPS International Conference (Cat. No. 01TH8569),25-28 July 2001,ieeexplore
10.1109/ICCS.2008.4737145,Design of new minimum decoding complexity quasi-orthogonal Space-Time Block Code for four transmit antennas,IEEE,Conferences,"A new Space-Time Block Code (STBC) achieving full rate and full diversity for general QAM and four transmit antennas is proposed. This code also possesses a quasi-orthogonal (QO) property like the conventional Minimum Decoding Complexity QO-STBC (MDC-QO-STBC), leading to joint ML detection of only two real symbols. The proposed code is shown to exhibit the identical error performance with the existing MDC-QO-STBC. However, the proposed code has an advantage in the transceiver implementation since this code can be modified so that the increase of PAPR occurs at only two transmit antennas, but the MDC-QO-STBC at all of transmit antennas.",https://ieeexplore.ieee.org/document/4737145/,2008 11th IEEE Singapore International Conference on Communication Systems,19-21 Nov. 2008,ieeexplore
10.1109/ITAIC.2019.8785568,Design of on-line monitoring system for catenary compensation device,IEEE,Conferences,"The safe operation of high-speed railway is directly affected by the reliability of catenary. Manual inspection is time/labor consumptive, bad real-time and etc. To solve these problems, a non-contact intelligent remote control monitoring system based on laser sensor is devised in this paper which can effectively monitor the catenary status and send alarm messages in time. On the respect of performance improvement, GPRS Network Communication Technology is applied to reduce the difficulty and cost of the access to the Internet of the intelligent terminal. A SD card is used to expand the store space to enhance the capacity of batch data processing. In order to decrease the consumption of outdoor electrical energy, a relay is utilized in circuit to control the turn-on and off of the solar power. At the same time, the control software is capable to analyze and store massive data with database technology. It turns out that the system has high reliability and real-time performance in functions such as acquiring accurate data and alarming timely by means of the spot test.",https://ieeexplore.ieee.org/document/8785568/,2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),24-26 May 2019,ieeexplore
10.1109/IJCNN.2012.6252851,Design of the Self-Constructing Fuzzy Neural Network controller for a sliding door system,IEEE,Conferences,"In this paper, the Self-Constructing Fuzzy Neural Network (SCFNN) controller suitable for real-time control of the speed control of the slide door is presented to track reference model. The structure and parameter learning can be done automatically and online. The structure learning is accordance with the partition of input space (error and change of error), and the parameter learning is based on the supervised gradient decent method. In this paper, the weights of SCFNN are generated from functional-link-based neural network (FLNN). The SCFNN adopted the FLNN, generating complex nonlinear combinations of input space to the weights of the SCFNN with FLNN. Finally, a slide door speed control system is implemented in this paper to verify the effectiveness of the proposed SCFNN with FLNN.",https://ieeexplore.ieee.org/document/6252851/,The 2012 International Joint Conference on Neural Networks (IJCNN),10-15 June 2012,ieeexplore
10.1109/CEC.2003.1299882,Design optimization for a novel class of high power microwave sources,IEEE,Conferences,"Significant benefits would follow from improving the signal growth rates of certain high-power microwave (HPM) sources, including the relativistic klystron oscillator (RKO). Optimization of the growth rate via analytical and standard numerical techniques is intractable because of the high dimensionality of the design space and the existence of many local optima. Instead, the growth rate is optimized using a real-value evolutionary algorithm (EA), which performs mutation, selection, and recombination on a population of candidate design parameters. Practical application of EAs requires the availability of a computationally efficient model of design quality. Two models of the RKO are developed relating the growth rate of the microwave output power to the design parameters. Both models have computationally efficient implementations, and one of them generalizes easily to a novel multi-cavity class of RKO devices, which has significantly better growth rates than standard two-cavity RKOs. Many design optimization problems of interest involve physical constraints. The GENOCOP evolutionary algorithm includes features which support the incorporation of physical constraints in the problem specification through the maintenance of separate search and reference populations, where the latter consists entirely of feasible individuals. It provides ""blind"" operators to recombine individuals from the two populations to produce new reference population individuals. However, the use of these blind operators can result in unnecessary modification of the search individual, and domain specific recombination operators can result in improved effectiveness. As with any optimization technique, GENOCOP also allows the use of either the penalty function or repair method for evaluation of infeasible individuals. Computational experiments are performed comparing the effectiveness of each possible combination of these constraint handling techniques.",https://ieeexplore.ieee.org/document/1299882/,"The 2003 Congress on Evolutionary Computation, 2003. CEC '03.",8-12 Dec. 2003,ieeexplore
10.1109/ASPDAC.2016.7428073,Design space exploration of FPGA-based Deep Convolutional Neural Networks,IEEE,Conferences,"Deep Convolutional Neural Networks (DCNN) have proven to be very effective in many pattern recognition applications, such as image classification and speech recognition. Due to their computational complexity, DCNNs demand implementations that utilize custom hardware accelerators to meet performance and energy-efficiency constraints. In this paper we propose an FPGA-based accelerator architecture which leverages all sources of parallelism in DCNNs. We develop analytical feasibility and performance estimation models that take into account various design and platform parameters. We also present a design space exploration algorithm for obtaining the implementation with the highest performance on a given platform. Simulation results with a real-life DCNN demonstrate that our accelerator outperforms other competing approaches, which disregard some sources of parallelism in the application. Most notably, our accelerator runs 1.9 faster than the state-of-the-art DCNN accelerator on the same FPGA device.",https://ieeexplore.ieee.org/document/7428073/,2016 21st Asia and South Pacific Design Automation Conference (ASP-DAC),25-28 Jan. 2016,ieeexplore
10.1109/ISCAS.2018.8351685,Design-Space Exploration of Pareto-Optimal Architectures for Deep Learning with DVFS,IEEE,Conferences,"Specialized computing engines are required to accelerate the execution of Deep Learning (DL) algorithms in an energy-efficient way. To adapt the processing throughput of these accelerators to the workload requirements while saving power, Dynamic Voltage and Frequency Scaling (DVFS) seems the natural solution. However, DL workloads need to frequently access the off-chip memory, which tends to make the performance of these accelerators memory-bound rather than computation-bound, hence reducing the effectiveness of DVFS. In this work we use a performance-power analytical model fitted on a parametrized implementation of a DL accelerator in a 28-nm FDSOI technology to explore a large design space and to obtain the Pareto points that maximize the effectiveness of DVFS in the sub-space of throughput and energy efficiency. In our model we consider the impact on performance and power of the off-chip memory using real data of a commercial low-power DRAM.",https://ieeexplore.ieee.org/document/8351685/,2018 IEEE International Symposium on Circuits and Systems (ISCAS),27-30 May 2018,ieeexplore
10.1109/COMPSAC.2008.76,Designing Simulated Context-Aware Telephone in Pervasive Spaces,IEEE,Conferences,"Personal context information is important to enable context-aware applications in pervasive spaces. As privacy, many investigators have pointed out that it must be in the controllable state by using personal privacy policy. The long term objective of this research was to extract privacy policy from personal context information database with machine learning method. A context-aware telephone system was designed to collect data, learn policy and examine the accuracy of policy. In the step of PDA simulation, the results showed that the method was possible to make suitable decision for telephone communication in consideration of privacy protection. In order to further validate the proposed privacy protection method, the pervasive space was designed and implemented to get more real data about using context-aware telephone. At the step of pervasive space simulation, we design and code the prototype of context-aware telephone system from the view of software engineering.",https://ieeexplore.ieee.org/document/4591762/,2008 32nd Annual IEEE International Computer Software and Applications Conference,28 July-1 Aug. 2008,ieeexplore
10.1109/MWSCAS.2006.381790,Designing a Genetic Algorithm for Function Approximation for Embedded and ASIC Applications,IEEE,Conferences,"In embedded systems and application specific integrated circuits (ASICs) that typically do not have a floating-point processor, measured data or function-sampled data is commonly described by an analytic function derived using standard numerical methods. The resultant errors are not caused by rounding but by translating a real solution to a restricted fixed-point environment. We have previously described a genetic algorithm that discovers a superior piece-wise polynomial approximation with coefficients restricted to the integer target space. In this paper we discuss details of the genetic algorithm implementation.",https://ieeexplore.ieee.org/document/4267414/,2006 49th IEEE International Midwest Symposium on Circuits and Systems,6-9 Aug. 2006,ieeexplore
10.1109/AERO.2017.7943637,Designing and implementing Machine Learning Algorithms for advanced communications using FPGAs,IEEE,Conferences,"Communications systems can obtain substantial benefits from increased intelligence. Improvements to communications include increased spectral situational awareness, spectral optimization, and robust operation in dynamic and demanding communications environments. Furthermore, complex communication systems require a high degree of autonomous intelligence to optimize performance under such varying conditions. Machine Learning Algorithms provide a means to increase the intrinsic intelligence of wideband communication systems. This paper considers the use of Machine Learning Algorithms to increase the intelligence of communication systems. Specifically, the focus of this paper is to sense and learn the communication environment in real-time and optimize system parameters to maximize end-to-end performance. Communications systems have existing adaptive capabilities in many subsystems such as equalization. The focus in this paper is top level system intelligence by learning from the environment, and based on the system capabilities determine an optimal mode in the solution space in real-time. Furthermore, the goal of this paper is to consider implementation of Machine Learning Algorithms using FPGAs. Design data for implementing Machine Learning Algorithms using FPGAs is provided in the paper as well as reference circuits for implementation. Finally, an example implementation of a Machine Learning Algorithm for intelligent communications is provided based on implementation in a Xilinx UltraScale FPGA.",https://ieeexplore.ieee.org/document/7943637/,2017 IEEE Aerospace Conference,4-11 March 2017,ieeexplore
10.1109/EMCTECH49634.2020.9261546,Designing of a Classifier for the Unstructured Text Formalization Model Based on Word Embedding,IEEE,Conferences,"The active use of artificial intelligence technologies has a direct positive impact on the development of society in various areas of human life. The article describes the developed model of processing and formalization of textual unstructured information in the form of a continuous flow of text information taken from the news feed of news agencies. A method for preprocessing text to reduce the execution time of the algorithm and save CPU resources is given. A method for representing words as a real vector is formed using various algorithms for training artificial neural networks and their properties. A model of the first stage of the text information processing system as a subsystem for classifying the subject of a news article text based on a vector representation of words, including a description of the word vectorization algorithm, an example of the type of word structure with a corresponding numeric vector, and a metric that determines the proximity of vectors to each other in space. The results of the experiment are obtained and a method for setting a decision criterion for the implemented classifier is proposed. The area of use of the proposed classifier is the sphere of information security. The results of the experiment can be indicators of the suitability of using the classifier as a definition of the subject of a news article.",https://ieeexplore.ieee.org/document/9261546/,2020 International Conference on Engineering Management of Communication and Technology (EMCTECH),20-22 Oct. 2020,ieeexplore
10.1109/SAINT.2012.14,Detecting Malicious Websites by Learning IP Address Features,IEEE,Conferences,"Web-based malware attacks have become one of the most serious threats that need to be addressed urgently. Several approaches that have attracted attention as promising ways of detecting such malware include employing various blacklists. However, these conventional approaches often fail to detect new attacks owing to the versatility of malicious websites. Thus, it is difficult to maintain up-to-date blacklists with information regarding new malicious websites. To tackle this problem, we propose a new method for detecting malicious websites using the characteristics of IP addresses. Our approach leverages the empirical observation that IP addresses are more stable than other metrics such as URL and DNS. While the strings that form URLs or domain names are highly variable, IP addresses are less variable, i.e., IPv4 address space is mapped onto 4-bytes strings. We develop a lightweight and scalable detection scheme based on the machine learning technique. The aim of this study is not to provide a single solution that effectively detects web-based malware but to develop a technique that compensates the drawbacks of existing approaches. We validate the effectiveness of our approach by using real IP address data from existing blacklists and real traffic data on a campus network. The results demonstrate that our method can expand the coverage/accuracy of existing blacklists and also detect unknown malicious websites that are not covered by conventional approaches.",https://ieeexplore.ieee.org/document/6305258/,2012 IEEE/IPSJ 12th International Symposium on Applications and the Internet,16-20 July 2012,ieeexplore
10.1109/MECO.2018.8405951,Detecting and counteracting benign faults and malicious attacks in cyber physical systems,IEEE,Conferences,"The use of cyber-physical system (CPS) is rapidly expanding and many of their applications require a highly reliable and secure implementation as they control critical infrastructures or even life-critical devices. Unfortunately, current techniques for achieving high reliability and security incur high overheads. In particular, integrating countermeasures against security attacks is problematic as security threats are often not well defined, evolve continuously, and as a result, many CPSs often remain vulnerable. We propose to exploits the physical plant state information to enhance both reliability and security. Our approach, which monitors the controlled plant state trajectory, allows for tunable fault-tolerance as well as detection of malicious attacks, and it achieves these at a low overhead. The plant state space consists of safe and marginal state subspaces. In the safe subspace the CPS will continue its safe operation even if the worst case control signal is applied. In contrast, any erroneous control applied when the plant state is marginal, may lead to a catastrophic system failure. Such an erroneous control output may be due to either a benign fault or a malicious security attack. As most of the time the plant will be deep within its safe subspace, we can avoid using expensive redundancy techniques and thus, reduce the computational load while still guaranteeing safe operation. When a marginal state of the plant is detected, it will signal the potential presence of a natural fault or malicious attack. Our scheme will counter this by switching to a critical mode involving higher levels of redundancy to combat natural failures as well as alternative mechanisms to defeat malicious attacks. A major challenge in our approach is to determine, in real-time, whether the current state of the physical plant is deep within its safe sub-space or is marginal. We have used various machine learning techniques for classifying the state and our results indicate that with a reasonable number of entries in a lookup table and with a short execution time, the required classification can be performed efficiently.",https://ieeexplore.ieee.org/document/8405951/,2018 7th Mediterranean Conference on Embedded Computing (MECO),10-14 June 2018,ieeexplore
10.1109/LCN.2008.4664303,Detection of anomalous network packets using lightweight stateless payload inspection,IEEE,Conferences,"A real-time packet-level anomaly detection approach for high-speed network intrusion prevention is described. The approach is suitable for small and fast hardware implementation and was designed to be embedded in network appliances. Each network packet is characterized using a novel technique that efficiently maps the payload histogram onto a simple pair of features using hypercube hash functions, which were chosen for their implementation efficiency in both hardware and software. This two-dimensional feature space is quantized into a binary bitmap representing the normal and anomalous feature regions. The potential loss of accuracy due to the reduction in feature space is countered by the ability of the bitmaps to capture nearly arbitrary shaped regions in the feature space. These bitmaps are used as the classifiers for real-time detection. The proposed method is extremely efficient in both the offline machine learning and real-time detection components. Results using the 1999 DARPA Intrusion Detection Evaluation Data Set yield a 100% detection of all applicable attacks, with extremely low false positive rate. The approach is also evaluated on real traffic captures.",https://ieeexplore.ieee.org/document/4664303/,2008 33rd IEEE Conference on Local Computer Networks (LCN),14-17 Oct. 2008,ieeexplore
10.1109/CAMSAP.2007.4497964,Determining the number of propagation paths from broadband mimo measurements via bootstrapped likelihoods and the false discovery rate criterion  part i: methodology,IEEE,Conferences,"In this paper, we propose a multiple hypotheses test for determining the number of propagation paths from broadband MIMO channel measurements. For this test, maximum likelihood (ML) estimates for propagation delay, direction of arrival, direction of departure, and Doppler shifts are required for each potential number of propagation paths. The ML-estimator is implemented via a variant of the space alternating generalized expectation-maximization (SAGE) algorithm. The proposed test is based on the Benjamini Hochberg procedure for guaranteeing a false discovery rate and employs the simple bootstrap approach for approximating the required p-values for the multiple test. In a companion paper, we apply the proposed test to real broadband MIMO antenna array measurements and discuss its performance.",https://ieeexplore.ieee.org/document/4497964/,2007 2nd IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing,12-14 Dec. 2007,ieeexplore
10.1109/COMPSAC.2018.10238,Development of Bicycle Simulator with Tilt Angle Control Tilt Angle,IEEE,Conferences,"Bicycles are a healthy and eco-friendly form of travel. Information about dangerous points (hazards) on roads is important for bicycle-riding safety. We proposed a method to detect road hazards using sensors attached to a bicycle in a prior conventional study. The conventional approach needs training data for machine learning, requiring the bicycle to travel over hazardous areas repeatedly. Not only is this is dangerous, it is also difficult to collect sufficiently large amounts of data. A bicycle simulator is a potential solution to this problem. Commercial bicycle simulators have been developed in Japan. However, the bicycle body is fixed, thus making the ride feel unnatural. This type of bicycle simulator is not suitable for gathering hazard data. We therefore propose a bicycle simulator capable of tilt angle control that runs in a three-dimensional (3D) virtual space. The built-in sensors send the speed and front-wheel angle information to the control unit. A new tilt angle is then calculated by the control unit and the information is sent to an AC servomotor to achieve this new tilt angle. Simultaneously, a 3D view of a virtual course is generated using the calculated results. By using our previously proposed rotation center tracking method, left and right steering actions can be observed by the system. This system allows for dangerous situations to be easily and repeatedly created with no danger to the rider.",https://ieeexplore.ieee.org/document/8377866/,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),23-27 July 2018,ieeexplore
10.1109/ROMAN.2008.4600679,Development of a 3D real time gesture recognition methodology for virtual environment control,IEEE,Conferences,"In this paper, we propose a real time 3D gesture recognition system that relies on the state based approach. The novelty of this work is the introduction of probabilistic neural networks (PNNs) to characterize the uncertain boundaries of each state. The 3D gestures are modeled as a sequence of states in a configuration space; the number of states and their spatial parameters are calculated by dynamic k-means clustering on the training data of the gesture without temporal information. Gesture recognition is performed using a simple Finite State Machine (FSM), where, each state transition depends only on the output of its corresponding PNN and optionally on its time restrictions (minimum and maximum time permitted in the state). If a recognizer reaches its final state, then it could be said that a gesture is recognized. The approach is illustrated with the implementation of a real time system that recognizes the semantic meaning of seven basic gestures of the Indian Dance, the description of the system and the technologies used, it will be described in detail in the paper.",https://ieeexplore.ieee.org/document/4600679/,RO-MAN 2008 - The 17th IEEE International Symposium on Robot and Human Interactive Communication,1-3 Aug. 2008,ieeexplore
10.1109/ICRAIE51050.2020.9358310,Development of a Neural Network Library for Resource Constrained Speech Synthesis,IEEE,Conferences,"Machine learning frameworks, like Tensorflow and PyTorch, use GPU hardware acceleration to deliver the needed performance. Since GPUs require a lot of power (and space) to operate, typical use cases involve high-performance servers, with the final deployment available as a cloud service. To address limitations of this approach, AI Accelerators have been proposed. In this context, we have designed and implemented a library of neural network algorithms, to efficiently run on edge devices, with AI Accelerators. Moreover, a unified interface has been provided, to allow easy experimentation with various neural networks applied to the same dataset. Here, let us stress that we do not propose new algorithms, but port known ones to, resource restricted, edge devices. The context is provided by a speech synthesis application for edge devices that is deployed on an NVIDIA Jetson Nano. This application is to be used by social robots for real-time off-cloud text-to-speech processing.",https://ieeexplore.ieee.org/document/9358310/,2020 5th IEEE International Conference on Recent Advances and Innovations in Engineering (ICRAIE),1-3 Dec. 2020,ieeexplore
10.1109/ICSENS.2009.5398535,Diagnostic models for sensor measurements in rocket engine tests,IEEE,Conferences,"This paper presents our ongoing work in the area of using virtual reality (VR) environments for the Integrated Systems Health Management (ISHM) of rocket engine test stands. Specifically, this paper focuses on the development of an intelligent valve model that integrates into the control center at NASA Stennis Space Center. The intelligent valve model integrates diagnostic algorithms and 3D visualizations in order to diagnose and predict failures of a large linear actuator valve (LLAV). The diagnostic algorithm uses auto-associative neural networks to predict expected values of sensor data based on the current readings. The predicted values are compared with the actual values and drift is detected in order to predict failures before they occur. The data is then visualized in a VR environment using proven methods of graphical, measurement, and health visualization. The data is also integrated into the control software using an ActiveX plug-in.",https://ieeexplore.ieee.org/document/5398535/,"SENSORS, 2009 IEEE",25-28 Oct. 2009,ieeexplore
10.1109/BIYOMUT.2010.5479790,Diffusion tensor fiber tracking based on unsupervised learning,IEEE,Conferences,"Using Hebbian learning rule and its special case Self-Organizing Map (SOM) as unsupervised learning, a solution is proposed for defining the fiber paths which is a critical problem in diffusion tensor literature, and synthetic diffusion patterns are analyzed by artificial neural network (ANN) approach. Unsupervised learning in training neural networks is a method, where network classification rules are self developed and which does not require any knowledge about the desired output. Only input data is presented to the ANN in the learning process of the network, in other words the input space of the unsupervised learning ANN is the diffusion tensor eigenvector data of each imaging matrix. The network then adjusts the weightings to determine patterns having similar characteristics and classification is done in that way. The resulting classification represents the principal diffusion direction and the weighted diffusion distribution tracked by the fibers. Verification of the application on synthetic data enabled the implementation of the method on real brain images. The aim of the proposed method is to accomplish brain fiber tracking based on learning algorithms according to the modeling studies accepted in artificial neural network literature. Implementing SOM for fiber path discrimination purposes was successful and future work relies in 3D diffusion tensor tractography.",https://ieeexplore.ieee.org/document/5479790/,2010 15th National Biomedical Engineering Meeting,21-24 April 2010,ieeexplore
10.1109/EMCTECH49634.2020.9261512,Digital Socio-Political Communication and its Transformation in the Technological Evolution of Artificial Intelligence and Neural Network Algorithms,IEEE,Conferences,"The study aims to analyze the specifics of determining the subjects of digital social and political communication in the context of the development of artificial intelligence technologies and neural network algorithms. The work uses a case-study design. As a research methodology, the method of critical analysis of the digital communications practice in the socio-political sphere, as well as discourse analysis of modern scientific research in the field of the development of artificial intelligence and neural network algorithms, are used. It is concluded that the implementation of technological solutions based on artificial intelligence and neural network algorithms into the processes of socio-political communications creates a problem of defining the subject of communication acts in the socio-political sphere. Society may face such communication practices in which hybrid subjectness is realized. In the conditions of hybrid subjectness, both real subjects and programmed neural network algorithms acting as real subjects (but only imitating own subjectivity) interact in common communication space. The originality of the work lies in the formulation of the author's hypothesis about the emergence of the phenomenon of hybrid subjectness in the space of modern socio-political communications and its potential in the aspect of influencing the mass consciousness of citizens.",https://ieeexplore.ieee.org/document/9261512/,2020 International Conference on Engineering Management of Communication and Technology (EMCTECH),20-22 Oct. 2020,ieeexplore
10.1109/ICCV.2009.5459193,Dimensionality reduction and principal surfaces via Kernel Map Manifolds,IEEE,Conferences,"We present a manifold learning approach to dimensionality reduction that explicitly models the manifold as a mapping from low to high dimensional space. The manifold is represented as a parametrized surface represented by a set of parameters that are defined on the input samples. The representation also provides a natural mapping from high to low dimensional space, and a concatenation of these two mappings induces a projection operator onto the manifold. The explicit projection operator allows for a clearly defined objective function in terms of projection distance and reconstruction error. A formulation of the mappings in terms of kernel regression permits a direct optimization of the objective function and the extremal points converge to principal surfaces as the number of data to learn from increases. Principal surfaces have the desirable property that they, informally speaking, pass through the middle of a distribution. We provide a proof on the convergence to principal surfaces and illustrate the effectiveness of the proposed approach on synthetic and real data sets.",https://ieeexplore.ieee.org/document/5459193/,2009 IEEE 12th International Conference on Computer Vision,29 Sept.-2 Oct. 2009,ieeexplore
10.1109/3DIM.2005.33,Discrete pose space estimation to improve ICP-based tracking,IEEE,Conferences,"Iterative closest point (ICP)-based tracking works well when the interframe motion is within the ICP minimum well space. For large interframe motions resulting from a limited sensor acquisition rate relative to the speed of the object motion, it suffers from slow convergence and a tendency to be stalled by local minima. A novel method is proposed to improve the performance of ICP-based tracking. The method is based upon the bounded Hough transform (BHT) which estimates the object pose in a coarse discrete pose space. Given an initial pose estimate, and assuming that the interframe motion is bounded in all 6 pose dimensions, the BHT estimates the current frame's pose. On its own, the BHT is able to track an object's pose in sparse range data both efficiently and reliably, albeit with a limited precision. Experiments on both simulated and real data show the BHT to be more efficient than a number of variants of the ICP for a similar degree of reliability. A hybrid method has also been implemented wherein at each frame the BHT is followed by a few ICP iterations. This hybrid method is more efficient than the ICP, and is more reliable than either the BHT or ICP separately.",https://ieeexplore.ieee.org/document/1443287/,Fifth International Conference on 3-D Digital Imaging and Modeling (3DIM'05),13-16 June 2005,ieeexplore
10.1109/IJCNN48605.2020.9207522,Discrete-Time Lyapunov based Kinematic Control of Robot Manipulator using Actor-Critic Framework,IEEE,Conferences,"Stability and optimality are the two foremost re-quirements for robotic systems that are deployed in critical operations and are to work for long hours or under limited energy resources. To address these, in this work we present a novel Lyapunov stability based discrete-time optimal kinematic control of a robot manipulator using actor-critic (AC) framework. The robot is actuated using optimal joint-space velocity control input to track a time-varying end-effector trajectory in its task space. In comparison to the existing near-optimal kinematic control solutions for robot manipulator under AC framework, proposed controller exhibits guaranteed analytical stability. We derive a novel critic weight update law based on Lyapunov stability, thus ensuring that the weights are updated along the negative gradient of Lyapunov function. This eventually ensures closed-loop system stability and convergence to the optimal control in discrete-time. Extensive simulations are performed on a 3D model of 6-DoF Universal Robot (UR) 10 in Gazebo, followed by implementation on real UR 10 robot manipulator to show the efficacy of the proposed scheme.",https://ieeexplore.ieee.org/document/9207522/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore
10.1109/ICMLA.2009.121,Discriminative Multi-stream Discrete Hidden Markov Models,IEEE,Conferences,"We propose a modified discrete HMM that handles multimodalities. We assume that the feature space is partitioned into subspaces generated by different sources of information. To combine these heteregoneous modalities we propose a multi-stream discrete HMM that assigns a relevance weight to each subspace. The relevance weights are set local and depend on the symbols and the states. In particular, we associate a partial probability with each symbol in each subspace. The overall observation state probability is then computed as an aggregation of the partial probabilities and their objective relevance weights based on a linear combination. The minimum classification error (MCE) objective based on the gradient probabilistic descent (GPD) optimization algorithm is reformulated to derive the update equations for the relevance weights and the partial state probabilities. The proposed approach is validated using synthetic and real data sets. The results are shown to outperform the baseline discrete HMM that treats all streams equally important.",https://ieeexplore.ieee.org/document/5381827/,2009 International Conference on Machine Learning and Applications,13-15 Dec. 2009,ieeexplore
10.1109/ICSE.2019.00112,Distance-Based Sampling of Software Configuration Spaces,IEEE,Conferences,"Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.",https://ieeexplore.ieee.org/document/8812049/,2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE),25-31 May 2019,ieeexplore
10.23919/MIPRO.2019.8756937,Distillation of a CNN for a high accuracy mobile face recognition system,IEEE,Conferences,"In face recognition systems, the use of convolutional neural networks (CNNs) permits to achieve good accuracy performances, which derive largely from a huge number of well-trained parameters. While using online services any mobile device can suffice for an accurate identification, in the offline scenario, implemented on a wearable mobile hardware, it is difficult to achieve both real-time responsiveness and high accuracy. In this paper we present a solution to replace a large open source face recognizer network (provided as part of the dlib libraries), distilling its learned knowledge into a less demanding CNN. The former is used as an expert oracle that provides the targets, while the latter is trained on the same input image, following a regression approach. In addition to lightness, our CNN is trained to use smaller input images, naturally allowing the recognition of identities in a wider distance range and with a reduced amount of computation. This eventually permits the porting of the network into a dedicated mobile accelerating hardware. The hypothesis we want to demonstrate is that since the feature space topology has been deeply explored during the training of the expert network, and due to the fact that no information is created during the up sampling of a tiny face to the input size of the expert oracle, the smaller network can provide the same accuracy at a reduced computational cost.",https://ieeexplore.ieee.org/document/8756937/,"2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",20-24 May 2019,ieeexplore
10.1109/ICCCNT49239.2020.9225309,Distributed Artificial Neural Network Model for Neutron Flux Mapping in Nuclear Reactors,IEEE,Conferences,"Neutron flux distribution inside the core of large size nuclear reactors is a function of space and time. An online Flux Mapping System (FMS) is needed to monitor the core during the reactor operation. FMS estimates the core flux distribution from the measurements of few in-core detectors using an appropriate algorithm. Here, a Distributed Artificial Neural Network (D-ANN) model is developed using parallel-forward multi-layer perceptron architecture to capture the spatial core flux variation in a nuclear reactor. The proposed D-ANN model is tested with simulated test case data of Advanced Heavy Water Reactor (AHWR) for multiple operating conditions of the reactor. The model estimates the neutron flux in all horizontal mesh locations (2-D) from the multiple networks distributed spatially across AHWR core. Estimation error using the proposed D-ANN model is found to be significantly lower than that with lumped ANN model. Validation exercises establish that this D-ANN model could effectively capture the spatial variations in the reactor core and therefore could be utilized for efficient flux mapping. The real time implementation of D-ANN based flux mapping method is also proposed.",https://ieeexplore.ieee.org/document/9225309/,"2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",1-3 July 2020,ieeexplore
10.1109/ICMLA.2013.17,Distributed Kernel Matrix Approximation and Implementation Using Message Passing Interface,IEEE,Conferences,"We propose a distributed method to compute similarity (also known as kernel and Gram) matrices used in various kernel-based machine learning algorithms. Current methods for computing similarity matrices have quadratic time and space complexities, which make them not scalable to large-scale data sets. To reduce these quadratic complexities, the proposed method first partitions the data into smaller subsets using various families of locality sensitive hashing, including random project and spectral hashing. Then, the method computes the similarity values among points in the smaller subsets to result in approximated similarity matrices. We analytically show that the time and space complexities of the proposed method are sub quadratic. We implemented the proposed method using the Message Passing Interface (MPI) framework and ran it on a cluster. Our results with real large-scale data sets show that the proposed method does not significantly impact the accuracy of the computed similarity matrices and it achieves substantial savings in running time and memory requirements.",https://ieeexplore.ieee.org/document/6784587/,2013 12th International Conference on Machine Learning and Applications,4-7 Dec. 2013,ieeexplore
10.1109/ICTON.2019.8840514,Distributed and Centralized Options for Self-Learning,IEEE,Conferences,"In general, the availability of enough real data from real fog computing scenarios to produce accurate Machine Learning (ML) models is rarely ensured since new equipment, techniques, etc., are continuously being deployed in the field. Although an option is to generate data from simulation and lab experiments, such data could not cover the whole features space, which would translate into ML models inaccuracies. In this paper, we propose a self-learning approach to facilitate ML deployment in real scenarios. A dataset for ML training can be initially populated based on the results from simulation and lab experiments and once ML models are generated, ML re-training can be performed after inaccuracies are detected to improve their precision. Illustrative numerical results show the benefits from the proposed self-learning approach for two general use cases of regression and classification.",https://ieeexplore.ieee.org/document/8840514/,2019 21st International Conference on Transparent Optical Networks (ICTON),9-13 July 2019,ieeexplore
10.1109/ITSC.2010.5624970,Distributed evolutionary estimation of dynamic traffic origin/destination,IEEE,Conferences,"This paper focuses on updating time varying demand matrices using real-time information. An Artificial Intelligence technique based on Distributed Evolutionary Algorithms (DEA), which is capable to exploit the use of grid computing, is developed. This EA-based demand estimation framework is implemented into a model that we call DynODE (Dynyamic O/D Estimator). DynODE provides a direct way of fusing information of varying types, with different levels of accuracy and from different sensors/sources. DynODE is integrated with an existing Dynamic Traffic Assignment platform (i.e. Dynasmart-P) and is evaluated on a medium size network for various search space sizes and for different quality of the apriori matrix. The obtained results, in terms of replicating observed vehicle counts and the closeness to the real demand, are promising and point to the robustness of the gradient-free framework and its high performance irrespective of the quality of the apriori travel information. The use of Distributed EA is also shown to provide good results within fast computing speeds.",https://ieeexplore.ieee.org/document/5624970/,13th International IEEE Conference on Intelligent Transportation Systems,19-22 Sept. 2010,ieeexplore
10.23919/ACC.2017.7963641,Distributed mean-field-type filter for vehicle tracking,IEEE,Conferences,"Particle filter is an effective tool for vehicle tracking. However, we need to maintain a large number of particles to keep a reasonable tracking accuracy for multi-target tracking in large scale state space. This paper proposes a new distributed mean-field-type filter to handle those noisy, partial-observed and high-dimensional data. The state space is decomposed and the particles are deployed locally and updated independently in the simplified subspaces. The filtering framework contains four operations: sampling, prediction, decomposition and correction. A mean-field term is included in the system dynamic so that the prediction is based on the previous state as well as its statistic distribution, which is estimated by a multi-frame learning procedure. The experiment on real data shows that our approach can achieve accurate tracking results with a small number of particles.",https://ieeexplore.ieee.org/document/7963641/,2017 American Control Conference (ACC),24-26 May 2017,ieeexplore
10.1109/ICASSP.2019.8683856,Diving Deep onto Discriminative Ensemble of Histological Hashing &amp; Class-Specific Manifold Learning for Multi-class Breast Carcinoma Taxonomy,IEEE,Conferences,"Histopathological images (HI) encrypt resolution dependent heterogeneous textures &amp; diverse color distribution variability, manifesting in micro-structural surface tissue convolutions &amp; inherently high coherency of cancerous cells posing significant challenges to breast cancer (BC) multi-classification. As such, multi-class stratification is sparsely explored &amp; prior work mainly focus on benign &amp; malignant tissue characterization only, which forestalls further quantitative analysis of subordinate classes like adenosis, mucinous carcinoma &amp; fibroadenoma etc, for diagnostic competence. In this work, a fully-automated, near-real-time &amp; computationally inexpensive robust multi-classification deep framework from HI is presented.The proposed scheme employs deep neural network (DNN) aided discriminative ensemble of holistic class-specific manifold learning (CSML) for underlying HI sub-space embedding &amp; HI hashing based local shallow signatures. The model achieves 95.8% accuracy pertinent to multi-classification, an 2.8% overall performance improvement &amp; 38.2% enhancement for Lobular carcinoma (LC) sub-class recognition rate as compared to the existing state-of-the-art on well known BreakHis dataset is achieved. Also, 99.3% recognition rate at 200 &amp; a sensitivity of 100% for binary grading at all magnification validates its suitability for clinical deployment in hand-held smart devices.",https://ieeexplore.ieee.org/document/8683856/,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",12-17 May 2019,ieeexplore
10.1109/SmartGridComm.2019.8909793,Domain-Adversarial Transfer Learning for Robust Intrusion Detection in the Smart Grid,IEEE,Conferences,"The smart grid faces growing cyber-physical attack threats aimed at the critical systems and processes communicating over the complex cyber-infrastructure. Thanks to the increasing availability of high-quality data and the success of deep learning algorithms, machine learning (ML)-based detection and classification have been increasingly effective and adopted against sophisticated attacks. However, many of these techniques rely on the assumptions that the training and testing datasets share the same distribution and the same class labels in a stationary environment. As such assumptions may fail to hold when the system dynamics shift and new threat variants emerge in a non-stationary environment, the capability of trained ML models to adapt in complex operating scenarios will be critical to their deployment in real-world smart grid communications. To this aim, this paper proposes a domain-adversarial transfer learning framework for robust intrusion detection against smart grid attacks. The framework introduces domain-adversarial training to create a mapping between the labeled source domain and the unlabeled target domain so that the classifiers can learn in a new feature space against unknown threats. The proposed framework with different baseline classifiers was evaluated using a smart grid cyber-attack dataset collected over a realistic hardware-in-the- loop security testbed. The results have demonstrated effective performance improvements of trained classifiers against unseen threats of different types and locations.",https://ieeexplore.ieee.org/document/8909793/,"2019 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)",21-23 Oct. 2019,ieeexplore
10.1109/SCCC.2001.972633,Domain-dependent option policies in autonomous robot learning,IEEE,Conferences,"In control-related applications such as robotics, determination of optimal solutions is made very difficult for many reasons. Among these stands the difficulty in finding out an appropriate model of the domain, as defined by the control agent (robot), environment where it acts and their interaction. Reinforcement learning is a theory which defines a collection of algorithms for determination of control actions under model-free assumptions, which allows control agents to learn optimal actions in an autonomous way. In reinforcement learning, a cost functional to be optimised is determined in advance. The agent then learns how to perform this optimisation via trial and error on its environment. A trial corresponds to execution of actions chosen by the agent, and the error is the immediate result (a real-valued reinforcement) of this action. In the work reported, we consider trials by a learning robotic agent which are not based on low level actions, but instead on sequences of actions (options or macro-operators). We analysed the performance both in terms of learning speed and quality of learned control-for options that correspond to mappings from states to action policies (O/sub /spl Pi// options). Experimental results show that careful (domain-dependent) selection of options (via methods such as discretised potential fields) produce much faster learning for option-based robots when compared to their action-based counterparts. Of critical importance, however, is the option mapping in regions of the state space where the options are not assumed to be necessary: as performance of reinforcement learning algorithms is strongly dependent on sufficient exploration of the state space, even in such regions a careful, ad-hoc selection of actions is of foremost importance.",https://ieeexplore.ieee.org/document/972633/,SCCC 2001. 21st International Conference of the Chilean Computer Science Society,9-9 Nov. 2001,ieeexplore
10.1109/SNPD.2007.475,Double Unmanned Aerial Vehicle's Path Planning for Scout via Cross-Entropy Method,IEEE,Conferences,"Cross-entropy has been recently applied to combinatorial optimization problems with satisfying results. This paper introduce the cross-entropy method theory, the way of model making, real time and robust, and the application, in military scout , Unmanned Aerial Vehicle(UAV) is a special tool, with so many advantage in real time and global space. A new method, based on cross-entropy method, was used for the optimal way of double UAV's path planning.",https://ieeexplore.ieee.org/document/4287760/,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",30 July-1 Aug. 2007,ieeexplore
10.1109/ASONAM.2018.8508284,DroidEye: Fortifying Security of Learning-Based Classifier Against Adversarial Android Malware Attacks,IEEE,Conferences,"To combat the evolving Android malware attacks, systems using machine learning techniques have been successfully deployed for Android malware detection. In these systems, based on different feature representations, various kinds of classifiers are constructed to detect Android malware. Unfortunately, as classifiers become more widely deployed, the incentive for defeating them increases. In this paper, we first extract a set of features from the Android applications (apps) and represent them as binary feature vectors; with these inputs, we then explore the security of a generic learning-based classifier for Android malware detection in the presence of adversaries. To harden the evasion, we first present count featurization to transform the binary feature space into continuous probabilities encoding the distribution in each class (either benign or malicious). To improve the system security while not compromising the detection accuracy, we further introduce softmax function with adversarial parameter to find the best trade-off between security and accuracy for the classifier. Accordingly, we develop a system named DroidEye which integrates our proposed method for Android malware detection. Comprehensive experiments on the real sample collection from Comodo Cloud Security Center are conducted to validate the effectiveness of DroidEye against adversarial Android malware attacks. Our proposed secure-learning paradigm is also applicable for other detection tasks, such as spammer detection in social media.",https://ieeexplore.ieee.org/document/8508284/,2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),28-31 Aug. 2018,ieeexplore
10.1109/VDAT50263.2020.9190415,DynRP- Non-Intrusive Profiler for Dynamic Reconfigurability,IEEE,Conferences,"Emerging technological areas such as machine learning, speech recognition, computer vision, autonomous robots, AI, bioinformatics involving big data, require implementation in complex heterogeneous accelerator platforms, to be able to handle data explosion with higher efficiency, lower power, and better performance. Dynamic reconfiguration in such platforms can help in run-time optimization to meet the design goals. The required optimal platform configuration can be achieved by a flexible design space exploration and appropriate task partitioning obtained through profiling computation and communication of processes in application code. This paper focuses on profiling, it being the key to the success of obtaining optimal platform configurations. It points to existing profiling techniques, their pros and cons vis--vis dynamic reconfigurable architectures, and the challenges in their design for obtaining optimal profiling performance. It further outlines desirable specifications for a profiler to allow dynamic real-time profiling for effective use of dynamic reconfiguration. DynRP, a non-intrusive hardware profiler for dynamic reconfiguration is proposed based on the desirable specifications, followed by its design and implementation details.",https://ieeexplore.ieee.org/document/9190415/,2020 24th International Symposium on VLSI Design and Test (VDAT),23-25 July 2020,ieeexplore
10.1109/ICRAE50850.2020.9310850,Dynamic Occlusion Handling for Real Time Object Perception,IEEE,Conferences,"An RGB-D based occlusion-handling camera position computation method for proper object perception has been designed and implemented. This proposal is an improved alternative to our previous optimisation-based approach where the contribution is twofold: this new method is geometric-based and it is also able to handle dynamic occlusions. This approach makes extensive use of a ray-projection model where a key aspect is that the solution space is defined within a sphere surface around the object. The method has been designed with a view to robotic applications and therefore provides robust and versatile features. Therefore, it does not require training nor prior knowledge of the scene, making it suitable for diverse applications and scenarios. Satisfactory results have been obtained with real time experiments.",https://ieeexplore.ieee.org/document/9310850/,2020 5th International Conference on Robotics and Automation Engineering (ICRAE),20-22 Nov. 2020,ieeexplore
10.1109/CloudNet.2014.6968974,Dynamic allocation and efficient distribution of data among multiple clouds using network coding,IEEE,Conferences,"Distributed storage has attracted large interest lately from both industry and researchers as a flexible, cost-efficient, high performance, and potentially secure solution for geographically distributed data centers, edge caching or sharing storage among users. This paper studies the benefits of random linear network coding to exploit multiple commercially available cloud storage providers simultaneously with the possibility to constantly adapt to changing cloud performance in order to optimize data retrieval times. The main contribution of this paper is a new data distribution mechanisms that cleverly stores and moves data among different clouds in order to optimize performance. Furthermore, we investigate the trade-offs among storage space, reliability and data retrieval speed for our proposed scheme. By means of real-world implementation and measurements using well-known and publicly accessible cloud service providers, we can show close to 9x less network use for the adaptation compared to more conventional dense recoding approaches, while maintaining similar download time performance and the same reliability.",https://ieeexplore.ieee.org/document/6968974/,2014 IEEE 3rd International Conference on Cloud Networking (CloudNet),8-10 Oct. 2014,ieeexplore
10.1109/WCICA.2016.7578244,Dynamic ensemble selection methods for heterogeneous data mining,IEEE,Conferences,"Big Data is often collected from multiple sources with possibly different features, representations and granularity and hence is defined as heterogeneous data. Such multiple datasets need to be fused together in some ways for further analysis. Data fusion at feature level requires domain knowledge and can be time-consuming and ineffective, but it could be avoided if decision-level fusion is applied properly. Ensemble methods appear to be an appropriate paradigm to do just that as each subset of heterogeneous data sources can be separately used to induce models independently and their decisions are then aggregated by a decision fusion function in an ensemble. This study investigates how heterogeneous data can be used to generate more diverse classifiers to build more accurate ensembles. A Dynamic Ensemble Selection Optimisation (DESO) framework is proposed, using the local feature space of heterogeneous data to increase diversity among classifiers and Simulated Annealing for optimisation. An implementation example of DESO - BaggingDES is provided with Bagging as a base platform of DESO, to test its performance and also explore the relationship between diversity and accuracy. Experiments are carried out with some heterogeneous datasets derived from real-world benchmark datasets. The statistical analyses of the results show that BaggingDES performed significantly better than the baseline method - decision tree, and reasonably better than the classic Bagging.",https://ieeexplore.ieee.org/document/7578244/,2016 12th World Congress on Intelligent Control and Automation (WCICA),12-15 June 2016,ieeexplore
10.1109/ICSMC.1995.537949,Dynamic path planning,IEEE,Conferences,"Path planning is dynamic when the path is continually recomputed as more information becomes available. A computational framework for dynamic path planning is proposed which has the ability to provide navigational directions during the computation of the plan. Path planning is performed using a potential field approach. We use a specific type of potential function-a harmonic function-which has no local minima. The implementation is parallel and consists of a collection of communicating processes, across a network of SPARC &amp; SGI workstations using a message passing software package called PVM. The computation of the plan is performed independently of the execution of the plan. A hierarchical coarse-to-fine procedure is used to guarantee a correct control strategy at the expense of accuracy. We have successfully navigated a Nomad robot around our lab space with no a priori map in real-time. The result of the described approach is a parallel implementation which permits dynamic path planning using available processor resources.",https://ieeexplore.ieee.org/document/537949/,"1995 IEEE International Conference on Systems, Man and Cybernetics. Intelligent Systems for the 21st Century",22-25 Oct. 1995,ieeexplore
10.1109/VRAIS.1995.512495,Dynamic registration correction in augmented-reality systems,IEEE,Conferences,"This paper addresses the problem of correcting visual registration errors in video-based augmented-reality systems. Accurate visual registration between real and computer-generated objects in combined images is critically important for conveying the perception that both types of object occupy the same 3-dimensional (3D) space. To date, augmented-reality systems have concentrated on simply improving 3D coordinate system registration in order to improve apparent (image) registration error. This paper introduces the idea of dynamically measuring registration error in combined images (2D error) and using that information to correct 3D coordinate system registration error which in turn improves registration in the combined images. Registration can be made exact in every combined image if a small video delay can be tolerated. Our experimental augmented-reality system achieves improved image registration, stability, and error tolerance from tracking system drift and jitter over current augmented-reality systems. No additional tracking hardware or other devices are needed on the user's head-mounted display. Computer-generated objects can be ""nailed"" to real-world reference points in every image the user sees with an easily-implemented algorithm. Dynamic error correction as demonstrated here will likely be a key component of future augmented-reality systems.",https://ieeexplore.ieee.org/document/512495/,Proceedings Virtual Reality Annual International Symposium '95,11-15 March 1995,ieeexplore
10.1109/CSNet47905.2019.9108976,Dynamic security management driven by situations: An exploratory analysis of logs for the identification of security situations,IEEE,Conferences,"Situation awareness consists of ""the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future"". Being aware of the security situation is then mandatory to launch proper security reactions in response to cybersecurity attacks. Security Incident and Event Management solutions are deployed within Security Operation Centers. Some vendors propose machine learning based approaches to detect intrusions by analysing networks behaviours. But cyberattacks like Wannacry and NotPetya, which shut down hundreds of thousands of computers, demonstrated that networks monitoring and surveillance solutions remain insufficient. Detecting these complex attacks (a.k.a. Advanced Persistent Threats) requires security administrators to retain a large number of logs just in case problems are detected and involve the investigation of past security events. This approach generates massive data that have to be analysed at the right time in order to detect any accidental or caused incident. In the same time, security administrators are not yet seasoned to such a task and lack the desired skills in data science. As a consequence, a large amount of data is available and still remains unexplored which leaves number of indicators of compromise under the radar. Building on the concept of situation awareness, we developed a situation-driven framework, called dynSMAUG, for dynamic security management. This approach simplifies the security management of dynamic systems and allows the specification of security policies at a high-level of abstraction (close to security requirements). This invited paper aims at exposing real security situations elicitation, coming from networks security experts, and showing the results of exploratory analysis techniques using complex event processing techniques to identify and extract security situations from a large volume of logs. The results contributed to the extension of the dynSMAUG solution.",https://ieeexplore.ieee.org/document/9108976/,2019 3rd Cyber Security in Networking Conference (CSNet),23-25 Oct. 2019,ieeexplore
10.1109/IROS.2015.7353649,Dynamically Pruned A* for re-planning in navigation meshes,IEEE,Conferences,"Modern simulations feature crowds of AI-controlled agents moving through dynamic environments, with obstacles appearing or disappearing at run-time. A dynamic navigation mesh can represent the traversable space of such environments. The A* algorithm computes optimal paths through the dual graph of this mesh. When an obstacle is inserted or deleted, the mesh changes and agents should re-plan their paths. Many existing re-planning algorithms are too memory-intensive for crowds, or they cannot easily be used on graphs where vertices and edges are added or removed. In this paper, we present Dynamically Pruned A* (DPA*), an extension of A* for re-planning optimal paths in dynamic navigation meshes. DPA* has similarities to adaptive algorithms that make the A* heuristic more informed based on previous queries. However, DPA* prunes the search using only the previous path and its relation to the dynamic event. We describe the four re-planning scenarios that can occur; DPA* uses different rules in each scenario. Our algorithm is memory-friendly and robust against structural changes in the graph, which makes it suitable for crowds in dynamic navigation meshes. Experiments show that DPA* performs particularly well in complex environments and when the dynamic event is visible to the agent. We integrate the algorithm into crowd simulation software to model large crowds in dynamic environments in real-time.",https://ieeexplore.ieee.org/document/7353649/,2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Sept.-2 Oct. 2015,ieeexplore
10.1109/SECON52354.2021.9491609,EFCam: Configuration-Adaptive Fog-Assisted Wireless Cameras with Reinforcement Learning,IEEE,Conferences,"Visual sensing has been increasingly employed in industrial processes. This paper presents the design and implementation of an industrial wireless camera system, namely, EFCam, which uses low-power wireless communications and edge-fog computing to achieve cordless and energy-efficient visual sensing. The camera performs image pre-processing (i.e., compression or feature extraction) and transmits the data to a resourceful fog node for advanced processing using deep models. EFCam admits dynamic configurations of several parameters that form a configuration space. It aims to adapt the configuration to maintain desired visual sensing performance of the deep model at the fog node with minimum energy consumption of the camera in image capture, pre-processing, and data communications, under dynamic variations of application requirement and wireless channel conditions. However, the adaptation is challenging due primarily to the complex relationships among the involved factors. To address the complexity, we apply deep reinforcement learning to learn the optimal adaptation policy. Extensive evaluation based on trace-driven simulations and experiments show that EFCam complies with the accuracy and latency requirements with lower energy consumption for a real industrial product object tracking application, compared with four baseline approaches incorporating hysteresis-based adaptation.",https://ieeexplore.ieee.org/document/9491609/,"2021 18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)",6-9 July 2021,ieeexplore
10.1109/BSN.2015.7299402,"EMI Spy: Harnessing electromagnetic interference for low-cost, rapid prototyping of proxemic interaction",IEEE,Conferences,"We present a wearable system that uses ambient electromagnetic interference (EMI) as a signature to identify electronic devices and support proxemic interaction. We designed a low cost tool, called EMI Spy, and a software environment for rapid deployment and evaluation of ambient EMI-based interactive infrastructure. EMI Spy captures electromagnetic interference and delivers the signal to a user's mobile device or PC through either the device's wired audio input or wirelessly using Bluetooth. The wireless version can be worn on the wrist, communicating with the user;s mobile device in their pocket. Users are able to train the system in less than 1 second to uniquely identify displays in a 2-m radius around them, as well as to detect pointing at a distance and touching gestures on the displays in real-time. The combination of a low cost EMI logger and an open source machine learning tool kit allows developers to quickly prototype proxemic, touch-to-connect, and gestural interaction. We demonstrate the feasibility of mobile, EMI-based device and gesture recognition with preliminary user studies in 3 scenarios, achieving 96% classification accuracy at close range for 6 digital signage displays distributed throughout a building, and 90% accuracy in classifying pointing gestures at neighboring desktop LCD displays. We were able to distinguish 1- and 2-finger touching with perfect accuracy and show indications of a way to determine power consumption of a device via touch. Our system is particularly well-suited to temporary use in a public space, where the sensors could be distributed to support a popup interactive environment anywhere with electronic devices. By designing for low cost, mobile, flexible, and infrastructure-free deployment, we aim to enable a host of new proxemic interfaces to existing appliances and displays",https://ieeexplore.ieee.org/document/7299402/,2015 IEEE 12th International Conference on Wearable and Implantable Body Sensor Networks (BSN),9-12 June 2015,ieeexplore
10.1109/AERO.1997.574422,ESL: a language for supporting robust plan execution in embedded autonomous agents,IEEE,Conferences,"ESL (Execution Support Language) is a language for encoding execution knowledge in embedded autonomous agents. It is similar in spirit to RAPs (1989), RS (1983), and RPL Reactive Plan Language, and its design owes much to these systems. Unlike its predecessors, ESL aims for a more utilitarian point in the design space. ESL was designed primarily to be a powerful and easy-to-use tool, not to serve as a representation for automated reasoning or formal analysis (although nothing precludes its use for these purposes). ESL consists of several sets of loosely coupled features that can be composed in arbitrary ways. It is currently implemented as a set of extensions to Common Lisp, and is being used to build the executive component of a control architecture for an autonomous spacecraft.",https://ieeexplore.ieee.org/document/574422/,1997 IEEE Aerospace Conference,13-13 Feb. 1997,ieeexplore
10.1109/ROMAN.2011.6005223,Effect of human guidance and state space size on Interactive Reinforcement Learning,IEEE,Conferences,"The Interactive Reinforcement Learning algorithm enables a human user to train a robot by providing rewards in response to past actions and anticipatory guidance to guide the selection of future actions. Past work with software agents has shown that incorporating user guidance into the policy learning process through Interactive Reinforcement Learning significantly improves the policy learning time by reducing the number of states the agent explores. We present the first study of Interactive Reinforcement Learning in real-world robotic systems. We report on four experiments that study the effects that teacher guidance and state space size have on policy learning performance. We discuss modifications made to apply Interactive Reinforcement Learning to a real-world system and show that guidance significantly reduces the learning rate, and that its positive effects increase with state space size.",https://ieeexplore.ieee.org/document/6005223/,2011 RO-MAN,31 July-3 Aug. 2011,ieeexplore
10.1109/IPCCC50635.2020.9391551,Efficient Architecture Paradigm for Deep Learning Inference as a Service,IEEE,Conferences,"Deep learning (DL) inference has been broadly used and shown excellent performance in many intelligent applications. Unfortunately, the high resource consumption and training efforts of sophisticated models present obstacles for regular users to enjoy it. Thus, Deep Learning Inference as a Service (DIaaS), offering online inference services on cloud, has earned great popularity among cloud tenants who can send their DIaaS inputs via RPCs across the internal network. However, such detached architecture paradigm is inappropriate to DIaaS because the high-dimensional inputs of DIaaS consume a lot of precious internal bandwidth and the service latency of DIaaS has to be low and stable. We therefore propose a novel architecture paradigm on cloud for DIaaS in order to address the above two problems without giving up the security and maintenance benefits. We first leverage the SGX technology, a strongly-protected user space enclave, to bring DIaaS computation to its input source as close as possible, i.e. co-locating a cloud tenant and its subscribed DIaaS in the same virtual machine. When the GPU acceleration is needed, we migrate this virtual machine to any available GPU host and transparently utilize the GPU via our backend computing stack installed on it. In this way the majority of internal bandwidth is saved compared to traditional paradigm. Furthermore, we greatly improve the efficiency of the proposed architecture paradigm, from the computation and I/O perspectives, by making the entire data flow more DL-oriented. Finally, we implement a prototype system and evaluate it in real-world scenarios. The experiments show that our locality-aware architecture achieves the average single CPU (GPU) based deep learning inference time 2.84X (4.87X) less than the traditional detached architecture on average.",https://ieeexplore.ieee.org/document/9391551/,2020 IEEE 39th International Performance Computing and Communications Conference (IPCCC),6-8 Nov. 2020,ieeexplore
10.1109/CVPR.2019.00790,Efficient Decision-Based Black-Box Adversarial Attacks on Face Recognition,IEEE,Conferences,"Face recognition has obtained remarkable progress in recent years due to the great improvement of deep convolutional neural networks (CNNs). However, deep CNNs are vulnerable to adversarial examples, which can cause fateful consequences in real-world face recognition applications with security-sensitive purposes. Adversarial attacks are widely studied as they can identify the vulnerability of the models before they are deployed. In this paper, we evaluate the robustness of state-of-the-art face recognition models in the decision-based black-box attack setting, where the attackers have no access to the model parameters and gradients, but can only acquire hard-label predictions by sending queries to the target model. This attack setting is more practical in real-world face recognition systems. To improve the efficiency of previous methods, we propose an evolutionary attack algorithm, which can model the local geometry of the search directions and reduce the dimension of the search space. Extensive experiments demonstrate the effectiveness of the proposed method that induces a minimum perturbation to an input face image with fewer queries. We also apply the proposed method to attack a real-world face recognition system successfully.",https://ieeexplore.ieee.org/document/8953400/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore
10.1109/DAC.2005.193910,Efficient SAT solving: beyond supercubes,IEEE,Conferences,"SAT (Boolean satisfiability) has become the primary Boolean reasoning engine for many EDA applications, so the efficiency of SAT solving is of great practical importance. Recently, Goldberg et al introduced supercubing, a different approach to search-space pruning, based on a theory that unifies many existing methods. Their implementation reduced the number of decisions, but no speedup was obtained. In this paper, we generalize beyond supercubes, creating a theory we call B-cubing, and show how to implement B-cubing in a practical solver. On extensive benchmark runs, using both real problems and synthetic benchmarks, the new technique is competitive on average with the newest version of ZChaff, is much faster in some cases, and is more robust.",https://ieeexplore.ieee.org/document/1510430/,"Proceedings. 42nd Design Automation Conference, 2005.",13-17 June 2005,ieeexplore
10.1109/ICCD.2001.955078,Efficient function approximation for embedded and ASIC applications,IEEE,Conferences,"In embedded systems and application specific integrated circuits (ASICs) that typically do not have a floating-point processor, measured data or function-sampled data is commonly described by means of an analytic function derived using standard numerical methods. The resultant errors are not caused by rounding the coefficients but by translating a real solution to a restricted fixed-point environment. A genetic algorithm has been constructed that discovers a superior piecewise polynomial approximation with coefficients restricted to the integer target space. This paper discusses the problem being solved and presents an overview of the implemented solution.",https://ieeexplore.ieee.org/document/955078/,Proceedings 2001 IEEE International Conference on Computer Design: VLSI in Computers and Processors. ICCD 2001,23-26 Sept. 2001,ieeexplore
10.1109/BDCloud.2015.51,Efficient k-Nearest Neighbors Search in High Dimensions Using MapReduce,IEEE,Conferences,"Finding the k-Nearest Neighbors (kNN) of a query object for a given dataset S is a primitive operation in many application domains. kNN search is very costly, especially many applications witness a quick increase in the amount and dimension of data to be processed. Locality sensitive hashing (LSH) has become a very popular method for this problem. However, most such methods can't obtain good performance in terms of search quality, search efficiency and space cost at the same time, such as RankReduce, which gains good search efficiency at the sacrifice of the search quality. Motivated by these, we propose a novel LSH-based inverted index scheme and design an efficient search algorithm, called H-c2kNN, which enables fast high-dimensional kNN search with excellent quality and low space cost. For efficiency and scalability concerns, we implemented our proposed approach to solve the kNN search in high dimensional space using MapReduce, which is a well-known framework for data-intensive applications and conducted extensive experiments to evaluate our proposed approach using both synthetic and real datasets. The results show that our proposed approach outperforms baseline methods in high dimensional space.",https://ieeexplore.ieee.org/document/7310711/,2015 IEEE Fifth International Conference on Big Data and Cloud Computing,26-28 Aug. 2015,ieeexplore
10.1109/NLPKE.2003.1275966,Efficient mining of textual associations,IEEE,Conferences,"We describe an efficient implementation for mining textual associations from text corpora. In order to tackle real world applications, efficient algorithms and data structures are needed to manage, in reasonable time and space, the overgrowing volume of text data. For that purpose, we introduce a global architecture based on masks, suffix arrays and multidimensional arrays to implement the SENTA extractor (Dias, 2002). In particular, SENTA has shown great flexibility and accuracy for mining textual associations such as collocations, cognates, morphemes and chunks. Our solution shows O(h(F) N log N) time complexity and O(N) space complexity where N is the size of the corpus and h(F) is a function of the context window size.",https://ieeexplore.ieee.org/document/1275966/,"International Conference on Natural Language Processing and Knowledge Engineering, 2003. Proceedings. 2003",26-29 Oct. 2003,ieeexplore
10.1109/IJCNN.1999.831156,Efficient training techniques for classification with vast input space,IEEE,Conferences,"Strategies to efficiently train a neural network for an aerospace problem with a large multidimensional input space are developed and demonstrated. The neural network provides classification for over 100,000,000 data points. A query-based strategy is used that initiates training using a small input set, and then augments the set in multiple stages to include important data around the network decision boundary. Neural network inversion and oracle query are used to generate the additional data, jitter is added to the query data to improve the results, and an extended Kalman filter algorithm is used for training. A causality index is discussed as a means to reduce the dimensionality of the problem based on the relative importance of the inputs.",https://ieeexplore.ieee.org/document/831156/,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),10-16 July 1999,ieeexplore
10.1109/ICPR.2000.902928,Eigensnakes for vessel segmentation in angiography,IEEE,Conferences,"We introduce a new deformable model, called eigensnake, for segmentation of elongated structures in a probabilistic framework. Instead of snake attraction by specific image features extracted independently of the snake, our eigensnake learns an optimal object description and searches for such image feature in the target image. This is achieved applying principal component analysis on image responses of a bank of Gaussian derivative filters. Therefore, attraction by eigensnakes is defined in terms of classification of image features. The potential energy for the snake is defined in terms of likelihood in the feature space and incorporated into a new energy minimising scheme. Hence, the snake deforms to minimise the mahalanobis distance in the feature space. A real application of segmenting and tracking coronary vessels in angiography is considered and the results are very encouraging.",https://ieeexplore.ieee.org/document/902928/,Proceedings 15th International Conference on Pattern Recognition. ICPR-2000,3-7 Sept. 2000,ieeexplore
10.1109/TAI.1993.633958,Elastic version space: a knowledge acquisition method with background knowledge adjustment,IEEE,Conferences,"Similarity based learning (SBL) is efficient in knowledge acquisition process, and it uses training examples to generate rules and refine them. Training examples collected in the real world are very often constructed with numerical attributes. In order to deal with these training examples, SBL needs background knowledge. Especially segments which specify value ranges of numerical attributes are discussed in the background knowledge. Elastic version space method is proposed here which integrates the version space method with the functions of segments adjustment. By defining the segments structure using margin segments in the background knowledge, the version space method itself adjusts the segments. In consequence, this method expands the region of application of version space. Empirical results applying to the man-power allocation problem are presented which shows that the elastic version space method is an effective SBL in the knowledge acquisition process.",https://ieeexplore.ieee.org/document/633958/,Proceedings of 1993 IEEE Conference on Tools with Al (TAI-93),8-11 Nov. 1993,ieeexplore
10.1109/ICCCNT45670.2019.8944554,Elucidating Farmers towards Smart Agricultural Farm Building through Cloud Model,IEEE,Conferences,"Agriculture is the main source of food and is undeniably the key reason for the survival of the beings in this planet. With the growing population, the precision-centric, informed, and optimized production of various food items is essential. There are new technologies, methodologies and best practices being conceived and conceptualized by the experts and researchers in the agriculture field. The various advancements in the information technology (IT) domain are showing some positive signs towards efficient and intelligent agriculture. The widely discussed innovations and disruptions in the IT space are bound to impact the agriculture activities in a strategically sound manner. A variety of changes in the way our farmers are toiling in the agriculture land parcels are being expected. With the emergence of software-defined cloud environments, big and real-time data analytics, the faster adoption of artificial intelligence (AI) advancements, knowledge visualization tools, the growing array of Smartphone applications are to bring in a raft of transformations in the lives of farmers in the days ahead. This paper is to describe the various ways and means of empowering our farmers with realtime information and insights through the cloud technology.",https://ieeexplore.ieee.org/document/8944554/,"2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",6-8 July 2019,ieeexplore
10.1109/ICNC.2010.5584314,Emotion generation for virtual human using Cognitive Map,IEEE,Conferences,"Affective computing is an indispensable aspect in harmonious human-computer interaction and artificial intelligence. Making computers have the ability of generating emotions is a challenging task of affective computing. The paper first introduces the basic affective elements, and the representation of affections in a computer. An emotion generation model using Cognitive Map is proposed. The model is used to generate emotion based on the evaluation of the overall influences of the mood, the personality, the previous emotion and the external stimulations. Then the paper describes a method to build a mapping from the emotion space to the facial expression space with a competitive network, and the implementation of facial expression animation. Finally, the paper constructs an intelligent virtual human system with facial expression, voice and vision communication. Experimental results show that the emotion generation model using Cognitive Map can produce realistic emotions similar to those of a real human.",https://ieeexplore.ieee.org/document/5584314/,2010 Sixth International Conference on Natural Computation,10-12 Aug. 2010,ieeexplore
10.1109/MILCOM.2017.8170755,Empirical statistical inference attack against PHY-layer key extraction in real environments,IEEE,Conferences,"Traditional cryptographic secret key establishment mechanisms are facing challenges with the fast growth of high-performance computing, and can be very costly in many settings, e.g. in wireless ad-hoc networks, since they consume scarce resources such as bandwidth and battery power. As an alternative, link-signature-based (LSB) secret key extraction techniques have received many interests in recent years. It is believed that these mechanisms are secure, based on the fundamental assumption that wireless signals received at two locations separated by more than half a wavelength apart are uncorrelated. However, recently it has been observed that in some circumstances this assumption does not hold, rendering LSB key extraction mechanisms vulnerable to attacks. This paper studies empirical statistical inference attacks (SIA) to LSB key extraction, whereby an attacker infers the signature of a target link, and henceforce recovers the secret key extracted from that signature, by observing the surrounding links. Different from prior work that assumes a theoretical link-correlation model for the inference, our study does not make any assumption on link correlation. Instead, ours is taking a machine learning method for link inference based on empirically measured link data. Machine learning (ML) algorithms are developed to launch SIA under various realistic scenarios. Our experiment results show that even without making assumptions on link correlation, the proposed inference algorithms are still quite effective, and can reduce the key search space by many orders of magnitudes compared to brutal force search.",https://ieeexplore.ieee.org/document/8170755/,MILCOM 2017 - 2017 IEEE Military Communications Conference (MILCOM),23-25 Oct. 2017,ieeexplore
10.1109/ECCTD.2017.8093280,Emulating CNN with template learning on FPGA,IEEE,Conferences,"A 2-D Cellular Neural Network structure with space invariant neural weights is widely used in image processing applications. Recent advances VLSI technology appears to be very promising to use discrete time CNNs for real time vision applications. In this paper, a system-on-chip implementation which consists of a new CNN emulator design and a processor which performs template learning algorithm is shown. SoC design is programmed to perform a sequential CNN operations on different input and state images with different templates. Furthermore, the presented SoC design allows that templates can be updated by a learning algoritm in run time. SoC design is realised on a target FPGA. Test results on FPGA and MATLAB are presented and compared with structural similarity map.",https://ieeexplore.ieee.org/document/8093280/,2017 European Conference on Circuit Theory and Design (ECCTD),4-6 Sept. 2017,ieeexplore
10.1109/ICIP.2017.8296489,Encyclopedia enhanced semantic embedding for zero-shot learning,IEEE,Conferences,"There are tremendous object categories in the real world besides those in image datasets. Zero-shot learning aims to recognize image categories which are unseen in the training set. A large number of previous zero-shot learning models use word vectors of the class labels directly as category prototypes in the semantic embedding space. But word vectors cannot obtain the global knowledge of an image category sufficiently. In this paper, we propose a new encyclopedia enhanced semantic embedding model to promote the discriminative capability of word vector prototypes with the global knowledge of each image category. The proposed model extracts the TF-IDF key words from encyclopedia articles to acquire the global knowledge of each category. The convex combination of the key words' word vectors acts as the prototypes of the object categories. The prototypes of seen and unseen classes build up the embedding space where the nearest neighbour search is implemented to recognize the unseen images. The experiments show that the proposed method achieves the state-of-the-art performance on the challenging ImageNet Fall 2011 1k2hop dataset.",https://ieeexplore.ieee.org/document/8296489/,2017 IEEE International Conference on Image Processing (ICIP),17-20 Sept. 2017,ieeexplore
10.1109/CVPR42600.2020.01463,End-to-End Adversarial-Attention Network for Multi-Modal Clustering,IEEE,Conferences,"Multi-modal clustering aims to cluster data into different groups by exploring complementary information from multiple modalities or views. Little work learns the deep fused representations and simutaneously discovers the cluster structure with a discriminative loss. In this paper, we present an End-to-end Adversarial-attention network for Multi-modal Clustering (EAMC), where adversarial learning and attention mechanism are leveraged to align the latent feature distributions and quantify the importance of modalities respectively. To benefit from the joint training, we introducea divergence-based clustering objective that not only encourages the separation and compactness of the clusters but also enjoy a clear cluster structure by embedding the simplex geometry of the output space into the loss. The proposed network consists of modality-specific feature learning, modality fusion and cluster assignment three modules. It can be trained from scratch with batch-mode based optimization and avoid an autoencoder pretraining stage. Comprehensive experiments conducted on five real-world datasets show the superiority and effectiveness of the proposed clustering method.",https://ieeexplore.ieee.org/document/9156700/,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 2020,ieeexplore
10.1109/CISIS.2011.82,Engineering SLS Algorithms for Statistical Relational Models,IEEE,Conferences,"We present high performing SLS algorithms for learning and inference in Markov Logic Networks (MLNs). MLNs are a state-of-the-art representation formalism that integrates first-order logic and probability. Learning MLNs structure is hard due to the combinatorial space of candidates caused by the expressive power of first-order logic. We present current work on the development of algorithms for learning MLNs, based on the Iterated Local Search (ILS) metaheuristic. Experiments in real-world domains show that the proposed approach improves accuracy and learning time over the existing state-of-the-art algorithms. Moreover, MAP and conditional inference in MLNs are hard computational tasks too. This paper presents two algorithms for these tasks based on the Iterated Robust Tabu Search (IRoTS) schema. The first algorithm performs MAP inference by performing a RoTS search within a ILS iteration. Extensive experiments show that it improves over the state-of the-art algorithm in terms of solution quality and inference times. The second algorithm combines IRoTS with simulated annealing for conditional inference and we show through experiments that it is faster than the current state-of-the-art algorithm maintaining the same inference quality.",https://ieeexplore.ieee.org/document/5989060/,"2011 International Conference on Complex, Intelligent, and Software Intensive Systems",30 June-2 July 2011,ieeexplore
10.1109/INCC.2008.4562704,Enhanced D-Tree - An Index Structure For Window Queries In Location Based Services,IEEE,Conferences,"Location based services is considered as a killer application in the wireless data market that provide information based on locations specified in the queries. We consider wireless data broadcasting as a way of disseminating information to a massive number of users. To address the issues of responsiveness, energy consumption and bandwidth contention in wireless communications, an index has to minimize the search time and maintain a small storage overhead. A linear index structure based on the D-tree is used to support location dependent queries, which is reduced to planar point queries. In this paper, we propose a modified D-tree index structure for processing location dependent information focusing on window queries via wireless data broadcast in a mobile environment. Hilbert curve is used to reduce the data regions that have to be searched from the whole space. Filtering mechanisms are used to find the real answer set. To improve the query processing two techniques namely, fixed grid assignment by which the given space is partitioned into several disjoint sub-grids and caching mechanism that improves access latency were used.",https://ieeexplore.ieee.org/document/4562704/,2008 IEEE International Networking and Communications Conference,1-3 May 2008,ieeexplore
10.1109/IADCC.2015.7154739,Enhanced SMOTE algorithm for classification of imbalanced big-data using Random Forest,IEEE,Conferences,"In the era of big data, the applications generating tremendous amount of data are becoming the main focus of attention as the wide increment of data generation and storage that has taken place in the last few years. This scenario is challenging for data mining techniques which are not arrogated to the new space and time requirements. In many of the real world applications, classification of imbalanced data-sets is the point of attraction. Most of the classification methods focused on two-class imbalanced problem. So, it is necessary to solve multi-class imbalanced problem, which exist in real-world domains. In the proposed work, we introduced a methodology for classification of multi-class imbalanced data. This methodology consists of two steps: In first step we used Binarization techniques (OVA and OVO) for decomposing original dataset into subsets of binary classes. In second step, the SMOTE algorithm is applied against each subset of imbalanced binary class in order to get balanced data. Finally, to achieve classification goal Random Forest (RF) classifier is used. Specifically, oversampling technique is adapted to big data using MapReduce so that this technique is able to handle as large data-set as needed. An experimental study is carried out to evaluate the performance of proposed method. For experimental analysis, we have used different datasets from UCI repository and the proposed system is implemented on Apache Hadoop and Apache Spark platform. The results obtained shows that proposed method outperforms over other methods.",https://ieeexplore.ieee.org/document/7154739/,2015 IEEE International Advance Computing Conference (IACC),12-13 June 2015,ieeexplore
10.1109/ICRA40945.2020.9197510,Episodic Koopman Learning of Nonlinear Robot Dynamics with Application to Fast Multirotor Landing,IEEE,Conferences,"This paper presents a novel episodic method to learn a robot's nonlinear dynamics model and an increasingly optimal control sequence for a set of tasks. The method is based on the Koopman operator approach to nonlinear dynamical systems analysis, which models the flow of observables in a function space, rather than a flow in a state space. Practically, this method estimates a nonlinear diffeomorphism that lifts the dynamics to a higher dimensional space where they are linear. Efficient Model Predictive Control methods can then be applied to the lifted model. This approach allows for real time implementation in on-board hardware, with rigorous incorporation of both input and state constraints during learning. We demonstrate the method in a real-time implementation of fast multirotor landing, where the nonlinear ground effect is learned and used to improve landing speed and quality.",https://ieeexplore.ieee.org/document/9197510/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore
10.1109/WCICA.2000.863468,Estimated force emulation for space robot using neural networks,IEEE,Conferences,"This paper introduces the telerobotic system estimated force emulation using neural networks. A delay-compensating 3D stereo-graphic simulator is implemented in SGI ONYX/4 RE/sup 2/. The estimated force emulation can protect the real robot in time from being damaged in collision. The neural network is used to learn the mapping between the contact force error and the accommodated position command to the controller of the space robot. Finally, the controller can feel the emulated force with a two-hand 6-DOF master arm using the force feedback interface.",https://ieeexplore.ieee.org/document/863468/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore
10.1109/CVPRW50498.2020.00337,Evading Deepfake-Image Detectors with White- and Black-Box Attacks,IEEE,Conferences,"It is now possible to synthesize highly realistic images of people who do not exist. Such content has, for example, been implicated in the creation of fraudulent socialmedia profiles responsible for dis-information campaigns. Significant efforts are, therefore, being deployed to detect synthetically-generated content. One popular forensic approach trains a neural network to distinguish real from synthetic content.We show that such forensic classifiers are vulnerable to a range of attacks that reduce the classifier to near- 0% accuracy. We develop five attack case studies on a state- of-the-art classifier that achieves an area under the ROC curve (AUC) of 0.95 on almost all existing image generators, when only trained on one generator. With full access to the classifier, we can flip the lowest bit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb 1% of the image area to reduce the classifier's AUC to 0.08; or add a single noise pattern in the synthesizer's latent space to reduce the classifier's AUC to 0.17. We also develop a black-box attack that, with no access to the target classifier, reduces the AUC to 0.22. These attacks reveal significant vulnerabilities of certain image-forensic classifiers.",https://ieeexplore.ieee.org/document/9150604/,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),14-19 June 2020,ieeexplore
10.1109/WiSEE44079.2020.9262700,Evaluating the Cognitive Network Controller with an SNN on FPGA,IEEE,Conferences,"The cognitive network controller (CNC) defines a learning agent that can adapt online and in near real-time the routing decisions needed for bundle transmissions in a space delay-tolerant network and other challenged networks. The agent uses a spiking neural network (SNN) as the learning element in a reinforcement learning loop, which incrementally optimizes the outbound link selection for each bundle based on its estimated routing cost. In this paper, a digital hardware implementation of the SNN element of the CNC is proposed, which helps to accelerate the routing decision-making process to faster-than-real-time levels. The design was tested on a Zynq Z7020 (PYNQ-Z2) SoC/FPGA board. A distributed implementation of the CNC is also proposed, which allows offloading the SNN execution from a DTN gateway to a remote device that hosts the FPGA implementation of the SNN. The methods were validated using an emulated satellite network testbed.",https://ieeexplore.ieee.org/document/9262700/,2020 IEEE International Conference on Wireless for Space and Extreme Environments (WiSEE),12-14 Oct. 2020,ieeexplore
10.1109/OCEANS.2014.7003088,Evaluation of Q-learning for search and inspect missions using underwater vehicles,IEEE,Conferences,"An application for offline Reinforcement Learning in the underwater domain is proposed. We present and evaluate the integration of the Q-learning algorithm into an Autonomous Underwater Vehicle (AUV) for learning the action-value function in simulation. Three separate experiments are presented. The first compares two search policies: the  - least visited, and random action, with respect to convergence time. The second experiment presents the effect of the learning discount factor, gamma, on the convergence time of the  - least visited search policy. The final experiment is to validate the use of a policy learnt offline on a real AUV. This learning phase occurs offline within the continuous simulation environment which had been discretized into a grid-world learning problem. Presented results show the system's convergence to a global optimal solution whilst following both sub-optimal policies during simulation. Future work is introduced, after discussion of our results, to enable the system to be used in a real world application. The results presented, therefore, form the basis for future comparative analysis of the necessary improvements such as function approximation of the state space.",https://ieeexplore.ieee.org/document/7003088/,2014 Oceans - St. John's,14-19 Sept. 2014,ieeexplore
10.1109/SNPD.2012.99,Evaluation of Realism of Dynamic Sound Space Using a Virtual Auditory Display,IEEE,Conferences,"We can perceive a sound position from binaural signals using mainly head-related transfer functions (HRTFs). Using the theorem presented herein, we can display a sound image to a specific position in virtual auditory space by HRTFs. However, HRTF is defined commonly in a free-field, and a virtual sound image is perceived as a dry source without reflection, reverberation, or ambient noise. Therefore, the virtual sound space might be unnatural. The authors developed a software-based virtual auditory display (VAD) that outputs audio signals for a set of headphones with a three-dimensional position sensor. The VAD software can display a dynamic virtual auditory space that is responsive to a listener's head movement. Subjective evaluations were conducted to clarify the relation between the perceived reality of virtual sound space and ambient sound. Evaluation results of the reality of the virtual sound space displayed by the VAD software are introduced.",https://ieeexplore.ieee.org/document/6299338/,"2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",8-10 Aug. 2012,ieeexplore
10.1109/DCOSS.2013.49,Event Prediction and Modeling of Variable Rate Sampled Data Using Dynamic Bayesian Networks,IEEE,Conferences,"Event detection is an important issue in sensor networks for a variety of real-world applications. Many events in real world are often correlated on a complex spatio-temporal level whereby they are manifested via observations over time and space proximities. In order to predict events in these spatiotemporal observations, the prediction model should be capable of modeling codependencies between data observed at various locations. In this paper, we propose a Dynamic Bayesian Network (DBN) with such spatio-temporal event prediction capability in sensor networks deployed for sensing environmental data. More specifically, we develop a DBN model with mixture distribution and a novel learning algorithm, for water level data prediction for different canals, using rainfall data at multiple locations. Experiments on real data demonstrates that our model and training method can provide accurate event prediction in real time for spatio-temporal sensor networks.",https://ieeexplore.ieee.org/document/6569444/,2013 IEEE International Conference on Distributed Computing in Sensor Systems,20-23 May 2013,ieeexplore
10.1109/IJCNN.2019.8852384,Evidence Transfer for Improving Clustering Tasks Using External Categorical Evidence,IEEE,Conferences,"In this paper we introduce evidence transfer for clustering, a deep learning method that can incrementally manipulate the latent representations of an autoencoder, according to external categorical evidence, in order to improve a clustering outcome. By evidence transfer we define the process by which the categorical outcome of an external, auxiliary task is exploited to improve a primary task, in this case representation learning for clustering. Our proposed method makes no assumptions regarding the categorical evidence presented, nor the structure of the latent space. We compare our method, against the baseline solution by performing k-means clustering before and after its deployment. Experiments with three different kinds of evidence show that our method effectively manipulates the latent representations when introduced with real corresponding evidence, while remaining robust when presented with low quality evidence.",https://ieeexplore.ieee.org/document/8852384/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore
10.1109/IJCNN.2019.8852006,Evolutionary Community Detection in Dynamic Social Networks,IEEE,Conferences,"Evolutionary clustering is a way of detecting the evolving patterns of communities in dynamic social networks. In principle, the detection process seeks to simultaneously maximize clustering accuracy at the current time step and minimize the clustering drift between two successive time steps. Several evolutionary clustering methods have been developed in an attempt to find the best trade-off between clustering accuracy and temporal smoothness, but the classic genetic operators in these methods do not make the best of the inter- and intra-connected relationships between nodes, which limits their effectiveness. To overcome this problem, we propose a novel migration operator to work in tandem with classic genetic operators to improve the discovery of evolving community structures. The operator is implemented within an existing genetic algorithm which relies on a genome representation under a decomposition framework that formulates evolutionary community detection as a multiobjective optimization problem. Moreover, we present a new method of calculating modularity directly from a genome matrix as the objective for measuring the snapshot quality, which results in a wider search space for finding the optimal solution. Experimental results over several synthetic networks and one real-world dynamic social network suggest that our method is superior to two other state-of-the-art methods in terms of both accuracy and smoothness in discovering evolving community structures in dynamic social networks.",https://ieeexplore.ieee.org/document/8852006/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore
10.1109/DeSE.2011.109,Evolutionary Environmental Modelling in Self-Managing Software Systems,IEEE,Conferences,"The promise of robust software that can self-manage significant aspects of its operation, including the ability to self-configure, self-heal, self-optimise and self-protect through having the requisite functionality to respond and adapt to changes in its operational environment is both seductive and compelling. There are a growing number of examples of partial implementations appearing in the literature and continued development across a number of areas can be expected in the future.One of the less travelled areas of research concerns the problem of developing an accurate and current model of the environment in which such adaptive systems will operate. It would seem a compelling argument that holding a current model of both the environment and the current capability of the system allowing the system to ""know itself"" are desirable additions to any adaptive system. As such they have a view of the complex space within which they can adapt and that without these properties the system could only be considered as purely reactive.Here, the use of Learning Classifier Systems and geneticalgorithms to provide the modelling element required of effective adaptive software systems is presented and evaluated. The work uses the virtual world platform of ""Second Life"" to represent anappropriate experimental environment. One outcome of this work is the restatement of some classical cybernetic principles to reflect the need for constant evolution.",https://ieeexplore.ieee.org/document/6150008/,2011 Developments in E-systems Engineering,6-8 Dec. 2011,ieeexplore
10.1109/CIG.2016.7860426,Evolutionary deckbuilding in hearthstone,IEEE,Conferences,"One of the most notable features of collectible card games is deckbuilding, that is, defining a personalized deck before the real game. Deckbuilding is a challenge that involves a big and rugged search space, with different and unpredictable behaviour after simple card changes and even hidden information. In this paper, we explore the possibility of automated deckbuilding: a genetic algorithm is applied to the task, with the evaluation delegated to a game simulator that tests every potential deck against a varied and representative range of human-made decks. In these preliminary experiments, the approach has proven able to create quite effective decks, a promising result that proves that, even in this challenging environment, evolutionary algorithms can find good solutions.",https://ieeexplore.ieee.org/document/7860426/,2016 IEEE Conference on Computational Intelligence and Games (CIG),20-23 Sept. 2016,ieeexplore
10.1109/ICDM.2005.59,Example-based robust outlier detection in high dimensional datasets,IEEE,Conferences,"Detecting outliers is an important problem. Most of its applications typically possess high dimensional datasets. In high dimensional space, the data becomes sparse which implies that every object can be regarded as an outlier from the point of view of similarity. Furthermore, a fundamental issue is that the notion of which objects are outliers typically varies between users, problem domains or, even, datasets. In this paper, we present a novel robust solution which detects high dimensional outliers based on user examples and tolerates incorrect inputs. It studies the behavior of projections of such a few examples, to discover further objects that are outstanding in the projection where many examples are outlying. Our experiments on both real and synthetic datasets demonstrate the ability of the proposed method to detect outliers corresponding to the user examples.",https://ieeexplore.ieee.org/document/1565793/,Fifth IEEE International Conference on Data Mining (ICDM'05),27-30 Nov. 2005,ieeexplore
10.1109/IJCNN.1991.155235,Experimental demonstration of large-scale holographic optical neural network,IEEE,Conferences,"The authors present an experimental implementation of the N/sup 4/ holographic optical neural network and demonstrations of the potential capabilities of large-scale and high-speed pattern recognition applications of this architecture. The extremely large space-bandwidth-product of the holographic materials makes it possible to construct large-scale static holographic optical neural networks (HONNs). A 1024-neuron HONN has been experimentally demonstrated for pattern recognition operations. A 32*32 optical neural network was trained to recognize different airplanes and to give the correct names at the output detector array. The computational time for training the four pairs of patterns took only a few minutes on a 386 microcomputer. It is estimated that with proper selection, over 200 pairs of heteroassociative patterns can be stored in the neural network of 1024 neurons for target recognition using the interpattern association (IPA) neural network model (T. Lu et al., 1990). By recording more holograms on a holographic plate, a 128*128 neural network and eventually a 256*256 neural network can be constructed, thus a high-speed real-time recognition of an enemy's aircraft can be performed.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/155235/,IJCNN-91-Seattle International Joint Conference on Neural Networks,8-12 July 1991,ieeexplore
10.1109/IWCSN.2017.8276520,Expert recommendation in oss projects based on knowledge embedding,IEEE,Conferences,"Modern Open Source Software (OSS) projects depend on the globally-distributed and synchronized software development. The online collaboration promotes more and more developers to join in OSS projects, while on the other hand, integrating new developers with teams is challenging and pivotal to the success of a project. In this paper, we propose a novel expert recommendation method, based on knowledge embedding, that realizes real-time recommendation for working developers. To capture structural information of source files in call graph, we use node2vec algorithm to convert file entities within projects into knowledge mappings within low-dimensional space, based on which we further propose four features to capture the work status and social relationship of developers. We then design a recommender system using random forest method to recommend appropriate experts for the developers. Experiments on 20 Apache OSS projects show that, compared with the baseline methods, our approach behaves significantly better in terms of a series of performance metrics.",https://ieeexplore.ieee.org/document/8276520/,2017 International Workshop on Complex Systems and Networks (IWCSN),8-10 Dec. 2017,ieeexplore
10.1109/CVPR.2019.00965,Explore-Exploit Graph Traversal for Image Retrieval,IEEE,Conferences,"We propose a novel graph-based approach for image retrieval. Given a nearest neighbor graph produced by the global descriptor model, we traverse it by alternating between exploit and explore steps. The exploit step maximally utilizes the immediate neighborhood of each vertex, while the explore step traverses vertices that are farther away in the descriptor space. By combining these two steps we can better capture the underlying image manifold, and successfully retrieve relevant images that are visually dissimilar to the query. Our traversal algorithm is conceptually simple, has few tunable parameters and can be implemented with basic data structures. This enables fast real-time inference for previously unseen queries with minimal memory overhead. Despite relative simplicity, we show highly competitive results on multiple public benchmarks, including the largest image retrieval dataset that is currently publicly available. Full code for this work is available here: https://github.com/layer6ai-labs/egt.",https://ieeexplore.ieee.org/document/8954266/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore
10.1109/SIEDS49339.2020.9106581,"Explorer51  Indoor Mapping, Discovery, and Navigation for an Autonomous Mobile Robot",IEEE,Conferences,"The nexus of robotics, autonomous systems, and artificial intelligence (AI) has the potential to change the nature of human guided exploration of indoor and outdoor spaces. Such autonomous mobile robots can be incorporated into a variety of applications, ranging from logistics and maintenance, to intelligence gathering, surveillance, and reconnaissance (ISR). One such example is that of a tele-operator using the robot to generate a map of the inside of a building while discovering and tagging the objects of interest. During this process, the tele-operator can also assign an area for the robot to navigate autonomously or return to a previously marked area/object of interest. Search and rescue and ISR abilities could be immensely improved with such capabilities. The goal of this research is to prototype and demonstrate the above autonomous capabilities in a mobile ground robot called Explorer51. Objectives include: (i) enabling an operator to drive the robot non-line of sight to explore a space by incorporating a first-person view (FPV) system to stream data from the robot to the base station; (ii) implementing automatic collision avoidance to prevent the operator from running the robot into obstacles; (iii) creating and saving 2D and 3D maps of the space in real time by using a 2D laser scanner, tracking, and depth/RGB cameras; (iv) locating and tagging objects of interest as waypoints within the map; (v) autonomously navigate within the map to reach a chosen waypoint.To accomplish these goals, we are using the AION Robotics R1 Unmanned Ground Vehicle (UGV) rover as the platform for Explorer51 to demonstrate the autonomous features. The rover runs the Robot Operating System (ROS) onboard an NVIDIA Jetson TX2 board, connected to a Pixhawk controller. Sensors include a 2D scanning LiDAR, depth camera, tracking camera, and an IMU. Using existing ROS packages such as Cartographer and TEB planner, we plan to implement ROS nodes for accomplishing these tasks. We plan to extend the mapping ability of the rover using Visual Inertial Odometry (VIO) using the cameras. In addition, we will explore the implementation of additional features such as autonomous target identification, waypoint marking, collision avoidance, and iterative trajectory optimization. The project will culminate in a series of demonstrations to showcase the autonomous navigation, and tele-operation abilities of the robot. Success will be evaluated based on ease of use by the tele-operator, collision avoidance ability, autonomous waypoint navigation accuracy, and robust map creation at high driving speeds.",https://ieeexplore.ieee.org/document/9106581/,2020 Systems and Information Engineering Design Symposium (SIEDS),24-24 April 2020,ieeexplore
10.1109/ICTAI.2009.24,Exploring Software Quality Classification with a Wrapper-Based Feature Ranking Technique,IEEE,Conferences,"Feature selection is a process of selecting a subset of relevant features for building learning models. It is an important activity for data preprocessing used in software quality modeling and other data mining problems. Feature selection algorithms can be divided into two categories, feature ranking and feature subset selection. Feature ranking orders the features by a criterion and a user selects some of the features that are appropriate for a given scenario. Feature subset selection techniques search the space of possible feature subsets and evaluate the suitability of each. This paper investigates performance metric based feature ranking techniques by using the multilayer perceptron (MLP) learner with nine different performance metrics. The nine performance metrics include overall accuracy (OA), default F-measure (DFM), default geometric mean (DGM), default arithmetic mean (DAM), area under ROC (AUC), area under PRC (PRC), best F-measure (BFM), best geometric mean (BGM) and best arithmetic mean (BAM). The goal of the paper is to study the effect of the different performance metrics on the feature ranking results, which in turn influences the classification performance. We assessed the performance of the classification models constructed on those selected feature subsets through an empirical case study that was carried out on six data sets of real-world software systems. The results demonstrate that AUC, PRC, BFM, BGM and BAM as performance metrics for feature ranking outperformed the other performance metrics, OA, DFM, DGMand DAM, unanimously across all the data sets and therefore are recommended based on this study. In addition, the performances of the classification models were maintained or even improved when over 85 percent of the features were eliminated from the original data sets.",https://ieeexplore.ieee.org/document/5364717/,2009 21st IEEE International Conference on Tools with Artificial Intelligence,2-4 Nov. 2009,ieeexplore
10.1109/IMIS.2012.137,Exploring Urban Dynamics Based on Pervasive Sensing: Correlation Analysis of Traffic Density and Air Quality,IEEE,Conferences,"Modern cities, with large population and complicated infrastructures, are complex entities with non-linear and dynamic properties that challenge the city management. Therefore, as the first step towards the goal of thorough understanding of the phenomena, pervasive urban sensing have become a cornerstone of future smart city that enhance the interplay between the cyber space and the physical world. We introduce a taxi-based pervasive urban sensing system and its key algorithm, aiming at the quantitative study of the correlation between human activities and environmental changes. Our contributions are twofold. First, we propose an urban crowd-sourcing framework that take automobiles as participatory mobile agents to the sensing tasks, and implemented a prototype in Beijing. Second, we design a Spatial-Temporal Manifold Learning (STML) algorithm to analyse the correlation between physical processes. Based on noisy and partially labelled dataset that are collected by pervasive urban sensor networks, we evaluate STML's performance by analysing correlation between the traffic density and the air quality. The results show great potential of STML for future urban sensing applications.",https://ieeexplore.ieee.org/document/6296824/,2012 Sixth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing,4-6 July 2012,ieeexplore
10.1109/BIOCAS.2010.5709623,Exploring olfactory sensory networks: Simulations and hardware emulation,IEEE,Conferences,"Olfactory stimuli are represented in a highdimensional space by neural networks of the olfactory system. A great deal of research in olfaction has focused on this representation within the first processing stage, the olfactory bulb (vertebrates) or antennal lobe (insects) glomeruli. In particular the mapping of chemical stimuli onto olfactory glomeruli and the relation of this mapping to perceptual qualities have been investigated. While a number of studies have illustrated the importance of inhibitory networks within the olfactory bulb or the antennal lobe for the shaping and processing of olfactory information, it is not clear how exactly these inhibitory networks are organized to provide filtering and contrast enhancement capabilities. In this work the aim is to study the topology of the proposed networks by using software simulations and hardware implementation. While we can study the dependence of the activity on each parameter of the theoretical models with the simulations, it is important to understand whether the models can be used in robotic applications for real-time odor recognition. We present the results of a linear simulation, a spiking simulation with I&amp;F neurons and a real-time hardware emulation using neuromorphic VLSI chips. We used an input data set of neurophysiological recordings from olfactory receptive neurons of insects, especially Drosophila.",https://ieeexplore.ieee.org/document/5709623/,2010 Biomedical Circuits and Systems Conference (BioCAS),3-5 Nov. 2010,ieeexplore
10.1109/ICRA48506.2021.9561040,Extendable Navigation Network based Reinforcement Learning for Indoor Robot Exploration,IEEE,Conferences,This paper presents a navigation network based deep reinforcement learning framework for autonomous indoor robot exploration. The presented method features a pattern cognitive non-myopic exploration strategy that can better reflect universal preferences for structure. We propose the Extendable Navigation Network (ENN) to encode the partially observed high-dimensional indoor Euclidean space to a sparse graph representation. The robots motion is generated by a learned Q-network whose input is the ENN. The proposed framework is applied to a robot equipped with a 2D LIDAR sensor in the GAZEBO simulation where floor plans of real buildings are implemented. The experiments demonstrate the efficiency of the framework in terms of exploration time.,https://ieeexplore.ieee.org/document/9561040/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore
10.1109/ICMLA51294.2020.00011,Extending SpArSe: Automatic Gesture Recognition Architectures for Embedded Devices,IEEE,Conferences,"Neural Architecture Search (NAS), which allows for automatically developing neural networks, has been mostly devoted to performance on a single metric, usually accuracy. New approaches have added more objectives, such as model size, in order to find networks suitable for resource-constrained platforms. SpArSe [1] is a multi-objective Bayesian optimization framework for automatically developing image classification convolutional neural networks (CNNs) for micro-controller units (MCUs). In this work, we first implement SpArSe and modify it to reduce search time, obtaining similar results regarding accuracy, model size, and maximum working memory but in less optimization time. Moreover, we extend the search space to include recurrent neural networks (RNNs) and add an inference latency objective for time-constrained tasks. Finally, we test our implementation in a gesture recognition task obtaining better results than previous manually tuned approaches for size and performance metrics, which validates the approach and its utility.",https://ieeexplore.ieee.org/document/9356164/,2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA),14-17 Dec. 2020,ieeexplore
10.1109/SOCA.2009.5410453,Extending service model to build an effective service composition framework for cyber-physical systems,IEEE,Conferences,"Cyber-Physical Systems (CPSs) are combinations of physical entities controlled by software systems to accomplish specified tasks under stringent real-time and physical entity constraints. As more and more physical entities are equipped with embedded computers, they are becoming more and more intelligent. However, the problem of effectively composing the services provided by cyber and physical entities to achieve specific goals still remains a challenge. Traditional service-oriented models and composition techniques are insufficient for CPS. In this paper, we present a novel Physical-Entity (PE) service-oriented model to address the problem, including the concepts of PE-ontology and PE-SOA specifications. Based on the model, we develop a two-level compositional reasoning approach, which divides the process into abstract and physical levels to expedite the composition process. With the assistance of the PE-ontology and PE-SOA, abstract level reasoning is performed by hiding the physical level details. This separation greatly reduces the search space for both levels through a divide-and-conquer technique. The model and the composition approach are illustrated using a simplified emergency response case study system.",https://ieeexplore.ieee.org/document/5410453/,2009 IEEE International Conference on Service-Oriented Computing and Applications (SOCA),14-15 Jan. 2009,ieeexplore
10.1109/SIEDS52267.2021.9483745,Extensions and Application of the Robust Shared Response Model to Electroencephalography Data for Enhancing Brain-Computer Interface Systems,IEEE,Conferences,"Brain Computer Interfaces (BCI) decode electroencephalography (EEG) data collected from the human brain to predict subsequent behavior. While this technology has promising applications, successfully implementing a model is challenging. The typical BCI control application requires many hours of training data from each individual to make predictions of intended activity specific to that individual. Moreover, there are individual differences in the organization of brain activity and low signal-to-noise ratios in noninvasive measurement techniques such as EEG. There is a fundamental bias-variance trade-off between developing a single model for all human brains vs. an individual model for each specific human brain. The Robust Shared Response Model (RSRM) attempts to resolve this tradeoff by leveraging both the homogeneity and heterogeneity of brain signals across people. RSRM extracts components that are common and shared across individual brains, while simultaneously learning unique representations between individual brains. By learning a latent shared space in conjunction with subject-specific representations, RSRM tends to result in better predictive performance on functional magnetic resonance imaging (fMRI) data relative to other common dimension reduction techniques. To our knowledge, we are the first research team attempting to expand the domain of RSRM by applying this technique to controlled experimental EEG data in a BCI setting. Using the openly available Motor Movement/ Imagery dataset, the decoding accuracy of RSRM exceeded models whose input was reduced by Principal Component Analysis (PCA), Independent Component Analysis (ICA), and subject-specific PCA. The results of our experiments suggest that RSRM can recover distributed latent brain signals and improve decoding accuracy of BCI tasks when dimension reduction is implemented as a feature engineering step. Future directions of this work include augmenting state-of-the art BCI with efficient reduced representations extracted by RSRM. This could enhance the utility of BCI technology in the real world. Furthermore, RSRM could have wide-ranging applications across other machine-learning applications that require classification of naturalistic data using reduced representations.",https://ieeexplore.ieee.org/document/9483745/,2021 Systems and Information Engineering Design Symposium (SIEDS),29-30 April 2021,ieeexplore
10.1109/IJCNN.1999.833481,Extracting knowledge from temporal clusters for real-time clustering,IEEE,Conferences,"Real-time anomaly detection is an increasing need as the desire for data analysis grows along with the amount of data stored on computers. Increasing the number of features used as input and the number of clusters created also increases the time needed to classify new inputs using clustering methods. Without intelligent reduction of the search space, the need to cluster in real-time forces the user to either limit the number of input features or reduce the number of total clusters created. However, the intelligent reduction of the search space can result in real-time anomaly detection without the loss of accuracy in classification. A hybrid neural network technique that allows for clustering of sequences, the extraction of regular grammars, and a method for using the grammars for real-time classification has been developed and is presented in this paper.",https://ieeexplore.ieee.org/document/833481/,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),10-16 July 1999,ieeexplore
10.1109/ICIT.2011.5754405,"FPGA based intelligent condition monitoring of induction motors: Detection, diagnosis, and prognosis",IEEE,Conferences,"This paper presents three intelligent methods for condition monitoring of induction motors in real-time. A structured neural network has been designed to prognosis of instantaneous faults. The inputs of neural network are the standard deviation and mean of feature signal obtained by Hilbert transform of one phase current signal. The stator related faults have been diagnosed by designing fuzzy logic. The amplitudes of three phase currents have been given to fuzzy logic and the condition of stator has been diagnosed. The last algorithm uses the phase space of the Hilbert transform of one phase current and detects broken rotor bar faults using negative selection algorithm. The contribution of the algorithm is the development of synchronously worked algorithms, optimized for low-cost Field Programmable Gate Array (FPGA) implementation. Extensive simulations were applied to test the performance of each algorithm, and the results show that the algorithms give high accuracy in detecting whether a possible fault has occurred in any component of the motor. The average detection time of the faults is above within 2 milliseconds or less.",https://ieeexplore.ieee.org/document/5754405/,2011 IEEE International Conference on Industrial Technology,14-16 March 2011,ieeexplore
10.1109/SCORED.2009.5443323,FPGA implementation of a space-time trellis decoder,IEEE,Conferences,This paper describes the real-time implementation of a space-time trellis encoder and decoder using the Xilinx Virtex-4FX12 FPGA. The code uses a generator matrix designed for 4-state space-time trellis (STT) that uses Quadrature Phase Shift Keying (QPSK) modulation scheme. The decoding process was done using Maximum Likelihood (ML) through the Viterbi Algorithm. The results show that the STT decoder can successfully decipher the encoded symbols from the STT encoder and that it can fully recover the original data in the absence of noise. The data rate of the decoder was 6.25 Msymbols/s. It was shown that 14% of the logic elements in Virtex 4 FPGA were used in implementing an encoder-decoder system.,https://ieeexplore.ieee.org/document/5443323/,2009 IEEE Student Conference on Research and Development (SCOReD),16-18 Nov. 2009,ieeexplore
10.1109/ICoICT.2015.7231422,Face image-based gender recognition using complex-valued neural network,IEEE,Conferences,"Automatic gender recognition is an emerging problem in computer visions. An accurate gender recognition system can be used to reduce the search space in face recognition system for about half. However, since there is no definitive features of sexual dimorphism on human face that can be applied to all kind of face shapes from any race and age, it needs more studies to optimize the recognition system. Thus, this research investigates the implementation of complex-valued neural network as a classifier to recognize human gender which is based on face image. The experiment is also aimed to study the comparison between complex-valued and real-valued neural network. The methods proposed in this paper include image processing, feature extraction, and classification. After the face image processed by using local binary pattern to accentuate face texture and gradient filter to define face outline, the features of the face image is extracted by using histogram of oriented gradient. Then, the dimension of the resulted vectors is reduced by using principal component analysis. The final feature vectors is then used in neural network training and neural network testing processes. This paper shows investigation results in implementing the methods for some measurement parameters. The accuracy level of real-valued neural network system is a bit lower than the average accuracy level of complex-valued one, with 78.2% and 80.2% average accuracy rate, respectively. The results also show that complex-valued neural network system can achieve convergency rate four times faster than the real-valued neural network, which implies on the training process time.",https://ieeexplore.ieee.org/document/7231422/,2015 3rd International Conference on Information and Communication Technology (ICoICT),27-29 May 2015,ieeexplore
10.1109/SSCI47803.2020.9308337,Fake-Face Image Classification using Improved Quantum-Inspired Evolutionary-based Feature Selection Method,IEEE,Conferences,"Deep learning models have been quite successful in discriminating synthesized or edited fake-face images. However, in the case of small training data, transfer-learning is rather preferable. This is a complex process for high dimensional feature space due to the curse of dimensionality. To mitigate the same, this paper proposes a new feature selection method for the classification of manually created fake-face images. In the proposed method, a pre-trained deep learning model is used to extract features of an image. Next, an optimal feature subset is selected from the extracted features through an improved quantum-inspired evolutionary algorithm. Lastly, the elicited features are considered to perform the classification. Experiments are conducted on a publicly available manually created fake-face image dataset, namely Real and Fake Face Detection by Yonsei University. The performance of the proposed method is compared with two methods in terms of classification accuracy and the number of selected features. The experimental comparisons exhibit that the proposed method achieves promising results among the considered methods.",https://ieeexplore.ieee.org/document/9308337/,2020 IEEE Symposium Series on Computational Intelligence (SSCI),1-4 Dec. 2020,ieeexplore
10.1109/ITSC48978.2021.9564641,Fast Collision Prediction for Autonomous Vehicles using a Stochastic Dynamics Model,IEEE,Conferences,"Autonomous Vehicles (AVs) have the potential to save millions of lives by reducing traffic deaths and accidents. However, despite recent advances, AVs have not met safety standard expectations for a variety of reasons, key among them being the difficulty in certifying AV safety. The development of model-based methods is essential for achieving more explainable tools that provide better safety assurances, in contrast to popular data-dependent end-to-end learning methods. This paper introduces a model-based collision prediction method that uses discretized Gaussian processes for future vehicle position estimation. It can incorporate road layout information, statistical agent dynamics, and be coupled with any trajectory prediction module. The discretization of the space together with a single Normal random variable for each vehicle trajectory allows fast and efficient computation for real-time deployment and computational intensive applications, such as simulation and training of Deep Learning and Reinforcement Learning models. The method can be applied to various scenarios by the adjustment of the model parameters that control dynamics uncertainty. Two scenarios extracted from real data are used as case studies.",https://ieeexplore.ieee.org/document/9564641/,2021 IEEE International Intelligent Transportation Systems Conference (ITSC),19-22 Sept. 2021,ieeexplore
10.1109/INFOCOM41043.2020.9155456,Fast Network Alignment via Graph Meta-Learning,IEEE,Conferences,"Network alignment (NA) - i.e., linking entities from different networks (also known as identity linkage) - is a fundamental problem in many application domains. Recent advances in deep graph learning have inspired various auspicious approaches for tackling the NA problem. However, most of the existing works suffer from efficiency and generalization, due to complexities and redundant computations.We approach the NA from a different perspective, tackling it via meta-learning in a semi-supervised manner, and propose an effective and efficient approach called Meta-NA - a novel, conceptually simple, flexible, and general framework. Specifically, we reformulate NA as a one-shot classification problem and address it with a graph meta-learning framework. Meta-NA exploits the meta-metric learning from known anchor nodes to obtain latent priors for linking unknown anchor nodes. It contains multiple sub-networks corresponding to multiple graphs, learning a unified metric space, where one can easily link entities across different graphs. In addition to the performance lift, Meta-NA greatly improves the anchor linking generalization, significantly reduces the computational overheads, and is easily extendable to multi-network alignment scenarios. Extensive experiments conducted on three real-world datasets demonstrate the superiority of Meta-NA over several state-of-the-art baselines in terms of both alignment accuracy and learning efficiency.",https://ieeexplore.ieee.org/document/9155456/,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications,6-9 July 2020,ieeexplore
10.1109/ICMLC.2004.1380499,Fast training of SVM for color-based image segmentation,IEEE,Conferences,"A novel method based on support vector machine (SVM) for color image segmentation is presented. Considering image segmentation is a two-class problem, a two-class SVM to classify pixels in color space is proposed. In order to speed up training and optimize parameters of SVM, the color quantization and sample selection approaches are presented to reduce the size of training set and to make the reduced set separable. Training with reduced and separable dataset, minimizing the number of support vectors is regard as an estimation criterion of generalization performance to kernel parameter optimization. The classifier can be trained on-line and implemented in real-time. The new algorithm has been used to color-based image segmentation, and it brings robust performance in practice.",https://ieeexplore.ieee.org/document/1380499/,Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826),26-29 Aug. 2004,ieeexplore
10.1109/WiSPNET51692.2021.9419438,Faster Training of Edge-attention Aided 6D Pose Estimation Model using Transfer Learning and Small Customized Dataset,IEEE,Conferences,"Computer Vision is the field of machine learning that deals with computers gaining knowledge from digital images/videos and performing tasks that human vision is capable of doing. It is widely used in the field of robotics for designing guidance systems where objects in the robot's field of view are identified and located. This research work is an application-specific project enabling a half-humanoid to find the 6D pose and bounding boxes of its hand and other objects within its field of view. We add an edge prediction head to the NOCS (Normalised Object Coordinate Space) model, which predicts the edges of each object from the predicted instance maps. An additional edge-agreement-loss found from the predicted edges is added to the total loss. This increases the attention to the edges and improves the accuracy of prediction of the instance masks. This edge-attention aided model is initialized with pre-trained weights of CAMERA and REAL dataset using transfer learning. The backbone layers of the model are frozen and the head layers alone are trained using a synthetic dataset (HAND dataset) we created using a software called blender. The model gives promising results when tested with objects kept in varying lighting conditions and at different distances from the camera. The use of transfer learning in models as large as the NOCS model allows us to train the model for a new class by only training the top few layers with a significantly small dataset.",https://ieeexplore.ieee.org/document/9419438/,"2021 Sixth International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)",25-27 March 2021,ieeexplore
10.1109/TFSA.1994.467254,Fault detection and identification using real-time wavelet feature extraction,IEEE,Conferences,"Development of real-time fault detection and identification technologies will allow a migration, in the respective theater of operation, from expensive scheduled based maintenance to the more efficient, less costly alternative of condition based maintenance. This paper presents successful initial results applying continuous wavelet transforms coupled with conventional neural networks to the development of a real-time fault detection and classification systems. The approach taken results in a general methodology which is shown to work equally well on fault-seeded, helicopter gear-box data and operational data from Navy shipboard pumps. The family of wavelet basis functions are specifically engineered to allow for real-time implementation. The wavelet basis functions have a time-scale decomposition mathematically inspired from biological systems and provides a clustering in feature space which allows for the development of simplified neural network classifiers. Application to various classes of fault data (helicopter and shipboard pump data) resulted in perfect detection, no false alarms with only modest deferral rates.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/467254/,Proceedings of IEEE-SP International Symposium on Time- Frequency and Time-Scale Analysis,25-28 Oct. 1994,ieeexplore
10.1109/CEC.2011.5949774,Fault identification with binary adaptive fireflies in parallel and distributed systems,IEEE,Conferences,"The efficient identification of hardware and software faults in parallel and distributed systems still remains a serious challenge in today's most prolific decentralized environments. System-level fault diagnosis is concerned with the detection of all faulty nodes in a set of interconnected units. This is accomplished by thoroughly examining the collection of outcomes of all tests carried out by the nodes under a particular test model. Such task has non-polynomial complexity and can be posed as a combinatorial optimization problem, whose optimal solution has been sought through bio-inspired methods like genetic algorithms, ant colonies and artificial immune systems. In this paper, we employ a swarm of artificial fireflies to quickly and reliably navigate across the search space of all feasible sets of faulty units under the invalidation and comparison test models. Our approach uses a binary encoding of the potential solutions (fireflies), an adaptive light absorption coefficient to accelerate the search and problem-specific knowledge to handle infeasible solutions. The empirical analysis confirms that the proposed algorithm outperforms existing techniques in terms of convergence speed and memory requirements, thus becoming a viable approach for real-time fault diagnosis in large-size systems.",https://ieeexplore.ieee.org/document/5949774/,2011 IEEE Congress of Evolutionary Computation (CEC),5-8 June 2011,ieeexplore
10.1109/ICMLA.2007.10,Feature Extraction from Microarray Expression Data by Integration of Semantic Knowledge,IEEE,Conferences,"Microarray techniques give biologists first peek into the molecular states of living tissues. Previous studies have proven that it is feasible to build sample classifiers using the gene expressional profiles. To build an effective sample classifier, dimension reduction process is necessary since classic pattern recognition algorithms do not work well in high dimensional space. In this paper, we present a novel feature extraction algorithm based on the concept of virtual genes by integrating microarray expression data sets with domain knowledge embedded in gene ontology (GO) annotations. We define semantic similarity to measure the functional associations between two genes using the annotation on each GO term. We then identify the groups of genes, called virtual genes, that potentially interact with each other for a biological function. The correlation in gene expression levels of virtual genes can be used to build a sample classifier. For a colon cancer data set, the integration of microarray expression data with GO annotations significantly improves the accuracy of sample classification by more than 10%.",https://ieeexplore.ieee.org/document/4457296/,Sixth International Conference on Machine Learning and Applications (ICMLA 2007),13-15 Dec. 2007,ieeexplore
10.1109/CyberSA52016.2021.9478199,Feature Vulnerability and Robustness Assessment against Adversarial Machine Learning Attacks,IEEE,Conferences,"Whilst machine learning has been widely adopted for various domains, it is important to consider how such techniques may be susceptible to malicious users through adversarial attacks. Given a trained classifier, a malicious attack may attempt to craft a data observation whereby the data features purposefully trigger the classifier to yield incorrect responses. This has been observed in various image classification tasks, including falsifying road sign detection and facial recognition, which could have severe consequences in real-world deployment. In this work, we investigate how these attacks could impact on network traffic analysis, and how a system could perform misclassification of common network attacks such as DDoS attacks. Using the CICIDS2017 data, we examine how vulnerable the data features used for intrusion detection are to perturbation attacks using FGSM adversarial examples. As a result, our method provides a defensive approach for assessing feature robustness that seeks to balance between classification accuracy whilst minimising the attack surface of the feature space.",https://ieeexplore.ieee.org/document/9478199/,"2021 International Conference on Cyber Situational Awareness, Data Analytics and Assessment (CyberSA)",14-18 June 2021,ieeexplore
10.1109/IJCNN.2013.6707020,Feature construction approach for email categorization based on term space partition,IEEE,Conferences,"This paper proposes a novel feature construction approach based on term space partition (TSP) aiming to establish a mechanism to make terms play more sufficient and rational roles in email categorization. Dominant terms and general terms are separated by performing a vertical partition of the original term space with respect to feature selection metrics, while spam terms and ham terms are separated by a transverse partition with respect to class tendency. Strategies for constructing discriminative features, named term ratio and term density, are designed on corresponding subspaces. Motivation and principle of the TSP approach is presented in detail, as well as the implementation. Experiments are conducted on five benchmark corpora using cross-validation to evaluate the proposed TSP approach. Comprehensive experimental results suggest that the TSP approach far outperforms the traditional and most widely used feature construction approach in spam filtering, which is named bag-of-words, in both performance and efficiency. In comparison with the heuristic and state-of-the-art approaches, namely CFC and LC, the proposed TSP approach shows obvious advantage in terms of accuracy and <sub>1</sub> measure, as well as high precision, which is warmly welcomed in real spam filtering. Furthermore, the TSP approach performs quite similar with CFC in efficiency of processing incoming emails, while much faster than LC. In addition, it is shown that the TSP approach cooperates well with both unsupervised and supervised feature selection metrics, which endows it with flexible capability in the real world.",https://ieeexplore.ieee.org/document/6707020/,The 2013 International Joint Conference on Neural Networks (IJCNN),4-9 Aug. 2013,ieeexplore
10.1109/ICMLC.2013.6890416,Feature extraction based on discriminant analysis with penalty constraint for hyperspectral image classification,IEEE,Conferences,"The main issue of hyperspectral image data (HSI) is its high dimensionality which conducts challenge in high dimensional data analysis community. Popular linear approaches can work effectively when the data is unimodal Gaussian class conditional independently distributions. Yet, they usually fail when applied to HSI data since the distribution of HSI data is usually unknown in reality. Locality preserving projection (LPP) addresses this problem approvingly, where the neighborhood information can be preserved in the reduced space. Based on typical behaviors of Fisher's linear discriminant analysis (LDA), a novel discriminant analysis framework under penalty constraint(PFDA), which extends the ideas of LDA and LPP, is developed in this paper. Benefiting from different construction of affinity matrix, our method can also preserve the locality embedding information effectively in the reduced space. Four types of PFDA are analyzed in this paper and the efficiency and effectiveness of proposed methods under penalty framework are demonstrated by both synthesis data and real hyperspectral remote sensing image data set.",https://ieeexplore.ieee.org/document/6890416/,2013 International Conference on Machine Learning and Cybernetics,14-17 July 2013,ieeexplore
10.1109/ICTAI50040.2020.00019,Feature-Aware Attentive Variational Auto-Encoder for Top-N Recommendation,IEEE,Conferences,"Personalized recommendation has become increasingly pervasive due to its great commercial value in business. Deep neural networks can automatically exvacate the behavior patterns from the historical interaction records, which has achieved excellent results in related tasks. Among them, the variational auto-encoders have been shown to be superior for learning to rank and recommendation on massive data. However, prior work neglects the association between user behavior and side information, which affects the quality of recommendation services to some extent. In this paper, we propose a feature-aware attentive variational auto-encoder for top-N recommendation. The attention mechanism is utilized to capture the relationship between user's representation and side information through a sub network, balancing the fusion weight of attributes in the main network. In addition, this method tries to construct combination of features in the high-dimensional embedding space, helping mining the promotion of side information at a finer scale. Experiments conducted on real-world datasets demonstrate the effectiveness over the state-of-art methods.",https://ieeexplore.ieee.org/document/9288316/,2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI),9-11 Nov. 2020,ieeexplore
10.1109/IJCNN.2019.8852314,Feature-Dependent Graph Convolutional Autoencoders with Adversarial Training Methods,IEEE,Conferences,"Graphs are ubiquitous for describing and modeling complicated data structures, and graph embedding is an effective solution to learn a mapping from a graph to a low-dimensional vector space while preserving relevant graph characteristics. Most existing graph embedding approaches either embed the topological information and node features separately or learn one regularized embedding with both sources of information, however, they mostly overlook the interdependency between structural characteristics and node features when processing the graph data into the models. Moreover, existing methods only reconstruct the structural characteristics, which are unable to fully leverage the interaction between the topology and the features associated with its nodes during the encoding-decoding procedure. To address the problem, we propose a framework using autoencoder for graph embedding (GED) and its variational version (VEGD). The contribution of our work is two-fold: 1) the proposed frameworks exploit a feature-dependent graph matrix (FGM) to naturally merge the structural characteristics and node features according to their interdependency; and 2) the Graph Convolutional Network (GCN) decoder of the proposed framework reconstructs both structural characteristics and node features, which naturally possesses the interaction between these two sources of information while learning the embedding. We conducted the experiments on three real-world graph datasets such as Cora, Citeseer and PubMed to evaluate our framework and algorithms, and the results outperform baseline methods on both link prediction and graph clustering tasks.",https://ieeexplore.ieee.org/document/8852314/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore
10.1109/ISIE45552.2021.9576369,Feature-based Egocentric Grasp Pose Classification for Expanding Human-Object Interactions,IEEE,Conferences,"This paper presents a framework for classifying human hand pose, especially in grasping object intuitively. First, we propose a system based on the stereo infra-red image as a sensor that can produce hand coordinates in 3-dimensional space. We use egocentric vision because it can get uniform and natural data with only a single sensor module. Second, we transformed the position to get the angle information for each joint on the finger. Third, we designed an intelligent system based on Multi-Layer Perceptron (MLP) to process angular data to obtain classification results according to the Cutkosky grasp taxonomy. Finally, we compared the results on several similar objects and evaluated their classification accuracy. In the validation phase, the results yielded an accuracy of 16 grasp pose classification is 89,60%. In real-time testing, the results yielded an accuracy of 81.93%. This result shows feature-based learning can reduce the complexity and training time of the MLP. Furthermore, a small amount of training data is sufficient for the training and implementation.",https://ieeexplore.ieee.org/document/9576369/,2021 IEEE 30th International Symposium on Industrial Electronics (ISIE),20-23 June 2021,ieeexplore
10.1109/HICSS.2003.1173646,Fifteen years of GSS in the field: a comparison across time and national boundaries,IEEE,Conferences,"It has been over two decades since group support systems (GSS) emerged on the information technology (IT) scene. GSS have now been commercialized and are present in an increasing number of domestic and international contexts but only lightly studied in real organizational settings. A criticism of studies has been that many of the organizations involved had a vested interest in the outcome that extended beyond that which would normally occur in a typical organization. An additional challenge has been made with respect to the generalizability of field study results across corporate and national cultures. This paper compares and contrasts findings from International Business Machines (IBM) and Boeing Aircraft Corporation in the US with those from two European companies: Nationale-Nederlanden (NN), the largest insurance firm in the Netherlands and European Aeronautic Defense and Space company, Military division (EADS-M). Attention is given to aspects of efficiency, effectiveness, and user satisfaction as well as group dynamics.",https://ieeexplore.ieee.org/document/1173646/,"36th Annual Hawaii International Conference on System Sciences, 2003. Proceedings of the",6-9 Jan. 2003,ieeexplore
10.1109/ICTAI.2011.39,Filtering by ULP Maximum,IEEE,Conferences,"Constraint solving over floating-point numbers is an emerging topic that found interesting applications in software analysis and testing. Even for IEEE-754 compliant programs, correct reasoning over floating-point computations is challenging and requires dedicated constraint solving approaches to be developed. Recent advances indicate that numerical properties of floating-point numbers can be used to efficiently prune the search space. In this paper, we reformulate the Marre and Michel property over floating-point addition/subtraction constraint to ease its implementation in real-world floating-point constraint solvers. We also generalize the property to the case of multiplication/division in order to benefit from its improvements in more cases.",https://ieeexplore.ieee.org/document/6103329/,2011 IEEE 23rd International Conference on Tools with Artificial Intelligence,7-9 Nov. 2011,ieeexplore
10.1109/ICTAI.2009.60,FlockStream: A Bio-Inspired Algorithm for Clustering Evolving Data Streams,IEEE,Conferences,"Existing density-based data stream clustering algorithms use a two-phase scheme approach consisting of an online phase, in which raw data is processed to gather summary statistics, and an offline phase that generates the clusters by using the summary data. In this paper we propose a data stream clustering method based on a multi-agent system that uses a decentralized bottom-up self-organizing strategy to group similar data points. Data points are associated with agents and deployed onto a 2D space, to work simultaneously by applying a heuristic strategy based on a bio-inspired model, known as flocking model. Agents move onto the space for a fixed time and, when they encounter other agents into a predefined visibility range, they can decide to form a flock if they are similar. Flocks can join to form swarms of similar groups. This strategy allows to merge the two phases of density-based approaches and thus to avoid the offline cluster computation, since a swarm represents a cluster. Experimental results show the capability of the bio-inspired approach to obtain very good results on real and synthetic data sets.",https://ieeexplore.ieee.org/document/5364942/,2009 21st IEEE International Conference on Tools with Artificial Intelligence,2-4 Nov. 2009,ieeexplore
10.1109/SYSCON.2019.8836965,Fog Computing for Real-Time Accident Identification and Related Congestion Control,IEEE,Conferences,"This paper focuses on developing (i) a benchmark application for Real-Time traffic incidence identification and related traffic management, using Real-Time congestion-aware navigation of smart vehicles (Edge nodes) with video feeds, (ii) an image database for Deep Learning used for recognition and classification of traffic incidences such as accidents and congestions, (iii) the System Level Software (or Middleware) required for Distributed Computing in such a heterogeneous Real-Time constrained system with Rapid Mobility - today's Internet-of-Everything (IoE), and (iv) a hardware prototype of the distributed computing and storage infrastructure. The video bandwidth requirement of 10-100 GigaBytes of data per minute per vehicular camera makes it a Big Data problem. With millions of smart vehicles projected to be deployed within the next 5 years, BigData from a single vehicle, multiplied with the large number of vehicles, presents a Big-Squared-Data computing space which will easily overwhelm any Cloud infrastructure with its Real-Time or near Real-Time demands. Hence the need for a Fog tier between the Edge nodes and the Cloud to bring distributed computation (servers) and storage closer to the Edge nodes. Such a Fog consists of multiple Fog instances, each one of which services cells or Virtual Clusters of Edge nodes. Results show that Fog-Cloud computing framework outperforms a Cloud-only platform by 79.7% reduction in total latency or response time.",https://ieeexplore.ieee.org/document/8836965/,2019 IEEE International Systems Conference (SysCon),8-11 April 2019,ieeexplore
10.1109/AIVR.2018.00046,FoodChangeLens: CNN-Based Food Transformation on HoloLens,IEEE,Conferences,"In this demonstration, we implemented food category transformation in mixed reality using both image generation and HoloLens. Our system overlays transformed food images to food objects in the AR space, so that it is possible to convert in consideration of real shape. This system has the potential to make meals more enjoyable. In this work, we use the Conditional CycleGAN trained with a large-scale food image data collected from the Twitter Stream for food category transformation which can transform among ten kinds of foods mutually keeping the shape of a given food. We show the virtual meal experience which is food category transformation among ten kinds of typical Japanese foods: ramen noodle, curry rice, fried rice, beef rice bowl, chilled noodle, spaghetti with meat source, white rice, eel bowl, and fried noodle. Note that additional results including demo videos can be see at https://negi111111.github.io/FoodChangeLensProjectHP/.",https://ieeexplore.ieee.org/document/8613665/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore
10.1109/R10-HTC.2018.8629835,Foody - Smart Restaurant Management and Ordering System,IEEE,Conferences,"Customers play a vital role in the contemporary food industry when determining the quality of the restaurant and its food. Restaurants give considerable attention to customers feedback about their service, since the reputation of the business depends on it. Key factors of evaluating customer satisfaction are, being able to deliver the services effectively to lessen the time of consumption, as well as maintaining a high quality of service. In most cases of selecting a prominent restaurant, customers focus on their choice of favorite food in addition to available seating and space options. Long waiting times and serving the wrong order is a common mistake that happens in every restaurant that eventually leads to customer dissatisfaction. Objectives of this online application Foody is to address these deficiencies and provide efficient and accurate services to the customer, by providing unique menus to each customer considering their taste. This concept is implemented as a mobile application using latest IT concepts such as Business Intelligence, Data Mining, Predictive Analysis and Artificial Intelligence. This includes graphics and 3D modeling that provide existent physical information related to food such as colors, sizes and further user can view the ingredients of the meal as well as the available tables. In addition, the app shows the real-time map to the restaurant. Current table reservation status is indicated by the color change of the table. Unique food recommendation and its order for each customer is generated by analyzing their social media information and the system notifies the customer the wait time by calculating it. Preparation of food and allocation is done subjectively. The expected outcome of the research is to develop a fully automated restaurant management system with the mentioned features as well as to avoid confusions between orders, provide better view of food and allow the customer to choose the menu according to their taste in a minimum time.",https://ieeexplore.ieee.org/document/8629835/,2018 IEEE Region 10 Humanitarian Technology Conference (R10-HTC),6-8 Dec. 2018,ieeexplore
10.1109/HUMANOIDS.2014.7041373,Footstep planning on uneven terrain with mixed-integer convex optimization,IEEE,Conferences,"We present a new method for planning footstep placements for a robot walking on uneven terrain with obstacles, using a mixed-integer quadratically-constrained quadratic program (MIQCQP). Our approach is unique in that it handles obstacle avoidance, kinematic reachability, and rotation of footstep placements, which typically have required non-convex constraints, in a single mixed-integer optimization that can be efficiently solved to its global optimum. Reachability is enforced through a convex inner approximation of the reachable space for the robot's feet. Rotation of the footsteps is handled by a piecewise linear approximation of sine and cosine, designed to ensure that the approximation never overestimates the robot's reachability. Obstacle avoidance is ensured by decomposing the environment into convex regions of obstacle-free configuration space and assigning each footstep to one such safe region. We demonstrate this technique in simple 2D and 3D environments and with real environments sensed by a humanoid robot. We also discuss computational performance of the algorithm, which is currently capable of planning short sequences of a few steps in under one second or longer sequences of 10-30 footsteps in tens of seconds to minutes on common laptop computer hardware. Our implementation is available within the Drake MATLAB toolbox [1].",https://ieeexplore.ieee.org/document/7041373/,2014 IEEE-RAS International Conference on Humanoid Robots,18-20 Nov. 2014,ieeexplore
10.1109/ROMAN.2008.4600635,Force skill training with a hybrid trainer model,IEEE,Conferences,"In this work, we present novel VR training strategies that incorporate a hybrid trainer model to train force. For modeling the trainer skill, weighted K-means algorithm in parameter space with LS optimization is implemented. The efficiency of the training strategies is verified via user tests in frame of a bone drilling training application. An objective evaluation method based on n dimensional Euclidean distances is introduced to assess user tests results. It is shown that the proposed strategies improve the student skill and accelerate force learning.",https://ieeexplore.ieee.org/document/4600635/,RO-MAN 2008 - The 17th IEEE International Symposium on Robot and Human Interactive Communication,1-3 Aug. 2008,ieeexplore
10.1109/GLOCOM.2017.8255034,FreeCount: Device-Free Crowd Counting with Commodity WiFi,IEEE,Conferences,"In the era of Internet of Things, crowd counting, which estimates the number of people within a region, becomes the underpinning for many emerging applications, such as occupancy estimation in smart building and queuing management and product placement in shopping center. Existing vision based crowd counting schemes require favorable lighting conditions and also raise privacy concerns. RF based approaches rely on specialized sensors and require users to carry RF devices. Thus, an accurate, reliable and non-intrusive crowd counting scheme is still desired. In this paper, we propose FreeCount, a device-free crowd counting scheme that is able to precisely estimate the number of people within a region using only commodity WiFi routers. To this end, the channel state information (CSI) data in PHY layer is obtained directly by upgrading the router's software. We propose an information theory based feature selection scheme to select the most representative features that are sensitive to human motion. To build a classifier that is robust to temporal and environmental disparities, we adopt transfer kernel learning, which minimizes the difference between the source and target distributions in the reproducing kernel Hilbert space, is adopted to process the real-time CSI feature data. Experiments were conducted in moderate sized rooms and the results demonstrated that FreeCount is able to accurately estimate the number of people with 96% crowd counting accuracy consistently over temporal and environmental variation.",https://ieeexplore.ieee.org/document/8255034/,GLOBECOM 2017 - 2017 IEEE Global Communications Conference,4-8 Dec. 2017,ieeexplore
10.1109/AICI.2009.369,Frequent Items Mining on Data Stream Based on Time Fading Factor,IEEE,Conferences,"Most of the existing algorithms for mining frequent items on data stream do not emphasis the importance of the recent data items. We present an algorithm using a fading factor to detect the data items with frequency counts exceeding a user-specified threshold. Our algorithm can detect -approximate frequent data items on data stream using O(<sup>-1</sup>) memory space and the processing time for each data item and a query is O(<sup>-1</sup>). Experimental results on several artificial datasets and real datasets show our algorithm has higher precision, requires less memory and consumes less computation time than other similar methods.",https://ieeexplore.ieee.org/document/5376330/,2009 International Conference on Artificial Intelligence and Computational Intelligence,7-8 Nov. 2009,ieeexplore
10.1109/IROS40897.2019.8967568,From Pixels to Buildings: End-to-end Probabilistic Deep Networks for Large-scale Semantic Mapping,IEEE,Conferences,"We introduce TopoNets, end-to-end probabilistic deep networks for modeling semantic maps with structure reflecting the topology of large-scale environments. TopoNets build a unified deep network spanning multiple levels of abstraction and spatial scales, from pixels representing geometry of local places to high-level descriptions of semantics of buildings. To this end, TopoNets leverage complex spatial relations expressed in terms of arbitrary, dynamic graphs. We demonstrate how TopoNets can be used to perform end-to-end semantic mapping from partial sensory observations and noisy topological relations discovered by a robot exploring large-scale office spaces. Thanks to their probabilistic nature and generative properties, TopoNets extend the problem of semantic mapping beyond classification. We show that TopoNets successfully perform uncertain reasoning about yet unexplored space and detect novel and incongruent environment configurations unknown to the robot. Our implementation of TopoNets achieves real-time, tractable and exact inference, which makes these new deep models a promising, practical solution to mobile robot spatial understanding at scale.",https://ieeexplore.ieee.org/document/8967568/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore
10.1109/IROS.2016.7759701,From indoor GIS maps to path planning for autonomous wheelchairs,IEEE,Conferences,"This work focuses on how to compute trajectories for an autonomous wheelchair based on indoor GIS maps, in particular on IndoorGML maps, which set the standard in this context. Good wheelchair trajectories are safe and comfortable for the user and the people sharing the space with him, turn gently, are high legible, and smooth (at least G<sup>2</sup> continuos). We derive a navigation graph from a given IndoorGML map. We define and solve an optimization problem to find the desired path: given a succession of cells to traverse, the path corresponds to the best composite Bzier trajectory for the wheelchair. We discuss a related multi-objective path planning problem. Experimental results and an implementation on real robots show the planner performance.",https://ieeexplore.ieee.org/document/7759701/,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),9-14 Oct. 2016,ieeexplore
10.1109/ICCI-CC.2014.6921432,From information revolution to intelligence revolution: Big data science vs. intelligence science,IEEE,Conferences,"The hierarchy of human knowledge is categorized at the levels of data, information, knowledge, and intelligence. For instance, given an AND-gate with 1,000-input pins, it may be described very much differently at various levels of perceptions in the knowledge hierarchy. At the data level on the bottom, it represents a 2<sup>1,000</sup> state space, known as `big data' in recent terms, which appears to be a big issue in engineering. However, at the information level, it just represents 1,000 bit information that is equivalent to the numbers of inputs. Further, at the knowledge level, it expresses only two rules that if all inputs are one, the output is one; and if any input is zero, the output is zero. Ultimately, at the intelligence level, it is simply an instance of the logical model of an AND-gate with arbitrary inputs. This problem reveals that human intelligence and wisdom are an extremely efficient and a fast convergent induction mechanism for knowledge and wisdom elicitation and abstraction where data are merely factual materials and arbitrary instances in the almost infinite state space of the real world. Although data and information processing have been relatively well studied, the nature, theories, and suitable mathematics underpinning knowledge and intelligence are yet to be systematically studied in cognitive informatics and cognitive computing. This will leads to a new era of human intelligence revolution following the industrial, computational, and information revolutions. This is also in accordance with the driving force of the hierarchical human needs from low-level material requirements to high-level ones such as knowledge, wisdom, and intelligence. The trend to the emerging intelligent revolution is to meet the ultimate human needs. The basic approach to intelligent revolution is to invent and embody cognitive computers, cognitive robots, and cognitive systems that extend human memory capacity, learning ability, wisdom, and creativity. Via intelligence revolution, an interconnected cognitive intelligent Internet will enable ordinary people to access highly intelligent systems created based on the latest development of human knowledge and wisdom. Highly professional systems may help people to solve typical everyday problems. Towards these objectives, the latest advances in abstract intelligence and intelligence science investigated in cognitive informatics and cognitive computing are well positioned at the center of intelligence revolution. A wide range of applications of cognitive computers have been developing in ICIC [http://www.ucalgary.ca/icic/] such as, inter alia, cognitive computers, cognitive robots, cognitive learning engines, cognitive Internet, cognitive agents, cognitive search engines, cognitive translators, cognitive control systems, cognitive communications systems, and cognitive automobiles.",https://ieeexplore.ieee.org/document/6921432/,2014 IEEE 13th International Conference on Cognitive Informatics and Cognitive Computing,18-20 Aug. 2014,ieeexplore
10.1109/IJCNN.2001.939561,Fuzzy clusters identification in the feature space using neural networks,IEEE,Conferences,Deals with the development of ARTMAP-like neural networks to analyze feature space for classification purposes. The proposed tool provides information about the value of membership functions of the unknown input vector to each class of interest. The designed ARTMAP-like system is called MF-ARTMAP based on the fact that membership functions are calculated. The functions shape is predefined as Gaussian with adaptation of mean value and variance in each feature space dimension during the training procedure. The parallel version of this approach is designed and implemented too. The parallel MF ARTMAP have some advantages over regular MF ARTMAP. The usefulness of this approach is presented on the benchmark classification problems e.g. circle in the square and spiral and on real-world data from satellite images over Slovakia. Classification accuracy is calculated using the contingency tables approach on actual and predicted classes of interest.,https://ieeexplore.ieee.org/document/939561/,IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222),15-19 July 2001,ieeexplore
10.1109/ISMA.2009.5164850,Fuzzy motion-based control for a bi-steerable mobile robot navigation,IEEE,Conferences,"This paper presents an implementation of a Fuzzy Motion Controller (FMC) to endow the mobile robot Robucar with capability to achieve the action behavior allowing smooth motion generation with intelligence in real-time. The robot state space (velocity and distances) is modeled in discrete intervals leading to linguistic variables. The fuzzy motion control rules are derived and used in a fuzzy inference mechanism to give the final control command to the robot actuators. Simulation and experimental results show FMC capabilities in generating smooth motions, illustrating then its adaptivity and intelligence.",https://ieeexplore.ieee.org/document/5164850/,2009 6th International Symposium on Mechatronics and its Applications,23-26 March 2009,ieeexplore
10.1109/FUZZ-IEEE.2016.7737911,Fuzzyfication of principle component analysis for data dimensionalty reduction,IEEE,Conferences,"Principal component analysis (PCA) extracts small uncorrelated data from original high dimensional data space and is widely used for data analysis. The methodology of classical PCA is based on orthogonal projection defined in convex vector space. Thus, a norm between two projected vectors is unavoidably smaller than the norm between any two objects before implementation of PCA. Due to this, in some cases when the PCA cannot capture the data structure, its implementation does not necessarily confirm the real similarity of data in the higher dimensional space, making the results unacceptable. In order to avoid this problem for the purposes of high dimensional data clustering, we propose new Fuzzy PCA (FPCA) algorithm. The novelty is in the extracted similarity structures of objects in high-dimensional space and dissimilarities between objects based on their cluster structure and dimension when the algorithm is implemented in conjunction with known fuzzy clustering algorithms as FCM, GK or GG algorithms. This is done by using the fuzzy membership functions and modification of the classical PCA approach by considering the similarity structures during the construction of projections in smaller dimensional space. The effectiveness of the proposed algorithm is tested on several benchmark data sets. We also evaluate the clustering efficiency by implementing validation measures.",https://ieeexplore.ieee.org/document/7737911/,2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),24-29 July 2016,ieeexplore
10.1109/INDIN.2009.5195905,GPS and sonar based area mapping and navigation by mobile robots,IEEE,Conferences,"In this paper, we have presented a GPS and sonar based area mapping and navigation scheme for a mobile robot. A mapping is achieved between the GPS space and the world coordinates of the mobile robot which enables us to generate direct motion commands for it. This mapping enables the robot to navigate among different GPS locations within the mapped area. The GPS data is extracted online to get the latitude and longitude information of a particular location. In the training phase, a 2-D axis transformation is used to relate local robot frame with the robot world coordinates and then the actual world coordinates are mapped from the GPS data using a RBFN (radial basis function network) based Neural Network. In the second phase, direct GPS data is used to get the mapping into the world coordinates of mobile robot using the trained network and the motion commands are generated accordingly. The physical placement of sonar devices, their ranging limits and beam opening angles are considered during navigation for possible collision detection and obstacle avoidance. This scheme is successfully implemented in real time with Pioneer mobile robot from ActivMedia Robotics and GPS receiver. The scheme is also tested in the simulation to justify its application in the real world.",https://ieeexplore.ieee.org/document/5195905/,2009 7th IEEE International Conference on Industrial Informatics,23-26 June 2009,ieeexplore
10.1109/INES52918.2021.9512921,Game Feature Validation of a Real-Time Game Space with an eXtended Classifier System,IEEE,Conferences,"The objective of this paper is the proposal of a new approach for the game feature validation of a game space with the eXtended Classifier System (XCS) algorithm. For initial ""proof-of-concept"" evaluation we used the game space of the Tic-Tac-Toe game, which was placed in a context of real-time characteristics. Evaluation was done with the XCS algorithm without pre-processing internal knowledge programmed (online mode). Evaluation data results were acquired under real-time constraints. No-loss-strategy was implemented for the game, with various scenarios tested to find out if the algorithm has the capability of finding invalid game feature design flaws. Results indicated that the XCS algorithm is able to deliver stable validation of game feature testing results, can be a powerful tool and, therefore, worthy of further research in the Gaming domain. Confirmation of this core concept testing with this type of algorithm (and similar ones) is necessary, since it paves the way for further research in more complex game environments, where the dimension size, the number of aspects, game states and game actions rises intensely.",https://ieeexplore.ieee.org/document/9512921/,2021 IEEE 25th International Conference on Intelligent Engineering Systems (INES),7-9 July 2021,ieeexplore
10.1109/VRAIS.1996.490517,Gaze-directed adaptive rendering for interacting with virtual space,IEEE,Conferences,"This paper presents a new method of rendering for interaction with 3D virtual space with the use of gaze detection devices. In this method, hierarchical geometric models of graphic objects are constructed prior to the rendering process. The rendering process first calculates the visual acuity, which represents the importance of a graphic object for a human operator, from the gaze position of the operator. Second, the process selects a level from the set of hierachical geometric models depending on the value of visual acuity. That is, a simpler level of detail is selected where the visual acuity is lower, and a more complicated level is used where it is higher. Then, the selected graphic models are rendered on the display. This paper examines three visual characteristics to calculate the visual acuity: the central/peripheral vision, the kinetic vision, and the fusional vision. The actual implementation and our testbed system are described, as well as the details of the visual acuity model.",https://ieeexplore.ieee.org/document/490517/,Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium,30 March-3 April 1996,ieeexplore
10.1109/BioCAS.2015.7348397,General-purpose LSM learning processor architecture and theoretically guided design space exploration,IEEE,Conferences,"This paper presents a general-purpose liquid state machine based neuromorphic learning processor with integrated training and recognition for real world pattern recognition problems. The proposed architecture consists of a generic preprocessor and one or multiple task processors. The pre-processor, or the reservoir, consists of a recurrent spiking neural network with fixed synaptic weights. Task processors are light weight and comprise a set of readout spiking neurons with plastic weights, which are tuned by a biologically plausible supervised learning rule. Importantly, we leverage the unique computational structure of the reservoir for highly efficient implementation of multiple tasks on the same learning processor. A novel theoretical measure of computational power, which is strongly correlated with the true learning performance, is proposed to facilitate fast design space exploration of the recurrent reservoir. We demonstrate the application of our processor architecture by mapping four recognition tasks onto a reconfigurable FPGA processor platform.",https://ieeexplore.ieee.org/document/7348397/,2015 IEEE Biomedical Circuits and Systems Conference (BioCAS),22-24 Oct. 2015,ieeexplore
10.1109/VRAIS.1996.490518,Generalized surface and volume decimation for unstructured tessellated domains,IEEE,Conferences,"A general algorithm for decimating unstructured discretized data sets is presented. The discretized space may be a planar triangulation, a general 3D surface triangulation, or a 3D tetrahedrization. The decimation algorithm enforces Dirichlet boundary conditions, uses only existing vertices, and assumes manifold geometry. Local dynamic vertex removal is performed without history information, while preserving the initial topology and boundary geometry. The research focuses on how to remove a vertex from an existing unstructured n-dimensional tessellation, not on the formulation of decimation criteria. Criteria for removing a candidate vertex may be based on geometric properties or any scalar governing function specific to the application.",https://ieeexplore.ieee.org/document/490518/,Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium,30 March-3 April 1996,ieeexplore
10.1109/PRDC50213.2020.00018,Generative Deep Learning for Internet of Things Network Traffic Generation,IEEE,Conferences,"The rapid development of the Internet of Things (IoT) has prompted a recent interest into realistic IoT network traffic generation. Security practitioners need IoT network traffic data to develop and assess network-based intrusion detection systems (NIDS). Emulating realistic network traffic will avoid the costly physical deployment of thousands of smart devices. From an attacker's perspective, generating network traffic that mimics the legitimate behavior of a device can be useful to evade NIDS. As network traffic data consist of sequences of packets, the problem is similar to the generation of sequences of categorical data, like word by word text generation. Many solutions in the field of natural language processing have been proposed to adapt a Generative Adversarial Network (GAN) to generate sequences of categorical data. In this paper, we propose to combine an autoencoder with a GAN to generate sequences of packet sizes that correspond to bidirectional flows. First, the autoencoder is trained to learn a latent representation of the real sequences of packet sizes. A GAN is then trained on the latent space, to learn to generate latent vectors that can be decoded into realistic sequences. For experimental purposes, bidirectional flows produced by a Google Home Mini are used, and the autoencoder is combined with a Wassertein GAN. Comparison of different network characteristics shows that our proposed approach is able to generate sequences of packet sizes that behave closely to real bidirectional flows. We also show that the synthetic bidirectional flows are close enough to the real ones that they can fool anomaly detectors into labeling them as legitimate.",https://ieeexplore.ieee.org/document/9320384/,2020 IEEE 25th Pacific Rim International Symposium on Dependable Computing (PRDC),1-4 Dec. 2020,ieeexplore
10.1109/SAMOS.2016.7818338,Genesys: Automatically generating representative training sets for predictive benchmarking,IEEE,Conferences,"Fast and efficient design space exploration is a critical requirement for designing computer systems, however, the growing complexity of hardware/software systems and significantly long run-times of detailed simulators often makes it challenging. Machine learning (ML) models have been proposed as popular alternatives that enable fast exploratory studies. The accuracy of any ML model depends heavily on the re-presentativeness of applications used for training the predictive models. While prior studies have used standard benchmarks or hand-tuned micro-benchmarks to train their predictive models, in this paper, we argue that it is often sub-optimal because of their limited coverage of the program state-space and their inability to be representative of the larger suite of real-world applications. In order to overcome challenges in creating representative training sets, we propose Genesys, an automatic workload generation methodology and framework, which builds upon key low-level application characteristics and enables systematic generation of applications covering a broad range of program behavior state-space without increasing the training time. We demonstrate that the automatically generated training sets improve upon the state-space coverage provided by applications from popular benchmarking suites like SPEC-CPU2006, MiBench, Media Bench, TPC-H by over llx and improve the accuracy of two machine learning based power and performance prediction systems by over 2.5x and 3.6x respectively.",https://ieeexplore.ieee.org/document/7818338/,"2016 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS)",17-21 July 2016,ieeexplore
10.1109/ICNN.1993.298714,Genetic algorithm based input selection for a neural network function approximator with applications to SSME health monitoring,IEEE,Conferences,"A genetic algorithm is used to select the inputs to a neural network function approximator. In the application considered, modeling critical parameters of the Space Shuttle main engine, the functional relationships among measured parameters if unknown and complex and the number of possible input parameters is quite large. Due to the optimization and space searching capabilities of genetic algorithms, they are employed to systematize the input selection process. The results suggest that the genetic algorithm can generate parameter lists of high quality without the explicit use of problem domain knowledge. Suggestions for improving the performance of the input selection process are provided.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/298714/,IEEE International Conference on Neural Networks,28 March-1 April 1993,ieeexplore
10.1109/KES.2000.885785,Genetic algorithm driven clustering for toxicity prediction,IEEE,Conferences,The pace of technological advancement in today's society has generated an enormous demand for methods facilitating the intelligent testing of the toxicity of new chemicals. Until now it was common use to make predictions based on 'real' tests. Recent investigations support the general assumption that macroscopic properties like toxicity and ecotoxicity strongly depend on microscopic features and the structure of the molecule. The authors have developed a computationally intelligent method for supervised training of regression systems. Their method selects those features needed to predict and calculate the toxicity. The proposed methodology relies on supervised clustering with genetic algorithms and local learning. Different molecular descriptors are computed and the correlation behaviour of the different descriptors in the descriptor space is studied.,https://ieeexplore.ieee.org/document/885785/,KES'2000. Fourth International Conference on Knowledge-Based Intelligent Engineering Systems and Allied Technologies. Proceedings (Cat. No.00TH8516),30 Aug.-1 Sept. 2000,ieeexplore
10.1109/ISSPIT.2011.6151580,Genetic algorithm implementation of multi-user detection in SDMA-OFDM systems,IEEE,Conferences,Number of supported users in the orthogonal frequency division multiplexing (OFDM) systems can be increased considerably using powerful multi-user detector (MUD) combined with space division multiple access (SDMA) techniques. This paper presents the results of implementing MUD in SDMA-OFDM systems based on an advanced genetic-algorithm (GA) optimization tool. The hardware implementation is performed using Field Programmable Gate array (FPGA) devices which allow the real time performance of the proposed tool. Results show that the GA scheme enhances the performance and provides BER near to that attained using maximum likelihood (ML) detector at considerably lower computation complexity. Investigation of the GA population size is presented and FPGA implementation is described based on the shared memory approach.,https://ieeexplore.ieee.org/document/6151580/,2011 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT),14-17 Dec. 2011,ieeexplore
10.1109/COGANN.1992.273945,Genetic sparse distributed memory,IEEE,Conferences,"Kanerva's 'sparse distributed memory' (SDM) is a type of self-organizing neural network which is able to extract a statistical summary from large volumes of data as it is being processed online. Genetic algorithms have been used to optimize the 'location address space' which corresponds to the mapping from the input layer to the hidden units in the neural network implementation of the sparse distributed memory. If treated as a global optimization problem, the genetic algorithm will attempt to optimize the sparse distributed memory so as to extract a single best statistical predictor. However, the real objective is to obtain not just a single global optimum, but to extract information about as many local optima as possible, since each local optimum in this particular definition of the search space represents a different and distinct data pattern that correlates with some output in which we may be interested. The implementation details of a genetic sparse distributed memory as well as modified algorithm designed to deal better with multiple data patterns are presented.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/273945/,[Proceedings] COGANN-92: International Workshop on Combinations of Genetic Algorithms and Neural Networks,6-6 June 1992,ieeexplore
10.1109/ISMAR-Adjunct51615.2020.00030,Ginput: a tool for fast hi-fi prototyping of gestural interactions in virtual reality,IEEE,Conferences,"Gestural interfaces in virtual reality (VR) expand the design space for user interaction, allowing spatial metaphors with the environment and more natural and immersive experiences. Typically, machine learning approaches recognize gestures with models that rely on a large number of samples for the training phase, which is an obstacle for rapidly prototyping gestural interactions. In this paper, we propose a solution designed for hi-fi prototyping of gestures within a virtual reality environment through a high-level Domain-Specific Language (DSL), as a subset of the natural language. The proposed DSL allows non-programmer users to intuitively describe a broad domain of poses and connect them for compound gestures. Our DSL was designed to be general enough for multiple input classes, such as body tracking, hand tracking, head movement, motion controllers, and buttons. We tested our solution for wands with VR designers and developers. Results showed that the tool gives non-programmers the ability to prototype gestures with ease and refine its recognition within a few minutes.",https://ieeexplore.ieee.org/document/9288432/,2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),9-13 Nov. 2020,ieeexplore
10.1109/FUZZY.2006.1681714,Granular Auto-regressive Moving Average (grARMA) Model for Predicting a Distribution from Other Distributions. Real-world Applications,IEEE,Conferences,"Industrial products are often output in batches at discrete times. A batch gives rise to distributions of measurements, one distribution per variable of interest. There may be a need for modeling to predict a distribution from other distributions. This work represents a distribution by a fuzzy interval number (FIN) interpreted as an information granule. Based on vector lattice theory it is shown that the lattice F<sub>+</sub> of positive FINs is a cone in a non-linearly tunable, metric, linear space. In conclusion, a multivariate granular autoregressive moving average (grARMA) model is proposed for predicting a distribution from other distributions. A recursive neural network implementation is shown. We report preliminary results regarding two real-world applications including, first, industrial fertilizer production and, second, environmental pollution monitoring along seashore in northern Greece. The far-reaching potential of novel techniques is discussed.",https://ieeexplore.ieee.org/document/1681714/,2006 IEEE International Conference on Fuzzy Systems,16-21 July 2006,ieeexplore
10.1109/ICME51207.2021.9428448,Graph Attention Neural Network for Image Restoration,IEEE,Conferences,"Self-similarity underpins modern non-local attention mechanism, which has been verified to be an effective prior for image restoration. However, most existing non-local attention restorers are implemented based on pixels, which tend to be biased due to image degeneration. Furthermore, most non-local methods for image restoration are restricted to construct fully-connected correlations in a regular Euclidean space so that all features within the search region have to participate in the feature aggregation process, no matter how similar the key feature is to the query feature. To rectify these weaknesses, in this paper, we propose a novel graph attention network for image restoration, dubbed GATIR, which establishes the non-local attention based on feature patches and utilizes the graph convolution to perform feature aggregation selectively in a non-Euclidean space. Experimental results demonstrate that our GATIR can achieve state-of-the-art performance on synthetic image denoising, real image denoising, image demosaicing, and compression artifact reduction tasks.",https://ieeexplore.ieee.org/document/9428448/,2021 IEEE International Conference on Multimedia and Expo (ICME),5-9 July 2021,ieeexplore
10.1109/IEEECONF51394.2020.9443359,Graph Diffusion Kernel LMS using Random Fourier Features,IEEE,Conferences,"This work introduces kernel adaptive graph filters that operate in the reproducing kernel Hilbert space. We propose a centralized graph kernel least mean squares (GKLMS) approach for identifying the nonlinear graph filters. The principles of coherence-check and random Fourier features (RFF) are used to reduce the dictionary size. Additionally, we leverage the graph structure to derive the graph diffusion KLMS (GDKLMS). The proposed GDKLMS requires only single-hop communication during successive time instants, making it viable for real-time network-based applications. In the distributed implementation, usage of RFF avoids the requirement of a centralized pre-trained dictionary in the case of coherence-check. Finally, the performance of the proposed algorithms is demonstrated in modeling a nonlinear graph filter via numerical examples. The results show that centralized and distributed implementations effectively model the nonlinear graph filters, whereas the random-feature-based solutions are shown to outperform coherence-check based solutions.",https://ieeexplore.ieee.org/document/9443359/,"2020 54th Asilomar Conference on Signals, Systems, and Computers",1-4 Nov. 2020,ieeexplore
10.1109/ICSMC.2008.4811307,Graph-based semi-supervised learning with manifold preprocessing for image classification,IEEE,Conferences,"In real worlds applications, some former research papers have shown that manifold learning tries to discover the non-linear low-dimensional data manifold from a high-dimensional space. Many natural images and the face images are believed to be sampled from a manifold. In this paper, we try to investigate whether discovering such manifold can aid the semi-supervised learning algorithms. We propose a novel graph-based learning algorithm locality preserving graph-based semi-supervised method (LLGSM), which firstly use both labeled and unlabeled examples as unlabeled to discover the manifolds of the data samples and then use the projected labeled examples together with projected unlabeled ones to do classification. Experiments performed on some public image data sets have demonstrated the effectiveness of our algorithm.",https://ieeexplore.ieee.org/document/4811307/,"2008 IEEE International Conference on Systems, Man and Cybernetics",12-15 Oct. 2008,ieeexplore
10.1109/BigData.2018.8622381,Guiding the Data Learning Process with Physical Model in Air Pollution Inference,IEEE,Conferences,"The surveillance of air pollution is becoming a highly concerned issue for city residents and urban administrators. Fixed air quality stations as well as mobile gas sensors have been deployed for air quality monitoring but with sparse observations over the entire temporal-spatial space. Therefore, an inference algorithm is essential for comprehensive fine-grained air pollution sensing. Conventional physically-based models can hardly be applied to all the scenarios, while pure data-driven methods suffer from sampling bias and overfitting problems. This paper presents a hybrid algorithm for air pollution inference by guiding the data learning process with physical model. The quantitative combination of knowledge from observed dataset and a discretized convective-diffusion model is performed within a multi-task learning scheme. Evaluations show that, benefited from physical guidance, our hybrid method obtains higher extrapolation ability and more robustness, achieving the same performance with 1/8 sample amount and obtaining 31.9% less error in noisy synthesized environment. In a real-world deployment in Tianjin, our algorithm outperforms the pure data-driven model with 9.69% less inference error over a 9-day PM<sub>2.5</sub> data collection.",https://ieeexplore.ieee.org/document/8622381/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore
10.1109/ICAML54311.2021.00056,Gymnasium Simulation Design and Implementation Based on 3D Virtual Building,IEEE,Conferences,"Based on the three-dimensional virtual building-simulation design and implementation method of the gymnasium, when the building space is subjected to virtual reality and simulation design, based on the constructed building space coordinate system and scale, the position and parameters of the building are used to construct a mathematical model of the building space component., The mathematical model of each component is integrated to build a mathematical model of the overall building space. OpenGL virtual reality technology is used to expand the target building based on the mathematical model of the target building space. The target building is given materials and texture characteristics to obtain the ideal 3D virtual view of the building space. The 3D virtual view of the building space is 3D rendered and displayed. The vivid 3D virtual renderings of the building space are animated using animation design technology. The experimental results show that the proposed method has good point-line rendering and overall rendering effect, can obtain more realistic 3D virtual building based gym simulation design results, and has high interactivity and practicability.",https://ieeexplore.ieee.org/document/9712032/,2021 3rd International Conference on Applied Machine Learning (ICAML),23-25 July 2021,ieeexplore
10.1109/ASP-DAC47756.2020.9045442,HL-Pow: A Learning-Based Power Modeling Framework for High-Level Synthesis,IEEE,Conferences,"High-level synthesis (HLS) enables designers to customize hardware designs efficiently. However, it is still challenging to foresee the correlation between power consumption and HLS-based applications at an early design stage. To overcome this problem, we introduce HL-Pow, a power modeling framework for FPGA HLS based on state-of-the-art machine learning techniques. HL-Pow incorporates an automated feature construction flow to efficiently identify and extract features that exert a major influence on power consumption, simply based upon HLS results, and a modeling flow that can build an accurate and generic power model applicable to a variety of designs with HLS. By using HL-Pow, the power evaluation process for FPGA designs can be significantly expedited because the power inference of HL-Pow is established on HLS instead of the time-consuming register-transfer level (RTL) implementation flow. Experimental results demonstrate that HL-Pow can achieve accurate power modeling that is only 4.67% (24.02 mW) away from onboard power measurement. To further facilitate power-oriented optimizations, we describe a novel design space exploration (DSE) algorithm built on top of HL-Pow to trade off between latency and power consumption. This algorithm can reach a close approximation of the real Pareto frontier while only requiring running HLS flow for 20% of design points in the entire design space.",https://ieeexplore.ieee.org/document/9045442/,2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC),13-16 Jan. 2020,ieeexplore
10.1109/ICDMW.2009.69,HOCT: A Highly Scalable Algorithm for Training Linear CRF on Modern Hardware,IEEE,Conferences,"Conditional Random Fields (CRFs) are widely used in machine learning and natural language processing fields. A number of methods have been developed for CRF training. However, even with state-of-the-art algorithms, the training of CRF is still very time and space consuming. This make it infeasible to use CRFs in large-scale data analysis tasks. This paper proposes an efficient algorithm, HOCT, for CRF training on modern computer architectures. First, software prefetching techniques are utilized to hide cache miss latency. Second, we exploit SIMD to process data in parallel. Third, when dealing with large data sets, we let HOCT instead of operating system to manage swapping operations. Our experiments on various real data sets show that HOCT yields a fourfold speedup when the data can fit in memory, and over a 30-fold speedup when the memory requirement exceeds the physical memory.",https://ieeexplore.ieee.org/document/5360418/,2009 IEEE International Conference on Data Mining Workshops,6-6 Dec. 2009,ieeexplore
10.1109/IJCNN.2018.8489438,Half-precision Floating Point on Spiking Neural Networks Simulations in FPGA,IEEE,Conferences,"The use of half-precision floating-point numbers (hFP) in simulations of spiking neural networks (SNN) was investigated. The hFP format is used successfully in computer graphics and video games for storage and data transfer. The IEEE 754-2008 standard settles that arithmetic operations must occur at least on single-precision floating-point format (sFP). This means that it is necessary to convert hFP to sFP for arithmetical operations and reconvert the results to hFP before storing it. The influence of successive conversions when simulating SNN is the main concern of this article. Three methods were used to evaluate the impact of hFP on SNNs: (i) F-I curve, (ii) subthreshold regime, and (iii) the time for the next spike. We have tested the leaky integrate-and-fire and the Izhikevich's neuron model; both presented similar results. The data show that SNNs simulated with sFP present equivalent results when compared to the ones simulated with hFP with identical topology. Such results are important because hFP requires half of the memory space, simpler buses, and lower bandwidth for transferring data. We may infer they require lower clock frequency consequently lower power consumption. These are essential factors for real-time simulation of SNN on embedded electronics. The sFP to hFP conversion circuits, and vice versa, may be implemented using few logical blocks in a field-programmable gate arrays (FPGA) with no relevant Iatency. We conclude that data in the hFP format are suitable for SNNs synthesized in FPGAs, even though such implementations require conversion circuits.",https://ieeexplore.ieee.org/document/8489438/,2018 International Joint Conference on Neural Networks (IJCNN),8-13 July 2018,ieeexplore
10.1109/IJCNN.2006.247206,Handwritten Signature Authentication using Artificial Neural Networks,IEEE,Conferences,"The main goal of this paper is to describe our research and implementation of a handwritten signature authentication system based on artificial neural networks. In this system the authentication process occurs in the following way: firstly, the users' signatures are read using a pen tablet device and then stored; after that some adjustments in position and scale are accomplished; representative signature features are extracted; the input space dimensionality is reduced using principal component analysis; and finally, the users' signatures are classified as authentic or not, through the use of a neural network. Several experiments were accomplished using a 2440 real signatures database, and the obtained results were very satisfactory.",https://ieeexplore.ieee.org/document/1716797/,The 2006 IEEE International Joint Conference on Neural Network Proceedings,16-21 July 2006,ieeexplore
10.1109/ISSC.2018.8585376,Hello &amp; Goodbye: Conversation Boundary Identification Using Text Classification,IEEE,Conferences,"One of the main challenges in discourse analysis is the process of segmenting text into meaningful topic segments. While this problem has been studied over the past thirty years, previous topic segmentation studies ignore crucial elements of a conversation: an opening and closing remark. Our motivation to revisit this problem space is the rise of instant message usage. We consider the problem of topic segmentation as a machine learning classification one. Using both enterprise and open source datasets, we address the question as to whether a machine learning algorithm can be trained to identify salutations and valedictions within multi-party real-time chat conversations. Our results show that both Nave Bayes (NB) and Support Vector Machine (SVM) algorithms provide a reasonable degree of precision(mean F1 score: 0.58).",https://ieeexplore.ieee.org/document/8585376/,2018 29th Irish Signals and Systems Conference (ISSC),21-22 June 2018,ieeexplore
10.1109/IJCNN48605.2020.9206610,Heterogeneous Information Network Embedding with Convolutional Graph Attention Networks,IEEE,Conferences,"Heterogeneous Information Networks (HINs) are prevalent in our daily life, such as social networks and bibliography networks, which contain multiple types of nodes and links. Heterogeneous information network embedding is an effective HIN analysis method, it aims at projecting network elements into a lower-dimensional vector space for further machine learning related evaluations, such as node classification, node clustering, and so on. However, existing HIN embedding methods mainly focus on extracting the semantic-related information or close neighboring relations, while the high-level proximity of the network is also important but not preserved. To address the problem, in this paper we propose CGAT, a semi-supervised heterogeneous information network embedding method. We optimize the graph attention network by adding additional convolution layers, thereby we can extract multiple types of semantics and preserve high-level information in HIN embedding at the same time. Also, we utilize label information in HINs for semi-supervised training to better obtain the model parameters and HIN embeddings. Experimental results on real-world datasets demonstrate the effectiveness and efficiency of the proposed model.",https://ieeexplore.ieee.org/document/9206610/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore
10.1109/ICDM.2014.147,Heterogeneous Metric Learning with Content-Based Regularization for Software Artifact Retrieval,IEEE,Conferences,"The problem of software artifact retrieval has the goal to effectively locate software artifacts, such as a piece of source code, in a large code repository. This problem has been traditionally addressed through the textual query. In other words, information retrieval techniques will be exploited based on the textual similarity between queries and textual representation of software artifacts, which is generated by collecting words from comments, identifiers, and descriptions of programs. However, in addition to these semantic information, there are rich information embedded in source codes themselves. These source codes, if analyzed properly, can be a rich source for enhancing the efforts of software artifact retrieval. To this end, in this paper, we develop a feature extraction method on source codes. Specifically, this method can capture both the inherent information in the source codes and the semantic information hidden in the comments, descriptions, and identifiers of the source codes. Moreover, we design a heterogeneous metric learning approach, which allows to integrate code features and text features into the same latent semantic space. This, in turn, can help to measure the artifact similarity by exploiting the joint power of both code and text features. Finally, extensive experiments on real-world data show that the proposed method can help to improve the performances of software artifact retrieval with a significant margin.",https://ieeexplore.ieee.org/document/7023378/,2014 IEEE International Conference on Data Mining,14-17 Dec. 2014,ieeexplore
10.1109/ICTAI.2019.00146,Heterogeneous Transfer Clustering for Partial Co-occurrence Data,IEEE,Conferences,"Heterogeneous transfer clustering can translate knowledge from some related heterogeneous source domains to the target domain without any supervision. Existing works usually use a large amount of complete co-occurrence data to learn the projection functions mapping heterogeneous data to a common latent feature subspace. However, in many real applications, it is not practical to collect abundant co-occurrence data, while the available co-occurrence data are always incomplete. Another commonly encountered problem is that the complex structure of real heterogeneous data may result in substantial degeneration in clustering performance. To address these issues, we propose a heterogeneous transfer clustering method specifically designed for partial co-occurrence data (HTCPC). It is superior to the existing methods in three facets. First, HTCPC fully uses the partial co-occurrence data in both source and target domains to learn a latent space, maximally extracting useful knowledge for clustering from limited information. Second, it incorporates multi-layer hidden representations, accurately preserving the complex hierarchical structure of data. Third, it enforces approximately orthogonal constraint in representations, effectively characterizing the latent subspace with minimal redundancy. An efficient algorithm has been derived and implemented to realize the proposed HTCPC. A series of experiments on the real datasets have illustrated the advantage of the proposed approach compared with state-of-the-art methods.",https://ieeexplore.ieee.org/document/8995324/,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),4-6 Nov. 2019,ieeexplore
10.1109/ISCID.2017.172,Heterogeneous Transfer with Deep Latent Correlation for Sentiment Analysis,IEEE,Conferences,"Most traditional methods of image sentiment analysis focus on the design of visual features, and the usefulness of texts associated to the images have not been sufficiently investigated. Heterogeneous transfer learning has recently gained much attention as a new machine learning paradigm in which knowledge can be transferred from source domain feature space to target domain feature space. This paper proposes a novel approach that exploits deep latent correlation between visual and textual modalities. In our proposed method, we build a latent embedding space for symmetric heterogeneous feature transfer. The latent space is able to generate domain-specific and maximally correlative cross-domain features which are regarded as the semantic-intensive visual feature representation and used to train sentiment polarity classifiers. The results of experiments conducted on real-world data sets show that the proposed approach can achieve better sentiment classification accuracy by using multi-layer neural network to capture deeper internal relations.",https://ieeexplore.ieee.org/document/8283267/,2017 10th International Symposium on Computational Intelligence and Design (ISCID),9-10 Dec. 2017,ieeexplore
10.1109/ROBOT.1993.292013,Hidden Markov model approach to skill learning and its application in telerobotics,IEEE,Conferences,"The problem of how human skill can be represented as a parametric model using a hidden Markov (HMM), and how an HMM-based skill model can be used to learn human skill, is discussed. The HMM is feasible for characterizing two stochastic processes, measurable action and immeasurable mental states that are involved in the skill learning. Based on the most likely performance criterion, the best action sequence can be selected from previously measured action data by modeling the skill as an HMM. This selection process can be updated in real-time by feeding new action data and modifying HMM parameters. The implementation of the proposed method in a teleoperation-controlled space robot is discussed. The results demonstrate the feasibility of the method.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/292013/,[1993] Proceedings IEEE International Conference on Robotics and Automation,2-6 May 1993,ieeexplore
10.1109/SMC.2018.00176,Hierarchical Control Architecture Regulating Competition between Model-Based and Context-Dependent Model-Free Reinforcement Learning Strategies,IEEE,Conferences,"Recent evidence in neuroscience and psychology suggests that a single reinforcement learning (RL) algorithm only accounts for less than 60% of the variance of human choice behavior in an uncertain and dynamic environment, where the amount of uncertainty in state-action-state transitions drift over time. The prediction performance further decreases when the size of the state space increases. We proposed a hierarchical context-dependent RL control framework that dynamically exerted control weights on model-based (MB) and multiple model-free (MF) RL strategies associated with different task goals. To properly assess the validity of the proposed method, we considered a two-stage Markov decision task (MDT) in which the three different types of context changed over time. We trained 57 different RL control models on a Caltech MDT data set; then, we assessed their prediction performance using a Bayesian model comparison. This large-scale computer simulation analysis revealed that the model providing the most accurate prediction was the version that implemented the competition between the MB and multiple goal-dependent MF RL strategies. The present study demonstrates the applicability of the goal-driven RL control to a variety of real-world human-robot interaction scenarios.",https://ieeexplore.ieee.org/document/8616172/,"2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",7-10 Oct. 2018,ieeexplore
10.1109/ISIC.1992.225127,Hierarchical architecture for multi-sensor robot cell operation,IEEE,Conferences,"The authors describe a hierarchical architecture designed to carry out experiments in multisensor integration and sensor-based control in robotics. The hierarchical model is composed of three major levels: a high-level information processing and planning structure at the top, a logic-branching control structure at the intermediate level, and a real-time continuous sensory feedback loop at the bottom level. The two lower control structures are addressed. The principal submodules of the intermediate structure are described, with particular emphasis on communication issues and on the available software mechanisms for configuration and online maintenance of the robot cell. The architecture of the real-time continuous control structure that composes the bottom level is also described. The application of the adaptive self-tuning scheme in controlling position and force, specified in task-space coordinates, is discussed. Practical issues and experimental results are summarized.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/225127/,Proceedings of the 1992 IEEE International Symposium on Intelligent Control,11-13 Aug. 1992,ieeexplore
10.1109/BigData.2013.6691727,Hierarchical feature learning from sensorial data by spherical clustering,IEEE,Conferences,"Surveillance sensors are a major source of unstructured Big Data. Discovering and recognizing spatiotemporal objects (e.g., events) in such data is of paramount importance to the security and safety of facilities and individuals. What kind of computational model is necessary for discovering spatiotemporal objects at the level of abstraction they occur? Hierarchical invariant feature learning is the crux to the problems of discovery and recognition in Big Data. We present a multilayered convergent neural architecture for storing repeating spatially and temporally coincident patterns in data at multiple levels of abstraction. A node is the canonical computational unit consisting of neurons. Neurons are connected in and across nodes via bottom-up, top-down and lateral connections. The bottom-up weights are learned to encode a hierarchy of overcomplete and sparse feature dictionaries from space- and time-varying sensorial data by recursive layer-by-layer spherical clustering. The model scales to full-sized high-dimensional input data and also to an arbitrary number of layers thereby having the capability to capture features at any level of abstraction. The model is fully-learnable with only two manually tunable parameters. The model is generalpurpose (i.e., there is no modality-specific assumption for any spatiotemporal data), unsupervised and online. We use the learning algorithm, without any alteration, to learn meaningful feature hierarchies from images and videos which can then be used for recognition. Besides being online, operations in each layer of the model can be implemented in parallelized hardware, making it very efficient for real world Big Data applications.",https://ieeexplore.ieee.org/document/6691727/,2013 IEEE International Conference on Big Data,6-9 Oct. 2013,ieeexplore
10.1109/NAECON46414.2019.9057909,High Speed Approximate Cognitive Domain Ontologies for Constrained Asset Allocation based on Spiking Neurons,IEEE,Conferences,"Cognitive agents are typically utilized in autonomous systems for automated decision making. These systems interact at real time with their environment and are generally heavily power constrained. Thus, there is a strong need for a real time agent running on a low power platform. The agent examined is the Cognitively Enhanced Complex Event Processing (CECEP) architecture. This is an autonomous decision support tool that reasons like humans and enables enhanced agent-based decision-making. It has applications in a large variety of domains including autonomous systems, operations research, intelligence analysis, and data mining. One of the most time consuming and key components of CECEP is the mining of knowledge from a repository described as a Cognitive Domain Ontology (CDO). One problem that is often tasked to CDOs is asset allocation. Given the number of possible solutions in this allocation problem, determining the optimal solution via CDO can be very time and energy consuming. A grid of isolated spiking neurons is capable of generating solutions to this problem very quickly, although some degree approximation is required to achieve the speedup. The approximate spiking approach presented in this work was able to complete nearly all allocation simulations with greater than 98% accuracy. Our results in this work show that constraining the possible solution space by creating specific rules for a scenario can alter the quality of the allocation result. We present a study compares allocation score and computation time for three different constraint implementation cases. Given the vast increase in speed, as well as the reduction computational requirements, the presented algorithm is ideal for moving asset allocation to low power embedded hardware.",https://ieeexplore.ieee.org/document/9057909/,2019 IEEE National Aerospace and Electronics Conference (NAECON),15-19 July 2019,ieeexplore
10.1109/ISUVR.2010.25,High-Performance Real-Time Face-Detection Architecture for HCI Applications,IEEE,Conferences,"This paper proposes a novel hardware structure and FPGA implementation method for real-time detection of multiple human faces with robustness against illumination variations and Rotated faces. These are designed to greatly improve face detection in various environments, using the Adaboost learning algorithm and MCT techniques, Rotation Transformation, which is robust against variable illumination and rotated faces. The overall structure of proposed hardware is composed of a Color Space Converter, Noise Filter, Memory Controller Interface, Rotation Transformation, MCT (Modified Census Transform), Candidate Detector/Confidence Mapper, Position Resizer, Data Grouper, Overlay Processor and Color Overlay Processor. The experiment was conducted in various environments using a QVGA Camera, LCD Display and Virtext5 XC5VLX330 FF1760 FPGA, made by Xilinx. Implementation and verification results showed that it is possible to detect at least 32 faces in a wide variety of sizes at a maximum speed of 149 frames per second in real time.",https://ieeexplore.ieee.org/document/5557933/,2010 International Symposium on Ubiquitous Virtual Reality,7-10 July 2010,ieeexplore
10.1109/IROS.2008.4651150,High-dimensional underactuated motion planning via task space control,IEEE,Conferences,"Kinodynamic planning algorithms have the potential to find feasible control trajectories which accomplish a task even in very nonlinear or constrained dynamical systems. Underactuation represents a particular form of a dynamic constraint, inherently present in many machines of interest (e.g., walking robots), and necessitates planning for long-term control solutions. A major limitation in motion planning techniques, especially for real-time implementation, is that they are only practical for relatively low degree-of-freedom problems. Here we present a model-based dimensionality reduction technique based on an extension of partial feedback linearization control into a task-space framework. This allows one to plan motions for a complex underactuated robot directly in a low-dimensional task-space, and to resolve redundancy with lower-priority tasks. We illustrate the potential of this approach with an extremely simple motion planning system which solves the swing-up problem for multi-link underactuated pendula, and discuss extensions to the control of walking.",https://ieeexplore.ieee.org/document/4651150/,2008 IEEE/RSJ International Conference on Intelligent Robots and Systems,22-26 Sept. 2008,ieeexplore
10.1109/ICAECT49130.2021.9392459,Highly Accurate Real time human Counter with Minimum Computation Cost,IEEE,Conferences,"During the ongoing Covid-19 Pandemic when we need to operate any public facility like museum, shopping mall, restaurants, or public dealing organizations, we not only need to keep the operations going but also have to ensure precautionary measures to ascertain their safety. As per all SOPs (Standard Operating Procedures) it is advisable to restrict number of visitors inside these enclosed spaces which are most likely to be weather controlled. Automatic safety compliance thus becomes imperative in such situations. Even though absolute compliance and alert signalling will require scrutiny and cross-checking at several levels, a beginning towards automation of compliance monitoring seems mandatory in the neo-normal era. Hence In this project we have designed a low cost rapidly implementable design to monitor the number of visitors inside the self-contained hall. The system will give signal once the maximum permissible visitor population is reached at a given time. Monitoring the optimal population and the density and enforcing visitor to wear mask even within the space manually is tantamount to imposing health hazards to the person who will physically have to monitor and it may as well render the visitors vulnerable. Here we have used Artificial Intelligence based model person detection and tracking. Real time tracking with accuracy is still an important area in computer vision. There are some commercial solution available for the problem but all of them either implemented considering ideal situation or need huge cost and infrastructure. But as a part of museums community we are passing through a financial crises as due to pandemic we closed for visitors. Hence neither we can afford costly system nor a system designed with ideal condition. This motivates us to develop a new system according to our criteria. Here we have modified the available solution for implementation in real world environment using very minimum hardware infrastructure requirements to work on real time with maximum possible efficiency. This system is not only useful for COVID-19 Situation but also its use can be extended beyond the boundary of museums for visitor density monitoring system for large public establishment with minimum computation cost.",https://ieeexplore.ieee.org/document/9392459/,"2021 International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)",19-20 Feb. 2021,ieeexplore
10.1109/ALLERTON.2009.5394528,Highly parallel decoding of space-time codes on graphics processing units,IEEE,Conferences,Graphics processing units (GPUs) with a few hundred extremely simple processors represent a paradigm shift for highly parallel computations. We use this emergent GPU architecture to provide a first demonstration of the feasibility of real time ML decoding (in software) of a high rate space-time block code that is representative of codes incorporated in 4th generation wireless standards such as WiMAX and LTE. The decoding algorithm is conditional optimization which reduces to a parallel calculation that is a natural fit to the architecture of low cost GPUs. Experimental results demonstrate that asymptotically the GPU implementation is more than 700 times faster than a standard serial implementation. These results suggest that GPU architectures have the potential to improve the cost / performance tradeoff of 4th generation wireless base stations. Additional benefits might include reducing the time required for system development and the time required for configuration and testing of wireless base stations.,https://ieeexplore.ieee.org/document/5394528/,"2009 47th Annual Allerton Conference on Communication, Control, and Computing (Allerton)",30 Sept.-2 Oct. 2009,ieeexplore
10.1109/6GSUMMIT49458.2020.9083875,Histograms to Quantify Dataset Shift for Spectrum Data Analytics: A SoC Based Device Perspective,IEEE,Conferences,"Cloud/software-based wireless resource controllers have been recently proposed to exploit radio frequency (RF) data analytics for a network control, configuration and management. For efficient resource controller design, tracking the right metrics in real-time (analytics) and making realistic predictions (deep learning) will play an important role to increase its efficiency. This factor becomes particularly critical as radio environments are generally dynamic, and the data sets collected may exhibit shift in distribution over time and/or space. When a trained model is deployed at the controller without taking into account dataset shift, a large amount of prediction errors may take place. This paper quantifies dataset shift in real wireless physical layer data by using a statistical distance method called earth mover's distance (EMD). It utilizes an FPGA to process in real-time the inphase and quadrature (IQ) samples to obtain useful information, such as histograms of wireless channel utilization (CU). We have prototyped the data processing modules on a Xilinx System on Chip (SoC) board using Vivado, Vivado HLS, SDK and MATLAB tools. The histograms are sent as low-overhead analytics to the resource controller server where they are processed to evaluate dataset shift. The presented results provide insight into dataset shift in real wireless CU data collected over multiple weeks in the University of Oulu using the implemented modules on SoC devices. The results can be used to design approaches that can prevent failures due to datashift in deep learning models for wireless networks.",https://ieeexplore.ieee.org/document/9083875/,2020 2nd 6G Wireless Summit (6G SUMMIT),17-20 March 2020,ieeexplore
10.1109/SECON.1991.147712,Hough transform system enhancements resulting from neural network implementation,IEEE,Conferences,A neural network implementation of a Hough transform system has been suggested to make real-time operation possible. That and other developments are summarized and combined to suggest a complete system. Convergence times under 30 mu s are obtainable with general-purpose operational amplifiers for the processing nodes or neurons. Enhancements are possible with an artificial neural network system such as multiple target detection with a simple hardware addition to the basic system. Approximation of the peak magnitude in parameter space is available from the multiple peak detection circuitry. The magnitude of the peak indirectly yields the length of the line segments in image space. The system can adapt to varying image intensities. This feature can be used to locate weak targets and avoid neuron amplifier saturation. A signal perturbation technique can be used to enhance resolution or reduce hardware requirements.&lt;<ETX>&gt;</ETX>,https://ieeexplore.ieee.org/document/147712/,IEEE Proceedings of the SOUTHEASTCON '91,7-10 April 1991,ieeexplore
10.1109/ACSOS49614.2020.00036,How far should I watch? Quantifying the effect of various observational capabilities on long-range situational awareness in multi-robot teams,IEEE,Conferences,"In our previous work, we showed that individual robots within a multi-robot team can gain long-distance situational awareness from passive observations of a single nearby neighbor without any explicit robot-to-robot communication. However, that prior work was developed only in simulation, and performance was not measured for real robot teams in physical space with realistic hardware limitations. Toward this end, we studied the performance of these methods in real robot scenarios with methods using more sophisticated techniques in machine learning to mitigate practical implementation problems. In this study, we further extend that work by characterizing the effects of changing history length and sensor range. Rather than finding that increasing history length and sensor range always yield better estimation performance, we find that the optimal history length and sensor range varies depending on the distance between the estimating robot and the robot being estimated. For estimation problems where the estimation target is nearby, longer histories actually degrade performance, and so sensor ranges could be increased instead. Conversely, for farther targets, history length is as valuable or more valuable than sensor range. Thus, just as optimal shutter speed varies with light availability and speed of the subject, passive situational awareness in multi-robot teams is best achieved with different strategies depending on proximity to locations of interest. All studies use the teams of Thymio II physical, two-wheeled robots in laboratory environments <sup>1</sup>.<sup>1</sup>Data and models used are available at https://github.com/PavlicLab/ACSOS2020_ReTLo_Extension.git.",https://ieeexplore.ieee.org/document/9196255/,2020 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS),17-21 Aug. 2020,ieeexplore
10.1109/ICCSE.2018.8468759,How to Enhance Chinese Word Segmentation Using Knowledge Graphs,IEEE,Conferences,"Chinese word segmentation is a very important problem for Chinese information processing. Chinese word segmentation results are the basis for computers to understand natural language. However, unlike most Western languages, Chinese words do not have fixed symbols like white space as word segmentation marks. Moreover, Chinese has a very complex grammar, and the word segmentation criteria are varied with the contexts. Therefore, Chinese word segmentation is a very difficult task. Many existing works have proposed many algorithms to solve this problem. However, to our best knowledge, none of them could outperform all the other methods. In this paper, we develop a novel algorithm based on semantics and contexts. We propose a semantic-based word similarity measure using the concept hierarchy in knowledge graphs, and use this measure to prune the different results which are generated by several state-of-the-art Chinese word segmentation methods. The idea is to respectively compute the concept similarity of these words to other words in the text, and choose the word with the highest concept similarity score. To evaluate the effectiveness of the proposed approach, we conduct a series of experiment on two real datasets. The results show that our method outperforms all the state-of-the-art algorithms by filtering out wrong results and retaining correct ones.",https://ieeexplore.ieee.org/document/8468759/,2018 13th International Conference on Computer Science & Education (ICCSE),8-11 Aug. 2018,ieeexplore
10.1109/IoTDI.2015.19,"Human SLAM, Indoor Localisation of Devices and Users",IEEE,Conferences,"The indoor localisation problem is more complex than just finding whereabouts of users. Finding positions of users relative to the devices of a smart space is even more important. Unfortunately, configuring such systems manually is a tedious process, requires expert knowledge, and is sensitive to changes in the environment. Moreover, many existing solutions do not take user privacy into account. We propose a new system, called Simultaneous Localisation and Configuration (SLAC), to address the problem of locating devices and users relative to those devices, and combine this problem into a single estimation problem. The SLAC algorithm, based on FastSLAM, is able to locate devices using the received signal strength indicator (RSSI) of devices and motion data from users. Simulations have been used to show the performance in a controlled environment and the effect of the amount of RSSI updates on the localisation error. Live tests in non-trivial environments showed that we can achieve room level accuracy and that the localisation can be performed in real time. This is all done locally, i.e. running on a user's device, with respect for privacy and without using any prior information of the environment or device locations. Although promising, more work is required to increase accuracy in larger environments and to make the algorithm more robust for environment noise caused by walls and other objects. Existing techniques, e.g. map fusing, can alleviate these problems.",https://ieeexplore.ieee.org/document/7471364/,2016 IEEE First International Conference on Internet-of-Things Design and Implementation (IoTDI),4-8 April 2016,ieeexplore
10.1109/CRV.2010.55,Human Upper Body Pose Recognition Using Adaboost Template for Natural Human Robot Interaction,IEEE,Conferences,"In this paper, we propose a novel Adaboost template to recognize human upper body poses from disparity images for natural human robot interaction (HRI). First, the upper body poses of standing persons are classified into seven categories of views. For each category, a mean template, variance template, and percentage template are generated. Then, the template region is divided into positive and negative regions, corresponding to the region of bodies and surrounding open space. A weak classifier is designed for each pixel in the template. A new EM-like Adaboost learning algorithm is designed to learn the Adaboost template. Different from existing Adaboost classifiers, we show that the Adaboost template can be used not only for recognition but also for adaptive top-down segmentation. By using Adaboost template, only a few positive samples for each category are required for learning. Comparison with conventional template matching techniques has been made. Experimental results show that significant improvements can be achieved in both cases. The method has been deployed in a social robot to estimate human attentions to the robot in real-time human robot interaction.",https://ieeexplore.ieee.org/document/5479162/,2010 Canadian Conference on Computer and Robot Vision,31 May-2 June 2010,ieeexplore
10.1109/IranianMVIP.2010.6313969,Human action recognition by RANSAC based salient features of skeleton history image using ANFIS,IEEE,Conferences,"In this paper, a new approach using Adaptive Neuro-Fuzzy Inference System (ANFIS) as a human action recognition system is proposed. ANFIS is an intelligence method which combines both fuzzy inference system and neural networks. The basis of the method is the representation of each action as a bivariate histogram that is computed from skeleton history image in one action duration. Skeleton image is extracted from the human silhouette in each frame then these images gather to generate skeleton history image. This approach automatically performs segmentation on the feature space with RANSAC algorithm to select some features yielded better results. Also some actions, which are similar in spatial features such as 'sit down' and 'stand up' but they are inverse in temporal domain, are discriminated with temporal window implemented in the first half duration. Real human action dataset, Weizmann, is selected for evaluation. The resulting average recognition rate of the proposed method is 98.3%.",https://ieeexplore.ieee.org/document/6313969/,2010 6th Iranian Conference on Machine Vision and Image Processing,27-28 Oct. 2010,ieeexplore
10.1109/VRAIS.1995.512487,Human figure synthesis and animation for virtual space teleconferencing,IEEE,Conferences,"Human figure animation is it widely researched area with many applications. This paper addresses specific issues that deal with the synthesis, animation and environmental interaction of human figures within a virtual space teleconferencing system. A layered representation of the human figure is adopted. Skeletal posture is determined from magnetic sensors on the body, using heuristics and inverse kinematics. This paper describes the use of implicit function techniques in the synthesis and animation of a polymesh geometric skin over the skeletal structure. Implicit functions perform detection and handling of collisions with an optimal worst case time complexity that is linear in the number polymesh vertices. Body deformations resulting from auto-collisions are handled elegantly and homogeneously as part of the environment. Further, implicit functions generate precise collision contact surfaces and have the capability to model the physical characteristics of muscles in systems that employ force feedback. The real time implementation within a virtual space teleconferencing system, illustrates this new approach, coupling polymesh and implicit surface based modeling and animation techniques.",https://ieeexplore.ieee.org/document/512487/,Proceedings Virtual Reality Annual International Symposium '95,11-15 March 1995,ieeexplore
10.1109/ROBIO.2011.6181717,Human-like gradual multi-agent Q-learning using the concept of behavior-based robotics for autonomous exploration,IEEE,Conferences,"In the last few years, the field of mobile robotics has made lots of advancements. These advancements are due to the extensive application of mobile robots for autonomous exploration. Mobile robots are being popularly used for applications in space, underwater explorations, underground coal mines monitoring, inspection in chemical/toxic/ nuclear factories etc. But if these environments are unknown/unpredictable, conventional/ classical robotics may not serve the purpose. In such cases robot learning is the best option. Learning from the past experiences, is one such way for real time application of robots for completely unknown environments. Reinforcement learning is one of the best learning methods for robots using a constant system-environment interaction. Both single and multi-agent concepts are available for implementation of learning. The current research work describes a multi-agent based reinforcement learning using the concept of behaviour-based robotics for autonomous exploration of mobile robots. The concept has also been tested both in indoor and outdoor environments using real-time robots.",https://ieeexplore.ieee.org/document/6181717/,2011 IEEE International Conference on Robotics and Biomimetics,7-11 Dec. 2011,ieeexplore
10.1109/FUZZY.1994.343840,Human-motion recognition by means of fuzzy associative inference,IEEE,Conferences,"A real time human motion recognition method is proposed that uses fuzzy associative inference. If transforms space time patterns into state transition patterns, which are then recognized by means of fuzzy associative inference using associative memories. The tracking data is given as time series data, from which the characteristic states are extracted. Each human motion has a specific state transition pattern that consists of characteristic states. To recognize these motions, the specific state transition patterns of the motions are defined as fuzzy rules and these fuzzy rules are implemented in a fuzzy associative memory system. This method is independent of the person being measured and the speed of the motion. In real time experiments, this method was able to recognize three basic tennis motions (forehand stroke, backhand stroke, and smash) for six unspecified people. The recognition ratio of the fuzzy associative memory system is better than that of conventional fuzzy inference and a multilayer perceptron.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/343840/,Proceedings of 1994 IEEE 3rd International Fuzzy Systems Conference,26-29 June 1994,ieeexplore
10.1109/ETFA45728.2021.9613547,Hybrid Feature Selection for High-Dimensional Manufacturing Data,IEEE,Conferences,"In manufacturing environment, hundreds of input parameters are related to product quality. To build an accurate machine learning model for quality prediction, it is necessary to find major input parameters which have a big influence in quality prediction. The procedure of identifying major factors out of original high-dimensional input parameters is called to be feature selection. This paper proposes a hybrid method for feature selection, which effectively reduces the searching space by leveraging feature subset chosen by Fast Correlation Based Filter (FCBF) and Relief-based feature selection. The computational complexity is proved to be quadratic in feature number, while most of the existing methods suffer from exponential computation complexity. This improvement is crucial especially when we deal with high-dimensional input parameters because it dramatically reduces the computational time. Further, the proposed method outperforms in prediction accuracy as well when it compares with the benchmarking method. It has been demonstrated by the implementation of our method into real-world manufacturing data sets and open source benchmarking data set.",https://ieeexplore.ieee.org/document/9613547/,2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA ),7-10 Sept. 2021,ieeexplore
10.1109/TAI.1998.744843,IBHYS: a new approach to learn users habits,IEEE,Conferences,"Learning interface agents search regularities in the user behavior and use them to predict user's actions. We propose a new inductive concept learning approach, called IBHYS, to learn such regularities. This approach limits the hypothesis search to a small portion of the hypothesis space by letting each training example build a local approximation of the global target function. It allows to simultaneously search several hypothesis spaces and to simultaneously handle hypotheses described in different languages. This approach is particularly suited for learning interface agents because it provides an incremental algorithm with low training time and decision time, which does not require the designer of the interface agent to describe in advance and quite carefully the repetitive patterns searched. We illustrate our approach with two autonomous software agents, the Apprentice and the Assistant, devoted to assist users of interactive programming environments and implemented in Objectworks Smalltalk-80. The Apprentice learns user's work habits using an IBHYS algorithm and the Assistant, based on what has been learnt, proposes to the programmer sequences of actions the user might want to redo. We show, with experimental results on real data, that IBHYS outperforms ID3 both in computing time and predictive accuracy.",https://ieeexplore.ieee.org/document/744843/,Proceedings Tenth IEEE International Conference on Tools with Artificial Intelligence (Cat. No.98CH36294),10-12 Nov. 1998,ieeexplore
10.1109/APSEC.2002.1182989,IBistro: a learning environment for knowledge construction in distributed software engineering courses,IEEE,Conferences,"We have taught several distributed software engineering project courses with students and real clients. During these projects, students in Pittsburgh and Munich, Germany collaborated in the development of a single system. Our experiences showed that software development is communication intensive and requires the collaboration of many stakeholders. Communication is challenging in distributed contexts: participants do not all know each other and work at different times and locations; the number of participants and their organization change during the project; and participants belong to different communities. Hence, to deal with the global marketplace, it is critical to provide students with distributed collaboration skills. To improve the teaching of collaboration in software engineering, we propose iBistro, an augmented, distributed, and ubiquitous communication space. iBistro aims to overcome problems resulting from miscommunications and information loss in informal or casual meetings. iBistro enables distributed groups to collaborate and cooperate in software projects and therefore provides an environment for learning in diverse aspects such as project management, programming skills, and social skills. With the addition of techniques from artificial intelligence, such as student modeling, and intelligent support mechanisms, such as computer supported group formation, distributed tutoring becomes feasible.",https://ieeexplore.ieee.org/document/1182989/,"Ninth Asia-Pacific Software Engineering Conference, 2002.",4-6 Dec. 2002,ieeexplore
10.1109/CSCN.2015.7390444,IEEE 1900.6b: Sensing support for spectrum databases,IEEE,Conferences,"A number of key examples of spectrum databases in wireless communications either persist or are in the process of being instantiated. Perhaps one of the most notable recent developments in this area is the spectrum databases that enable secondary usage of TV White Space (TVWS), authorized by regulators such as the FCC in the US, Ofcom in the UK, and various others internationally. Such developments have moved away from spectrum sensing for detection and secondary usage of TV band spectrum opportunities. However, it is clear that spectrum sensing might still viably assist opportunistic spectrum usage, even from a regulatory point of view, both in TVWS and in other forms of spectrum sharing. This also might be the case in wireless communications in general (e.g., in the context of self-organizing networks), particularly when spectrum sensing methods are employed to enhance or verify the operation of spectrum databases. To this end, the IEEE 1900.6 working group is undertaking an amendment standard project, IEEE 1900.6b, on spectrum sensing support for spectrum databases. This paper addresses the IEEE 1900.6 background, and reasoning for the 1900.6b amendment standard, as well as the use cases for the amendment standard and the deployment scenarios and benefits for such standardized spectrum sensing support for spectrum databases. It also provides qualitative arguments of the benefits of the approach using real information from an operational TVWS spectrum database compared with measurements at the same location. It is shown that spectrum sensing to support such a database might viably increase the amount of TV band spectrum available at that location for opportunistic usage, with 4 Watts EIRP, from around 24 MHz to around 240 MHz.",https://ieeexplore.ieee.org/document/7390444/,2015 IEEE Conference on Standards for Communications and Networking (CSCN),28-30 Oct. 2015,ieeexplore
10.1109/MDM48529.2020.00023,IFLoc: Indoor Height Estimation by Telco Data,IEEE,Conferences,"Understanding the fine-grained distribution of telecommunication (Telco) signals in terms of a three-dimensional (3D) space is important for Telco operators to manage, operate and optimize Telco networks. It is particularly true in nowadays urban cities with a large number of high buildings. One of the key tasks is to infer the location height of mobile devices, e.g., the floor within a high building where mobile devices are located. However, precise height estimation is challenging due to complex Telco signal propagation within an indoor 3D space, sparse cell tower deployment and scarce training samples. To tackle these issues, in this paper, we propose an indoor MR height estimation framework, namely IFLoc, via a machine learning model. IFLoc first builds a training MR database via a pre-processing step to comfortably tag raw MR samples by precisely inferred height from auxiliary data such as GPS and barometer readings. Next, IFLoc trains a regression model for height estimation by a set of developed techniques including 3D space division, post-processing techniques, feature augmentation and an improved SVR (Supported Vector Regression) model. Our evaluation on eight real datasets collected within five representative high buildings in Shanghai validates that IFLoc outperforms state-of-the-art counterparts in particularly with scarce training data.",https://ieeexplore.ieee.org/document/9162333/,2020 21st IEEE International Conference on Mobile Data Management (MDM),30 June-3 July 2020,ieeexplore
10.1109/ICA-ACCA.2018.8609763,"Identification and Process Control for MISO systems, with Artificial Neural Networks and PID Controller",IEEE,Conferences,"Industrial processes with multiple input and single manipulated variables are very complex systems to control in automatic models. Such is the case with processes related to gases extraction or transport phenomena. The present research is focused on the development of a control algorithm (automatic control strategy), based on artificial neural networks, to identify an industrial process by using process historical records, as well as knowledge from the operation itself. The output of the identification stage feeds a classic PID controller to perform control actions (hybrid controller). Here, an actuator or final control element is modeled, estimating its space-state dynamic equation. With the estimated model, a local control loop is conformed controlling the main process or manipulated variable. For this, the process of gases transport in a copper smelter plant was chosen, where the necessary data and scenarios for the proposed control algorithm testing was obtained. This application attempts to present a solution to problems inherent to manual control, multiple key variables coexisting in a system, mechanical stress in equipment due to manual actions, etc. The control strategy is based on a computer simulation made with real process data, showing improvement of the transient periods in the final actuators due to control signals, as well as establishing that these kinds of technologies could be implemented in both, an existing plant hardware/software or in a conventional control system.",https://ieeexplore.ieee.org/document/8609763/,2018 IEEE International Conference on Automation/XXIII Congress of the Chilean Association of Automatic Control (ICA-ACCA),17-19 Oct. 2018,ieeexplore
10.1109/GIOTS.2017.8016227,Identifying parking spaces &amp; detecting occupancy using vision-based IoT devices,IEEE,Conferences,"The increasing number of vehicles in high density, urban areas is leading to significant parking space shortages. While systems have been developed to enable visibility into parking space vacancies for drivers, most rely on costly, dedicated sensor devices that require high installation costs. The proliferation of inexpensive Internet of Things (IoT) devices enables the use of compute platforms with integrated cameras that could be used to monitor parking space occupancy. However, even with camera-captured images, manual specification of parking space locations is required before such devices can be used by drivers after device installation. In this paper, we leverage machine learning techniques to develop a method to dynamically identify parking space topologies based on parked vehicle positions. More specifically, we designed and evaluated an occupation detection model to identify vacant parking spaces. We built a prototype implementation of the whole system using a Raspberry Pi and evaluated it on a real-world urban street near the University of Washington campus. The results show that our clustering-based learning technique coupled with our occupation detection pipeline is able to correctly identify parking spaces and determine occupancy without manual specication of parking space locations with an accuracy of 91%. By dynamically aggregating identied parking spaces from multiple IoT devices using Amazon Cloud Services, we demonstrated how a complete, city-wide parking management system can be quickly deployed at low cost.",https://ieeexplore.ieee.org/document/8016227/,2017 Global Internet of Things Summit (GIoTS),6-9 June 2017,ieeexplore
10.1109/CCWC47524.2020.9031165,Image Classification on NXP i.MX RT1060 using Ultra-thin MobileNet DNN,IEEE,Conferences,"Deep Neural Networks play a very significant role in computer vision applications like image classification, object recognition and detection. They have achieved great success in this field but the main obstacles for deploying a DNN model into an Autonomous Driver Assisted System (ADAS) platform are limited memory, constrained resources, and limited power. MobileNet is a very efficient and light DNN model which was developed mainly for embedded and computer vision applications, but researchers still faced many constraints and challenges to deploy the model into resource-constrained microprocessor units. Design Space Exploration of such CNN models can make them more memory efficient and less computationally intensive. We have used the Design Space Exploration technique to modify the baseline MobileNet V1 model and develop an improved version of it. This paper proposes seven modifications on the existing baseline architecture to develop a new and more efficient model. We use Separable Convolution layers, the width multiplier hyperparamater, alter the channel depth and eliminate the layers with the same output shape to reduce the size of the model. We achieve a good overall accuracy by using the Swish activation function, Random Erasing technique and a choosing good optimizer. We call the new model as Ultra-thin MobileNet which has a much smaller size, lesser number of parameters, less average computation time per epoch and negligible overfitting, with a little higher accuracy as compared to the baseline MobileNet V1. Generally, when an attempt is made to make an existing model more compact, the accuracy decreases. But here, there is no trade off between the accuracy and the model size. The proposed model is developed with the intent to make it deployable in a realtime autonomous development platform with limited memory and power and, keeping the size of the model within 5 MB. It could be successfully deployed into NXP i.MX RT1060 ADAS platform due to its small model size of 3.9 MB. It classifies images of different classes in real-time, with an accuracy of more than 90% when it is run on the above-mentioned ADAS platform. We have trained and tested the proposed architecture from scratch on the CIFAR-10 dataset.",https://ieeexplore.ieee.org/document/9031165/,2020 10th Annual Computing and Communication Workshop and Conference (CCWC),6-8 Jan. 2020,ieeexplore
10.1109/GLOBECOM42002.2020.9348202,Image Download and Rate Allocation of Internet-of-Things Analytics at Gateways in Smart Cities,IEEE,Conferences,"Internet-of-Things (IoT) devices are connected to the Internet through a gateway, which can host IoT analytics encapsulated in containers to convert raw sensor data into more condensed processed data. In this paper, we study two research problems to maximize the overall Quality-of-Service (QoS) level of all IoT analytics that run on both data center servers and gateways. The first problem is to select additional IoT analytics to deploy on a gateway to save upload bandwidth due to transmitting raw sensor data. The second problem is to allocate the residue upload bandwidth among all IoT analytics to maximize the overall QoS level. We propose several algorithms to solve these two research problems. We have implemented real testbeds to evaluate our proposed system and algorithms. Our experiment results reveal that the proposed algorithms: (i) capitalize the download bandwidth and storage space of the gateway for saving the upload bandwidth consumption and (ii) achieve high QoS levels without overloading the network and gateway.",https://ieeexplore.ieee.org/document/9348202/,GLOBECOM 2020 - 2020 IEEE Global Communications Conference,7-11 Dec. 2020,ieeexplore
10.1109/ICM52667.2021.9664950,Image Inpainting and Classification Agent Training Based on Reinforcement Learning and Generative Models with Attention Mechanism,IEEE,Conferences,"What distinguishes the field of artificial intelligence (AI) from others is to develop fully independent agents that learn optimal behavior, change, and evolve solely through the communication of trial and error with the surrounding environment. Reinforcement learning (RL) can be seen in multiple aspects of Machine Learning (ML), provided the environment, reward, actions, the state will be defined. Agent training in previous years is seen to only relate to robotics, games, and self-driving cars. While trying to divert the focus of researchers from the view of self-driving cars, games, robots, etc. Here, we investigated using reinforcement learning in the aspect of task completion. We deployed our architecture in an inpainting task where the agent generates the distorted or missing image content into an eminent fidelity completed the image by using reinforcement learning to influence the generative model utilized. The Generative Adversary Network (GAN) problem of not being steady and challenging to train was overwhelmed by utilizing latent space representation. The dimension is reduced compared to the distorted or corrupted image in training the GAN. Then reinforcement learning was deployed to pick the correct GAN input to get the images latent space representation that is most suitable for the current input of the missing or distorted image region. In this paper, we also learned that the trained agent enhances the accuracy in a classification task of images with missing data. We successfully examined the classification enhancement on images missing 30%, 50%, and 70%.",https://ieeexplore.ieee.org/document/9664950/,2021 International Conference on Microelectronics (ICM),19-22 Dec. 2021,ieeexplore
10.1109/ELECTR.1991.718282,Imaging And Controls For Mars Robots With Neural Networks,IEEE,Conferences,"Two aspects of the design of space robots is covered implemented by neural networks and by hybrid approach with artificial intelligence. One is a neurocontroller for a real-time autonomous system. An optical control system developed saves the time for the image processing that analyzes an image sensor through the environment and induces a transformation over the sensor array. A prototype of the neurocontroller is able to learn and control by itself. The second aspect deals with the design of a Servo Control System for a Robot with the capability of ""learning in Unanticipated Situations"" incorporated in the system. The robot is assumed to be employed to perform useful tasks in an alien evironment. The model developed is shown to provide the robot with the capability to recover from unanticipated situations that can lead to the disruption of its normal operation, and to learn to avoid such situations in the future. These two aspects will be integrated for a design of a very intelligent autonomous space robot.",https://ieeexplore.ieee.org/document/718282/,"Electro International, 1991",16-18 April 1991,ieeexplore
10.23919/ITUK50268.2020.9303205,Immersive Technologies for Development: An Analysis of Agriculture,IEEE,Conferences,"Agricultural development is key to any economic development. Immersive technology plays a catalytic role and offers smart and sustainable choices to farmers who want to improve on agricultural productivity, and to agricultural training institutes that seek to use modern technology to advance pedagogy and reduce fatalities and operating costs in the learning space, among others. Currently, there are limited descriptive literature reviews in the area of immersive technology in agriculture, hereafter referred to as AVR-Agric. This paper presents a systematic literature review (SLR), which offers a structured, methodical, and rigorous approach to the understanding of the trend of research in AVR-Agric, and the least and most researched issues. This study explores and examines the current trends in the immersive technology-based agriculture areas, and provides a credible intellectual guide for future research direction. The SLR was limited to existing applications and peer-reviewed conference and journal articles published from 2006 to 2020. The results showed that virtual reality was implemented in 41% of the papers reviewed, augmented reality was found in 53%, while only 6% considered mixed-reality applications. The study also showed that developments that incorporate IoT, blockchain, and machine-learning technologies are still at their stage of exploration and advancement.",https://ieeexplore.ieee.org/document/9303205/,2020 ITU Kaleidoscope: Industry-Driven Digital Transformation (ITU K),7-11 Dec. 2020,ieeexplore
10.1109/ICASSP.2000.859315,Implementation and comparison of three architectures for gesture recognition,IEEE,Conferences,"Several systems for automatic gesture recognition have been developed using different strategies and approaches. In these systems the recognition engine is mainly based on three algorithms: dynamic pattern matching, statistical classification, and neural networks (NN). In this paper three architectures for the recognition of dynamic gestures using the above mentioned techniques or a hybrid combination of them are presented and compared. For all architectures a common preprocessor receives as input a sequence of color images, and produces as output a sequence of feature vectors of continuous parameters. The first two systems are hybrid architectures consisting of a combination of neural networks and hidden Markov models (HMM). NNs are used for the classification of single feature vectors while HMMs for the modeling of sequences of them with the aim to exploit the properties of both these tools. More precisely, in the first system a Kohonen feature map (SOM) clusters the input space. Further, each code-book is transformed into a symbol from a discrete alphabet and fed into a discrete HMM for classification. In the second approach a radial basis function (RBF) network is directly used to compute the HMM state observation probabilities. In the last system only dynamic programming techniques are employed. An input sequence of feature vectors is matched by some predefined templates by using the dynamic time warping (DTW) algorithm. Preliminary experiments with our baseline systems achieved a recognition accuracy up to 92%. All systems use input from a monocular color video camera, are user-independent but so far, they are not yet real-time.",https://ieeexplore.ieee.org/document/859315/,"2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.00CH37100)",5-9 June 2000,ieeexplore
10.1109/GCCE46687.2019.9015335,Implementation of 3D Drawing to Interactive Display System,IEEE,Conferences,"Our laboratory has been developing an entertainment system called the light crayon. This system can draw on a screen with light pointer. In this paper, we propose a new system that can draw on 3D(three-dimensional) space with Mixed Reality technology.",https://ieeexplore.ieee.org/document/9015335/,2019 IEEE 8th Global Conference on Consumer Electronics (GCCE),15-18 Oct. 2019,ieeexplore
10.1109/I-SMAC52330.2021.9640863,Implementation of Machine Learning Classifier for DTN Routing,IEEE,Conferences,"This paper presents, better routing method in Delay Tolerant Network using Machine learning. Delay Tolerant Network is a wireless network, in which nodes are changing its positions dynamically in an unexpected way due to that Round trip time and error rates are very high. Examples are Disaster area, under the sea, Space communication, etc. In the proposed method neighbouring nodes are predicted by machine learning classifiers. These nodes use message history delivery information to deliver the message on destination. With the help of Bundle protocol implementation IBR-DTN [3], collects network traffic status and real-world location trace. These information uses to emulate DTN environment by Common Open Research Emulator (CORE) [2]. The new application is used to predict the results, preparation for the network history data, analysis and classification-based routing.",https://ieeexplore.ieee.org/document/9640863/,"2021 Fifth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",11-13 Nov. 2021,ieeexplore
10.1109/CNNA.2010.5430286,Implementation of a drosophila-inspired orientation model on the Eye-Ris platform,IEEE,Conferences,"A behavioral model, recently derived from experiments on fruit-flies, was implemented, with successful comparative experiments on orientation control in real robots. This model has been firstly implemented in a standard CNN structure, using an algorithm based on classical, space-invariant templates. Subsequently, the Eye-Ris platform was utilised for the implementation of the whole strategy, at the aim to constitute a stand alone smart sensor for orientation control in bio-inspired robotic platforms. The Eye-Ris vl.2 is a visual system, made by Anafocus, that employs a fully-parallel mixed-signal array sensor-processor chip. Some experiments are reported using a commercial roving platform, the Pioneer P3-AT, showing the reliability of the proposed implementation and usefulness in higher level perceptual tasks.",https://ieeexplore.ieee.org/document/5430286/,2010 12th International Workshop on Cellular Nanoscale Networks and their Applications (CNNA 2010),3-5 Feb. 2010,ieeexplore
10.1109/FUZZ.2001.1007312,Implementation of a general real-time visual anomaly detection system via soft computing,IEEE,Conferences,"The intelligent visual system detects anomalies or defects in real time under normal lighting operating conditions. The application is basically a learning machine that integrates fuzzy logic (FL), artificial neural network (ANN), and genetic algorithm (GA) schemes to process the image, run the learning process, and finally detect the anomalies or defects. The system acquires the image, performs segmentation to separate the object being tested from the background, preprocesses, segments, and retrieves regions. FL provides a powerful framework for knowledge representation and overcomes uncertainty and vagueness typically found in image analysis. An application prototype currently runs on a regular PC under Windows NT, and preliminary work has been performed to build an embedded version with multiple image processors. The application prototype is being tested at the Kennedy Space Center to detect anomalies along slide basket cables utilized by the astronauts to evacuate the Shuttle launch pad in an emergency.",https://ieeexplore.ieee.org/document/1007312/,10th IEEE International Conference on Fuzzy Systems. (Cat. No.01CH37297),2-5 Dec. 2001,ieeexplore
10.1109/FUZZ.2003.1206576,Implementation of a general real-time visual anomaly detection system via soft computing,IEEE,Conferences,"An intelligent visual system prototype was built to detect anomalies or defects in real time under normal lighting operating conditions. The application is basically a learning machine that integrates fuzzy logic (FL), artificial neural network (ANN), and genetic algorithm (GA) schemes to process the image, run the learning process, and finally detect the anomalies or defects. The system acquires the image, performs segmentation to separate the object being tested from the background, preprocesses the image using fuzzy reasoning, performs the final segmentation using fuzzy reasoning techniques to retrieve regions with potential anomalies or defects, and finally retrieves them using a learning model built via ANN and GA techniques. FL provides a powerful framework for knowledge representation and overcomes uncertainty and vagueness typically found in image analysis. ANN provides learning capabilities, and GA leads to robust learning results. An application prototype currently runs on a regular PC under Windows NT, and preliminary work has been performed to build an embedded version with multiple image processors. The application prototype is being tested at the Kennedy Space Center (KSC), Florida, to visually detect anomalies along slide basket cables utilized by the astronauts to evacuate the NASA Shuttle launch pad in an emergency. The potential applications of this anomaly detection system in an open environment are quite wide. Another current, potentially viable application at NASA is in detecting anomalies of the NASA Space Shuttle Orbiter's radiator panels.",https://ieeexplore.ieee.org/document/1206576/,"The 12th IEEE International Conference on Fuzzy Systems, 2003. FUZZ '03.",25-28 May 2003,ieeexplore
10.1109/IJCNN.2008.4633972,Implementation of a neural network based visual motor control algorithm for A 7 DOF redundant manipulator,IEEE,Conferences,"This paper deals with visual-motor coordination of a 7 dof robot manipulator for pick and place applications. Three issues are dealt with in this paper - finding a feasible inverse kinematic solution without using any orientation information, resolving redundancy at position level and finally maintaining the fidelity of information during clustering process thereby increasing accuracy of inverse kinematic solution. A 3-dimensional KSOM lattice is used to locally linearize the inverse kinematic relationship. The joint angle vector is divided into two groups and their effect on end-effector position is decoupled using a concept called function decomposition. It is shown that function decomposition leads to significant improvement in accuracy of inverse kinematic solution. However, this method yields a unique inverse kinematic solution for a given target point. A concept called sub-clustering in configuration space is suggested to preserve redundancy during learning process and redundancy is resolved at position level using several criteria. Even though the training is carried out off-line, the trained network is used online to compute the required joint angle vector in only one step. The accuracy attained is better than the current state of art. The experiment is implemented in real-time and the results are found to corroborate theoretical findings.",https://ieeexplore.ieee.org/document/4633972/,2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),1-8 June 2008,ieeexplore
10.1109/CCAAW.2017.8001607,Implementation of a space communications cognitive engine,IEEE,Conferences,"Although communications-based cognitive engines have been proposed, very few have been implemented in a full system, especially in a space communications system. In this paper, we detail the implementation of a multi-objective reinforcement-learning algorithm and deep artificial neural networks for the use as a radio-resource-allocation controller. The modular software architecture presented encourages re-use and easy modification for trying different algorithms. Various trade studies involved with the system implementation and integration are discussed. These include the choice of software libraries that provide platform flexibility and promote reusability, choices regarding the deployment of this cognitive engine within a system architecture using the DVB-S2 standard and commercial hardware, and constraints placed on the cognitive engine caused by real-world radio constraints. The implemented radio-resource-allocation-management controller was then integrated with the larger space-ground system developed by NASA Glenn Research Center (GRC).",https://ieeexplore.ieee.org/document/8001607/,2017 Cognitive Communications for Aerospace Applications Workshop (CCAA),27-28 June 2017,ieeexplore
10.1109/GeoInformatics.2011.5980945,Implementation of a vector-based cellular automata model for simulating land-use changes,IEEE,Conferences,"In the last few years, cellular automata (CA) models have been increasingly used to simulate the complex land-use changes. However, the traditional regular raster-based CA models are sensitive to the cell size and the neighborhood configuration used in the models, which restrict its ability to simulate the real world. By representing space as irregular shape and size geographic objects and defining artificial neural network as transition rule, a vector-based CA model is constructed and applied to the simulation and prediction of land-use changes. By taking the north branch of Yangtze River estuary as an example, studies illustrated that the vector-based CA model can make full use of artificial neural network to obtain the variable space parameters and simplify the land-use transfer rule. It is concluded that the vector-based CA model produces more realistic spatial patterns than those generated by a raster-based CA model.",https://ieeexplore.ieee.org/document/5980945/,2011 19th International Conference on Geoinformatics,24-26 June 2011,ieeexplore
10.1109/IJCNN.2005.1556443,Implementation of an MLP-based DOA system using a reduced number of MM-wave antenna elements,IEEE,Conferences,"It is required to know the direction of arrival (DOA) of a signal in many applications, such as car tracking or reception optimization for satellite antenna. However, several reflections of different intensities highly affect the sensor outputs and the estimation quality. The system presented here is divided into three parts: a pair of three-element antenna arrays which receives the main beam and the reflected one, a radio frequency combiner which generates power signals and multilayer perceptron neural networks used to invert the mapping between the DOA space and the combiner output space. Simulations are carried out including a model of a simple reflection over a road. They are validated by experimental dataset provided by real antenna array outputs coming from tests using an asphalt reflector in an anechoic chamber.",https://ieeexplore.ieee.org/document/1556443/,"Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.",31 July-4 Aug. 2005,ieeexplore
10.1109/CAIPT.2017.8320692,Implementation of real-time static hand gesture recognition using artificial neural network,IEEE,Conferences,"Sign language is a language that requires the combination of hand gesture, orientation, movement of the hands, arms, body, and facial to simultaneously express the thoughts of the speaker. This paper implements static hand gesture recognition in recognizing the alphabetical sign from A to Z, number from 0 to 9, and additional punctuation mark such as Period, Question Mark, and Spacein Sistem Isyarat Bahasa Indonesia (SIBI). Hand gestures are obtained by evaluating the contour representation from image segmentation of the glove wore by user and then is classified using Artificial Neural Network based on the training model previously built from 100 images for each gesture. The accuracy rate of hand gesture translation is calculated to be 90%. Speech translation recognized NATO phonetic letter as the speech input for translation.",https://ieeexplore.ieee.org/document/8320692/,2017 4th International Conference on Computer Applications and Information Processing Technology (CAIPT),8-10 Aug. 2017,ieeexplore
10.1109/FPT.2018.00015,Implementing NEF Neural Networks on Embedded FPGAs,IEEE,Conferences,"Low-power, high-speed neural networks are critical for providing deployable embedded AI applications at the edge. We describe an FPGA implementation of Neural Engineering Framework (NEF) networks with online learning that outperforms mobile GPU implementations by an order of magnitude or more. Specifically, we provide an embedded Python-capable PYNQ FPGA implementation supported with a High-Level Synthesis (HLS) workflow that allows sub-millisecond implementation of adaptive neural networks with low-latency, direct I/O access to the physical world. We tune the precision of the different intermediate variables in the code to achieve competitive absolute accuracy against slower and larger floating-point reference designs. The online learning component of the neural network exploits immediate feedback to adjust the network weights to best support a given arithmetic precision. As the space of possible design configurations of such networks is vast and is subject to a target accuracy constraint, we use the Hyperopt hyper-parameter tuning tool instead of manual search to find Pareto optimal designs. Specifically, we are able to generate the optimized designs in under 500 iterations of Vivado HLS before running the complete Vivado place-and-route phase on that subset. For neural network populations of 64-4096 neurons and 1-8 representational dimensions our optimized FPGA implementation generated by Hyperopt has a speedup of 10-484 over a competing cuBLAS implementation on the Jetson TX1 GPU while using 2.4-9.5 less power. Our speedups are a result of HLS-specific reformulation (15 improvement), precision adaptation (4 improvement), and low-latency direct I/O access (1000 improvement).",https://ieeexplore.ieee.org/document/8742303/,2018 International Conference on Field-Programmable Technology (FPT),10-14 Dec. 2018,ieeexplore
10.1109/IJCNN.2015.7280832,Improved Manifold Learning with competitive Hebbian rule,IEEE,Conferences,"Manifold Learning methods aim to find meaningful low-dimensional structures hidden in their high-dimensional observations. Recently, they are faced with critical problems of how to reduce computational and space complexity in big data applications, how to determine neighborhood size adaptive to different data sets and how to deal with new observations in an out-of-sample mode. This paper presents a new method called TLOE (Topology Learning and Out-of-sample Embedding) to deal with the above three problems. TLOE uses the competitive Hebbian rule to construct the topology preserving network on a given manifold. It is capable of: 1) automatical selection of the right number and position of landmarks, 2) adaptive determination of neighborhood sizes for landmarks and 3) online embedding of new observations. Experiments on both synthetic and real-world data sets show its promising results.",https://ieeexplore.ieee.org/document/7280832/,2015 International Joint Conference on Neural Networks (IJCNN),12-17 July 2015,ieeexplore
10.1109/IAMA.2009.5228045,Improved classification association rule mining,IEEE,Conferences,"Classification aims to define an abstract model of a set of classes, called classifier, which is built from a set of labeled data, the training set. However, in large or correlated data sets, association rule mining may yield huge rule sets. Hence several pruning techniques have been proposed to select a small subset of high-quality rules. Since the availability of a ldquorichrdquo rule set may improve the accuracy of the classifier, we argue that rule pruning should be reduced to a minimum. A small subset of high-quality rules is first considered. When this set is not able to classify the data, a larger rule set is exploited. This second set includes rules usually discarded by previous approaches. To cope with the need of mining large rule sets and to efficiently use them for classification, a compact form is proposed to represent a complete rule set in a space-efficient way and without information loss. An extensive experimental evaluation on real and synthetic data sets shows that improves the classification accuracy with respect to previous approaches.",https://ieeexplore.ieee.org/document/5228045/,2009 International Conference on Intelligent Agent & Multi-Agent Systems,22-24 July 2009,ieeexplore
10.1109/NSSMIC.1994.474587,Improved resolution via 3D iterative reconstruction for PET volume imaging,IEEE,Conferences,"The authors have implemented iterative filtered backprojection (IFBP) and maximum likelihood by expectation maximization (ML-EM) algorithms in 3D space and applied them to phantom and real PET data. Transaxial resolution improves /spl ap/50% and axial resolution improves /spl ap/15% for IFBP at 15 iterations without a sieve compared to FBP. With a sieve, the improvements are reduced to /spl ap/6%. 3D ML-EM reconstruction shows similar resolution improvement with a much slower convergence rate compared to IFBP. The improvements in resolution from both IFBP and ML-EM are apparent in 3D FDG brain data.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/474587/,Proceedings of 1994 IEEE Nuclear Science Symposium - NSS'94,30 Oct.-5 Nov. 1994,ieeexplore
10.1145/3238147.3238206,Improving Automatic Source Code Summarization via Deep Reinforcement Learning,IEEE,Conferences,"Code summarization provides a high level natural language description of the function performed by code, as it can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space, suffering from two major drawbacks: a) Their encoders only consider the sequential content of code, ignoring the tree structure which is also critical for the task of code summarization; b) Their decoders are typically trained to predict the next word by maximizing the likelihood of next ground-truth word with previous ground-truth word given. However, it is expected to generate the entire sequence from scratch at test time. This discrepancy can cause an exposure bias issue, making the learnt decoder suboptimal. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework (i.e., actor-critic network). The actor network provides the confidence of predicting the next word according to current state. On the other hand, the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations. We employ an advantage reward composed of BLEU metric to train both networks. Comprehensive experiments on a real-world dataset show the effectiveness of our proposed model when compared with some state-of-the-art methods.",https://ieeexplore.ieee.org/document/9000003/,2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE),3-7 Sept. 2018,ieeexplore
10.1109/HPDC.2006.1652187,Improving Resource Matching Through Estimation of Actual Job Requirements,IEEE,Conferences,"Heterogeneous clusters and grid infrastructures are becoming increasingly popular. In these computing infrastructures, machines have different resources (e.g., memory sizes, disk space, and installed software packages). These differences give rise to a problem of over-provisioning, that is, sub-optimal utilization of a cluster due to users requesting resource capacities greater than what their jobs actually need. Our analysis of a real workload file (LANL CM 5) revealed differences of up to two orders of magnitude between requested memory capacity and actual memory usage. The problem of over-provisioning has received very little attention so far. We discuss different approaches for applying machine learning methods to estimate the actual resource capacities used by jobs. These approaches are independent of the scheduling policies and the dynamic resource-matching schemes used. Our simulations show that these methods can yield an improvement of over 50% in utilization (throughput) of heterogeneous clusters",https://ieeexplore.ieee.org/document/1652187/,2006 15th IEEE International Conference on High Performance Distributed Computing,19-23 June 2006,ieeexplore
10.1109/HPCA.2018.00018,In-Situ AI: Towards Autonomous and Incremental Deep Learning for IoT Systems,IEEE,Conferences,"Recent years have seen an exploration of data volumes from a myriad of IoT devices, such as various sensors and ubiquitous cameras. The deluge of IoT data creates enormous opportunities for us to explore the physical world, especially with the help of deep learning techniques. Traditionally, the Cloud is the option for deploying deep learning based applications. However, the challenges of Cloud-centric IoT systems are increasing due to significant data movement overhead, escalating energy needs, and privacy issues. Rather than constantly moving a tremendous amount of raw data to the Cloud, it would be beneficial to leverage the emerging powerful IoT devices to perform the inference task. Nevertheless, the statically trained model could not efficiently handle the dynamic data in the real in-situ environments, which leads to low accuracy. Moreover, the big raw IoT data challenges the traditional supervised training method in the Cloud. To tackle the above challenges, we propose In-situ AI, the first Autonomous and Incremental computing framework and architecture for deep learning based IoT applications. We equip deep learning based IoT system with autonomous IoT data diagnosis (minimize data movement), and incremental and unsupervised training method (tackle the big raw IoT data generated in ever-changing in-situ environments). To provide efficient architectural support for this new computing paradigm, we first characterize the two In-situ AI tasks (i.e. inference and diagnosis tasks) on two popular IoT devices (i.e. mobile GPU and FPGA) and explore the design space and tradeoffs. Based on the characterization results, we propose two working modes for the In-situ AI tasks, including Single-running and Co-running modes. Moreover, we craft analytical models for these two modes to guide the best configuration selection. We also develop a novel two-level weight shared In-situ AI architecture to efficiently deploy In-situ tasks to IoT node. Compared with traditional IoT systems, our In-situ AI can reduce data movement by 28-71%, which further yields 1.4X-3.3X speedup on model update and contributes to 30-70% energy saving.",https://ieeexplore.ieee.org/document/8327001/,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),24-28 Feb. 2018,ieeexplore
10.1109/STPEC52385.2021.9718693,Incipient Faults Detection in Induction Motor using MLP-NN and RBF-NN-based Fault Classifier,IEEE,Conferences,"Stator winding inter-turn faults (SITFs) diagnosis has enormously exploited motor current signatures, and eccentricity faults (EFs) detection has significantly investigated vibration and current signals. However, the motor-current signature analysis sometimes may need other diagnostics techniques for indicating the faulty events. The measurement of vibration signals using an accelerometer is an expensive task. The researchers appreciably implemented artificial intelligence (AI) systems for incipient fault detection in induction motors (IMs). State of the art mainly signifies fault detection techniques for IMs based on the statistical parameters as an input frame to the neural networks (NN). However, in this paper, the proposed neural networks are equipped with the twelve real input parameters to meet the desired fault identification and classification. The NN is trained by deliberately creating the SITFs, EFs, and both faults simultaneously. Multilayer perceptron (MLP) and radial basis function (RBF) based NN models are designed and verified for optimal performance in fault detection techniques. The experimental data set of a three-phase, 420 V, 4 pole squirrel cage induction motor (SCIM) is harnessed to develop the proposed NN. The proposed fault classifier has proven to have reduced mean square error (MSE) and better classification accuracy as compared to classifiers with statistical parameters used as an input feature space.",https://ieeexplore.ieee.org/document/9718693/,"2021 IEEE 2nd International Conference on Smart Technologies for Power, Energy and Control (STPEC)",19-22 Dec. 2021,ieeexplore
10.1109/ICTAI.2019.00226,Incorporating Domain Knowledge in Learning Word Embedding,IEEE,Conferences,"Word embedding is a Natural Language Processing (NLP) technique that automatically maps words from a vocabulary to vectors of real numbers in an embedding space. It has been widely used in recent years to boost the performance of a variety of NLP tasks such as named entity recognition, syntactic parsing and sentiment analysis. Classic word embedding methods such as Word2Vec and GloVe work well when they are given a large text corpus. When the input texts are sparse as in many specialized domains (e.g., cybersecurity), these methods often fail to produce high-quality vectors. In this paper, we describe a novel method, called Annotation Word Embedding (AWE), to train domain-specific word embeddings from sparse texts. Our method is generic and can leverage diverse types of domain knowledge such as domain vocabulary, semantic relations and attribute specifications. Specifically, our method encodes diverse types of domain knowledge as text annotations and incorporates the annotations in word embedding. We have evaluated AWE in two cybersecurity applications: identifying malware aliases and identifying relevant Common Vulnerabilities and Exposures (CVEs). Our evaluation results have demonstrated the effectiveness of our method over state-of-the-art baselines.",https://ieeexplore.ieee.org/document/8995379/,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),4-6 Nov. 2019,ieeexplore
10.1109/ICMSAO.2013.6552635,Incremental Bayesian network structure learning in high dimensional domains,IEEE,Conferences,"The recent advances in hardware and software has led to development of applications generating a large amount of data in real-time. To keep abreast with latest trends, learning algorithms need to incorporate novel data continuously. One of the efficient ways is revising the existing knowledge so as to save time and memory. In this paper, we proposed an incremental algorithm for Bayesian network structure learning. It could deal with high dimensional domains, where whole dataset is not completely available, but grows continuously. Our algorithm learns local models by limiting search space and performs a constrained greedy hill-climbing search to obtain a global model. We evaluated our method on different datasets having several hundreds of variables, in terms of performance and accuracy. The empirical evaluation shows that our method is significantly better than existing state of the art methods and justifies its effectiveness for incremental use.",https://ieeexplore.ieee.org/document/6552635/,"2013 5th International Conference on Modeling, Simulation and Applied Optimization (ICMSAO)",28-30 April 2013,ieeexplore
10.1109/DEVLRN.2014.6983001,Incremental training of Restricted Boltzmann Machines using information driven saccades,IEEE,Conferences,"In the context of developmental robotics, a robot has to cope with complex sensorimotor spaces by reducing their dimensionality. In the case of sensor space reduction, classical approaches for pattern recognition use either hardcoded feature detection or supervised learning. We believe supervised learning and hard-coded feature extraction must be extended with unsupervised learning of feature representations. In this paper, we present an approach to learn representations using space-variant images and saccades. The saccades are driven by a measure of quantity of information in the visual scene, emerging from the activations of Restricted Boltzmann Machines (RBMs). The RBM, a generative model, is trained incrementally on locations where the system saccades. Our approach is implemented using real data captured by a NAO robot in indoor conditions.",https://ieeexplore.ieee.org/document/6983001/,4th International Conference on Development and Learning and on Epigenetic Robotics,13-16 Oct. 2014,ieeexplore
10.1109/ASIANCON51346.2021.9544615,Indian Sign language Recognition Using Color Space Model and Thresholding,IEEE,Conferences,"Sign language is for deaf and mute people. Population as large as India's provides for an enormous section of people using Indian sign language to communicate. Unfortunately, a major chunk of the population does not understand sign language which limits communication between those with disabilities and wider population. We are proposing a system to harness this communication chasm. First the images are taken from webcam in RGB color space. Preprocessing and semantic segmentation are applied on the input image. By employing a simple background, the RGB segmented image is transformed to gray scale and background noise is removed. Otsu's segmentation method is used to segment the image. Convolutional Neural Networks were employed, utilizing skin segmented hand images as the input. On the training data, a classification accuracy of 99.33% was attained for 36 static hand gestures from Indian Sign Language. The above mentioned model performed significantly well in real time implementation.",https://ieeexplore.ieee.org/document/9544615/,2021 Asian Conference on Innovation in Technology (ASIANCON),27-29 Aug. 2021,ieeexplore
10.1109/BWCCA.2011.52,Indoor Location Fingerprinting Based on Data Reduction,IEEE,Conferences,"Agent localization in indoor wireless environments is a challenging issue. Numerous techniques have been developed. Location fingerprinting, which is based on received signal strength measurements, is a frequently used approach for indoor applications. In this paper, we examine the possibility to obtain the location fingerprinting method characterized with more accurate mapping between the signal-space and the physical-space. An implemented well-known weighted k-nearest neighbor (WkNN) method is enhanced by two steps: a) pre-processing by the unsupervised learning technique during radio map building and b) post-processing of initial estimates obtained by the WkNN localization method. In this post-processing step signal-space and physical-space are transformed and mapped using two techniques of the dimension reduction: principal component analysis and multidimensional scaling. The aim of this transformation step is to de-correlate and refine initially obtained location estimates. Parameters such as number of access points and number of nearest reference nodes are examined for their impact on accuracy of the presented localization techniques. Performances are examined and verified through the experiments with real environment data.",https://ieeexplore.ieee.org/document/6103053/,"2011 International Conference on Broadband and Wireless Computing, Communication and Applications",26-28 Oct. 2011,ieeexplore
10.1109/SMC.2013.183,Indoor Positioning with Virtual Fingerprint Mapping by Using Linear and Exponential Taper Functions,IEEE,Conferences,"A 2D localization system is constructed by using Wireless Sensor Nodes (WSN) to create a Virtual Fingerprint map. Linear and exponential taper functions are utilized with the received signal strength distributions between the fingerprint nodes to generate virtual fingerprint maps. Thus, a real and virtual combined fingerprint map is generated across the test area. k-NN and k-NN weighted algorithms have been implemented on virtual fingerprint maps to find the coordinates of the unknown objects. The system Localization accuracies of less than a grid space are obtained in calculations.",https://ieeexplore.ieee.org/document/6721936/,"2013 IEEE International Conference on Systems, Man, and Cybernetics",13-16 Oct. 2013,ieeexplore
10.1109/ITAIC.2019.8785597,Induction motor control system based on FOC algorithm,IEEE,Conferences,"In today's increasingly severe energy crisis, new energy vehicles have zero emissions, high efficiency, environmentally friendly features are sought after by people, influenced by policies and future market trends, the development of a highly efficient, versatile motor control It is particularly important. In this paper, the mathematical model of the high-power asynchronous motor of new energy vehicles is constructed. Analysis of its control principle, By analyzing the mathematical model of the three-wire AC asynchronous motor under the three-phase static coordinates, the three-phase coordinate system of the motor is decoupled into a two-phase rotating coordinate system similar to the DC motor by using the Clark transform Park variation mathematical formula. By detecting the phase current, the torque component and the excitation component of the motor are used in the above method, and the magnetic flux observer of the motor is designed by the method of magnetic field orientation. Based on the FOC field oriented control algorithm, TI's TMS320F28035 high-performance digital signal processor has developed an efficient and versatile induction motor control system with a speed loop and a current loop double closed loop. Controls the A/D to sample the voltage and current of the controller and build its current model. The real-time speed of the motor is detected using the incremental photoelectric encoder M/T method. The DSP-enhanced PWM (EPWM) module is used to generate high-precision flexible space vector pulse width modulation (SVPWM) signals, which greatly improves the control efficiency and speed range of the motor. By downloading the prepared control software to the controller hardware platform, the signal output test and the motor load test show that the controlled motor runs smoothly, the speed is stable, the corresponding speed is fast, and the parameters can be easily modified to adapt different motors, which saves the cost and development cycle for the enterprise.",https://ieeexplore.ieee.org/document/8785597/,2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),24-26 May 2019,ieeexplore
10.1109/GIOTS49054.2020.9119497,Industrial IoT and Digital Twins for a Smart Factory : An open source toolkit for application design and benchmarking,IEEE,Conferences,"The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the digital twin concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to close the gap between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry open data, and is bench-marked with universal testing tools.",https://ieeexplore.ieee.org/document/9119497/,2020 Global Internet of Things Summit (GIoTS),3-3 June 2020,ieeexplore
10.23919/ACC45564.2020.9147268,Inferential Methods for Additive Manufacturing Feedback,IEEE,Conferences,"Adaptive manufacturing has revolutionized desktop prototyping and the production of physical models for non-load bearing or stress inducing applications. Many extrusion-based printers are available for purchase by entrepreneurial enthusiasts or businesses with manufacturing space limitations. These low-cost printers allow for quick prototyping but are not designed or intended for high quality production or high-cycle production, requiring extensive user tuning and upkeep to maintain the printer in usable condition. In a quest to apply modern deep learning and reinforcement learning based models, this work focuses on the development of control systems and infrastructure needed to resolve many of these intrinsic limitations of desktop 3D printers. A series of real-time agents were designed and deployed to actively monitor the printing of every layer and make continuous corrections in the printing parameters and G-code commands to reduce the variance in the tensile strength of homogeneous parts printed in a large batch.",https://ieeexplore.ieee.org/document/9147268/,2020 American Control Conference (ACC),1-3 July 2020,ieeexplore
10.1109/MLSP.2014.6958856,Inferring clinical depression from speech and spoken utterances,IEEE,Conferences,"In this paper, we investigate the problem of detecting depression from recordings of subjects' speech using speech processing and machine learning. There has been considerable interest in this problem in recent years due to the potential for developing objective assessments from real-world behaviors, which may provide valuable supplementary clinical information or may be useful in screening. The cues for depression may be present in what is said (content) and how it is said (prosody). Given the limited amounts of text data, even in this relatively large study, it is difficult to employ standard method of learning models from n-gram features. Instead, we learn models using word representations in an alternative feature space of valence and arousal. This is akin to embedding words into a real vector space albeit with manual ratings instead of those learned with deep neural networks [1]. For extracting prosody, we employ standard feature extractors such as those implemented in openSMILE and compare them with features extracted from harmonic models that we have been developing in recent years. Our experiments show that our features from harmonic model improve the performance of detecting depression from spoken utterances than other alternatives. The context features provide additional improvements to achieve an accuracy of about 74%, sufficient to be useful in screening applications.",https://ieeexplore.ieee.org/document/6958856/,2014 IEEE International Workshop on Machine Learning for Signal Processing (MLSP),21-24 Sept. 2014,ieeexplore
10.1109/ICMLC.2008.4620601,Infrared passenger flow collection system based on RBF neural net,IEEE,Conferences,"As the current level of the people-counting, on the consideration of the cost of the acquisition equipment and the acquisition accuracy, the customer-counting system is constructed based on RBF neural net using technology of infrared photoelectric sensor. For better differential count, extant data segmentation and the method of feature extraction is improved based on the feature of passenger-counting data continuous space-time sequence. Compared with traditional customer-counting using sensor, the accuracy of real-time customer-counting is improved in this method and the situation of customers who entry at the same time can be identified with lower error rate. It is helpful for both principle research and actual application.",https://ieeexplore.ieee.org/document/4620601/,2008 International Conference on Machine Learning and Cybernetics,12-15 July 2008,ieeexplore
10.1109/SPW53761.2021.00016,Innocent Until Proven Guilty (IUPG): Building Deep Learning Models with Embedded Robustness to Out-Of-Distribution Content,IEEE,Conferences,"Deep Neural Network classifiers trained with the conventional Categorical Cross-Entropy loss face problems in real-world environments such as a tendency to produce overly confident posterior distributions on out-of-distribution inputs, sensitivity to adversarial noise, and lost performance due to distributional shift. We hypothesize that a central shortcoming -an inability to effectively process out-of-distribution content within inputs-exacerbates each of these setbacks. In response, we propose a novel learning framework called Innocent Until Proven Guilty which prototypes training data clusters or classes within the input space while uniquely leveraging noise and inherently random classes to discover noise-resistant, uniquely identifiable features of the modeled classes. In evaluation, we leverage both academic computer vision datasets and realworld JavaScript and URL datasets for malware classification. Across these interdisciplinary settings, we observe favorable classification performance on test data, decreased loss of performance due to recency bias, decreased false-positive responses on noise samples, and decreased vulnerability in several noisebased attack simulations when compared to a baseline network of equal topology trained with Categorical Cross-Entropy. To the best of our knowledge, ours is the first work that demonstrates significantly decreased vulnerability to blackbox append attacks on malware. By applying the well-known FastGradient Sign Method, we show the potential to combine our framework with existing adversarial learning techniques and discover favorable performance by a significant margin. Our framework is general enough for use with any network topology that could otherwise be trained with Categorical Cross-Entropy.",https://ieeexplore.ieee.org/document/9474279/,2021 IEEE Security and Privacy Workshops (SPW),27-27 May 2021,ieeexplore
10.1109/ICMLC.2007.4370315,Integrating Incremental Feature Weighting into Nave Bayes Text Classifier,IEEE,Conferences,"In the real-world operational environment, text classification systems should handle the problem of incomplete training set and no prior knowledge of feature space. In this regard, the most appropriate algorithm for operational text classification is the Nave Bayes since it is easy to incrementally update its pre-learned classification model and feature space. Our work mainly focuses on improving Nave Bayes classifier through feature weighting strategy. The basic idea is that parameter estimation of Nave Bayes can consider the degree of feature importance as well as feature distribution. In addition, we have extended a conventional algorithm for incremental feature update for developing a dynamic feature space in operational environment. Through experiments using the Reuters-21578 and the 20Newsgroup benchmark collections, we show that the traditional multinomial Nave Bayes classifier can be significantly improved by <sup>2</sup>-statistic based feature weighting.",https://ieeexplore.ieee.org/document/4370315/,2007 International Conference on Machine Learning and Cybernetics,19-22 Aug. 2007,ieeexplore
10.1109/AERO.1997.577990,Integrating autonomy technologies into an embedded spacecraft system-flight software system engineering for new millennium,IEEE,Conferences,"Deep Space 1 (DS1) is the first deep-space mission of NASA's New Millennium technology validation program. The DS1 flight software will validate five autonomy technologies: 1) Planner/Scheduler, which receives ground or on-board requests for spacecraft activities and schedules them to resolve any resource conflicts or timing constraints; 2) Smart Executive, which expands planned activities into lower-level commands, deduces required hardware configurations or other actions, and provides detection and avoidance of constraint violations; 3) Mode Identification and Reconfiguration engine, which incorporates models of hardware and software behavior, detects discrepancies due to hardware or software failures, and requests recovery actions via the Smart Executive. 4) Autonomous Navigation, which determines the spacecraft trajectory from images of asteroids against the celestial sphere, and autonomously adjusts the trajectory to reach the target asteroid or comet. 5) Beacon Monitoring, which uses radio carrier modification and telemetry summarization to simplify ground monitoring of spacecraft health. Integration of these technologies into the spacecraft flight software architecture has presented a number of system engineering challenges, Some of these technologies were developed in a research-oriented, non-real-time, artificial intelligence organizational culture while spacecraft software is typically developed in a strong real-time, algorithmically-oriented culture. The Navigation technology has been developed in a ground-based environment. Integration of these different cultures and mutual education of the software team has been achieved. An early rapid prototype of an existing spacecraft design proved very valuable in educating the team members and in working out the development process.",https://ieeexplore.ieee.org/document/577990/,1997 IEEE Aerospace Conference,13-13 Feb. 1997,ieeexplore
10.1109/IST.2013.6729736,Intelligent Tracking Teaching System based on monocular active vision,IEEE,Conferences,"Teacher detection and tracking is the most important and fundamental functionality in the implementation of the Intelligent Tracking Teaching System (ITTS). In order to track teacher's movement in real-time, face tracking in rostrum region is initiated by the normalized size face adaboost detection followed with the Expectation Maximization (EM) algorithm based on HSV color space and prediction of face position. The split line and position-based visual servo were adopted to realize the tracking strategy, which is trying to keep the teacher in the middle of image by controlling pan/tilt/zoom monocular camera in either rostrum region or classroom region. Furthermore, the student camera will adaptive pan/tilt during the interaction process between teacher and students, and a real-time display on the GUI window is given. The experiment results demonstrated fast and extremely smooth pursuit for teacher's motion despite time-varying variation in illumination, with sustained robustness to the change of pose (e.g., from frontal face to nearly back of head).",https://ieeexplore.ieee.org/document/6729736/,2013 IEEE International Conference on Imaging Systems and Techniques (IST),22-23 Oct. 2013,ieeexplore
10.1109/ITNEC52019.2021.9587115,Interactive Augmented Reality Application Design based on Mobile Terminal,IEEE,Conferences,"Augmented Reality could realize the integration of virtuality and reality and bring users an enhanced information space feeling. In this context, the design of interactive Augmented Reality application based on mobile terminal is described. Under the integrated development environment of Unity3D, ARFoundation and TensorFlowSharp are adopted to integrate virtual information with markers, and an application with functions of command interaction, plane recognition, image recognition and object recognition are implemented. The design scheme is introduced, and the realization processes are discussed in detail. The test shows that the Augmented Reality application provide rich information and natural way of interacting in three-dimensional virtual-real fusion mode, which improves the interactive experience and brings novelty and immersion to users. The immersive and interactive space improves the interactive experience and strengthens the effect of information transmission, which has certain application significance.",https://ieeexplore.ieee.org/document/9587115/,"2021 IEEE 5th Information Technology,Networking,Electronic and Automation Control Conference (ITNEC)",15-17 Oct. 2021,ieeexplore
10.1109/SNPD.2018.8441031,Interactive Distance Media Learning Collaborative Based on Virtual Reality with Solar System Subject,IEEE,Conferences,"The emergence of the Internet and various computer devices has revolutionized the learning process in various schools and colleges, both in terms of media and interaction methods. E-learning and distance learning is one of the revolutions of how education can be passed well through cyberspace media that are connected, organized, and integrated with each other. In this research will be developed distance learning media using virtual reality technology, where teachers and students can make communication and made an immersive learning process although they stayed in different places. However, they can have the same face-to-face conversation, sitting in one table, and made virtual meeting with their avatars like meet in the real world in space. As an example learning subject, this project will present solar system learning with interactive virtual reality media.",https://ieeexplore.ieee.org/document/8441031/,"2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",27-29 June 2018,ieeexplore
10.1109/STFSSD.2009.29,Intra Media Synchronization for Actual Feeling Video Model,IEEE,Conferences,"The video clips only have simple information that these are just recorded data. There is a need to make the video clips actual sensation and look real to satisfy users for resolve the needs. We suggest new video model, called AFVM (Actual Feeling Video Model), that is based on ontology concept. The purpose of this study is to provide AFVM and new format of space where containing divided videos for actual image expression.",https://ieeexplore.ieee.org/document/4804590/,2009 Software Technologies for Future Dependable Distributed Systems,17-17 March 2009,ieeexplore
10.1109/SP40000.2020.00073,Intriguing Properties of Adversarial ML Attacks in the Problem Space,IEEE,Conferences,"Recent research efforts on adversarial ML have investigated problem-space attacks, focusing on the generation of real evasive objects in domains where, unlike images, there is no clear inverse mapping to the feature space (e.g., software). However, the design, comparison, and real-world implications of problem-space attacks remain underexplored.This paper makes two major contributions. First, we propose a novel formalization for adversarial ML evasion attacks in the problem-space, which includes the definition of a comprehensive set of constraints on available transformations, preserved semantics, robustness to preprocessing, and plausibility. We shed light on the relationship between feature space and problem space, and we introduce the concept of side-effect features as the byproduct of the inverse feature-mapping problem. This enables us to define and prove necessary and sufficient conditions for the existence of problem-space attacks. We further demonstrate the expressive power of our formalization by using it to describe several attacks from related literature across different domains.Second, building on our formalization, we propose a novel problem-space attack on Android malware that overcomes past limitations. Experiments on a dataset with 170K Android apps from 2017 and 2018 show the practical feasibility of evading a state-of-the-art malware classifier along with its hardened version. Our results demonstrate that ""adversarial-malware as a service"" is a realistic threat, as we automatically generate thousands of realistic and inconspicuous adversarial applications at scale, where on average it takes only a few minutes to generate an adversarial app. Yet, out of the 1600+ papers on adversarial ML published in the past six years, roughly 40 focus on malware [15]-and many remain only in the feature space.Our formalization of problem-space attacks paves the way to more principled research in this domain. We responsibly release the code and dataset of our novel attack to other researchers, to encourage future work on defenses in the problem space.",https://ieeexplore.ieee.org/document/9152781/,2020 IEEE Symposium on Security and Privacy (SP),18-21 May 2020,ieeexplore
10.1109/ICCAIRO.2018.00041,Iterative Algorithm for Solving a Split Common Null Point Problem for Demicontractive Operators,IEEE,Conferences,"In this paper, first we introduce an iterative algorithm which does not require prior knowledge of operator norm and prove strong convergence theorem for approximating a solution of split common null point problem of demicontractive mappings in a real Hilbert space. Widely known the computation of algorithms involving the operator norm for solving split common null point problem may be difficult and for this reason, authors have recently started constructing iterative algorithms with a way of selecting the step-sizes such that the implementation of the algorithm does not require the calculation or estimation of the operator norm. We introduce a new algorithm for solving the split common null point problem for demicontractive mappings with a way of selecting the step-sizes such that the implementation of the algorithm does not require the calculation or estimation of the operator norm and then prove strong convergence of the sequence in real Hilbert spaces. Finally, we give some numerical examples to illustrate our main result.",https://ieeexplore.ieee.org/document/8698385/,"2018 International Conference on Control, Artificial Intelligence, Robotics & Optimization (ICCAIRO)",19-21 May 2018,ieeexplore
10.1109/SAHCN.2015.7338346,JICE: Joint data compression and encryption for wireless energy auditing networks,IEEE,Conferences,"Fine-grained real-time metering is a fundamental service of wireless energy auditing networks, where metering data is transmitted from embedded power meters to gateways for centralized processing, storage, and forwarding. Due to limited meter capability and wireless bandwidth, the increasing sampling rates and network scales needed to support new energy auditing applications pose significant challenges to metering data fidelity and secrecy. This paper exploits the compression and encryption properties of compressive sensing (CS) to design a joint data compression and encryption (JICE) approach that addresses these two challenges simultaneously. Compared with a conventional signal processing pipeline that compresses and encrypts data sequentially, JICE reduces computation and storage complexities due to its simple design. It thus leaves more processor time and available buffer space for handling lossy wireless transmissions. Moreover, JICE features a machine-learning-based reconfiguration mechanism that adapts its signal representation basis to changing power patterns autonomously. On a smart plug platform, we implemented JICE and several baseline approaches including downsampling, lossless compression, and the pipeline approach. Extensive testbed experiments show that JICE achieves higher data delivery ratios and lower recovery distortions under a range of realistic settings. In particular, JICE increases the number of meters supported by a gateway by 50%, compared with the pipeline approach, while keeping a distortion rate lower than 5%.",https://ieeexplore.ieee.org/document/7338346/,"2015 12th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)",22-25 June 2015,ieeexplore
10.1109/ICMLA.2018.00037,Joint Adversarial Domain Adaptation for Resilient WiFi-Enabled Device-Free Gesture Recognition,IEEE,Conferences,"Human gesture recognition plays a critical role in numerous applications of human-computer interaction. By analyzing how gesture alters the WiFi propagation among WiFi-enabled IoT devices to identify the gestures in a device-free manner could be a promising solution. However, existing methods require tedious data collection and labeling process each time being implemented in a new environment. The classifier constructed by SVM or random forest is vulnerable to spatial dynamics. In this paper, we proposed JADA, a novel unsupervised Joint adversarial domain adaptation (JADA) scheme that realizes accurate and resilient WiFi-enabled device-free gesture recognition without collecting and labeling training data in new environments. After constructing a source encoder and a source classifier in the source domain by convolutional neural network, JADA trains a target encoder and also fine-tunes the source encoder through adversarial learning to map both unlabeled target data and labeled source data to a domain-invariant feature space such that a domain discriminator cannot distinguish the domain labels of the data. After training a shared classifier with the labeled source data while fixing the parameters of the source encoder, we employ the trained target encoder to embed the test target samples into the domain-invariant feature space and infer its class using the shared classifier. We develop a novel Channel State Information (CSI) enabled IoT platform that could obtain fine-grained CSI time series data directly from IoT devices and transform them into CSI frames. Real-world experiments with COTS WiFi routers were conducted in 2 indoor environments. The experimental results demonstrate that JADA achieves 98.75% gesture recognition accuracy in the original environment. Moreover, when the environmental scenario is altered, it is able to reduce the domain discrepancy across domains without collecting any labeled data in the new context.",https://ieeexplore.ieee.org/document/8614062/,2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA),17-20 Dec. 2018,ieeexplore
10.1109/IECON.1995.483992,Joint torque optimization for redundant manipulators using neural networks,IEEE,Conferences,"One of the important applications for the resolution of redundant manipulators is torque optimization. To achieve this objective, finding out the most desirable configuration from the infinite number of possible configurations that satisfy the end-effector constraint is required. It has been previously shown that the pseudoinverse plays a crucial role in doing such calculations. In this work, the Tank-Hopfield (TH) network is adopted for pseudoinverse calculations and the connection weights of the network can be directly obtained from the known matrices at each sampling time. At acceleration level, the joint acceleration commands related to torque optimization are generated from the outputs of the network. Incorporating the TH network into the Null-Space (NS) algorithm allows a torque optimization to be implemented in real-time. Simulation results for a three-link planar manipulator are given to prove that the proposed scheme is efficient and practical.",https://ieeexplore.ieee.org/document/483992/,Proceedings of IECON '95 - 21st Annual Conference on IEEE Industrial Electronics,6-10 Nov. 1995,ieeexplore
10.1109/IVCNZ.2009.5378371,Kernel PCA of HOG features for posture detection,IEEE,Conferences,"Motivated by the non-linear manifold learning ability of the kernel principal component analysis (KPCA), we propose in this paper a method for detecting human postures from single images by employing KPCA to learn the manifold span of a set of HOG features that can effectively represent the postures. The main contribution of this paper is to apply the KPCA as a non-linear learning and open-set classification tool, which implicitly learns a smooth manifold from noisy data that scatter over the feature space. For a new instance of HOG feature, its distance to the manifold that is measured by its reconstruction error when mapping into the kernel space serves as a criterion for detection. And by combining with a newly developed KPCA approximation technique, the detector can achieve almost real-time speed with neglectable loss of performance. Experimental results have shown that the proposed method can achieve promising detection rate with relatively small size of positive training dataset.",https://ieeexplore.ieee.org/document/5378371/,2009 24th International Conference Image and Vision Computing New Zealand,23-25 Nov. 2009,ieeexplore
10.1109/ICIIECS.2015.7193182,Kernel centric machine learning classifiers for anomaly detection with real bank datasets,IEEE,Conferences,"The machine learning is more effective today in anomaly detection to improve the classification accuracy. The use of powerful kernel based learning is very practical in current trends may expose accurate results in real time database applications. In this context, we need to use the new and adorned machine learning classifiers. In this paper we have given very successful and emerged kernels SVM (Support Vector Machines) which uses the marginal hyperplane uniquely determine the classes by mapping of data and KPCA (Kernel Principal Component Analysis) is an extension to PCA. Both used to classify the data and detecting anomalies by transforming input space into high dimensional feature space. The SVM kernel is use non-linear mapping function and inner product replace with kernel ingredients. KPCA extract principal components from set of corresponding eigenvectors and used as threshold with reference to kernel width. The SVM and KPCA are implemented by taking one real-time bank dataset and other from UCI machine learning repository sets. Finally performance compared with non-kernel techniques (CART, k-NN, PLSDA, PCA) applied on same datasets using training and test set combinations.",https://ieeexplore.ieee.org/document/7193182/,"2015 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS)",19-20 March 2015,ieeexplore
10.1109/FMEC49853.2020.9144828,Keynote speech 3: Big data Computing and Machine Learning for Intelligent Transportation and Connected Vehicles,IEEE,Conferences,"We are developing machine learning algorithms and software to fuse real-time feeds from video cameras and traffic sensor data to generate real-time detection, classification, and space-time trajectories of individual vehicles and pedestrians. This information is then transmitted to a cloud-based system and then synthesized to create a real-time city-wide traffic palette. I will discuss our research on: Smart intersections: Space-time trajectories are used to understand and improve the safety and efficiency of the intersection. Using conflict points of the vehicle-pedestrian trajectories, we identify potential collisions, or near-misses, and how they are related to the state of the signal cycle (transition from green to yellow, from yellow to red, etc.) and the presence of other vehicles and pedestrians.  Smart system: We are developing efficient signal re-timing for different corridors by time of day and day of the week to reflect the changes in network demand. We are also developing machine learning techniques for real-time detection of incidents and accidents on arterial networks.  Smart interactions with connected and autonomous vehicles: We have developed signalized intersection control strategies and sensor fusion algorithms for jointly optimizing vehicle trajectories and signal control for a mixture of autonomous vehicles and traditional vehicles at every intersection",https://ieeexplore.ieee.org/document/9144828/,2020 Fifth International Conference on Fog and Mobile Edge Computing (FMEC),20-23 April 2020,ieeexplore
10.1109/PerComWorkshops51409.2021.9431114,Keynote: Explainable-by-design Deep Learning,IEEE,Conferences,"MACHINE and AI justifiably attract the attention and interest not only of the wider scientific community and industry, but also society and policy makers. However, even the most powerful (in terms of accuracy) algorithms such as deep learning (DL) can give a wrong output, which may be fatal. Due to the opaque and cumbersome model structure used by DL, some authors started to talk about a dystopian black box society. Despite the success in this area, the way computers learn is still principally different from the way people acquire new knowledge, recognise objects and make decisions. People do not need a huge amount of annotated data. They learn by example, using similarities to previously acquired prototypes, not by using parametric analytical models. Current ML approaches are focused primarily on accuracy and overlook explainability, the semantic meaning of the internal model representation, reasoning and its link with the problem domain. They also overlook the efforts to collect and label training data and rely on assumptions about the data distribution that are often not satisfied. The ability to detect the unseen and unexpected and start learning this new class/es in real time with no or very little supervision is critically important and is something that no currently existing classifier can offer. The challenge is to fill this gap between high level of accuracy and the semantically meaningful solutions. The most efficient algorithms that have fuelled interest towards ML and AI recently are also computationally very hungry - they require specific hardware accelerators such as GPU, huge amounts of labeled data and time. They produce parametrised models with hundreds of millions of coefficients, which are also impossible to interpret or be manipulated by a human. Once trained, such models are inflexible to new knowledge. They cannot dynamically evolve their internal structure to start recognising new classes. They are good only for what they were originally trained for. They also lack robustness, formal guarantees about their behaviour and explanatory and normative transparency. This makes problematic use of such algorithms in high stake complex problems such as aviation, health, bailing from jail, etc. where the clear rationale for a particular decision is very important and the errors are very costly. All these challenges and identified gaps require a dramatic paradigm shift and a radical new approach. In this talk the speaker will present such a new approach towards the next generation of computationally lean ML and AI algorithms that can learn in real-time using normal CPUs on computers, laptops, smartphones or even be implemented on chip that will change dramatically the way these new technologies are being applied. It is explainable-by-design. It focuses on addressing the open research challenge of developing highly efficient, accurate ML algorithms and AI models that are transparent, interpretable, explainable and fair by design. Such systems are able to self-learn lifelong, and continuously improve without the need for complete retraining, can start learning from few training data samples, explore the data space, detect and learn from unseen data patterns, collaborate with humans or other such algorithms seamlessly.",https://ieeexplore.ieee.org/document/9431114/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore
10.1109/AIMSEC.2011.6009874,Kinematics simulation of upper limb rehabilitant robot based on virtual reality techniques,IEEE,Conferences,"The wearable exoskeletal robot for upper extremity rehabilitation is taken as the research object. According to D-H method, an accurate three-dimensional mechanism model for the robot system is established by SolidWorks software. The virtual set was generated in Simulink/VRML to carry out dynamic simulation. The variable parameters were set based on robotic practical joint range movement. The simulation of all joints and terminal trajectory and space motion area provided theoretical basis for position control, remote control and trajectory planning, realizing the rehabilitation robot visualizations and system interaction.",https://ieeexplore.ieee.org/document/6009874/,"2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",8-10 Aug. 2011,ieeexplore
10.1109/CVPR.2019.00655,L3-Net: Towards Learning Based LiDAR Localization for Autonomous Driving,IEEE,Conferences,"We present L3-Net - a novel learning-based LiDAR localization system that achieves centimeter-level localization accuracy, comparable to prior state-of-the-art systems with hand-crafted pipelines. Rather than relying on these hand-crafted modules, we innovatively implement the use of various deep neural network structures to establish a learning-based approach. L3-Net learns local descriptors specifically optimized for matching in different real-world driving scenarios. 3D convolutions over a cost volume built in the solution space significantly boosts the localization accuracy. RNNs are demonstrated to be effective in modeling the vehicle's dynamics, yielding better temporal smoothness and accuracy. We comprehensively validate the effectiveness of our approach using freshly collected datasets. Multiple trials of repetitive data collection over the same road and areas make our dataset ideal for testing localization systems. The SunnyvaleBigLoop sequences, with a year's time interval between the collected mapping and testing data, made it quite challenging, but the low localization error of our method in these datasets demonstrates its maturity for real industrial implementation.",https://ieeexplore.ieee.org/document/8954371/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore
10.1109/IJCNN.2010.5596809,Label propagation through neuronal synchrony,IEEE,Conferences,"Semi-Supervised Learning (SSL) is a machine learning research area aiming the development of techniques which are able to take advantage from both labeled and unlabeled samples. Additionally, most of the times where SSL techniques can be deployed, only a small portion of samples in the data set is labeled. To deal with such situations in a straightforward fashion, in this paper we introduce a semi-supervised learning approach based on neuronal synchrony in a network of coupled integrate-and-fire neurons. For that, we represent the input data set as a graph and model each of its nodes by an integrate-and-fire neuron. Thereafter, we propagate the class labels from the seed samples to unlabeled samples through the graph by means of the emerging synchronization dynamics. Experimentations on synthetic and real data show that the introduced technique achieves good classification results regardless the feature space distribution or geometrical shape.",https://ieeexplore.ieee.org/document/5596809/,The 2010 International Joint Conference on Neural Networks (IJCNN),18-23 July 2010,ieeexplore
10.1109/CVPR.2008.4587406,Large margin pursuit for a Conic Section classifier,IEEE,Conferences,"Learning a discriminant becomes substantially more difficult when the datasets are high-dimensional and the available samples are few. This is often the case in computer vision and medical diagnosis applications. A novel conic section classifier (CSC) was recently introduced in the literature to handle such datasets, wherein each class was represented by a conic section parameterized by its focus, directrix and eccentricity. The discriminant boundary was the locus of all points that are equi-eccentric relative to each class-representative conic section. Simpler boundaries were preferred for the sake of generalizability. In this paper, we improve the performance of the two-class classifier via a large margin pursuit. When formulated as a non-linear optimization problem, the margin computation is demonstrated to be hard, especially due to the high dimensionality of the data. Instead, we present a geometric algorithm to compute the distance of a point to the non-linear discriminant boundary generated by the CSC in the input space. We then introduce a large margin pursuit in the learning phase so as to enhance the generalization capacity of the classifier. We validate the algorithm on real datasets and show favorable classification rates in comparison to many existing state-of-the-art binary classifiers as well as the CSC without margin pursuit.",https://ieeexplore.ieee.org/document/4587406/,2008 IEEE Conference on Computer Vision and Pattern Recognition,23-28 June 2008,ieeexplore
10.1109/FOAN.2017.8215259,Large-scale location-aware services in access: Hierarchical building/floor classification and location estimation using Wi-Fi fingerprinting based on deep neural networks,IEEE,Conferences,"One of key technologies for future large-scale location-aware services in access is a scalable indoor localization technique. In this paper, we report preliminary results from our investigation on the use of deep neural networks (DNNs) for hierarchical building/floor classification and floor-level location estimation based on Wi-Fi fingerprinting, which we carried out as part of a feasibility study project on Xi'an Jiaotong-Liverpool University (XJTLU) Campus Information and Visitor Service System. To take into account the hierarchical nature of the building/floor classification problem, we propose a new DNN architecture based on a stacked autoencoder for the reduction of feature space dimension and a feed-forward classifier for multi-label classification with argmax functions to convert multi-label classification results into multi-class classification ones. We also describe the demonstration of a prototype DNN-based indoor localization system for floor-level location estimation using real received signal strength (RSS) data collected at one of the buildings on the XJTLU campus. The preliminary results for both building/floor classification and floor-level location estimation clearly show the strengths of DNN-based approaches, which can provide near state-of-the-art performance with less parameter tuning and higher scalability.",https://ieeexplore.ieee.org/document/8215259/,2017 International Workshop on Fiber Optics in Access Network (FOAN),6-8 Nov. 2017,ieeexplore
10.1109/IROS45743.2020.9341460,Latent Replay for Real-Time Continual Learning,IEEE,Conferences,"Training deep neural networks at the edge on light computational devices, embedded systems and robotic platforms is nowadays very challenging. Continual learning techniques, where complex models are incrementally trained on small batches of new data, can make the learning problem tractable even for CPU-only embedded devices enabling remarkable levels of adaptiveness and autonomy. However, a number of practical problems need to be solved: catastrophic forgetting before anything else. In this paper we introduce an original technique named ""Latent Replay"" where, instead of storing a portion of past data in the input space, we store activations volumes at some intermediate layer. This can significantly reduce the computation and storage required by native rehearsal. To keep the representation stable and the stored activations valid we propose to slow-down learning at all the layers below the latent replay one, leaving the layers above free to learn at full pace. In our experiments we show that Latent Replay, combined with existing continual learning techniques, achieves state-of-the-art performance on complex video benchmarks such as CORe50 NICv2 (with nearly 400 small and highly non-i.i.d. batches) and OpenLORIS. Finally, we demonstrate the feasibility of nearly real-time continual learning on the edge through the deployment of the proposed technique on a smartphone device.",https://ieeexplore.ieee.org/document/9341460/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore
10.1109/WACV51458.2022.00373,Latent to Latent: A Learned Mapper for Identity Preserving Editing of Multiple Face Attributes in StyleGAN-generated Images,IEEE,Conferences,"Several recent papers introduced techniques to adjust the attributes of human faces generated by unconditional GANs such as StyleGAN. Despite efforts to disentangle the attributes, a request to change one attribute often triggers unwanted changes to other attributes as well. More importantly, in some cases, a human observer would not recognize the edited face to belong to the same person. We propose an approach where a neural network takes as input the latent encoding of a face and the desired attribute changes and outputs the latent space encoding of the edited image. The network is trained offline using unsupervised data, with training labels generated by an off-the-shelf attribute classifier. The desired attribute changes and conservation laws, such as identity maintenance, are encoded in the training loss. The number of attributes the mapper can simultaneously modify is only limited by the attributes available to the classifier  we trained a network that handles 35 attributes, more than any previous approach. As no optimization is performed at deployment time, the computation time is negligible, allowing real-time attribute editing. Qualitative and quantitative comparisons with the current state-of-the-art show our method is better at conserving the identity of the face and restricting changes to the requested attributes.",https://ieeexplore.ieee.org/document/9706683/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),3-8 Jan. 2022,ieeexplore
10.23919/ECC.2001.7076545,Lateral auto-pilot design for an agile missile using dynamic fuzzy neural networks,IEEE,Conferences,"This paper presents a new approach, which exploits the recently developed Dynamic Fuzzy Neural Networks (DFNN) learning algorithm. The DFNN is based on extended Radial Basis Function (RBF) neural networks, which are functionally equivalent to Takagi-Sugeno-Kang (TSK) fuzzy systems. The algorithm comprises 4 parts: (1) Criteria of rules generation; (2) Allocation of premise parameters; (3) Determination of consequent parameters and (4) Pruning technology. The salient characteristics of the approach are: (1) A hierarchical on-line self-organizing learning paradigm is employed so that not only parameters can be adjusted, but also the determination of structure can be self-adaptive without partitioning the input space a priori; (2) Fast learning speed can be achieved so that the system can be implemented in real time. The application of the proposed approach is demonstrated in application to a demanding, highly nonlinear, missile control design task. Scheduling on instantaneous incidence (a rapidly varying quantity) is well known to lead to considerable difficulties with classical gain-scheduling methods. It is shown that the methods proposed here can, however, be used to successfully design an effective intelligent controller.",https://ieeexplore.ieee.org/document/7076545/,2001 European Control Conference (ECC),4-7 Sept. 2001,ieeexplore
10.1109/FIE49875.2021.9637402,Learning Autonomous Driving in Tangible Practice: Development and On-Road Applications of a 1/10-Scale Autonomous Vehicle,IEEE,Conferences,"This Innovative Practice Work-In-Progress Paper presents a case of learning autonomous driving in tangible practice. As technology sustainably enhances the quality of life, intelligent systems continue to contribute solutions to some of the biggest challenges faced by humans. Autonomous vehicles offer humans the opportunity to increase transportation safety by reducing human errors on the road, preventing accidents, improving human productivity by reducing commuting time, and possibly mitigating air pollution. There is a critical shortage of educational and training programs in autonomous vehicles due to the high cost of full-size vehicles, computing and sensor equipment, and big lab space needed. To address this problem, we develop a 1/10-scale autonomous vehicle powered by pre-collision detection, lane tracking, and road sign recognition systems. The pre-collision system is built using ultrasonic sensors, and the Proportional-Integral-Derivative (PID) control is implemented to manipulate the vehicle's safety response. The Open-Source Computer Vision Library (OpenCV) is exploited to detect and process real-time on-road streaming video to enable lane-tracking and road sign recognition. AI techniques are utilized for the model training. Preliminary results of this work are presented and analyzed. We also discuss the future directions of this study.",https://ieeexplore.ieee.org/document/9637402/,2021 IEEE Frontiers in Education Conference (FIE),13-16 Oct. 2021,ieeexplore
10.1109/IROS51168.2021.9636547,Learning Contact-Rich Assembly Skills Using Residual Admittance Policy<sup>*</sup>,IEEE,Conferences,"Contact-rich assembly tasks may result in large and unpredictable forces and torques when the locations of the contacting parts are uncertain. The ability to correct the trajectory in response to haptic feedback and accomplish the task despite location uncertainties is an important skill. We hypothesize that this skill would facilitate generalization and support direct transfer from simulations to real world. To reduce sample complexity, we propose to learn a residual admittance policy (RAP). RAP is learned to correct the movements generated by a baseline policy in the framework of dynamic movement primitives. Given the reference trajectories generated by the baseline policy, the action space of RAP is limited to the admittance parameters. Using deep reinforcement learning, a deep neural network is trained to map task specifications to proper admittance parameters. We demonstrate that RAP handles uncertainties in board location, generalizes well over space, size and shape, and facilitates quick transfer learning. Most impressively, we demonstrate that the policy learned in simulations achieves similar robustness to uncertainties, generalization and performance when deployed on an industrial robot (UR5e) without further training. See accompanying video for demonstrations.",https://ieeexplore.ieee.org/document/9636547/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore
10.1109/ICPR48806.2021.9413196,Learning Defects in Old Movies from Manually Assisted Restoration,IEEE,Conferences,"We propose to detect defects in old movies, as the first step of a larger framework of old movies restoration by inpainting techniques. The specificity of our work is to learn a film restorer's expertise from a pair of sequences, composed of a movie with defects, and the same movie which was semiautomatically restored with the help of a specialized software. In order to detect those defects with minimal human interaction and further reduce the time spent for a restoration, we feed a U-Net with consecutive defective frames as input to detect the unexpected variations of pixel intensity over space and time. Since the output of the network is a mask of defect location, we first have to create the dataset of mask frames on the basis of restored frames from the software used by the film restorer, instead of classical synthetic ground truth, which is not available. These masks are estimated by computing the absolute difference between restored frames and defectuous frames, combined with thresholding and morphological closing. Our network succeeds in automatically detecting real defects with more precision than the manual selection with an all-encompassing shape, including some the expert restorer could have missed for lack of time.",https://ieeexplore.ieee.org/document/9413196/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore
10.23919/CNSM46954.2019.9012710,Learning From Evolving Network Data for Dependable Botnet Detection,IEEE,Conferences,"This work presents an emerging problem in real-world applications of machine learning (ML) in cybersecurity, particularly in botnet detection, where the dynamics and the evolution in the deployment environments may render the ML solutions inadequate. We propose an approach to tackle this challenge using Genetic Programming (GP) - an evolutionary computation based approach. Preliminary results show that GP is able to evolve pre-trained classifiers to work under evolved (expanded) feature space conditions. This indicates the potential use of such an approach for botnet detection under non-stationary environments, where much less data and training time are required to obtain a reliable classifier as new network conditions arise.",https://ieeexplore.ieee.org/document/9012710/,2019 15th International Conference on Network and Service Management (CNSM),21-25 Oct. 2019,ieeexplore
10.1109/ICTAI.2016.0048,Learning User Embedding Representation for Gender Prediction,IEEE,Conferences,"Predicting the gender of users in social media has aroused great interests in recent years. Almost all existing studies rely on the the content features extracted from the main texts like tweets or reviews. It is sometimes difficult to extract content information since many users do not write any posts at all. In this paper, we present a novel framework which uses only the users' ids and their social contexts for gender prediction. The key idea is to represent users in the embedding connection space. A user often has the social context of family members, schoolmates, colleagues, and friends. This is similar to a word and its contexts in documents, which motivates our study. However, when modifying the word embedding technique for user embedding, there are two major challenges. First, unlike the syntax in language, no rule is responsible for the composition of the social contexts. Second, new users were not seen when learning the representations and thus they do not have embedding vectors. Two strategies circular ordering and incremental updating are proposed to solve these problems. We evaluate our methodology on two real data sets. Experimental results demonstrate that our proposed approach is significantly better than the traditional graph representation and the state-of-the-art graph embedding baselines. It also outperforms the content based approaches by a large margin.",https://ieeexplore.ieee.org/document/7814608/,2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI),6-8 Nov. 2016,ieeexplore
10.1109/IJCNN.1993.716991,Learning goal-directed navigation as attractor dynamics for a sensory motor system. (An experiment by the mobile robot YAMABICO),IEEE,Conferences,"This paper describes experimental results based on the authors' prior-proposed scheme: learning of sensory-based, goal-directed behavior. The scheme was implemented on the mobile robot ""YAMABICO"" and learning of a set of goal-directed navigations were conducted. The experiment assumed that the robot receives no global information such as position nor prior environment model. Instead, the robot was trained to learn adequate maneuvering in the adopted workspace by building a correct mapping between a spatio-temporal sequence of sensory inputs and maneuvering outputs on a neural structure. The experimental results showed that sufficient training generated rigid dynamical structure of a fixed point and limit cycling in the sensory-based state space, which realized robust navigations of homing and cyclic routing even against certain changes of environment as well as miscellaneous noises in the real world.",https://ieeexplore.ieee.org/document/716991/,"Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)",25-29 Oct. 1993,ieeexplore
10.1109/CEC.2017.7969562,Learning of a tracker model from multi-radar data for performance prediction of air surveillance system,IEEE,Conferences,"A valid model of the air surveillance system performance is highly valued when making decisions related to the optimal control of the system. We formulate a model for a multi-radar tracker system by combining a radar performance model with a tracker performance model. A tracker as a complex software system is hard to model mathematically and physically. Our novel approach is to utilize machine learning to create a tracker model based on measurement data from which the input and target output for the model are calculated. The measured data comprises the time series of 3D coordinates of cooperative aircraft flights, the corresponding target detection recordings from multiple radars, and the related multi-radar track recordings. The collected data is used to calculate performance measures for the radars and the tracker at specific locations in the air space. We apply genetic programming to learning such rules from radar performance measures that explain tracker performance. The easily interpretable rules are intended to reveal the real behavior of the system providing comprehension for its control and further development. The learned rules allow predicting tracker performance level for the system control in all radar geometries, modes, and conditions at any location. In the experiments, we show the feasibility of our approach to learning a tracker model and compare our rule learner with two tree classifiers, another rule learner, a neural network, and an instance-based classifier using the real air surveillance data. The tracker model created by our rule learner outperforms the models by the other methods except for the neural network whose prediction performance is equal.",https://ieeexplore.ieee.org/document/7969562/,2017 IEEE Congress on Evolutionary Computation (CEC),5-8 June 2017,ieeexplore
10.1109/ICPR.2016.7899802,Learning opposites using neural networks,IEEE,Conferences,"Many research works have successfully extended algorithms such as evolutionary algorithms, reinforcement agents and neural networks using opposition-based learning (OBL). Two types of the opposites have been defined in the literature, namely type-I and type-II. The former are linear in nature and applicable to the variable space, hence easy to calculate. On the other hand, type-II opposites capture the oppositeness in the output space. In fact, type-I opposites are considered a special case of type-II opposites where inputs and outputs have a linear relationship. However, in many real-world problems, inputs and outputs do in fact exhibit a nonlinear relationship. Therefore, type-II opposites are expected to be better in capturing the sense of opposition in terms of the input-output relation. In the absence of any knowledge about the problem at hand, there seems to be no intuitive way to calculate the type-II opposites. In this paper, we introduce an approach to learn type-II opposites from the given inputs and their outputs using the artificial neural networks (ANNs). We first perform opposition mining on the sample data, and then use the mined data to learn the relationship between input x and its opposite x. We have validated our algorithm using various benchmark functions to compare it against an evolving fuzzy inference approach that has been recently introduced. The results show the better performance of a neural approach to learn the opposites. This will create new possibilities for integrating oppositional schemes within existing algorithms promising a potential increase in convergence speed and/or accuracy.",https://ieeexplore.ieee.org/document/7899802/,2016 23rd International Conference on Pattern Recognition (ICPR),4-8 Dec. 2016,ieeexplore
10.1109/ICTAI.2019.00220,Learning to Drive via Apprenticeship Learning and Deep Reinforcement Learning,IEEE,Conferences,"With the implementation of reinforcement learning (RL) algorithms, current state-of-art autonomous vehicle technology have the potential to get closer to full automation. However, most of the applications have been limited to game domains or discrete action space which are far from the real world driving. Moreover, it is very tough to tune the parameters of reward mechanism since the driving styles vary a lot among the different users. For instance, an aggressive driver may prefer driving with high acceleration whereas some conservative drivers prefer a safer driving style. Therefore, we propose an apprenticeship learning in combination with deep reinforcement learning approach that allows the agent to learn the driving and stopping behaviors with continuous actions. We use gradient inverse reinforcement learning (GIRL) algorithm to recover the unknown reward function and employ REINFORCE as well as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal policy. The performance of our method is evaluated in simulation-based scenario and the results demonstrate that the agent performs human like driving and even better in some aspects after training.",https://ieeexplore.ieee.org/document/8995417/,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),4-6 Nov. 2019,ieeexplore
10.1109/CSSE.2008.720,Learning to Rank with Bayesian Evidence Framework,IEEE,Conferences,"The problem of ranking has recently gained attention in data learning. The goal ranking is to learn a real-valued ranking function that induces a ranking or ordering over an instance space. In this paper, we apply popular Bayesian techniques on ranking support vector machine. We propose a novel differentiable loss function called trigonometric loss function with the desirable characteristic of natural normalization in the likelihood function, and then set up a Bayesian framework. In this framework, Bayesian inference is used to implement model adaptation, while keeping the merits of ranking SVM. Experimental results on data sets indicate the usefulness of this approach.",https://ieeexplore.ieee.org/document/4722718/,2008 International Conference on Computer Science and Software Engineering,12-14 Dec. 2008,ieeexplore
10.23919/DATE.2019.8714959,Learning to infer: RL-based search for DNN primitive selection on Heterogeneous Embedded Systems,IEEE,Conferences,"Deep Learning is increasingly being adopted by industry for computer vision applications running on embedded devices. While Convolutional Neural Networks' accuracy has achieved a mature and remarkable state, inference latency and throughput are a major concern especially when targeting low-cost and low-power embedded platforms. CNNs' inference latency may become a bottleneck for Deep Learning adoption by industry, as it is a crucial specification for many real-time processes. Furthermore, deployment of CNNs across heterogeneous platforms presents major compatibility issues due to vendor-specific technology and acceleration libraries.In this work, we present QS-DNN, a fully automatic search based on Reinforcement Learning which, combined with an inference engine optimizer, efficiently explores through the design space and empirically finds the optimal combinations of libraries and primitives to speed up the inference of CNNs on heterogeneous embedded devices. We show that, an optimized combination can achieve 45x speedup in inference latency on CPU compared to a dependency-free baseline and 2x on average on GPGPU compared to the best vendor library. Further, we demonstrate that, the quality of results and time ""to-solution"" is much better than with Random Search and achieves up to 15x better results for a short-time search.",https://ieeexplore.ieee.org/document/8714959/,"2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)",25-29 March 2019,ieeexplore
10.1109/ICCPS.2018.00048,Learning-Based Control Design for Deep Brain Stimulation,IEEE,Conferences,"By employing low-voltage electrical stimulation of the basal ganglia (BG) regions of the brain, deep brain stimulation (DBS) devices are used to alleviate the symptoms of several neurological disorders, including Parkinson's disease (PD). Recently, we have developed a Basal Ganglia Model (BGM) that can be utilized for design and evaluation of DBS devices. In this work, we focus on the use of a hardware (FPGA) implementation of the BGM platform to facilitate development of new control policies. Specifically, we introduce a design-time framework that allows for development of suitable control policies, in the form of electrical pulses with variable temporal patterns, while supporting tradeoffs between energy efficiency and efficacy (i.e., Quality-of-Control) of the therapy. The developed framework exploits machine learning and optimization based methods for design-space exploration where predictive behavior for any control configuration (i.e., temporal pattern) is obtained using the BGM platform that simulates physiological response to the considered control in real-time. To illustrate the use of the developed framework, in our demonstration we present how the BGM can be utilized for physiologically relevant BG modeling and design-state exploration for DBS controllers, as well as show the effectiveness of obtained controllers that significantly outperform conventional DBS controllers.",https://ieeexplore.ieee.org/document/8443755/,2018 ACM/IEEE 9th International Conference on Cyber-Physical Systems (ICCPS),11-13 April 2018,ieeexplore
10.1109/GLOBECOM42002.2020.9322310,Learning-Based Massive Beamforming,IEEE,Conferences,"Developing resource allocation algorithms with strong real-time and high efficiency has been an imperative topic in wireless networks. Conventional optimization-based iterative resource allocation algorithms often suffer from slow convergence, especially for massive multiple-input-multiple-output (MIMO) beamforming problems. This paper studies learningbased efficient massive beamforming methods for multi-user MIMO networks. The considered massive beamforming problem is challenging in two aspects. First, the beamforming matrix to be learned is quite high-dimensional in case with a massive number of antennas. Second, the objective is often time-varying and the solution space is not fixed due to some communication requirements. All these challenges make learning representation for massive beamforming an extremely difficult task. In this paper, by exploiting the structure of the most popular WMMSE beamforming solution, we propose convolutional massive beamforming neural networks (CMBNN) using both supervised and unsupervised learning schemes with particular design of network structure and input/output. Numerical results demonstrate the efficacy of the proposed CMBNN in terms of running time and system throughput.",https://ieeexplore.ieee.org/document/9322310/,GLOBECOM 2020 - 2020 IEEE Global Communications Conference,7-11 Dec. 2020,ieeexplore
10.1109/SYNCHROINFO.2019.8814156,Lightweight Machine Learning Classifiers of IoT Traffic Flows,IEEE,Conferences,"IoT traffic flows have different from traditional devices statistics and their classification become an important task because of the exponentially growing number of smart devices. Conventional Deep Packet Inspection systems that rely on inspection of open fields in TLS and DNS packets, and the trend of encrypting the open fields makes machine learning based systems the only viable option for future networks. Moreover, computational complexity of models becomes crucial for large-scale operations. In this work, we investigated whether simple models, such as Logistic Regression, SVM with linear kernel, and a Decision Tree, have suitable for real-world deployments performance of multiclass classification of IoT traces, given thoughtful features engineering. We introduced a new flow feature of categorical type that describes a set of TCP-flag fields within a flow. In addition, removal of correlated features and feature space transformation via PCA method showed their usefulness in terms of prediction complexity reduction. In order to account for online classification mode, we limited the maximal number of packets within a flow to 10. Moreover, to estimate the upper-bound performance with given features, we compared the simple algorithms with Random Forest, Gradient Boosting and a feed-forward neural network. We performed 4-fold cross-validation of models by metrics Accuracy and F1-measure. The test results demonstrated that the introduced feature increases F1-measure for logistic regression from 99.1% in the base case to 99.6%, thus closely approaching more computationally expensive models. Overall, the evaluation results demonstrated feasibility of a lightweight model for IoT flow classification task with the suitable for a practical deployment performance.",https://ieeexplore.ieee.org/document/8814156/,"2019 Systems of Signal Synchronization, Generating and Processing in Telecommunications (SYNCHROINFO)",1-3 July 2019,ieeexplore
10.1109/ETFA.2018.8502485,Linear Classification of Badly Conditioned Data,IEEE,Conferences,"We present a method for the fast and robust linear classification of badly conditioned data. In our considerations, badly conditioned data are such data which are numerically difficult to handle. Due to, e.g. a large number of features or a large number of objects representing classes as well as noise, outliers or incompleteness, the common software computation of the discriminating linear combination of features between classes fails or is extremely time consuming. The theoretical foundations of our approach are based on the single feature ranking, which allows fast calculation of the approximative initial classification boundary. For the increasing of classification accuracy of this boundary, the refinement is performed in the lower dimensional space. Our approach is tested on several datasets from UCI Reposi-tiory. Experimental results indicate high classification accuracy of the approach. For the modern real industrial applications such a method is especially suitable in the Cyber-Physical-System environments and provides a part of the workflow for the automated classifier design.",https://ieeexplore.ieee.org/document/8502485/,2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA),4-7 Sept. 2018,ieeexplore
10.1109/AERO.2016.7500514,Link performance analysis of multi-user detection techniques for W-band multi-beam satellites,IEEE,Conferences,"The existing literature about broadband satellite communications clearly indicates the exploitation of Extremely High Frequency (EHF) bandwidth portions and the massive employment of multi-beam satellites as key enabling technologies for terabit connectivity in the Space. In particular, increasing the reuse factor of frequency sub-bands, it is possible to boost multi-beam satellite capacity, provided that co-channel interference is conveniently reduced at the terminal side. To this aim, suitable multi-user detection techniques are required. In this paper, we shall analyze the link performance of basic theoretical multi-user detection techniques, namely: optimum Maximum-Likelihood (ML) and sub-optimum Minimum Mean Squared Error (MMSE) detection, in the framework of a W-band (70-110 GHz) multi-gigabit geostationary satellite system. At our best knowledge, some partial analyses have been proposed in literature, mostly in terms of reachable capacity and, anyway, not considering the multi-gigabit W-band case. The results shown in this paper will be the basis for the practical implementation of multi-user detection techniques for real-world multi-beam terminals that will have to consider stringent constraints in terms of affordable computational complexity.",https://ieeexplore.ieee.org/document/7500514/,2016 IEEE Aerospace Conference,5-12 March 2016,ieeexplore
10.1109/ISCAS51556.2021.9401231,Live Demonstration: Real-Time Multi-Hand Segmentation on Exhibition,IEEE,Conferences,"In this paper, we proposed a multi-hand segmentation on exhibition. In exhibition there are many objects with similar color such as skin clothes and the decoration close to skin color. First we made a lot of virtual image to make the datasets closed to the exhibition, and combined the palm and back of hand into same picture. Secondly we proposed a robustness neural network call ""Unet- Encoder Network (Unet-EN)"" to train this datasets. We use pruning method to reduce the parameter and increase the speed. We implemented on NVIDIA Jetson TX2. As a result, it can be implemented on some skin color space and supported multi-hand segmentation.",https://ieeexplore.ieee.org/document/9401231/,2021 IEEE International Symposium on Circuits and Systems (ISCAS),22-28 May 2021,ieeexplore
10.1109/RADAR.2017.7944428,Localized random projections for space-time adaptive processing,IEEE,Conferences,"High-dimensional multi-sensor radar data suffers from the well known curse of dimensionality. For example, in radar space time adaptive processing (STAP), training data from neighboring range cells is limited, since the statistical properties vary significantly over range and azimuth. Therefore, precluding straightforward implementation of standard detectors, for example, the whitening minimum variance distortionless response filter. Using random projections, we can reduce the dimension of the radar problem by random sampling, i.e. by projecting the data into a random d-dimensional subspace. The Johnson-Lindenstrauss (JL) theorem provides theoretical guarantees which explicitly states that the low dimensional data after random projections is only very slightly perturbed when compared to the data from the original problem in an l<sub>2</sub> norm sense. Random projections offers significant computational savings permitting possible real time solutions, however, at the cost of reducing the clairvoyant SINR for radar STAP. To alleviate this issue of SINR loss, we use localized random projections where the random projection matrix incorporates the look angle information, thereby minimizing the noise and interference effects from other angles, and increasing the SINR. We show that the resulting detector is CFAR, and the transformation matrix satisfies all the necessary conditions for the the JL theorem to hold.",https://ieeexplore.ieee.org/document/7944428/,2017 IEEE Radar Conference (RadarConf),8-12 May 2017,ieeexplore
10.1109/IJCNN.2014.6889658,Long-term learning behavior in a recurrent neural network for sound recognition,IEEE,Conferences,"In this paper, the long-term learning properties of an artificial neural network model, designed for sound recognition and computational auditory scene analysis in general, are investigated. The model is designed to run for long periods of time (weeks to months) on low-cost hardware, used in a noise monitoring network, and builds upon previous work by the same authors. It consists of three neural layers, connected to each other by feedforward and feedback excitatory connections. It is shown that the different mechanisms that drive auditory attention emerge naturally from the way in which neural activation and intra-layer inhibitory connections are implemented in the model. Training of the artificial neural network is done following the Hebb principle, dictating that ""Cells that fire together, wire together"", with some important modifications, compared to standard Hebbian learning. As the model is designed to be on-line for extended periods of time, also learning mechanisms need to be adapted to this. The learning needs to be strongly attention- and saliency-driven, in order not to waste available memory space for sounds that are of no interest to the human listener. The model also implements plasticity, in order to deal with new or changing input over time, without catastrophically forgetting what it already learned. On top of that, it is shown that also the implementation of short-term memory plays an important role in the long-term learning properties of the model. The above properties are investigated and demonstrated by training on real urban sound recordings.",https://ieeexplore.ieee.org/document/6889658/,2014 International Joint Conference on Neural Networks (IJCNN),6-11 July 2014,ieeexplore
10.1109/ISQED.2018.8357325,Low cost and power CNN/deep learning solution for automated driving,IEEE,Conferences,"Automated driving functions, like highway driving and parking assist, are increasingly getting deployed in high-end cars with the ultimate goal of realizing self-driving car using Deep learning techniques like convolution neural network (CNN). For mass-market deployment, the embedded solution is required to address the right cost and performance envelope along with security and safety. In the case of automated driving, one of the key functionality is finding drivable free space, which is addressed using deep learning techniques like CNN. These CNN networks pose huge computing requirements in terms of hundreds of GOPS/TOPS (Giga or Tera operations per second), which seems beyond the capability of today's embedded SoC. This paper covers various techniques consisting of fixed-point conversion, sparse multiplication, fusing of layers and network pruning, for tailoring on the embedded solution. These techniques are implemented on the device by means of optimized Deep learning library for inference. The paper concludes by demonstrating the results of a CNN network running in real time on TI's TDA2X embedded platform producing a high-quality drivable space output for automated driving.",https://ieeexplore.ieee.org/document/8357325/,2018 19th International Symposium on Quality Electronic Design (ISQED),13-14 March 2018,ieeexplore
10.1109/IJCNN.2005.1556213,Low order modeling for multiple moving sound synthesis using head-related transfer functions' principal basis vectors,IEEE,Conferences,An algorithm for simulating multiple moving-sound sources in a virtual auditory space is developed. Sound sources are localized with balanced model approximation of principal basis vectors extracted from the head-related transfer functions (HRTFs) dataset. This approach enables a low complexity implementation and real-time rendering of multiple sound sources in motion.,https://ieeexplore.ieee.org/document/1556213/,"Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.",31 July-4 Aug. 2005,ieeexplore
10.1109/ICASSP39728.2021.9414107,Low-Complexity Parameter Learning for OTFS Modulation Based Automotive Radar,IEEE,Conferences,"Orthogonal time frequency space (OTFS) as an emerging modulation technique in the 5G and beyond era exploits full time-frequency diversity and is robust against doubly-selective channels in high mobility scenarios. In this work, we consider an OTFS modulation based automotive joint radar-communication system and focus on the design of low-complexity parameter estimation algorithm for radar targets. It is well known that target parameter estimation in OTFS radar is computationally much more expensive than the orthogonal frequency division multiplex based platform, which hampers low-cost and real-time implementation. In this context, an efficient Bayesian learning scheme is proposed for OTFS automotive radars, which leverages the structural sparsity of radar channel in the delay-Doppler domain. We also reduce the dimension of the measurement matrix by incorporating the prior knowledge on the motion parameter limit of the true targets. Numerical simulation results are presented to demonstrate the superior performance of the proposed method in comparison with the state-of-the-art.",https://ieeexplore.ieee.org/document/9414107/,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",6-11 June 2021,ieeexplore
10.1109/TENCON.2018.8650452,ML-Based Approach to Detect DDoS Attack in V2I Communication Under SDN Architecture,IEEE,Conferences,"The need for Internet-based services is increasing at a tremendous pace in smart cities. The driver and occupants of the vehicle access Internet, and different intelligent transportation system (ITS) related services such as real-time traffic information, parking space availability, downloading the map, etc., in a vehicle to infrastructure communication (V2I) mode. In a highly dynamic network environment like vehicular network, software-defined networking (SDN) promises to be an ideal solution. However, it also opens doors for various distributed denial of service (DDoS) attacks. An attacker can easily flood short-lived spoofed flows and exhaust network resources. This motivates us to find a solution to detect the attacks in a V2I communication under SDN. In this paper, we propose a machine learning (ML) based DDoS attack detection. The proposed system uses various ML schemes, and few of them found to be accurate with a high detection rate and a relatively low false alarm rate.",https://ieeexplore.ieee.org/document/8650452/,TENCON 2018 - 2018 IEEE Region 10 Conference,28-31 Oct. 2018,ieeexplore
10.1109/WACV.2009.5403083,ML-fusion based multi-model human detection and tracking for robust human-robot interfaces,IEEE,Conferences,"A novel stereo vision system for real-time human detection and tracking on a mobile service robot is presented in this paper. The system integrates the individually enhanced stereo-based human detection, HOG-based human detection, color-based tracking, and motion estimation for the robust detection and tracking of humans with large appearance and scale variations in real-world environments. A new framework of maximum likelihood based multi-model fusion is proposed to fuse these four human detection and tracking models according to the detection-track associations in 3D space, which is robust to the possible missed detections, false detections, and duplicated responses from the individual models. Multi-person tracking is implemented in a sequential near-to-far way, which well alleviates the difficulties caused by human-over-human occlusions. Extensive experimental results demonstrate the robustness of the proposed system under real-world scenarios with large variations in lighting conditions, cluttered backgrounds, human clothes and postures, and complex occlusion situations. Significant improvements in human detection and tracking have been achieved. The system has been deployed on six robot butlers to serve drinks, and showed encouraging performance in open ceremony events.",https://ieeexplore.ieee.org/document/5403083/,2009 Workshop on Applications of Computer Vision (WACV),7-8 Dec. 2009,ieeexplore
10.1109/ICDE51399.2021.00146,MLCask: Efficient Management of Component Evolution in Collaborative Data Analytics Pipelines,IEEE,Conferences,"With the ever-increasing adoption of machine learning for data analytics, maintaining a machine learning pipeline is becoming more complex as both the datasets and trained models evolve with time. In a collaborative environment, the changes and updates due to pipeline evolution often cause cumbersome coordination and maintenance work, raising the costs and making it hard to use. Existing solutions, unfortunately, do not address the version evolution problem, especially in a collaborative environment where non-linear version control semantics are necessary to isolate operations made by different user roles. The lack of version control semantics also incurs unnecessary storage consumption and lowers efficiency due to data duplication and repeated data pre-processing, which are avoidable.In this paper, we identify two main challenges that arise during the deployment of machine learning pipelines, and address them with the design of versioning for an end-to-end analytics system MLCask. The system supports multiple user roles with the ability to perform Git-like branching and merging operations in the context of the machine learning pipelines. We define and accelerate the metric-driven merge operation by pruning the pipeline search tree using reusable history records and pipeline compatibility information. Further, we design and implement the prioritized pipeline search, which gives preference to the pipelines that probably yield better performance. The effectiveness of MLCask is evaluated through an extensive study over several real-world deployment cases. The performance evaluation shows that the proposed merge operation is up to 7.8x faster and saves up to 11.9x storage space than the baseline method that does not utilize history records.",https://ieeexplore.ieee.org/document/9458924/,2021 IEEE 37th International Conference on Data Engineering (ICDE),19-22 April 2021,ieeexplore
10.1109/ICMLA.2018.00161,Machine Cognition of Violence in Videos Using Novel Outlier-Resistant VLAD,IEEE,Conferences,"Understanding highly accurate and real-time violent actions from surveillance videos is a demanding challenge. Our primary contribution of this work is divided into two parts. Firstly, we propose a computationally efficient Bag-of-Words (BoW) pipeline along with improved accuracy of violent videos classification. The novel pipeline's feature extraction stage is implemented with densely sampled Histogram of Oriented Gradients (HOG) and Histogram of Optical Flow (HOF) descriptors rather than Space-Time Interest Point (STIP) based extraction. Secondly, in encoding stage, we propose Outlier-Resistant VLAD (OR-VLAD), a novel higher order statistics-based feature encoding, to improve the original VLAD performance. In classification, efficient Linear Support Vector Machine (LSVM) is employed. The performance of the proposed pipeline is evaluated with three popular violent action datasets. On comparison, our pipeline achieved near perfect classification accuracies over three standard video datasets, outperforming most state-of-the-art approaches and having very low number of vocabulary size compared to previous BoW Models.",https://ieeexplore.ieee.org/document/8614186/,2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA),17-20 Dec. 2018,ieeexplore
10.1109/I3DA48870.2021.9610915,Machine Learning-Based Room Classification for Selecting Binaural Room Impulse Responses in Augmented Reality Applications,IEEE,Conferences,"A key attribute of augmented reality (AR) applications is the matching reverberation of virtual sounds to the room acoustics of the real environment. However, especially in real-time scenarios where the properties of rapidly changing surroundings are unknown, creating a persistently coherent sound field synthesis within a real space is a challenging problem. While AR devices and their sensors can usually provide depth information within the field of view of the user, retrieving a complete geometric model requires significant time and user activity. Prior acoustic measurements or scans of the deployment area also severely limit many use cases, especially in the consumer sector. In this paper, we propose an automatic system that provides a fast selection of room categories and their corresponding binaural reverberation using only monoscopic images as input information. The proposed system combines existing approaches of machine learning (ML) based room classification and parametric synthesis of binaural room impulse responses (BRIRs) to provide room reverberation for arbitrary indoor environments. As a proof of concept, we present a demonstrator developed in Cycling74s Max linked to a python-based ML model. For the ML model, we use the convolutional neural network (CNN) GoogLeNet architecture trained on a subset of the Places365 data set. This subset contains 20 custom indoor room categories which are composed of the original categories that share similar acoustic properties. The demonstrator captures images and automatically selects binaural reverberation based on the predictions of the ML classifier. Monophonic stimuli are reverberated and presented using dynamic headphone-based binauralization.",https://ieeexplore.ieee.org/document/9610915/,2021 Immersive and 3D Audio: from Architecture to Automotive (I3DA),8-10 Sept. 2021,ieeexplore
10.1109/ISPASS.2017.7975264,Machine learning for performance and power modeling/prediction,IEEE,Conferences,"Effective design space exploration relies on fast and accurate pre-silicon performance and power models. Simulation is commonly used for understanding architectural tradeoffs, however many emerging workloads cannot even run on many full-system simulators. Even if you manage to run an emerging workload, it may be a tiny part of the workload, because detailed simulators are prohibitively slow. This talk presents some examples of how machine learning can be used to solve some of the problems haunting the performance evaluation field. An application for machine learning is in cross-platform performance and power prediction. If one model is slow to run real-world benchmarks/workloads, is it possible to predict/estimate its performance/power by using runs on another platform? Are there correlations that can be exploited using machine learning to make cross-platform performance and power predictions? A methodology to perform cross-platform performance/power predictions will be presented in this talk. Another application illustrating the use of machine learning to calibrate analytical power estimation models will be discussed. Yet another application for machine learning has been to create max power stressmarks. Manually developing and tuning so called stressmarks is extremely tedious and time-consuming while requiring an intimate understanding of the processor. In our past research, we created a framework that uses machine learning for the automated generation of stressmarks. In this talk, the methodology of the creation of automatic stressmarks will be explained. Experiments on multiple platforms validating the proposed approach will also be described.",https://ieeexplore.ieee.org/document/7975264/,2017 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),24-25 April 2017,ieeexplore
10.1109/TAI.2002.1180806,Maintenance scheduling of oil storage tanks using tabu-based genetic algorithm,IEEE,Conferences,"Due to the entry of Taiwan into WTO and the recently liberalized Petroleum Management Law, the oil market in Taiwan is liberalized and thus is becoming more competitive. However, the space limitation and the residents' increasing awareness of environmental protection issues in the island make international vendors unavoidably have to rent tanks from domestic oil companies. In order to help the leaseholder maximize revenue by increasing the availability of tanks, an efficient maintenance scheduling is needed. This paper introduces a tabu-based genetic algorithm (TGA) and its implementation for solving a real-world maintenance scheduling problem of oil storage tanks. TGA incorporates a tabu list to prevent inbreeding and utilizes an aspiration criterion to supply moderate selection pressure so that the selection efficiency is improved, and the population diversity is maintained. The experimental results validate that TGA outperform GA in terms of solution quality and convergence efficiency. Keywords: Tabu-based genetic algorithm, maintenance scheduling, tabu search, genetic algorithm.",https://ieeexplore.ieee.org/document/1180806/,"14th IEEE International Conference on Tools with Artificial Intelligence, 2002. (ICTAI 2002). Proceedings.",4-6 Nov. 2002,ieeexplore
10.1109/AIKE.2018.00051,Management of Subdivided Dynamic Indoor Environments by Autonomous Scanning System,IEEE,Conferences,"With the development of sensing technologies, various spatial applications have been expanding into indoor spaces. For smooth spatial services, grasping indoor space information is most essential task. However, the indoor spaces is not only becoming increasingly complex, but also frequently changed than outdoor spaces. This makes it hard to provide an accurate location based service in an indoor space. This paper propose a way of managing a dynamic indoor environment by defining a multi-layered indoor model in terms of an object mobility. It allows an indoor space to be managed more elaborate and realistic than up-to-date indoor models which only consider an indoor floor plan. We firstly define a classification of indoor objects based on their characteristic to frequently change location, and propose three-layers indoor model followed by the classified objects with its mobility. Secondly, we design and implement an autonomous scanning system to understand changes of indoor situation quickly and automatically. The system is made up of a combination of IoT devices, including a programmable robot, lidar scanner and single-board computer. Finally, we demonstrate an implementation of the system with constructing the proposed model from a real indoor environment.",https://ieeexplore.ieee.org/document/8527483/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore
10.1109/SiPS.2018.8598454,Mapping Systolic Arrays onto 3D Circuit Structures: Accelerating Convolutional Neural Network Inference,IEEE,Conferences,"In recent years, numerous designs have used systolic arrays to accelerate convolutional neural network (CNN) inference. In this work, we demonstrate that we can further speed up CNN inference and lower its power consumption by mapping systolic arrays onto 3D circuit structures as opposed to conventional 2D structures. Specifically, by operating in 3D space, a wide systolic array consisting of a number of subarrays can efficiently implement wide convolutional layers prevalent in state of the art CNNs. Additionally, by accumulating intermediate results along the third dimension, systolic arrays can process partitioned data channels in parallel with reduced data skew for lowered inference latency. We present a building block design using through-silicon vias (TSVs) for the 3D realization of systolic subarrays. We validate the 3D scheme using a 2.5D FPGA design and demonstrate that when mapped onto 3D structures wide systolic arrays can scale up in size without increasing wiring length in interconnecting subarrays. Further, by taking full advantage of 3D structures, we are able to pipeline inference across multiple layers of a CNN over a series of systolic arrays, dramatically reducing the inference time per input sample. These improvements lead to significantly reduced inference latency, which is especially important for real-time applications where it is common to process samples one at a time.",https://ieeexplore.ieee.org/document/8598454/,2018 IEEE International Workshop on Signal Processing Systems (SiPS),21-24 Oct. 2018,ieeexplore
10.1109/ICIP.2009.5414267,Margin and domain integrated classification,IEEE,Conferences,"Multi-category classification is an ongoing research topic with numerous applications. In this paper, a novel approach called margin and domain integrated classifier (MDIC) is addressed. It handles multi-class problems as a combination of several target classes plus outliers. The basic idea behind the proposed approach is that target classes possess structured characteristics while outliers scatter around in the feature space. In our approach the domain description and large-margin discrimination are adjustable and therefore higher classification accuracy leads to better performance. The properties of MDIC are analyzed and the performance comparisons using synthetic and real data are presented.",https://ieeexplore.ieee.org/document/5414267/,2009 16th IEEE International Conference on Image Processing (ICIP),7-10 Nov. 2009,ieeexplore
10.1109/DSAA.2019.00059,Maximum Relevance and Minimum Redundancy Feature Selection Methods for a Marketing Machine Learning Platform,IEEE,Conferences,"In machine learning applications for online product offerings and marketing strategies, there are often hundreds or thousands of features available to build such models. Feature selection is one essential method in such applications for multiple objectives: improving the prediction accuracy by eliminating irrelevant features, accelerating the model training and prediction speed, reducing the monitoring and maintenance workload for feature data pipeline, and providing better model interpretation and diagnosis capability. However, selecting an optimal feature subset from a large feature space is considered as an NP-complete problem. The mRMR (Minimum Redundancy and Maximum Relevance) feature selection framework solves this problem by selecting the relevant features while controlling for the redundancy within the selected features. This paper describes the approach to extend, evaluate, and implement the mRMR feature selection methods for classification problem in a marketing machine learning platform at Uber that automates creation and deployment of targeting and personalization models at scale. This study first extends the existing mRMR methods by introducing a non-linear feature redundancy measure and a model-based feature relevance measure. Then an extensive empirical evaluation is performed for eight different feature selection methods, using one synthetic dataset and three real-world marketing datasets at Uber to cover different use cases. Based on the empirical results, the selected mRMR method is implemented in production for the marketing machine learning platform. A description of the production implementation is provided and an online experiment deployed through the platform is discussed.",https://ieeexplore.ieee.org/document/8964172/,2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA),5-8 Oct. 2019,ieeexplore
10.1109/ICDE48307.2020.00144,Maxson: Reduce Duplicate Parsing Overhead on Raw Data,IEEE,Conferences,"JSON is a very popular data format in many applications in Web and enterprise. Recently, many data analytical systems support the loading and querying JSON data. However, JSON parsing can be costly, which dominates the execution time of querying JSON data. Many previous studies focus on building efficient parsers to reduce this parsing cost, and little work has been done on how to reduce the occurrences of parsing. In this paper, we start with a study with a real production workload in Alibaba, which consists of over 3 million queries on JSON. Our study reveals significant temporal and spatial correlations among those queries, which result in massive redundant parsing operations among queries. Instead of repetitively parsing the JSON data, we propose to develop a cache system named Maxson for caching the JSON query results (the values evaluated from JSONPath) for reuse. Specifically, we develop effective machine learning-based predictor with combining LSTM (long shortterm memory) and CRF (conditional random field) to determine the JSONPaths to cache given the space budget. We have implemented Maxson on top of SparkSQL. We experimentally evaluate Maxson and show that 1) Maxson is able to eliminate the most of duplicate JSON parsing overhead, 2) Maxson improves end-to-end workload performance by 1.5-6.5.",https://ieeexplore.ieee.org/document/9101499/,2020 IEEE 36th International Conference on Data Engineering (ICDE),20-24 April 2020,ieeexplore
10.1109/IVCNZ51579.2020.9290736,Melanoma and Nevi Classification using Convolution Neural Networks,IEEE,Conferences,"Early identification of melanoma skin cancer is vital for the improvement of patients' prospects of five year disease free survival. The majority of malignant skin lesions present at a general practice level where a diagnosis is based on a clinical decision algorithm. As a false negative diagnosis is an unacceptable outcome, clinical caution tends to result in a low positive predictive value of as low at 8%. There has been a large burden of surgical excisions that retrospectively prove to have been unnecessary.This paper proposes a method to identify melanomas in dermoscopic images using a convolution neural network (CNN). The proposed method implements transfer learning based on the ResNet50 CNN, pretrained using the ImageNet dataset. Datasets from the ISIC Archive were implemented during training, validation and testing. Further tests were performed on a smaller dataset of images taken from the Dermnet NZ website and from recent clinical cases still awaiting histological results to indicate the trained network's ability to generalise to real cases. The 86% test accuracy achieved with the proposed method was comparable to the results of prior studies but required significantly less pre-processing actions to classify a lesion and was not dependant on consistent image scaling or the presence of a scale on the image. This method also improved on past research by making use of all of the information present in an image as opposed to focusing on geometric and colour-space based aspects independently.",https://ieeexplore.ieee.org/document/9290736/,2020 35th International Conference on Image and Vision Computing New Zealand (IVCNZ),25-27 Nov. 2020,ieeexplore
10.1109/IIAI-AAI.2015.290,Memetic Self-Configuring Genetic Programming for Solving Machine Learning Problems,IEEE,Conferences,"A hybridization of self-configuring genetic programming algorithms (SelfCGPs) with a local search in the space of trees is fulfilled to improve their performance for symbolic regression problem solving and artificial neural network automated design. The local search is implemented with two neighborhood systems (1-level and 2-level neighborhoods), three strategies of a tree scanning (""full"", ""incomplete"" and ""truncated"") and two ways of a movement between adjacent trees (transition by the first improvement and the steepest descent). The Lamarckian local search is applied on each generation to ten percent of best individuals. The performance of all developed memetic algorithms is estimated on a representative set of test problems of the functions approximation as well as on real-world machine learning problems. It is shown that developed memetic algorithms require comparable amount of computational efforts but outperform the original SelfCGPs both for the symbolic regression and neural network design. The best variant of the local search always uses the steepest descent but different tree scanning strategies, namely, full scanning for the solving of symbolic regression problems and incomplete scanning for the neural network automated design. Additional advantage of the approach proposed is a possibility of the automated features selection.",https://ieeexplore.ieee.org/document/7373977/,2015 IIAI 4th International Congress on Advanced Applied Informatics,12-16 July 2015,ieeexplore
10.1109/ICRA40945.2020.9196540,Meta Reinforcement Learning for Sim-to-real Domain Adaptation,IEEE,Conferences,"Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.",https://ieeexplore.ieee.org/document/9196540/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore
10.1109/ICASSP39728.2021.9413978,Meta-Learning for Cross-Channel Speaker Verification,IEEE,Conferences,"Automatic speaker verification (ASV) has been successfully deployed for identity recognition. With increasing use of ASV technology in real-world applications, channel mismatch caused by the recording devices and environments severely degrade its performance, especially in the case of unseen channels. To this end, we propose a meta speaker embedding network (MSEN) via meta-learning to generate channel-invariant utterance embeddings. Specifically, we optimize the differences between the embeddings of a support set and a query set in order to learn a channel-invariant embedding space for utterances. Furthermore, we incorporate distribution optimization (DO) to stabilize the performance of MSEN. To quantitatively measure the effect of MSEN on unseen channels, we specially design the generalized cross-channel (GCC) evaluation. The experimental results on the HI-MIA corpus demonstrate that the proposed MSEN reduce considerably the impact of channel mismatch, while significantly outperforms other state-of-the-art methods.",https://ieeexplore.ieee.org/document/9413978/,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",6-11 June 2021,ieeexplore
10.1109/IJCNN.1991.170383,Micro-hardware implementation of a pattern recognition algorithm on a neuron-based multiprocessor system in a real-time environment,IEEE,Conferences,"The authors present an experimental study on a system which implements a pattern recognition algorithm (W.C. Lin, K.S. Fu, 1965, 1966) with a set of artificial neuron elements emulated on a multi-microprocessor hardware system. The algorithm is believed to be good for parallel processing utilizing the concept of information content, or entropy. The system used for the implementation study consists of four commercially available iSBC 286/12 single board microcomputers. They are configured to operate in parallel within the environment of the real-time operating system iRMX286. Thus, it is a multi-microprocessor and multitasking real-time processing hardware system. It is shown that the system with four microprocessors is 3.8 times faster than a single processor in operation speed with a recognition rate of 95%. Since the algorithm is a regionalism process in the feature space, the number of regions, and thus the number of microprocessors operating in parallel, can be increased to satisfy specific real-time requirements.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/170383/,[Proceedings] 1991 IEEE International Joint Conference on Neural Networks,18-21 Nov. 1991,ieeexplore
10.1109/SOFTWAREMINING.2017.8100847,Mining temporal intervals from real-time system traces,IEEE,Conferences,"We introduce a novel algorithm for mining temporal intervals from real-time system traces with linear complexity using passive, black-box learning. Our interest is in mining nfer specifications from spacecraft telemetry to improve human and machine comprehension. Nfer is a recently proposed formalism for inferring event stream abstractions with a rule notation based on Allen Logic. The problem of mining Allen's relations from a multivariate interval series is well studied, but little attention has been paid to generating such a series from symbolic time sequences such as system traces. We propose a method to automatically generate an interval series from real-time system traces so that they may be used as inputs to existing algorithms to mine nfer rules. Our algorithm has linear runtime and constant space complexity in the length of the trace and can mine infrequent intervals of arbitrary length from incomplete traces. The paper includes results from case studies using logs from the Curiosity rover on Mars and two other realistic datasets.",https://ieeexplore.ieee.org/document/8100847/,2017 6th International Workshop on Software Mining (SoftwareMining),3-3 Nov. 2017,ieeexplore
10.1109/AMS.2008.128,Misuse Intrusion Detection Using a Fuzzy-Metaheuristic Approach,IEEE,Conferences,"In this paper, we use simulated annealing heuristics for constructing an intrusion detection system (IDS). The proposed IDS combines the learning ability of simulated annealing heuristics with the approximate reasoning method of fuzzy systems. The use of simulated annealing is an effort to effectively explore the large search space related to intrusion detection problems, and find the optimum set of fuzzy if-then rules. The aim of this paper is to present the capability of simulated annealing based fuzzy system to deal with intrusion detection classification problem as a new real-world application area. Experiments were performed with KDD-Cup99 intrusion detection benchmark data set.",https://ieeexplore.ieee.org/document/4530516/,2008 Second Asia International Conference on Modelling & Simulation (AMS),13-15 May 2008,ieeexplore
10.1109/DSAA49011.2020.00024,Mix2Vec: Unsupervised Mixed Data Representation,IEEE,Conferences,"Unsupervised representation learning on mixed data is highly challenging but rarely explored. It has to tackle significant challenges related to common issues in real-life mixed data, including sparsity, dynamics and heterogeneity of attributes and values. This work introduces an effective and efficient unsupervised deep representer called Mix2Vec to automatically learn a universal representation of dynamic mixed data with the above complex characteristics. Mix2Vec is empowered with three effective mechanisms: random shuffling prediction, prior distribution matching, and structural informativeness maximization, to tackle the aforementioned challenges. These mechanisms are implemented as an unsupervised deep neural representer Mix2Vec. Mix2Vec converts complex mixed data into vector space-based representations that are universal and comparable to all data objects and transparent and reusable for both unsupervised and supervised learning tasks. Extensive experiments on four large mixed datasets demonstrate that Mix2Vec performs significantly better than state-of-the-art deep representation methods. We also empirically verify the designed mechanisms in terms of representation quality, visualization and capability of enabling better performance of downstream tasks.",https://ieeexplore.ieee.org/document/9260035/,2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA),6-9 Oct. 2020,ieeexplore
10.1109/CW.2019.00011,Mixed Reality User Interface for Astronauts Procedure Viewer,IEEE,Conferences,"This paper introduces a Proof-of-Concept (PoC) for Mixed Reality (MR) system to support an astronaut's manual work, the system is called MobiPV4Hololens. It has been developed in the European Space Agency's (ESA) project called ""MobiPV4Hololens - Prototype a Media Helmet for MobiPV Implemented Using Microsoft (MS) HoloLens"". The MS HoloLens mixed reality platform was integrated as the hands-free user interface to the ESA Mobile Procedure Viewer system called MobiPV. Based on the user evaluation most of the users believe that the MobiPV4Hololens is beneficial in supporting procedure execution.",https://ieeexplore.ieee.org/document/8919243/,2019 International Conference on Cyberworlds (CW),2-4 Oct. 2019,ieeexplore
10.1109/ROBOT.1998.681416,Mobile robot exploration and map-building with continuous localization,IEEE,Conferences,"Our research addresses how to integrate exploration and localization for mobile robots. A robot exploring and mapping an unknown environment needs to know its own location, but it may need a map in order to determine that location. In order to solve this problem, we have developed ARIEL, a mobile robot system that combines frontier based exploration with continuous localization. ARIEL explores by navigating to frontiers, regions on the boundary between unexplored space and space that is known to be open. ARIEL finds these regions in the occupancy grid map that it builds as it explores the world. ARIEL localizes by matching its recent perceptions with the information stored in the occupancy grid. We have implemented ARIEL on a real mobile robot and tested ARIEL in a real-world office environment. We present quantitative results that demonstrate that ARIEL can localize accurately while exploring, and thereby build accurate maps of its environment.",https://ieeexplore.ieee.org/document/681416/,Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146),20-20 May 1998,ieeexplore
10.1109/SAMI.2015.7061869,Model Predictive Control of a Ball and Plate laboratory model,IEEE,Conferences,"The papers presents an implementation of the predictive state space control algorithm, called Model Predictive Control (MPC). This control algorithm is verified on the Ball and Plate laboratory model, called B&amp;P_KYB, for the reference trajectory tracking. The control algorithm is first verified using the derived nonlinear simulation model in Matlab/Simulink. Since simulation results are acceptable, an experiment is realized on the real laboratory model. The results of the experiment are demonstrated as the time response of the ball position and the voltage.",https://ieeexplore.ieee.org/document/7061869/,2015 IEEE 13th International Symposium on Applied Machine Intelligence and Informatics (SAMI),22-24 Jan. 2015,ieeexplore
10.1109/VRAIS.1995.512486,Model based vision as feedback for virtual reality robotics environments,IEEE,Conferences,"Task definition methods for robotic systems are often difficult to use. The ""on-line"" programming methods are often time expensive or risky for the human operator or the robot itself. On the other hand, ""off-line"" techniques are tedious and complex. In addition operator training is costly and time consuming. In a Virtual Reality Robotics Environment (VRRE), users are not asked to write down complicated functions, but can operate complex robotic systems in an intuitive and cost-effective way. However a VRRE is only effective if all the environment changes and object movements are fed-back to the virtual manipulating system. The paper describes the use of a VRRE for a semi-autonomous robot system comprising an industrial 5-axis robot, its virtual equivalent and a model based vision system used as feed-back. The user is immersed in a 3-D space built out of models of the robot's environment. He directly interacts with the virtual ""components"", defining tasks and dynamically optimizing them. A model based vision system locates objects in the real workspace to update the VRRE through a bi-directional communication link. In order to enhance the capabilities of the VRRE, a reflex-type behavior based on vision has been implemented. By locally (independently of the VRRE) controlling the real robot, the operator is discharged of small environmental changes due to transmission delays. Thus once the tasks have been optimized on the VRRE, they are sent to the real robot and a semi autonomous process ensures their correct execution thanks to a camera directly mounted on the robot's end effector. On the other hand if the environmental changes are too important, the robot stops, re-actualizes the VRRE with the new environmental configuration, and waits for task redesign. Because the operator interacts with the robotic system at a task oriented high level, VRRE systems are easily portable to other robotics environments (mobile robotics and micro assembly).",https://ieeexplore.ieee.org/document/512486/,Proceedings Virtual Reality Annual International Symposium '95,11-15 March 1995,ieeexplore
10.1109/ROBOT.1992.220046,Model-driven pose correction,IEEE,Conferences,"Pose determination for robot navigation is discussed. The problem is to maintain the system's instantaneous precept of its position and orientation in space for performing various tasks. The authors describe a system in which models were used to guide the sensory interpretation and to correct expectations. In this system, simulated images were used to analyze the real images and to correct the pose parameters. The reported techniques have been implemented and experiments with real images in a real environment have been performed.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/220046/,Proceedings 1992 IEEE International Conference on Robotics and Automation,12-14 May 1992,ieeexplore
10.1109/ICTAI.2006.86,Modeling Workflow Systems with Genetic Planner and Scheduler,IEEE,Conferences,"Workflow systems have been widely employed by organizations in general; however, the generation of process models is still an area to be explored. Some works are concentrated in the use of planning techniques to solve problems; however, one of the difficulties in applying such techniques in workflow problems is the size of the search space required for the real world problems. An alternative is the use of evolutionary computing techniques, particularly genetic algorithms that in general are more suitable for these problems. In this context, we will present an architecture based on the use of a genetic planner in order to allow the automatic generation of process modeling. A simulation environment is also proposed by using scheduling techniques based on the use of genetic algorithms to identify the most suitable process model",https://ieeexplore.ieee.org/document/4031922/,2006 18th IEEE International Conference on Tools with Artificial Intelligence (ICTAI'06),13-15 Nov. 2006,ieeexplore
10.1109/SNPD.2012.138,Modelling Large Complex Systems Using Multi-agent Technology,IEEE,Conferences,"The paper outlines a method for modelling largescale commercial, social, socio-technological and engineering problems. The method is derived from twelve years of experience in designing and implementing large complex systems for real-time scheduling of taxis, air taxis, car rentals, seagoing tankers, trucks, space crafts; dynamic data mining; dynamic knowledge discovery and semantic search. The same approach has been also used for designing adaptive engineering systems and for research into social issues such as eradication of poverty.",https://ieeexplore.ieee.org/document/6299317/,"2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",8-10 Aug. 2012,ieeexplore
10.1109/ICIINFS.2017.8300340,Modelling and simulation analysis of the genetic-fuzzy controller for speed regulation of a sensored BLDC motor using MATLAB/SIMULINK,IEEE,Conferences,"This paper presents the speed regulation of a Sensored BLDC (Brushless Direct Current) Motor through a Genetic-Fuzzy controller, where the Sensored BLDC motor was modeled in MATLAB Simulink environment according to State-Space analysis approach. When designing Fuzzy Logic controllers (FLCs) there is no generalized defined approach and these controllers are mainly based on linguistically defined variables which are non-linear elements, that are impossible to model accurately. Our test results shows that the fuzzy controller's output highly depends on the fuzzy rules. In some situations, very experienced and a skillful expert's solutions (fuzzy rules) even may not satisfy the desired output. In many cases FLCs rule bases have been designed according to trial-and-error method which makes the optimization of the solution very difficult. As a solution, FLC of the Sensored BLDC motor was tuned through a stochastic search optimization technique which is based on GA (Genetic Algorithm) and the GA parameters (Crossover, Mutation rates etc.) adapted through another TSK-FLC (Takagi-Sugeno-Kang type FLC) in real-time. The optimization stochastic search process was implemented using a fitness function index (i.e. a predefined threshold level) which is calculated from the population (randomly generated solutions by the GA) based on the E (Error), MAE (Mean Absolute Error) and the RMSE (Root Mean Square Error). The simulated test results shows the proposed control technique has effectively reduced the maximum overshoot, settling time, steady state error and the rise time by 12%, 15%, 11% and 1% respectively. But further research is needed to optimize the search algorithm to increase the Genetic-Fuzzy controller's efficiency and the stability to withstand external disturbances while increasing the frequency for various desired input signal wave pattern trajectories.",https://ieeexplore.ieee.org/document/8300340/,2017 IEEE International Conference on Industrial and Information Systems (ICIIS),15-16 Dec. 2017,ieeexplore
10.1109/CCECE.1996.548197,Modelling of mines with a generic object model,IEEE,Conferences,"This article presents a generic object model (GNOME) and its application to the modelling of mines. GNOME is an hierarchical model and is composed essentially of two parts: one is topological and the other is quantitative. The objects are represented by their attributes (geometry, density, colour, etc.) which allow one to define the object-attributes (the objects in the attribute representation space) and thus the relations between them in attribute data and between different attributes. Several results from software using the mine representation following GNOME are shown.",https://ieeexplore.ieee.org/document/548197/,Proceedings of 1996 Canadian Conference on Electrical and Computer Engineering,26-29 May 1996,ieeexplore
10.1109/NCVPRIPG.2013.6776185,Monitoring a large surveillance space through distributed face matching,IEEE,Conferences,Large space with many cameras require huge storage and computational power to process these data for surveillance applications. In this paper we propose a distributed camera and processing based face detection and recognition system which can generate information for finding spatiotemporal movement pattern of individuals over a large monitored space. The system is built upon Hadoop Distributed File System using map reduce programming model. A novel key generation scheme using distance based hashing technique has been used for distribution of the face matching task. Experimental results have established effectiveness of the technique.,https://ieeexplore.ieee.org/document/6776185/,"2013 Fourth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)",18-21 Dec. 2013,ieeexplore
10.1109/ICMA.2019.8816292,Monitoring and Warning for Digital Twin-driven Mountain Geological Disaster,IEEE,Conferences,"With the unprecedented increase in the complexity of human space activities, geological disasters are increasing. In recent years, with the development of information technology and artificial intelligence, the monitoring and prediction of geological disasters requires real-time, dynamic and the early warning process to be more intelligent. A digital twin can better combine physical space and information space. Through the fusion of these two spaces, a digital disaster monitoring and early warning system based on digital twin driving is established. Firstly, the system uses Internet of Things, data-driven technology, and so on, combined with the BP neural network algorithm for dynamic prediction of geological disasters. Secondly, the disaster information is used to alert nearby residents through the Internet and base stations. On this basis, the operational mechanism, key technologies and implementation methods of geological disaster monitoring and early warning based on digital twin driving are discussed.",https://ieeexplore.ieee.org/document/8816292/,2019 IEEE International Conference on Mechatronics and Automation (ICMA),4-7 Aug. 2019,ieeexplore
10.1109/SMC52423.2021.9659268,Multi-Exemplar Learning Particle Swarm Optimization for Regional Traffic Signal Timing Optimization with Multi-Intersections,IEEE,Conferences,"Traffic congestion has become one of the major problems of smart travel. The application of evolutionary computation (EC) for traffic signal timing optimization (TSTO) can effectively alleviate traffic congestion at a single intersection. However, while in more complicated regional traffic signal timing optimization (RTSTO) problems, the canonical EC algorithm such as particle swarm optimization (PSO) still has limitation due to population prematurity. In this paper, a multi-exemplar learning (MEL) strategy is adopted to improve the diversity of the population, so that the particles can have more opportunities to explore the search space. Furthermore, multiple traffic indicators are used in this paper to measure the comprehensive performance of the solution. Moreover, the microsimulation software is adopted to evaluate the solution to simulate the real-world intersection, making the obtained solution more practical in real-world application. Experiments are conducted to investigate the effectiveness and efficiency of MEL-PSO. The results show that the MEL-PSO algorithm is more effective and efficient than the compared algorithms on RTSTO problems.",https://ieeexplore.ieee.org/document/9659268/,"2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",17-20 Oct. 2021,ieeexplore
10.1109/ICTAI.2011.89,Multi-agent Simulation Design Driven by Real Observations and Clustering Techniques,IEEE,Conferences,"The multi-agent simulation consists in using a set of interacting agents to reproduce the dynamics and the evolution of the phenomena that we seek to simulate. It is considered now as an alternative to classical simulations based on analytical models. But, its implementation remains difficult, particularly in terms of behaviors extraction and agents modelling. This task is usually performed by the designer who has some expertise and available observation data on the process. In this paper, we propose a novel way to make use of the observations of real world agents to model simulated agents. The modelling is based on clustering techniques. Our approach is illustrated through an example in which the behaviors of agents are extracted as trajectories and destinations from video sequences analysis. This methodology is investigated with the aim to apply it, in particular, in a retail space simulation for the evaluation of marketing strategies. This paper presents experiments of our methodology in the context of a public area modelling.",https://ieeexplore.ieee.org/document/6103379/,2011 IEEE 23rd International Conference on Tools with Artificial Intelligence,7-9 Nov. 2011,ieeexplore
10.1109/IJCNN.2015.7280577,Multi-kernel probability distribution regressions,IEEE,Conferences,"This paper presents a multi-layer reproducing kernel Hilbert space (RKHS) approach for probability distribution to real and probability distribution to function regressions. The approach maps the distributions into RKHS by distribution embeddings and, then, constructs a multi-layer RKHS within which the multi-kernel distribution regression can be implemented using an existing kernel regression algorithm, such as kernel recursive least squares (KRLS). The numerical simulations on synthetic data obtained via Gaussian mixtures show that the proposed approach outperforms existing probability distribution (DR) regression algorithms by achieving smaller mean squared errors (MSEs) and requiring less training samples.",https://ieeexplore.ieee.org/document/7280577/,2015 International Joint Conference on Neural Networks (IJCNN),12-17 July 2015,ieeexplore
10.1109/BigDataSecurity-HPSC-IDS49724.2020.00042,Multi-label Classification for Clinical Text with Feature-level Attention,IEEE,Conferences,"Multi-label text classification, which tags a given plain text with the most relevant labels from a label space, is an important task in the natural language process. To diagnose diseases, clinical researchers use a machine-learning algorithm to do multi-label clinical text classification. However, conventional machine learning methods can neither capture deep semantic information nor the context of words strictly. Diagnostic information from the EHRs (Electronic Health Records) is mainly constructed by unstructured clinical free text which is an obstacle for clinical feature extraction. Moreover, feature engineering is time-consuming and labor-intensive. With the rapid development of deep learning, we apply neural network models to resolve this problem mentioned above. To favor multi-label classification on EHRs, we propose FAMLC-BERT (Feature-level Attention for Multi-label classification on BERT) to capture semantic features from different layers. The model uses feature-level attention with BERT to recognize the labels of EHRs. We empirically compared our model with other state-of-the-art models on real-world documents collected from the hospital. Experiments show that our model achieved significant improvements compared to other selected benchmarks.",https://ieeexplore.ieee.org/document/9123057/,"2020 IEEE 6th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)",25-27 May 2020,ieeexplore
10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00189,Multi-network Embedding for Missing Point-of-Interest Identification,IEEE,Conferences,"The large volume of data flowing throughout location-based social networks (LBSNs) provides an opportunity for human mobility behavior understanding and prediction. However, data quality issues (e.g., historical check-in POI missing, data sparsity) limit the effectiveness of existing LBSN-oriented studies, e.g., Point-of-Interest (POI) recommendation or prediction. Contrary to previous efforts in next POI recommendation or prediction, we focus on identifying the missing POI which the user has visited at a past specific time and proposed a multi-network Embedding (MNE) method. Specifically, the model jointly captures temporal cyclic effect, user preference and sequence transition influence in a unified way by embedding five relational information graphs into a shared dimensional space from both POI- and category-instance levels. The proposed model also incorporates region-level spatial proximity to explore geographical influence, and derives the ranking score list of candidates for missing POI identification. We conduct extensive experiments to evaluate the performance of our model on two real large-scale datasets, and the experimental results show its superiority over other competitors. Significantly, it also proves that the proposed model can be naturally transferred to general next POI recommendation and prediction tasks with competitive performances.",https://ieeexplore.ieee.org/document/9644693/,"2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)",30 Sept.-3 Oct. 2021,ieeexplore
10.1109/ICCP.2013.6646076,Multi-objective DSE algorithms' evaluations on processor optimization,IEEE,Conferences,"Very complex micro-architectures, like complex superscalar/SMT or multicore systems, have lots of configurations. Exploring this huge design space and trying to optimize multiple objectives, like performance, power consumption and hardware complexity is a real challenge. In this paper, using the multi-objective design space exploration tool FADSE, we tried to optimize the hardware parameters of the complex superscalar Grid ALU Processor. We compared how different heuristic algorithms handle the DSE optimization. Three of these algorithms are taken from the jMetal library (NSGAII, SPEA2 and SMPSO) while the other two, CNSGAII and MOHC were implemented by us. We show that in this huge design space the differences between the best found individuals by every algorithm are very small, only the time in which they got to these solutions differs. In order to accelerate the DSE process we also did a feature selection through machine learning techniques and ran all DSE algorithms again with a smaller number of input parameters.",https://ieeexplore.ieee.org/document/6646076/,2013 IEEE 9th International Conference on Intelligent Computer Communication and Processing (ICCP),5-7 Sept. 2013,ieeexplore
10.1109/FUZZY.1997.616431,Multilayered fuzzy behavior fusion for reactive control of an autonomous mobile robot,IEEE,Conferences,"Fuzzy linguistic rules provide an intuitive and powerful means for defining control behavior. Most applications that use fuzzy control feature a single layer of fuzzy inference, mapping a function from one or two inputs to equally few outputs. Highly complex systems, however, may benefit from qualitative rules as well if the control task is properly partitioned. This paper presents a modular fuzzy control architecture and inference engine. A control function is broken down into multiple agents, each of which samples a subset of a large sensor input space. Additional fuzzy agents are employed to fuse the recommendations of the local agents. Real-time implementation without special hardware is possible by using singleton output values during fuzzy rule evaluation. Using this system, a fuzzy behavior-based reactive control system has been implemented on an autonomous mobile robot MARGE, with great success.",https://ieeexplore.ieee.org/document/616431/,Proceedings of 6th International Fuzzy Systems Conference,5-5 July 1997,ieeexplore
10.1109/SMCIA.2005.1466971,Multiobjective route selection for car navigation system using genetic algorithm,IEEE,Conferences,"Route planning is an important problem for a car navigation system. Given a set of origin-destination pair, there could be many possible routes for a driver. Search for shortest route from one point to another on a weighted graph is a well known problem and has several solutions like Dijkstra algorithm, Bellman-Ford algorithm etc. But in case of car navigation systems the shortest path may not be the best one from the point of view of driver's satisfaction. So, for a practical car navigation system in dynamical environment, we need to specify multiple and separate good (near optimal) choices according to multiple criteria which make the search space too large to find out the solution in real time by deterministic algorithms. Genetic algorithms (GA) are now widely used to solve search problems with applications in practical routing and optimization problems. GA includes a variety of quasi optimal solutions, which can be obtained in a given time. In this work we propose a GA based algorithm to find out simultaneously several alternate routes depending on different criterion according to driver's choice such as shortest path by distance, path which contains minimum number of turns, path passing through mountains or by the side of a river etc. The proposed algorithm has been evaluated by simulation experiment using real road map compared to other existing GA based algorithms. It has been found that the proposed algorithm is quite efficient in finding alternate non overlapping routes with different characteristics.",https://ieeexplore.ieee.org/document/1466971/,"Proceedings of the 2005 IEEE Midnight-Summer Workshop on Soft Computing in Industrial Applications, 2005. SMCia/05.",28-30 June 2005,ieeexplore
10.1109/CEC.2007.4424774,Multiple sensors data integration using MFAM for mobile robot navigation,IEEE,Conferences,"The mobile robot navigation with complex environment needs more input space to match the environmental data into robot outputs in order to perform realistic task. At the same time, the number of rules at the rule base needs to be optimized to reduce the computing time and to provide the possibilities for real time operation. In this paper, the optimization of fuzzy rules using a modified fuzzy associative memory (MFAM) is designed and implemented. MFAM provides good flexibility to use multiple input space and reduction of rule base for robot navigation. This paper presents the MFAM model to generate the rule base for robot navigation. The behavior rules obtained from MFAM model are tested using simulation and real world experiments, and the results are discussed in the paper and compared with the existing methods.",https://ieeexplore.ieee.org/document/4424774/,2007 IEEE Congress on Evolutionary Computation,25-28 Sept. 2007,ieeexplore
10.1109/WCICA.2000.862545,Multipurpose virtual-reality-based motion simulator,IEEE,Conferences,"Public security has become an important issue everywhere. Especially, the safe manipulation and control of various machines and vehicles has gained special attention such that the authorities keep emphasizing the strict training and censoring of human operators. Currently, such training and censoring process usually relies on the actual machines, equipment, or vehicles in the real sites. This not only has high demands in space, time and cost, but also causes another public security problem. In this connection, the world-wide trend is to tackle the above dilemma by using virtual reality (VR). However, the current researches or products on VR are more matured in the software display part of VR. How to combine 3D VR display with motion platform to achieve the aforementioned training and censoring purposes is an important research issue. This paper focuses on this research issue, and the goal is to develop a multipurpose virtual-reality-based motion simulation system to meet the requirements of public security in training and censoring of human operators.",https://ieeexplore.ieee.org/document/862545/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore
10.1109/IJCNN.2017.7966020,Multiscale Hebbian neural network for cyber threat detection,IEEE,Conferences,"The recent blaze in cyber espionage has posed unprecedented challenges to the cutting edge network intrusion detection systems in terms of accurate and precise classification of dynamically evolving threats. Along with the traditional signature based detection, the supervised and unsupervised machine learning algorithms are also being deployed to detect advance anomalies. However, due to the class overlap between the threat and legitimate data over feature space, satisfactory detection results cannot be obtained. This necessitates the introduction of cognition in the domain of cyber-security. In this paper, a wavelet based multiscale Hebbian learning approach in neural networks is introduced to address the challenge of class overlap. Contrary to inherently linear single scale Hebbian learning, the proposed methodology is able to distinguish non-linear and overlapping classification boundaries sufficiently well. A comparison of presented techniques with fundamental gradient descent based neural network shows promising results. Experimental results on simulated and real-world UNSW-NB15 dataset have been presented to support the claim.",https://ieeexplore.ieee.org/document/7966020/,2017 International Joint Conference on Neural Networks (IJCNN),14-19 May 2017,ieeexplore
10.1109/KIMAS.2003.1245070,Multisensor image fusion &amp; mining: from neural systems to COTS software,IEEE,Conferences,"We summarize our methods for the fusion of multisensor imagery based on concepts derived from neural models of visual processing and pattern learning and recognition. These methods have been applied to real-time fusion of night vision sensors in the field, airborne multispectral and hyperspectral imaging systems, and space-based multiplatform multimodality sensors. The methods enable color fused 3D visualization, as well as interactive exploitation and data mining in the form of human-guided machine learning and search for targets and cultural features. Over the last year we have developed a user-friendly system integrated into a COTS exploitation environment known as ERDAS Imagine. We demonstrate fusion and interactive mining of low-light Visible/SWIR/MWIR/LWIR night imagery, and IKONOS multispectral imagery. We also demonstrate how target learning and search can be enabled over extended operating conditions by allowing training over multiple scenes. This is illustrated for detecting small boats in coastal waters using fused Visible/MWIR/LWIR imagery.",https://ieeexplore.ieee.org/document/1245070/,IEMC '03 Proceedings. Managing Technologically Driven Organizations: The Human Side of Innovation and Change (IEEE Cat. No.03CH37502),30 Sept.-4 Oct. 2003,ieeexplore
10.1109/WARSD.2003.1295180,Multisensor image fusion and mining: from neural systems to COTS software with application to remote sensing AFE,IEEE,Conferences,"We summarize our methods for the fusion of multisensor/spectral imagery based on concepts derived from neural models of visual processing (adaptive contrast enhancement, opponent-color contrast, multi-scale contour completion, and multi-scale texture enhancement) and semi-supervised pattern learning and recognition. These methods have been applied to the problem of aided feature extraction (AFE) from remote sensing airborne multispectral and hyperspectral imaging systems, and space-based multi-platform multi-modality imaging sensors. The methods enable color fused 3D visualization, as well as interactive exploitation and data mining in the form of human-guided machine learning and search for objects, landcover, and cultural features. This technology has been evaluated on space-based imagery for the National Imagery and Mapping Agency, and real-time implementation has also been demonstrated for terrestrial fused-color night imaging. We have recently incorporated these methods into a commercial software platform (ERDAS Imagine) for imagery exploitation. We describe the approach and user interfaces, and show results for a variety of sensor systems with application to remote sensing feature extraction including EO/IR/MSI/SAR imagery from Landsat and Radarsat, multispectral Ikonos imagery, and Hyperion and HyMap hyperspectral imagery.",https://ieeexplore.ieee.org/document/1295180/,"IEEE Workshop on Advances in Techniques for Analysis of Remotely Sensed Data, 2003",27-28 Oct. 2003,ieeexplore
10.1109/CCA.1993.348267,Multivariable neural network vibration control based on output feedback,IEEE,Conferences,"This paper presents a multivariable direct adaptive control concept for vibration suppression in flexible space structures. The adaptive controller is implemented by a combination of forward neural networks. Tuning of the controller gains (neural network synaptic weights) takes place in real-time and is performed by a nonlinear least squares algorithm. The control scheme is based on output rather than state feedback, an approach motivated from the fact that, in most applications, the system state is not readily available. The results are demonstrated by simulation using a high fidelity 6-input 6-output dynamic model of the testbed at the JPL/USAF-PL Large Spacecraft Control Laboratory.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/348267/,Proceedings of IEEE International Conference on Control and Applications,13-16 Sept. 1993,ieeexplore
10.1109/IJCNN.2018.8489481,NEGAN:Network Embedding based on Generative Adversarial Networks,IEEE,Conferences,"Network embedding, also known as graph representation, is a classical topic in data mining. It has been widely used in real-world network applications such as node classification and community detection. However, it remains open to find a method that is scalable and preserves both structure and content information. Based on generative adversarial networks, we propose an unsupervised network embedding framework NEGAN, which is featured by combining graph topology and node content. In NEGAN, network nodes are mapped to the target space in a highly flexible non-linear way, guided by the content of the nodes. This mapping is learned from the generator of the generative adversarial networks, and node adjacency in the input network is preserved. Experiments on real datasets show that NEGAN outperforms all the existing methods on many scenarios including node classification, visualization and community detection tasks.",https://ieeexplore.ieee.org/document/8489481/,2018 International Joint Conference on Neural Networks (IJCNN),8-13 July 2018,ieeexplore
10.1109/MICRO.2016.7783724,NEUTRAMS: Neural network transformation and co-design under neuromorphic hardware constraints,IEEE,Conferences,"With the recent reincarnations of neuromorphic computing comes the promise of a new computing paradigm, with a focus on the design and fabrication of neuromorphic chips. A key challenge in design, however, is that programming such chips is difficult. This paper proposes a systematic methodology with a set of tools to address this challenge. The proposed toolset is called NEUTRAMS (Neural network Transformation, Mapping and Simulation), and includes three key components: a neural network (NN) transformation algorithm, a configurable clock-driven simulator of neuromorphic chips and an optimized runtime tool that maps NNs onto the target hardware for better resource utilization. To address the challenges of hardware constraints on implementing NN models (such as the maximum fan-in/fan-out of a single neuron, limited precision, and various neuron models), the transformation algorithm divides an existing NN into a set of simple network units and retrains each unit iteratively, to transform the original one into its counterpart under such constraints. It can support both spiking neural networks (SNNs) and traditional artificial neural networks (ANNs), including convolutional neural networks (CNNs) and multilayer perceptrons (MLPs) and recurrent neural networks (RNNs). With the combination of these tools, we have explored the hardware/software co-design space of the correlation between network error-rates and hardware constraints and consumptions. Doing so provides insights which can support the design of future neuromorphic architectures. The usefulness of such a toolset has been demonstrated with two different designs: a real Complementary Metal-Oxide-Semiconductor (CMOS) neuromorphic chip for both SNNs and ANNs and a processing-in-memory architecture design for ANNs.",https://ieeexplore.ieee.org/document/7783724/,2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),15-19 Oct. 2016,ieeexplore
10.1145/3326285.3329056,NFVdeep: Adaptive Online Service Function Chain Deployment with Deep Reinforcement Learning,IEEE,Conferences,"With the evolution of network function virtualization (NFV), diverse network services can be flexibly offered as service function chains (SFCs) consisted of different virtual network functions (VNFs). However, network state and traffic typically exhibit unpredictable variations due to stochastically arriving requests with different quality of service (QoS) requirements. Thus, an adaptive online SFC deployment approach is needed to handle the real-time network variations and various service requests. In this paper, we firstly introduce a Markov decision process (MDP) model to capture the dynamic network state transitions. In order to jointly minimize the operation cost of NFV providers and maximize the total throughput of requests, we propose NFVdeep, an adaptive, online, deep reinforcement learning approach to automatically deploy SFCs for requests with different QoS requirements. Specifically, we use a serialization-and-backtracking method to effectively deal with large discrete action space. We also adopt a policy gradient based method to improve the training efficiency and convergence to optimality. Extensive experimental results demonstrate that NFVdeep converges fast in the training process and responds rapidly to arriving requests especially in large, frequently transferred network state space. Consequently, NFVdeep surpasses the state-of-the-art methods by 32.59% higher accepted throughput and 33.29% lower operation cost on average.",https://ieeexplore.ieee.org/document/9068634/,2019 IEEE/ACM 27th International Symposium on Quality of Service (IWQoS),24-25 June 2019,ieeexplore
10.1109/IECON.2000.972604,NN controller of the constrained robot under unknown constraint,IEEE,Conferences,"In this paper, the problems faced in the constrained force control is studied (uncertainties in dynamic model and the unknown constraints). A neural network (NN) controller is proposed based on the derived dynamic model of robot in the task space. The feed-forward neural network is used to adaptively compensate for the uncertainties in the robot dynamics. Training signals are proposed for the feed-forward neural network controller. The NN weights are tuned online, with no off-line learning phase required. An online estimation algorithm is developed to estimate the local shape of the constraint surface by using measured data on the force and position of the end-effector. The suggested controller is simple in structure and can be implemented easily. Real-time experiments are conducted using the five-bar robot to demonstrate the effectiveness of the proposed controller.",https://ieeexplore.ieee.org/document/972604/,"2000 26th Annual Conference of the IEEE Industrial Electronics Society. IECON 2000. 2000 IEEE International Conference on Industrial Electronics, Control and Instrumentation. 21st Century Technologies",22-28 Oct. 2000,ieeexplore
10.1109/CVPR46437.2021.01403,NPAS: A Compiler-aware Framework of Unified Network Pruning and Architecture Search for Beyond Real-Time Mobile Acceleration,IEEE,Conferences,"With the increasing demand to efficiently deploy DNNs on mobile edge devices, it becomes much more important to reduce unnecessary computation and increase the execution speed. Prior methods towards this goal, including model compression and network architecture search (NAS), are largely performed independently, and do not fully consider compiler-level optimizations which is a must-do for mobile acceleration. In this work, we first propose (i) a general category of fine-grained structured pruning applicable to various DNN layers, and (ii) a comprehensive, compiler automatic code generation framework supporting different DNNs and different pruning schemes, which bridge the gap of model compression and NAS. We further propose NPAS, a compiler-aware unified network pruning and architecture search. To deal with large search space, we propose a meta-modeling procedure based on reinforcement learning with fast evaluation and Bayesian optimization, ensuring the total number of training epochs comparable with representative NAS frameworks. Our framework achieves 6.7ms, 5.9ms, and 3.9ms ImageNet inference times with 78.2%, 75% (MobileNet-V3 level), and 71% (MobileNet-V2 level) Top-1 accuracy respectively on an off-the-shelf mobile phone, consistently outperforming prior work.",https://ieeexplore.ieee.org/document/9578043/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore
10.1109/ICPPW.2009.6,Network Anomaly Detection Using Dissimilarity-Based One-Class SVM Classifier,IEEE,Conferences,"We present a new network anomaly detection system using dissimilarity-based one-class support vector machine( DSVMC). we transform the raw data into a dissimilarity space using Dissimilarity Representations (DR). DR describe objects by their dissimilarities to a set of target class. DSVMC are constructed on these DR. We propose a framework of anomaly detection using DSVMC. A new strategy of prototype selection has been proposed to obtain better DR. We not only offer a better approach in strategy to describe to distribution of large training dataset but also reduce the computational cost of prototype selection largely. In order to deploy the ADS in real-time detection application, we use Kernel Primary Component Analysis (KPCA) to reduce the dimension of transformed data. Evaluation has been made among traditional one-class classifiers, the dissimilarity-based one class SVM classifier without optimization of DR (WSVMC) and our DSVMC on KDDCUP' 99 dataset. The results show that DSVMC can achieve high detection rate than WSVMC and more robust performance than traditional one-class classifiers.",https://ieeexplore.ieee.org/document/5364550/,2009 International Conference on Parallel Processing Workshops,22-25 Sept. 2009,ieeexplore
10.1109/IJCNN.2004.1380187,Neural network approach for user activity monitoring in computer networks,IEEE,Conferences,"A system is proposed for user activity monitoring in computer networks. The system is based on the use of neural networks and is implemented using agent approach. The monitoring system allows to detect anomalies in user activity, and consists of two components-on-line and off-line. On-line monitoring is carried out in real time and is used to predict the processes started by an user on the basis of previous ones. Off-line monitoring is carried out at the end of the day and is based on the analysis of statistical parameters of user behavior (user signature). Both on-line and off-line monitoring use neural network approach to detect anomalies in user behavior. Proposed system was verified on real data obtained in Intranet of Space Research Institute of NASU-NSAU and Institute of Physics and Technologies of National Technical University of Ukraine ""Kiev Polytechnic Institute"".",https://ieeexplore.ieee.org/document/1380187/,2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541),25-29 July 2004,ieeexplore
10.1109/NNSP.2003.1318072,Neural network classifiers for automated video surveillance,IEEE,Conferences,"In automated visual surveillance applications, detection of suspicious human behaviors is of great practical importance. However due to random nature of human movements, reliable classification of suspicious human movements can be very difficult. Artificial neural network (ANN) classifiers can perform well however their computational requirements can be very large for real time implementation. In this paper, a data-based modeling neural network such as modified probabilistic neural network (MPNN) is introduced which partitions the decision space nonlinearly in order to achieve reliable classification, however still with acceptable computations. The experiment shows that the compact MPNN attains good classification performance compared to that of other larger conventional neural network based classifiers such as multilayer perceptron (MLP) and self organising map (SOM).",https://ieeexplore.ieee.org/document/1318072/,2003 IEEE XIII Workshop on Neural Networks for Signal Processing (IEEE Cat. No.03TH8718),17-19 Sept. 2003,ieeexplore
10.1109/CEC.1999.785521,Neural network training with constrained integer weights,IEEE,Conferences,"Presents neural network training algorithms which are based on the differential evolution (DE) strategies introduced by Storn and Price (J. of Global Optimization, vol. 11, pp. 341-59, 1997). These strategies are applied to train neural networks with small integer weights. Such neural networks are better suited for hardware implementation than the real weight ones. Furthermore, we constrain the weights and biases in the range [-2/sup k/+1, 2/sup k/-1], for k=3,4,5. Thus, they can be represented by just k bits. These algorithms have been designed keeping in mind that the resulting integer weights require less bits to be stored and the digital arithmetic operations between them are more easily implemented in hardware. Obviously, if the network is trained in a constrained weight space, smaller weights are found and less memory is required. On the other hand, the network training procedure can be more effective and efficient when large weights are allowed. Thus, for a given application, a trade-off between effectiveness and memory consumption has to be considered. We present the results of evolution algorithms for this difficult task. Based on the application of the proposed class of methods on classical neural network benchmarks, our experience is that these methods are effective and reliable.",https://ieeexplore.ieee.org/document/785521/,Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406),6-9 July 1999,ieeexplore
10.1109/MELCON.2000.879972,Neural networks arbitration for automatic edge detection of DNA bands in low-contrast images,IEEE,Conferences,"Low-contrast images, such as DNA autoradiograph images, provide a challenge for edge detection techniques, where the detection of the DNA bands within the images and locating their position is vital. In addition, the speed of recognition, high computational cost, and real-time implementation are also problems that haunt image processing. Thus, new measures are required to solve these problems. This paper reports on a new approach to solving the aforementioned problems. The novel idea is based on combining neural network arbitration and scale space analysis to automatically select one optimum scale for the entire image at which scale space edge detection can be applied. This approach to edge detection is formalised in the automatic edge detection scheme (AEDS). The AEDS is implemented on a real-life application namely, the detection of bands within low-contrast DNA autoradiograph images. An accurate comparison is drawn between the AEDS and the grammar-based multiscale analysis technique (GBMAT).",https://ieeexplore.ieee.org/document/879972/,2000 10th Mediterranean Electrotechnical Conference. Information Technology and Electrotechnology for the Mediterranean Countries. Proceedings. MeleCon 2000 (Cat. No.00CH37099),29-31 May 2000,ieeexplore
10.1109/ISCAS.1994.409629,Neural networks using bit stream arithmetic: a space efficient implementation,IEEE,Conferences,"In this paper an expandable digital architecture that provides an efficient implementation base for large neural networks, is presented. The architecture uses the circuit for arithmetic operations on delta encoded signals to carry out the large number of required parallel synaptic calculations. All real valued quantities are encoded on delta bit streams. The actual digital circuitry is simple and highly regular, thus allowing very efficient space usage of fine grained FPGAs.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/409629/,Proceedings of IEEE International Symposium on Circuits and Systems - ISCAS '94,30 May-2 June 1994,ieeexplore
10.1109/ISIC.1989.238676,Neuromorphic learning of continuous-valued mappings in the presence of noise: application to real-time adaptive control,IEEE,Conferences,The ability of feedforward neural net architectures to learn continuous-valued mappings in the presence of noise is demonstrated in relation to parameter identification and real-time adaptive control applications. Factors and parameters influencing the learning performance of such nets in the presence of noise are identified. Their effects are discussed through a computer simulation of the back-error-propagation algorithm by taking the example of the cart-pole system controlled by a nonlinear control law. Adequate sampling of the state space is found to be essential for canceling the effect of the statistical fluctuations and allowing learning to take place.&lt;<ETX>&gt;</ETX>,https://ieeexplore.ieee.org/document/238676/,Proceedings. IEEE International Symposium on Intelligent Control 1989,25-26 Sept. 1989,ieeexplore
10.1109/IPDPS.2014.59,Nitro: A Framework for Adaptive Code Variant Tuning,IEEE,Conferences,"Autotuning systems intelligently navigate a search space of possible implementations of a computation to find the implementation(s) that best meets a specific optimization criteria, usually performance. This paper describes Nitro, a programmer-directed auto tuning framework that facilitates tuning of code variants, or alternative implementations of the same computation. Nitro provides a library interface that permits programmers to express code variants along with meta-information that aids the system in selecting among the set of variants at run time. Machine learning is employed to build a model through training on this meta-information, so that when a new input is presented, Nitro can consult the model to select the appropriate variant. In experiments with five real-world irregular GPU benchmarks from sparse numerical methods, graph computations and sorting, Nitro-tuned variants achieve over 93% of the performance of variants selected through exhaustive search. Further, we describe optimizations and heuristics in Nitro that substantially reduce training time and other overheads.",https://ieeexplore.ieee.org/document/6877283/,2014 IEEE 28th International Parallel and Distributed Processing Symposium,19-23 May 2014,ieeexplore
10.1109/IPIN.2019.8911773,No-Sweat Detective: No Effort Anomaly Detection for Wi-Fi-Based Localization,IEEE,Conferences,"At present, Wi-Fi localization is the main approach to estimating location indoors. However, age deterioration of the localization model due to dynamic environmental changes degrades its accuracy. Therefore, periodic model recalibration is inescapable. Existing methods for doing this use transfer learning and a small set of additional and supervised datasets. However, the reference points to obtain these datasets are determined either randomly or comprehensively. Such poor datasets catastrophically destabilize model recovery after recalibration because overfitting occurs. We propose a new approach that detects anomalous reference points to gain felicitous supervised datasets in order to prevent overfitting. Unsupervised datasets obtained from off-the-shelf mobile navigation applications, i.e., user logs uploaded from phones, are used. Our approach is implemented in a system we call ""No-Sweat Detective"". The results of an experiment in a controlled environment demonstrate that No-Sweat Detective can detect anomalies caused by environmental changes, and the results of a five-month experiment show that No-Sweat Detective has redundancy against a complex open-space environment in the real world. In addition, it could suppress model age deterioration by up to 10.9% compared to existing methods.",https://ieeexplore.ieee.org/document/8911773/,2019 International Conference on Indoor Positioning and Indoor Navigation (IPIN),30 Sept.-3 Oct. 2019,ieeexplore
10.1109/IJCNN.2019.8851949,Noise-Aware Network Embedding for Multiplex Network,IEEE,Conferences,"Network embedding aims at learning the latent representations of nodes while preserving the complex structure of the underlying graph. Real-world networks are usually related with each other via common nodes, the so-called multiplex network. To make the data mining work on the multiplex network more actionable, it become urgent and essential to transform it into low-dimension vector space. Recently, several works have been proposed to leverage the complementary information for embedding. However, they suffer from sacrificing distinct properties of the counterparts in different layers, as they preserve much noise information into embedding vectors. In this paper, we propose a Noise-Aware Network Embedding approach for Multiplex Network, namely NANE. Unlike previous works, NANE considers the roles of an identical node in different layers, and adopts a more robust and flexible strategy to rationally integrate the cross-layer information while keeping the unique characteristic of each layer. We perform extensive evaluations on several real-world datasets. The experimental results demonstrate that our NANE can achieve better performance on link prediction task and significantly outperform previous methods especially in noisy multiplex network scenarios.",https://ieeexplore.ieee.org/document/8851949/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore
10.1109/UEMCON.2018.8796576,Non-Intrusive Activity Detection and Prediction in Smart Residential Spaces,IEEE,Conferences,"Non-intrusive human activity detection and prediction is an important and challenging problem in smart and pervasive spaces. The advantages of such a design are the reduced dependency on the users and fewer security/privacy concerns. However, these also make it difficult to effectively and accurately understand the activities in real-time. In residential spaces, this can be even a bigger challenge due to nonuniform space boundaries and multiple people sharing the space. In this paper, we present a system, that consists of both hardware and software components, capable of detecting and predicting human activities in a smart residential environment. Our system deploys a finite state machine-based activity detection with 96% accuracy in real-time. Afterwards, we use several machine learning methods to create an effective activity prediction framework. We demonstrate that we can achieve up to 98.5% activity prediction accuracy with 4ms delay, making it a perfect real-time system example. Since our smart and pervasive space implementation does not use any intrusive sensor or data acquisition unit (such as wearables, camera, or audio sources), we reduce the dependency to the user and potential security/privacy issues.",https://ieeexplore.ieee.org/document/8796576/,"2018 9th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",8-10 Nov. 2018,ieeexplore
10.1109/MLSP.2011.6064611,Non-parametric co-clustering of large scale sparse bipartite networks on the GPU,IEEE,Conferences,"Co-clustering is a problem of both theoretical and practical importance, e.g., market basket analysis and collaborative filtering, and in web scale text processing. We state the co-clustering problem in terms of non-parametric generative models which can address the issue of estimating the number of row and column clusters from a hypothesis space of an infinite number of clusters. To reach large scale applications of co-clustering we exploit that parameter inference for co-clustering is well suited for parallel computing. We develop a generic GPU framework for efficient inference on large scale sparse bipartite networks and achieve a speedup of two orders of magnitude compared to estimation based on conventional CPUs. In terms of scalability we find for networks with more than 100 million links that reliable inference can be achieved in less than an hour on a single GPU. To efficiently manage memory consumption on the GPU we exploit the structure of the posterior likelihood to obtain a decomposition that easily allows model estimation of the co-clustering problem on arbitrary large networks as well as distributed estimation on multiple GPUs. Finally we evaluate the implementation on real-life large scale collaborative filtering data and web scale text corpora, demonstrating that latent mesoscale structures extracted by the co-clustering problem as formulated by the Infinite Relational Model (IRM) are consistent across consecutive runs with different initializations and also relevant for interpretation of the underlaying processes in such large scale networks.",https://ieeexplore.ieee.org/document/6064611/,2011 IEEE International Workshop on Machine Learning for Signal Processing,18-21 Sept. 2011,ieeexplore
10.1109/IJCNN.2019.8851992,Nonlinear Transformation for Multiple Auxiliary Information in Music Recommendation,IEEE,Conferences,"Online music recommender systems are becoming increasingly prevalent because of the popularity of digital music and music recommendation generally caters to users by discovering songs that match their preferences. However, these systems have to face a challenge: how to recommend new songs in a situation where prior knowledge is scarce. Some researches take auxiliary information into consideration in new recommendation approaches to deal with this problem. Nevertheless, they rarely pay attention to complex relationships among different feature spaces when they map those information to a latent space. To this end, this paper proposes an approach that uses non-linear transformation to integrate different auxiliary information into the songs latent representations. Unlike other studies which directly map auxiliary information to the feature space, the proposed music recommendation model (NeuTrans) maps different information features to low-dimensional vector representation by non-linear neural networks. Specifically, the NeuTrans separately employs matrix factorization and attribute network embedding to extract auxiliary information (historical interaction, network structure and attributes of songs). The feature space of different information is obtained by nonlinearly mapping the feature space of the songs. Experimental analysis on two real-world datasets shows that our framework outperforms the state-of-the- art approaches on Top-N music recommendation.",https://ieeexplore.ieee.org/document/8851992/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore
10.1109/IJCNN.2011.6033598,Nonlinear multi-model ensemble prediction using dynamic Neural Network with incremental learning,IEEE,Conferences,"This paper introduces several nonlinear multi-model ensemble techniques for multiple chaotic models in high-dimensional phase space by means of artificial neural networks. A chaotic model is built by way of the time-delayed phase space reconstruction of the time series from observables. Several predictive global and local models, including Multi-layered Perceptron Neural Network (MLP-NN), are constructed and a number of multi-model ensemble techniques are implemented to produce more accurate hybrid models. One of these techniques is the nonlinear multi-model ensemble using one kind of dynamic neural network so-called Focused Time Delay Neural Network (FTDNN) with batch and incremental learning algorithms. The proposed techniques were used and tested for predicting storm surge dynamics in the North Sea. The results showed that the accuracy of multi-model ensemble predictions is generally improved in comparison to the one by single models. An FTDNN with incremental learning is more desirable for real-time operation, however in our experiments it was less accurate than batch learning.",https://ieeexplore.ieee.org/document/6033598/,The 2011 International Joint Conference on Neural Networks,31 July-5 Aug. 2011,ieeexplore
10.1109/EESCO.2015.7253955,Notice of Removal: Hybrid intelligent trail to search engine answering machine: Squat appraisal on pedestal technology (hybrid search machine),IEEE,Conferences,"Arched type Swing in loom of information retrieval system is observed with record progression to information fetch, to knowledge data processing, to intelligent information progression. Subsequent processing machines like document retrieval, text summarization, search engines, rule based machines, expert systems have been developed. These machines have dedicated performance with retrieval measure in particular dimension. Machine learning methods have facilitated reasoning machine with ability like humans. Still a corner in research argues highly intelligent time constraint fact seeking real world information processing machine. Hybrid technology is integration of optimized approaches at various levels of information processing. We proposed a hybrid search answer machine with four techniques of optimization question reformulation (from user-intent, profile), search method (semantic concept, context, machine learning), answer presentation (ranking algorithm), decision support  (comparative analysis to choose best techniques to retrieve results). Data corpus is heart of any IR system large dataset facilitates good search which argue to distributed data and computing. Intelligence is reformation proceeds that excel our time and dataset. The machine is designed to facilitated updatable training dataset for fact seeking knowledge acquirement it trains over data. Muti-agent model distributed search methodology is proposed. In precise Hybrid extraction of hybrid models is performed. Semantic context (concept based) user profiled; best machine learning, decision supportive multiagent distributed search system is proposed. This paper gives underlying technologies overview, with examinations of 30 papers is done as with recent review of technology advancement. The review outcomes are orderly placed with 3 research query answering. The outputs of query structure a trail to search engine answering machine. We facilitate research done by scholars on technology perspective we integrate them to draw a sketch of hybrid search answering. In domain a point of reference concepts of research are studied, with comparative views on advance in IR. We identify the benchmark of research methods blueprint and explore space of research in area of intelligent machine implementation.",https://ieeexplore.ieee.org/document/7253955/,"2015 International Conference on Electrical, Electronics, Signals, Communication and Optimization (EESCO)",24-25 Jan. 2015,ieeexplore
10.1109/INDIN45523.2021.9557354,Novelty Detection for Iterative Learning of MIMO Fuzzy Systems,IEEE,Conferences,"This paper proposes a methodology for iterative learning of multi-input multi-output (MIMO) fuzzy models focusing on dynamic system identification. The first step of the proposed method is the learning of the antecedent part of the fuzzy system, which is learned iteratively, where fuzzy rules can be added or merged based on the presented novelty detection and similarity criteria defined by a recursive extension of the Gath-Geva clustering algorithm. Then, the consequent part consists in the direct implementation of a non-recursive fuzzy approach that uses global least squares, Observer Kalman Filter Identification (OKID) and the Eigensystem Realization Algorithm (ERA). The proposed method is validated using experimental data from a real quadrotor aerial robot, a nonlinear dynamic system. Using quantitative performance metrics, the proposed method is compared with Hammerstein-Wiener models (H.-W.), nonlinear autoregressive models with exogenous input (NARX), and state-space models using subspace method with time-domain data (N4SID), other MIMO system identification techniques. The proposed method achieved better results compared to other techniques, showing the importance and versatility of learning based on novelty detection for MIMO problems.",https://ieeexplore.ieee.org/document/9557354/,2021 IEEE 19th International Conference on Industrial Informatics (INDIN),21-23 July 2021,ieeexplore
10.1109/ROBIO.2009.4913200,Object orientation recognition based on SIFT and SVM by using stereo camera,IEEE,Conferences,"The goal of this research is to recognize an object and its orientation in space by using stereo camera. The principle of object orientation recognition in this paper was based on the scale invariant feature transform (SIFT) and support vector machine (SVM). SIFT has been successfully implemented on object recognition but it had a problem recognizing the object orientation. For many autonomous robotics applications, such as using a vision-guided industrial robot to grab a product, not only correct object recognition will be needed in this process but also object orientation recognition is required. In this paper we used SVM to recognize object orientation. SVM has been known as a promising method for classification accuracy and its generalization ability. The stereo camera system adopted in this research provided more useful information compared to single camera one. The object orientation recognition technique was implemented on an industrial robot in a real application. The proposed camera system and recognition algorithms were used to recognize a specific object and its orientation and then guide the industrial robot to perform some alignment operations on the object.",https://ieeexplore.ieee.org/document/4913200/,2008 IEEE International Conference on Robotics and Biomimetics,22-25 Feb. 2009,ieeexplore
10.1109/IEMBS.2007.4353406,Obstacle Avoidance for Power Wheelchair Using Bayesian Neural Network,IEEE,Conferences,"In this paper we present a real-time obstacle avoidance algorithm using a Bayesian neural network for a laser based wheelchair system. The raw laser data is modified to accommodate the wheelchair dimensions, allowing the free- space to be determined accurately in real-time. Data acquisition is performed to collect the patterns required for training the neural network. A Bayesian frame work is applied to determine the optimal neural network structure for the training data. This neural network is trained under the supervision of the Bayesian rule and the obstacle avoidance task is then implemented for the wheelchair system. Initial results suggest this approach provides an effective solution for autonomous tasks, suggesting Bayesian neural networks may be useful for wider assistive technology applications.",https://ieeexplore.ieee.org/document/4353406/,2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,22-26 Aug. 2007,ieeexplore
10.1109/IECON.2006.347441,Obstacle avoidance algorithm based on biological patterns for anthropomorphic robot manipulator,IEEE,Conferences,"This study addresses the problem of collision-free controlling of 3-DOF (degree of freedom) anthropomorphic manipulators with given a priori unrestricted trajectory. The robot constraints resulting from the physical robot's actuators are also taken into account during the robot movement. Obstacle avoidance algorithm is based on penalty function, which is minimized when collision is predicted. Mathematical construction of penalty function and minimization process allows modeling of variety behaviors of robot elusion moves. Implementation of artificial neural network (ANN) inside the control process gives the additional flexibility needed to remember most important robot behaviors based on biological pattern of human arm moves. Thanks to the fast collisions' detection, the presented algorithm appears to be applicable to the industrial real-time implementations. Numerical simulations of the anthropomorphic manipulator operating in three dimensional space with obstacles is also presented",https://ieeexplore.ieee.org/document/4152937/,IECON 2006 - 32nd Annual Conference on IEEE Industrial Electronics,6-10 Nov. 2006,ieeexplore
10.23919/OCEANS44145.2021.9705808,Ocean current observations by infrared and visual Large Scale Particle Image Velocimetry (LSPIV),IEEE,Conferences,"The natural dynamics of tidal currents continuously change the appearance of the Wadden Sea. Especially coastal regions and bays with shallow waters are strongly influenced by tidal dynamics. Meanwhile, in offshore study areas with harsh environmental conditions as well as dynamic ocean currents and water levels, continuous in-situ investigations of the prevailing current conditions requires a high effort. In-situ measurement technology is thereby maintenance-intensive and partly limited in time or space by limited accessibility or restricted by the measurement method. In hence, remote sensing techniques based on video image information, such as Large Scale Particle Image Velocimetry (LSPIV) for sensing, and computer vision algorithms for long-term investigation of the prevailing dynamic near-surface ocean flow conditions, have become highly relevant. However, in some environmental situations there may be not sufficient or significant natural textures evaluable in the visual camera images, or only insufficient contrast ratio under varying ambient illumination at the water observation site, to perform image based velocimetry. Thus, we consider with this study an extended approach of visual and infrared video data analysis by an automated LSPIV measurement technique under real offshore deployment conditions. The treatise of this paper first introduces the reader to the subject area and technologies as well as the principles of the LSPIV measurement method. Followed by the depiction of the motivation for an extended approach to visual and infrared video data analysis by an automated LSPIV measurement method in real offshore applications. Subsequently, related research is discussed. Thereafter, the remote sensing setup and sensor-test-bed-system for offshore deployment on an observation platform is presented, therein we also addresses routines of necessary calibration procedures for camera sensors. We then depict details of our automated LSPIV measurement procedure. This is followed with an overview of the preceding validation procedures and LSPIV multispectral remote sensing results of long-term monitoring of horizontal flow dynamics over several days. Finally, we discuss uncertainties of the LSPIV velocity measurement method encountered in real applications and conclude with a brief outlook on further developments and applications.",https://ieeexplore.ieee.org/document/9705808/,OCEANS 2021: San Diego  Porto,20-23 Sept. 2021,ieeexplore
10.1109/IAdCC.2013.6514374,Off-line signature verification using Neural Networks,IEEE,Conferences,"This paper proposes a signature verification system that can authenticate a signature to avoid forgery cases. In the real world environment, it is often very difficult for any verification system to handle a huge collection of data, and to detect the genuine signatures with relatively good accuracy. Consequently, some artificial intelligence technique are used that can learn from the huge data set, in its training phase and can respond accurately, in its application phase without consuming much storage memory space and computational time. In addition, it should also have the ability to continuously update its knowledge from real time experiences. One such adaptive machine learning technique called a Multi-Layered Neural Network Model (NN Model) is implemented for the purpose of this work. Initially, a huge set of data is generated by collecting the images of several genuine and forgery signatures. The quality of the images is improved by using image processing followed by further extracting certain unique standard statistical features in its feature extraction phase. This output is given as the input to the above proposed NN Model to further improve its decision making capabilities. The performance of the proposed model is evaluated by calculating the fault acceptance and rejection rates for a small set of data. Further possible developments of this model are also outlined.",https://ieeexplore.ieee.org/document/6514374/,2013 3rd IEEE International Advance Computing Conference (IACC),22-23 Feb. 2013,ieeexplore
10.23919/ACC.1988.4789994,On Discrete Inner-Outer and Spectral Factorizations,IEEE,Conferences,"In this paper, reliable algorithms are developed to perform inner-outer, coprime, and spectral factorizations for discrete FDLTI systems. It is shown that the discrete algebraic Riccati equation plays an important role in obtaining state-space representations for all key factorizations. The implementation of algorithms can be carried out efficiently using real matrix operations.",https://ieeexplore.ieee.org/document/4789994/,1988 American Control Conference,15-17 June 1988,ieeexplore
10.1109/VETECS.2012.6239909,On the Effect of Gaussian Imperfect Channel Estimations on the Performance of Space Modulation Techniques,IEEE,Conferences,"Space modulation techniques, such as spatial modulation (SM) and space shift keying (SSK), are efficient low complexity implementation of multiple input multiple output (MIMO) systems. In such techniques, a single transmit-antenna is activated during each time instant and the activated antenna index is used to convey information. Due to the novel method of conveying information, a major criticism arises on the practicality of such techniques in the presence of real-time imperfections such as channel estimation errors. Therefore, the aim of this paper is to shed light on this issue. The performance of such systems are analyzed in the presence of Gaussian imperfect channel estimations. More specifically, the performance of SSK system consisting of N<sub>t</sub> transmit and N<sub>r</sub> receive antennas with maximum-likelihood (ML) detection and imperfect channel state information (CSI) at the receiver is studied. The exact average bit error probability (ABEP) over Rayleigh fading channels is obtained in closed-form for N<sub>t</sub> = 2 and arbitrary N<sub>r</sub>; while union upper bound is used to compute the ABEP when N<sub>t</sub> &gt;; 2 and arbitrary N<sub>r</sub>. Furthermore, simple and general asymptotic expression for the ABEP is derived and analyzed. Besides, the effect of imperfect CSI on the performance of SM, Alamouti and SSK schemes considering different number of channel estimation pilots are studied and compared via numerical Monte Carlo simulations. It is shown that, on the contrary to the raised criticism, space modulation techniques are more robust to channel estimation errors than Alamouti since the probability of error is determined by the differences between channels associated with the different transmit antennas rather than the actual channel realization.",https://ieeexplore.ieee.org/document/6239909/,2012 IEEE 75th Vehicular Technology Conference (VTC Spring),6-9 May 2012,ieeexplore
10.1109/ICITACEE.2016.7892481,On the implementation of ZFS (Zettabyte File System) storage system,IEEE,Conferences,"Digital data storage is very critical in computer systems. Storage devices used to store data may at any time suffer from damage caused by its lifetime, resource failure or factory defects. Such damage may lead to loss of important data. The risk of data loss in the event of device damage can be minimized by building a storage system that supports redundancy. The design of storage based on ZFS (Zettabyte File System) aims at building a storage system that supports redundancy and data integrity without requiring additional RAID controllers. When the system fails on one of its hard drive, the stored data remains secure and data integrity is kept assured. In addition to providing redundancy, the ZFS-based storage system also supports data compression for savings on storage space. The results show that the ZFS with LZ4 compression has the highest read and write speed. For real benchmark, there is no significant difference in reading speed for a variety of different variables, whereas a significant increase in speed occurs when writing compressible files on the ZFS system with compression configuration.",https://ieeexplore.ieee.org/document/7892481/,"2016 3rd International Conference on Information Technology, Computer, and Electrical Engineering (ICITACEE)",19-20 Oct. 2016,ieeexplore
10.1109/ACC.1995.532356,On the localization of feedforward networks,IEEE,Conferences,"Interference in neural networks occurs when learning in one area of the input space causes unlearning in another area. Networks that are less susceptible to interference are called spatially local networks. These networks are often used in neurocontrol, in online applications, where, because of the real time nature of the task, interference is often a problem. Although there are heuristics as to what makes a network local, there is no theoretical framework for measuring localization. This paper provides a formal definition of interference and localization that will allow measurement of a network's local properties. These definitions will be useful in developing learning algorithms that make networks more local. This may lead to faster learning over the entire input domain.",https://ieeexplore.ieee.org/document/532356/,Proceedings of 1995 American Control Conference - ACC'95,21-23 June 1995,ieeexplore
10.1109/CEC.2002.1007008,On the similarities between binary-coded GA and real-coded GA in wide search space,IEEE,Conferences,"This study sets out to identify a real-world system by employing binary-coded and real-coded genetic algorithms (GAs). However, in a nascent stage of setting configurations of GAs, it is difficult to determine the feasible boundaries of each parameter of the system. In this paper, both GAs are implemented based on the similar mechanisms of crossover and mutation, and performed on discretized linear, logarithmic, and hybrid search spaces with corresponding encoding methods. Through simple probabilistic analysis, it follows that logarithmic space and hybrid space searches are far more advantageous to general linear space search in wide bound searching. The identification results of a simple electrical circuit support this expectation and confirm that real-coded GA (RCGA) of logarithmic and hybrid space searches are the best way to tackle the real-world problem.",https://ieeexplore.ieee.org/document/1007008/,Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600),12-17 May 2002,ieeexplore
10.1109/IRSEC48032.2019.9078164,On the use of Deep Learning Approaches for Occupancy prediction in Energy Efficient Buildings,IEEE,Conferences,"Occupancy forecasting is considered as a crucial input for improving the performance of predictive control strategies in energy efficient buildings. In fact, accurate occupancy forecast is the key enabler for context-drive control of active systems (e.g. heating, ventilation, and lighting). This paper focuses on forecasting occupants' number using real-time measurements of CO2 concentration and its forecasting values. The main aim is to evaluate the accuracy of forecasting occupants' number by applying the steady state model (1) [16] on the CO2 forecast using recent deep learning approaches. The LSTM, a recurrent neural network based deep learning algorithm, is deployed to forecast the CO2 level in a dedicated space, a testlab deployed in our university for conducting experiments and assess approaches for energy efficiency in buildings. Preliminary results show the effectiveness of LSTM in forecasting occupants' number, which reaches 70% in accuracy.",https://ieeexplore.ieee.org/document/9078164/,2019 7th International Renewable and Sustainable Energy Conference (IRSEC),27-30 Nov. 2019,ieeexplore
10.1109/IPDPSW.2017.131,On-FPGA real-time processing of biological signals from high-density MEAs: a design space exploration,IEEE,Conferences,"High-density microelectrode arrays (HDMEAs) are promising tools to tackle fundamental questions in neuroscience and brain diseases with unprecedented experimental capabilities. The acquisition of the biological signals sampled by such MEAs, that usually involves filtering, preliminary processing and finally data storage, is an intrinsically parallel and computation-intensive activity, particularly in systems targeting thousands of recording channels acquired with sub-millisecond time resolution. Within several applications, these operations need to be performed in real-time. A promising solution offering an adequate performance level relies on parallel hardware structures, making FPGA devices the perfect target technology. In this paper, we present an evaluation of an acquisition and processing system, to be implemented on an FPGA device, which is conceived to be connected to multi-channel CMOS-MEAs and is specifically designed for in-vitro and in-vivo recordings of neural activity. The template, implemented on reconfigurable logic, performs the first steps of the computing chain: filtering and adaptive detection of neural spikes. The filtered samples together with information regarding the presence of spikes are stored in an external DDR memory, for further elaboration and communication with the external environment. We performed a design space exploration measuring resource utilization and precision of the detection algorithm for different use-cases, corresponding to different state-of-the-art HDMEAs, and for different application parameters, such as the filtering scheme, number of parallel input channels, and sampling frequency. A prototype instance of the proposed platform, implemented on a low-end Xilinx Zynq SoC, allows to process more than 1 Gbps of data coming from up to 4096 18-kHz channels, within a time latency of 1.8 ms.",https://ieeexplore.ieee.org/document/7965040/,2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),29 May-2 June 2017,ieeexplore
10.1109/NAECON.2018.8556744,Onboard Image Processing for Small Satellites,IEEE,Conferences,"In general, the computational ability of spacecraft and satellites has lagged behind terrestrial computers by several generations. Moore's Law turns the supercomputers of yesterday into the laptops of today, but space computing remains relatively underpowered due to the harsh radiation environment and low risk-tolerance of most space missions. Space missions are generally low risk because of the high cost of components and launch. However, launch costs are drastically decreasing and innovations such as CubeSats are changing the risk equation. By accepting more risk and utilizing commercial of the shelf (COTS) parts, it is possible to cheaply build and launch extremely capable computing platforms into space. High performance satellites will be required for advanced interplanetary exploration due to latency challenges. The long transmission times between planets means satellites or robotic explorers need onboard processing to perform tasks in real-time. This paper explores one possible application that could be hosted onboard the next generation of high performance satellites, performing object classification on satellite imagery. Automation of satellite imagery processing is currently performed by servers or workstations on Earth, but this paper will show that those algorithms can be moved onboard satellites by using COTS components. First traditional computer vision techniques such as edge detection and sliding windows are used to detect possible objects on the open ocean. Then a modern neural network architecture is used to classify the object as a ship or not. This application is implemented on a Nvidia Jetson TX2 and measurements of the application's power use confirm that it fits within the Size Weight and Power (SWAP) requirements of SmallSats and possibly even CubeSats.",https://ieeexplore.ieee.org/document/8556744/,NAECON 2018 - IEEE National Aerospace and Electronics Conference,23-26 July 2018,ieeexplore
10.1109/SMC.2018.00121,One-Class Classification Using Quasi-Linear Support Vector Machine,IEEE,Conferences,"This paper proposes a novel method for one-class classification by using support vector machine (SVM) based on a divide-and-conquer strategy. An s% winner-take-all autoencoder is applied to realize a sophisticated partitioning which divides the dataset into many clusters. For each cluster, data points are separated from the origin in the feature space like a traditional one-class SVM (OCSVM). By designing a gated linear network, and generating the gate signal from the autoencoder, the proposed OCSVM is implemented in an exact same way as a standard OCSVM with a quasi-linear kernel composed by using a base kernel with the gate signals. Comparing to a traditional OCSVM, the proposed quasi-linear OCSVM is expected to capture a more compact region in the input space. The compact region will decrease the probability of outlier objects falling inside the domain of classifier, which give a better performance. The proposed quasi-linear OCSVM method is applied to different real-world datasets, and simulation results confirm the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/8616117/,"2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",7-10 Oct. 2018,ieeexplore
10.1109/SMC42975.2020.9283154,One-array Differential Evolution Algorithm with a Novel Replacement Strategy for Numerical Optimization,IEEE,Conferences,"Differential Evolution (DE) algorithm is an efficient metaheuristic algorithm in solving complex real-world optimization problems. DE algorithm benefits from two populations for updating individuals, while it might cause memory problems in practice during solving large-scale optimization problems; especially when they are used in an embedded system. One strategy to tackle this problem is utilizing a one-array scheme which benefits from only one population, leading to a half-space memory. This paper proposes a novel DE algorithm based on one-array DE and a random replacement strategy; it adds an additional competition to the selection operator to make better use of the new individual that it might be potentially noteworthy. The positive feature of the introduced replacement strategy is that it does not need any extra computational budget. Also, due to employing one-array strategy, the proposed scheme has a lower memory complexity. Our experiments on CEC-2017 benchmark function with dimensions 30, 50, and 100 clearly illustrate the effectiveness of the proposed DE algorithm.",https://ieeexplore.ieee.org/document/9283154/,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",11-14 Oct. 2020,ieeexplore
10.1109/CCDC.2018.8407436,One-dimensional data augmentation using a wasserstein generative adversarial network with supervised signal,IEEE,Conferences,"In recent years, Generative Adversarial Network (GAN) is widely applied in many domains; however, there is still some difficulties in training the network, which are mainly caused by mode collapse, vanishing gradient of generator and indirect assessment criteria of generated samples. In this paper, the supervised signal is introduced into Wasserstein Generative Adversarial Network (WGAN) on the application of one-dimensional data augmentation to alleviate this difficulty. In the proposed method, besides generating fake samples, a well trained generative model is implemented to reconstruct the real samples , whose input data are latent space samples obtained from autoencoder (AE). In addition, the mode collapse can be prevented by the new model through ensuring that the supervised signal grounded in all the available training data. The performance of our method is verified based on parameters of electronic equipment and stock index systematically and quantitatively, and the superiority of the algorithm is demonstrated by the experiment results both in convergence rate and the quality of samples compared with WGAN and VAEGAN.",https://ieeexplore.ieee.org/document/8407436/,2018 Chinese Control And Decision Conference (CCDC),9-11 June 2018,ieeexplore
10.1109/ICPR48806.2021.9412087,One-stage Multi-task Detector for 3D Cardiac MR Imaging,IEEE,Conferences,"Fast and accurate landmark location and bounding box detection are important steps in 3D medical imaging. In this paper, we propose a novel multi-task learning framework, for real-time, simultaneous landmark location and bounding box detection in 3D space. Our method extends the famous single-shot multibox detector (SSD) from single-task learning to multitask learning and from 2D to 3D. Furthermore, we propose a post-processing approach to refine the network landmark output, by averaging the candidate landmarks. Owing to these settings, the proposed framework is fast and accurate. For 3D cardiac magnetic resonance (MR) images with size 22422464, our framework runs ~128 volumes per second (VPS) on GPU and achieves 6.75mm average point-to-point distance error for landmark location, which outperforms both state-of-the-art and baseline methods. We also show that segmenting the 3D image cropped with the bounding box results in both improved performance and efficiency.",https://ieeexplore.ieee.org/document/9412087/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore
10.23919/INM.2017.7987462,Online learning and adaptation of network hypervisor performance models,IEEE,Conferences,"Software Defined Networking (SDN) paved the way for a logically centralized entity, the SDN controller, to excerpt near real-time control over the forwarding state of a network. Network hypervisors are an in-between layer to allow multiple SDN controllers to share this control by slicing the network and giving each controller the power over a part of the network. This makes network hypervisors a critical component in terms of reliability and performance. At the same time, compute virtualization is ubiquitous and may not guarantee statically assigned resources to the network hypervisors. It is therefore important to understand the performance of network hypervisors in environments with varying compute resources. In this paper we propose an online machine learning pipeline to synthesize a performance model of a running hypervisor instance in the face of varying resources. The performance model allows precise estimations of the current capacity in terms of control message throughput without time-intensive offline benchmarks. We evaluate the pipeline in a virtual testbed with a popular network hypervisor implementation. The results show that the proposed pipeline is able to estimate the capacity of a hypervisor instance with a low error and furthermore is able to quickly detect and adapt to a change in available resources. By exploring the parameter space of the learning pipeline, we discuss its characteristics in terms of estimation accuracy and convergence time for different parameter choices and use cases. Although we evaluate the approach with network hypervisors, our work can be generalized to other latency-sensitive applications with similar characteristics and requirements as network hypervisors.",https://ieeexplore.ieee.org/document/7987462/,2017 IFIP/IEEE Symposium on Integrated Network and Service Management (IM),8-12 May 2017,ieeexplore
10.1109/IROS.2017.8202247,Online learning for human classification in 3D LiDAR-based tracking,IEEE,Conferences,"Human detection and tracking are essential aspects to be considered in service robotics, as the robot often shares its workspace and interacts closely with humans. This paper presents an online learning framework for human classification in 3D LiDAR scans, taking advantage of robust multi-target tracking to avoid the need for data annotation by a human expert. The system learns iteratively by retraining a classifier online with the samples collected by the robot over time. A novel aspect of our approach is that errors in training data can be corrected using the information provided by the 3D LiDAR-based tracking. In order to do this, an efficient 3D cluster detector of potential human targets has been implemented. We evaluate the framework using a new 3D LiDAR dataset of people moving in a large indoor public space, which is made available to the research community. The experiments analyse the real-time performance of the cluster detector and show that our online learned human classifier matches and in some cases outperforms its offline version.",https://ieeexplore.ieee.org/document/8202247/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore
10.1109/INDIN.2014.6945484,Online passive learning of timed automata for cyber-physical production systems,IEEE,Conferences,"Model-based approaches are very often used for diagnosis in production systems. And since the manual creation of behavior models is a tough task, many learning algorithms have been constructed for the automatic model identification. Most of them are tested and evaluated on artificial datasets on personal computers only. However, the implementation on cyber-physical production systems puts additional requirements on learning algorithms, for instance the real-time aspect or the usage of memory space. This paper analyzes the requirements on learning algorithms for cyber-physical production systems and presents an appropriate online learning algorithm, the Online Timed Automaton Learning Algorithm, OTALA. It is the first online passive learning algorithm for timed automata which in addition copes without negative learning examples. An analysis of the algorithm and comparison with offline learning algorithms completes this contribution.",https://ieeexplore.ieee.org/document/6945484/,2014 12th IEEE International Conference on Industrial Informatics (INDIN),27-30 July 2014,ieeexplore
10.1109/IJCNN.2016.7727712,Online variational Bayesian Support Vector Regression,IEEE,Conferences,"Traditional Support Vector Regression (SVR) solvers require user pre-specified penalty (regularization) parameter as input and typically model the training data with maximum a posterior (MAP) principle. The resultant point estimates can be affected seriously by inappropriate regularization, outliers and noise, especially when training online. In this paper, we address the aforementioned problems by developing a Bayesian SVR model with the pseudo-likelihood and data augmentation idea. Then we perform variational posterior inference in an augmented variable space and the approximate posterior of model weights, rather than point estimates as in traditional SVR, are used to make robust predictions. Besides, once the approximate posterior is obtained from a given set of data, we can regard it as model prior when dealing with new arrival data, which leads to a natural way to extend our batch model to the online scenario. Experiments on several benchmark regression problems as well as a real vehicle accident rate prediction task show that our models have superior performance while inferring penalty parameter automatically.",https://ieeexplore.ieee.org/document/7727712/,2016 International Joint Conference on Neural Networks (IJCNN),24-29 July 2016,ieeexplore
10.1109/INDIN.2006.275808,"Ontology for Cognitics, Closed-Loop Agility Constraint, and Case Study - a Mobile Robot with Industrial-Grade Components",IEEE,Conferences,"The paper refers to intelligent industrial automation. The objective is to present key elements and methods for best practice, as well as some results obtained. The first part presents an ontology for automated cognition (cognitics), where, based on information and time, the main cognitive concepts, including those of complexity, knowledge, expertise, learning, intelligence abstraction, and concretization are rigorously defined, along with corresponding metrics and specific units. Among important conclusions at this point are the fact that reality is much too complex to be approached better than through much simplified models, in very restricted contexts. Another conclusion is the necessity to be focused on goal. Extensions are made here for group behavior. The second part briefly presents a basic law governing the choice of overall control architecture: achievable performance level of control system in terms of agility, relative to process dynamics, dictates the type of approaches which is suitable, in a spectrum which ranges from simple threshold-based switching, to classical closed-loop calculus (PID, state space multivariable systems, etc.), up to ""impossible"" cases where additional controllers must be considered, leading to cascaded, hierarchical control structures. For complex cases such as latter ones, new tools and methodologies must be designed, as is typical in O<sup>3</sup>NEIDA initiative, at least for software components. Finally, a large part of the paper presents a case study, a mobile robot, i.e. an embedded autonomous system with distributed, networked control, featuring industry-grade components, designed with the main goal of robust functionality. The case illustrates several of the concepts introduced earlier in the paper.",https://ieeexplore.ieee.org/document/4053562/,2006 4th IEEE International Conference on Industrial Informatics,16-18 Aug. 2006,ieeexplore
10.1109/iThings.2014.13,Ontology-Based Semantic Modeling and Evaluation for Internet of Things Applications,IEEE,Conferences,"Internet of Things (IoT) is a major paradigm shift from traditional Internet applications that connects things / objects to the Internet for automatic sensing, information processing, understanding, reasoning, decision and response to virtual and physical world. Due to its highly distributed and heterogeneous characteristics, it requires more semantic interactions between different scenarios. Therefore, interoperability is one of the most fundamental requirements for IoT systems. Currently, semantic modeling for IoT applications has significant deficiency in its reusability and interoperability. In this paper, we propose a semantic information model for IoT applications, named OntoIoT. This model has a two-level architecture including a set of general upper ontology and flexible interfaces for different application domain ontology extensions. We have preliminarily implemented the OntoIoT framework and domain extension in selected scenarios of smart space and healthcare. A data-driven and other ontology evaluation approaches are used to prove the scalability and interoperability of OntoIoT. Result demonstrates that, compared to other test ontologies, Onto IoT is more suitable for the given application scenario.",https://ieeexplore.ieee.org/document/7059638/,"2014 IEEE International Conference on Internet of Things (iThings), and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom)",1-3 Sept. 2014,ieeexplore
10.1109/IVS.2014.6856509,Ontology-based context awareness for driving assistance systems,IEEE,Conferences,"Within a vehicle driving space, different entities such as vehicles and vulnerable road users are in constant interaction which governs their behaviour. Whilst smart sensors provide information about the state of the perceived objects, considering the spatio-temporal relationships between them with respect to the subject vehicle remains a challenge. This paper proposes to fill this gap by using contextual information to infer how perceived entities are expected to behave, and thus what are the consequences of these behaviours on the subject vehicle. For this purpose, an ontology is formulated about the vehicle, perceived entities and context (map information) to provide a conceptual description of all road entities with their interaction. It allows for inferences of knowledge about the situation of the subject vehicle with respect to the environment in which it is navigating. The framework is applied to the navigation of a vehicle as it approaches road intersections, to demonstrate its applicability. Results from the real-time implementation on a vehicle operating under controlled conditions are included. They show that the proposed ontology allows for a coherent understanding of the interactions between the perceived entities and contextual data. Further, it can be used to improve the situation awareness of an ADAS (Advanced Driving Assistance System), by determining which entities are the most relevant for the subject vehicle navigation.",https://ieeexplore.ieee.org/document/6856509/,2014 IEEE Intelligent Vehicles Symposium Proceedings,8-11 June 2014,ieeexplore
10.1109/WI.2005.108,Ontology-based information integration in virtual learning environment,IEEE,Conferences,"A good virtual learning environment should deliver relevant learning materials to learners at the most appropriate time and locations to facilitate learners' acquisition of knowledge and skills. In this paper, we propose ontology-based information integration in virtual learning environment using ontology and Web services. Relevant concepts extracted from domain ontology provide ontology-based browsing space that allows users to browse and select relevant terms of interest and increases the degree of relevancy. By using Web services to integrate learning materials from heterogeneous public domain data sources, applications do not need to know the internal structure and working of public domain data sources, and reuse existing applications and recourses. We use gene ontology, PubMed eUtils and Google Web APIs to demonstrate our idea. The implementation involves techniques in image and video processing, database management, programming, and multimedia learning materials presentation.",https://ieeexplore.ieee.org/document/1517949/,The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05),19-22 Sept. 2005,ieeexplore
10.1109/ISMAR52148.2021.00016,"OpenRDW: A Redirected Walking Library and Benchmark with Multi-User, Learning-based Functionalities and State-of-the-art Algorithms",IEEE,Conferences,"Redirected walking (RDW) is a locomotion technique that guides users on virtual paths, which might vary from the paths they physically walk in the real world. Thereby, RDW enables users to explore a virtual space that is larger than the physical counterpart with near-natural walking experiences. Several approaches have been proposed and developed; each using individual platforms and evaluated on a custom dataset, making it challenging to compare between methods. However, there are seldom public toolkits and recognized benchmarks in this field. In this paper, we introduce OpenRDW, an open-source library and benchmark for developing, deploying and evaluating a variety of methods for walking path redirection. The OpenRDW library provides application program interfaces to access the attributes of scenes, to customize the RDW controllers, to simulate and visualize the navigation process, to export multiple formats of the results, and to evaluate RDW techniques. It also supports the deployment of multi-user real walking, as well as reinforcement learning-based models exported from TensorFlow or PyTorch. The OpenRDW benchmark includes multiple testing conditions, such as walking in size varied tracking spaces or shape varied tracking spaces with obstacles, multiple user walking, etc. On the other hand, procedurally generated paths and walking paths collected from user experiments are provided for a comprehensive evaluation. It also contains several classic and state-of-the-art RDW techniques, which include the above mentioned functionalities.",https://ieeexplore.ieee.org/document/9583831/,2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR),4-8 Oct. 2021,ieeexplore
10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00137,Optimal FPGA-oriented lightweight network architecture search under multi-objective constraints,IEEE,Conferences,"The neural architecture search (NAS) technology has developed rapidly, but most algorithms mainly target the GPUs, causing the searched models to lack consideration of the target hardware platform. This paper proposes a novel NAS algorithm specific for the unique calculation and storage character of FPGA to accelerate the deployment efficiency of neural networks and automated build the FPGA-oriented neural structure. Firstly, we design the search space based on the lightweight candidate blocks. Then, we measure the real simulation latency of each candidate block on the FPGA platform and for the first time using it to direct the NAS process. Eventually, this paper proposed a differentiable representation function of hardware latency and built a multi-objective NAS framework based on real simulation latency and FLOPS. In addition, by analyzing the relationship between the network convergence process and the model size, we achieve an optimal trade-off balance between precision and latency and build a more suitable network structure for deployment on FPGA. Compared with state-of-the-art manual and automated lightweight models, our method has relatively better accuracy and latency performance. Extensive experiments on the ImageNet dataset show that the proposed method can achieve 2x speedup compared with the similar accuracy manual lightweight network (MobileNet V2) and even get 3x speedup compared with the equivalent auto lightweight network (DARTS).",https://ieeexplore.ieee.org/document/9644718/,"2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)",30 Sept.-3 Oct. 2021,ieeexplore
10.1109/ICPR.2008.4761060,Optimal feature weighting for the discrete HMM,IEEE,Conferences,"We propose a modified discrete HMM that includes a feature weighting discrimination component. We assume that the feature space is partitioned into subspaces and that the relevance weights of the different subspaces depends on the symbols and the states. In particular, we associate a partial probability with each symbol in each subspace. The overall observation state probability is then computed as an aggregation of the partial probabilities and their relevance weights. We consider two aggregation models. The first one is based on a linear combination, while the second one is based on a geometric combination. For both models, we reformulate the Baum-Welch learning algorithm and derive the update equations for the relevance weights and the partial state probabilities. The proposed approach is validated using synthetic and real data sets. The results are shown to outperform the baseline HMM.",https://ieeexplore.ieee.org/document/4761060/,2008 19th International Conference on Pattern Recognition,8-11 Dec. 2008,ieeexplore
10.1109/SII.2012.6426933,Optimization of obstacle avoidance using reinforcement learning,IEEE,Conferences,Walking through narrow space for multi-legged robot is optimized using reinforcement learning in this paper. The walking is generated by the virtual repulsive force from the estimated obstacle position and the virtual impedance field. The resulted action depends on the parameter of the virtual impedance coefficients. The reinforcement learning is employed to find an optimal motion. The temporal walking through motion consists of each parameter optimized for a situation. Optimization of integrated walking through motion is finally achieved evaluating walking in compound encountering obstacle on simulator. The resulted motion is implemented to a real multi-legged robot and results show the effectiveness of the proposed method.,https://ieeexplore.ieee.org/document/6426933/,2012 IEEE/SICE International Symposium on System Integration (SII),16-18 Dec. 2012,ieeexplore
10.1109/ICCVE45908.2019.8965211,Optimizing coverage of simulated driving scenarios for the autonomous vehicle,IEEE,Conferences,"Self-driving cars and advanced driver-assistance systems are perceived as a game-changer in the future of road transportation. However, their validation is mandatory before industrialization; testing every component should be assessed intensively in order to mitigate potential failures and avoid unwanted problems on the road. In order to cover all possible scenarios, virtual simulations are used to complement real-test driving and aid in the validation process. This paper focuses on the validation of the command law during realistic virtual simulations. Its aim is to detect the maximum amount of failures while exploring the input search space of the scenarios. A key industrial restriction, however, is to launch simulations as little as possible in order to minimize computing power needed. Thus, a reduced model based on a random forest model helps in decreasing the number of simulations launched. It accompanies the algorithm in detecting the maximum amount of faulty scenarios everywhere in the search space. The methodology is tested on a tracking vehicle use case, which produces highly effective results.",https://ieeexplore.ieee.org/document/8965211/,2019 IEEE International Conference on Connected Vehicles and Expo (ICCVE),4-8 Nov. 2019,ieeexplore
10.1109/ICSP51882.2021.9408823,Over-the-Air Radar Emitter Signal Classification Based on SDR,IEEE,Conferences,"At present, in the field of radar emitter classification, theoretical simulation is mostly used to carry out algorithm research. However, there are few schemes to study signal classification in real electromagnetic environment using actual hardware. Therefore, this paper proposes a radar emitter classification scheme based on HackRF Software Defined Radio (SDR) and deep learning to solve the problem of weak engineering practice. Firstly, the GNU Radio development environment is used to realize the integration design of real space signal transceiver and time-frequency analysis algorithm application on HackRF hardware platform. Then, a classification model with 11 layers network is constructed to automatically extract the deep features of intra-pulse signal time-frequency image. Finally, the classification performance of eight kinds of signals in real electromagnetic environment is tested. The total recognition accuracy of this scheme is more than 83% under 6dB low Signal-to-Noise Ratio (SNR), which proves the effectiveness of the scheme, and provides an important basis for practical engineering application in the future.",https://ieeexplore.ieee.org/document/9408823/,2021 6th International Conference on Intelligent Computing and Signal Processing (ICSP),9-11 April 2021,ieeexplore
10.1109/HPCC-SmartCity-DSS50907.2020.00081,Overcoming Memory Constraint for Improved Target Classification Performance on Embedded Deep Learning Systems,IEEE,Conferences,"Pattern recognition applications such as face recognition, detection of broken eggs, and classification of agricultural products are all using image classification in deep neural networks to improve the quality of services. However, traditional cloud inference models suffer from several problems such as network delay fluctuations and privacy leakage. In this regard, most real-time applications currently need to be deployed on edge computing devices. Constrained by the computing power and memory limitations of edge devices, the use of an efficient memory manager for model reasoning is the key to improving the quality of service. This study firstly explored the incremental loading strategy of model weights for the model reasoning. Next, the memory space at runtime is optimized through data layout reorganization from the spatial dimension. In particular, our proposed schemes are orthogonal and transparent to the model. Experimental results demonstrate that the proposed approach reduced the memory consumption by 43.74% on average without additional reasoning time overhead.",https://ieeexplore.ieee.org/document/9408027/,2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS),14-16 Dec. 2020,ieeexplore
10.1109/BigData.2015.7363884,PAIRS: A scalable geo-spatial data analytics platform,IEEE,Conferences,"Geospatial data volume exceeds hundreds of Petabytes and is increasing exponentially mainly driven by images/videos/data generated by mobile devices and high resolution imaging systems. Fast data discovery on historical archives and/or real time datasets is currently limited by various data formats that have different projections and spatial resolution, requiring extensive data processing before analytics can be carried out. A new platform called Physical Analytics Integrated Repository and Services (PAIRS) is presented that enables rapid data discovery by automatically updating, joining, and homogenizing data layers in space and time. Built on top of open source big data software, PAIRS manages automatic data download, data curation, and scalable storage while being simultaneously a computational platform for running physical and statistical models on the curated datasets. By addressing data curation before data being uploaded to the platform, multi-layer queries and filtering can be performed in real time. In addition, PAIRS offers a foundation for developing custom analytics. Towards that end we present two examples with models which are running operationally: (1) high resolution evapo-transpiration and vegetation monitoring for agriculture and (2) hyperlocal weather forecasting driven by machine learning for renewable energy forecasting.",https://ieeexplore.ieee.org/document/7363884/,2015 IEEE International Conference on Big Data (Big Data),29 Oct.-1 Nov. 2015,ieeexplore
10.1109/CNNA.2002.1035066,PDE based histogram modification with embedded morphological processing of the level-sets,IEEE,Conferences,"This paper describes parallel histogram modification techniques with embedded morphological preprocessing methods within the CNN-UM framework. The procedure is formulated in terms of nonlinear partial differential equations (PDE) and approximated through finite differences in space, resulting in coupled nonlinear ordinary differential equations (ODE). The I/O mapping of the system (containing both local and global couplings) can be calculated by a complex analogic (analog and logic) algorithm executed on a stored program nonlinear array processor, called the cellular nonlinear network universal machine (CNN-UM). We describe and illustrate how implementation of the algorithm results in an adaptive multi-thresholding scheme when histogram modification is combined with embedded morphological processing at a finite (low) number of grayscale levels. This has obvious advantages if the further processing steps are segmentation and/or recognition. Experimental results processing real-life and echocardiography images are measured on different hardware/software platforms, including a 64/spl times/64 CNN-UM chip (ACE4k).",https://ieeexplore.ieee.org/document/1035066/,Proceedings of the 2002 7th IEEE International Workshop on Cellular Neural Networks and Their Applications,24-24 July 2002,ieeexplore
10.1109/ICISCE50968.2020.00212,PTR-MirrorGAN: Parallel Text Redescription module for advancing MirrorGAN,IEEE,Conferences,"During generating images from a given text description, two aspects of consistency are usually considered: visual consistency and semantic consistency. Although there has been a great improvement in the use of generative adversarial networks to generate images, it remains a challenging project to ensure that the semantics of the generated images are consistent with the semantics of the input text descriptions. This study proposed a Real-Fake Images Parallel Text Redescription Module (RFIPTRM) for advancing MirrorGAN based on the idea of mirroring and reconstructed the loss of the image generation model by adding the loss between the real text and the text from generated images to the genarators for balancing the textual semantic space. Intuitively, there may be inconsistency between the textual semantic space structure of our real text and the generated image, so we conducted adversarial learning between redescriptive text from the real images and the real text to improve the inconsistency of this text space structure. Comprehensive experiments conducted on the public benchmark datasets CUB and COCO have proved that PTR-MirrorGAN is superior to other methods and achieved a better generation effect.",https://ieeexplore.ieee.org/document/9532027/,2020 7th International Conference on Information Science and Control Engineering (ICISCE),18-20 Dec. 2020,ieeexplore
10.1109/AISIG.1989.47322,PX1: a space shuttle mission operations knowledge-based system project,IEEE,Conferences,"A knowledge-based system (KBS) prototyping project is examined. The work entails reasoning about the impact of component failures in space shuttle orbiter subsystems. The effort is directed toward the development of a system that will recognize passive component failures as potential safety hazards, rather than toward an active failure identification or diagnostic tool. The system was designed to be integrated as a knowledge-based processing system utilizing input from a procedure-based processing system. Implementation of the consolidated system will occur when real-time telemetry data is available at the workstations. The system may be used standalone in the meantime. Flight controllers at the Johnson Space Center will use this prototype to help develop requirements for a space shuttle mission operations tool.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/47322/,[1989] Proceedings. The Annual AI Systems in Government Conference,27-31 March 1989,ieeexplore
10.1109/ICDM.2008.119,Paired Learners for Concept Drift,IEEE,Conferences,"To cope with concept drift, we paired a stable online learner with a reactive one. A stable learner predicts based on all of its experience, whereas are active learner predicts based on its experience over a short, recent window of time. The method of paired learning uses differences in accuracy between the two learners over this window to determine when to replace the current stable learner, since the stable learner performs worse than does there active learner when the target concept changes. While the method uses the reactive learner as an indicator of drift, it uses the stable learner to predict, since the stable learner performs better than does the reactive learner when acquiring target concept. Experimental results support these assertions. We evaluated the method by making direct comparisons to dynamic weighted majority, accuracy weighted ensemble, and streaming ensemble algorithm (SEA) using two synthetic problems, the Stagger concepts and the SEA concepts, and three real-world data sets: meeting scheduling, electricity prediction, and malware detection. Results suggest that, on these problems, paired learners outperformed or performed comparably to methods more costly in time and space.",https://ieeexplore.ieee.org/document/4781097/,2008 Eighth IEEE International Conference on Data Mining,15-19 Dec. 2008,ieeexplore
10.1109/BigData.2018.8622065,Parallel Large-Scale Neural Network Training For Online Advertising,IEEE,Conferences,"Neural networks have shown great successes in many fields. Due to the complexity of the training pipeline, however, using them in an industrial setting is challenging. In online advertising, the complexity arises from the immense size of the training data, and the dimensionality of the sparse feature space (both can be hundreds of billions). To tackle these challenges, we built TrainSparse (TS), a system that parallelizes the training of neural networks with a focus on efficiently handling large-scale sparse features. In this paper, we present the design and implementation of TS, and show the effectiveness of the system by applying it to predict the ad conversion rate (pCVR), one of the key problems in online advertising. We also compare several methods for dimensionality reduction on sparse features in the pCVR task. Experiments on real-world industry data show that TS achieves outstanding performance and scalability.",https://ieeexplore.ieee.org/document/8622065/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore
10.1109/CLUSTR.2006.311867,Parallel Morphological/Neural Classification of Remote Sensing Images Using Fully Heterogeneous and Homogeneous Commodity Clusters,IEEE,Conferences,"The wealth spatial and spectral information available from last-generation Earth observation instruments has introduced extremely high computational requirements in many applications. Most currently available parallel techniques treat remotely sensed data not as images, but as unordered listings of spectral measurements with no spatial arrangement. In thematic classification applications, however, the integration of spatial and spectral information can be greatly beneficial. Although such integrated approaches can be efficiently mapped in homogeneous commodity clusters, low-cost heterogeneous networks of computers (HNOCs) have soon become a standard tool of choice in Earth and planetary missions. In this paper, we develop a new morphological/neural parallel algorithm for commodity cluster-based analysis of high-dimensional remotely sensed image data sets. The algorithms accuracy and parallel performance are tested (in the context of a real precision agriculture application) using two parallel platforms: a fully heterogeneous cluster made up of 16 workstations at University of Maryland, and a massively parallel Beowulf cluster at NASA's Goddard Space Flight Center",https://ieeexplore.ieee.org/document/4100373/,2006 IEEE International Conference on Cluster Computing,25-28 Sept. 2006,ieeexplore
10.1109/ICAPR.2009.40,Parallel Point Symmetry Based Clustering for Gene Microarray Data,IEEE,Conferences,"Point symmetry-based clustering is an important unsupervised learning tool for recognizing symmetrical convex or non-convex shaped clusters, even in the microarray datasets. To enable fast clustering of this large data, in this article, a distributed space and time-efficient scalable parallel approach for point symmetry-based K-means algorithm has been proposed. A natural basis for analyzing gene expression data using this symmetry-based algorithm, is to group together genes with similar symmetrical patterns of expression. This new parallel implementation satisfies the quadratic reduction in timing, as well as the space and communication overhead reduction without sacrificing the quality of clustering solution. The parallel point symmetry based K-means algorithm is compared with another newly implemented parallel symmetry-based K-means and existing parallel K-means over four artificial, real-life and benchmark microarray datasets, to demonstrate its superiority,both in timing and validity.",https://ieeexplore.ieee.org/document/4782807/,2009 Seventh International Conference on Advances in Pattern Recognition,4-6 Feb. 2009,ieeexplore
10.1109/ROBOT.1991.131722,Parallel robot motion planning,IEEE,Conferences,"A fast, parallel method for computing configuration space maps is presented. The method is made possible by recognizing that one can compute a family of primitive maps which can be combined by superposition based on the distribution of real obstacles. This motion planner has been implemented for the first three degrees-of-freedom of a Puma robot in *Lisp on a Connection Machine with 8 K processors. A six degree-of-freedom version of the algorithm which performs a sequential search of the six-dimensional configuration space, building three-dimensional cross sections in parallel, has also been implemented.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/131722/,Proceedings. 1991 IEEE International Conference on Robotics and Automation,9-11 April 1991,ieeexplore
10.1109/ICCWorkshops50388.2021.9473673,Passive WiFi CSI Sensing Based Machine Learning Framework for COVID-Safe Occupancy Monitoring,IEEE,Conferences,"The COVID-19 pandemic requires social distancing to prevent transmission of the virus. Monitoring social distancing is difficult and expensive, especially in ""travel corridors"" such as elevators and commercial spaces. This paper describes a low-cost and non-intrusive method to monitor social distancing within a given space, using Channel State Information (CSI) from passive WiFi sensing. By exploiting the frequency selective behaviour of CSI with a cubic SVM classifier, we count the number of people in an elevator with an accuracy of 92%, and count the occupancy of an office to 97%. As opposed to using a multi-class counting approach, this paper aggregates CSI for the occupancies below and above a COVID-Safe limit. We show that this binary classification approach to the COVID safe decision problem has similar or better accuracy outcomes with much lower computational complexity, allowing for real-world implementation on IoT embedded devices. Robustness and scalability is demonstrated through experimental validation in practical scenarios with varying occupants, different environment settings and interference from other WiFi devices.",https://ieeexplore.ieee.org/document/9473673/,2021 IEEE International Conference on Communications Workshops (ICC Workshops),14-23 June 2021,ieeexplore
10.1109/TAI.1995.479619,Path consistency revisited,IEEE,Conferences,"One of the main factors limiting the use of path consistency algorithms in real life applications is their high space complexity. C. Han and C. Lee (1988) presented a path consistency algorithm, PC-4, with O(n/sup 3/a/sup 3/) space complexity, which makes it practicable only for small problems. The author presents a new path consistency algorithm, PC-5, which has an O(n/sup 3/a/sup 2/) space complexity while retaining the worst case time complexity of PC-4. Moreover, the new algorithm exhibits a much better average case time complexity. The new algorithm is based on the idea (due to C. Bessiere (1994)) that, at any time, only a minimal amount of support has to be found and recorded for a labeling to establish its viability; one has to look for at new support only if the current support is eliminated. The author also shows that PC-5 can be improved further to yield an algorithm, PC5++, with even better average case performance and the same space complexity.",https://ieeexplore.ieee.org/document/479619/,Proceedings of 7th IEEE International Conference on Tools with Artificial Intelligence,5-8 Nov. 1995,ieeexplore
10.1109/ICCC51575.2020.9345104,Payload-based Anomaly Detection for Industrial Internet Using Encoder Assisted GAN,IEEE,Conferences,"Payload-based anomaly detection has been proved effective in discovering Internet misbehavior and potential intrusions, but highly relies on the unstructured feature engineering to generalize the distribution of normal payloads. This kind of generalization may not adapt well to the emerging industrial Internet, where the normal behaviors are more diverse and usually embedded in the raw payloads' local structures. In this paper, we tackle this generalization problem and propose a very different solution to payload-based anomaly detection without the need of feature engineering. Our basic idea is to learn the raw structures of normal payloads directly by a generative adversarial network (GAN), in which we have a generator (i.e., a reversed convolutional decoder) to sample raw payloads from a latent space as well as a discriminator (i.e., a convolutional classifier) to guide the generator produce raw payloads approximating the normal structures. We also deploy an assisted convolutional encoder to map the true payloads back to the latent space and combine with the GAN's decoder (i.e., generator) to reconstruct the payload structures. We consider anomalies appear in condition the re-constructed payloads are largely deviated from the true ones, since our encoder-decoder architecture is trained able to rebuild only the normal payload structures. We have evaluated our solution using extensive experiments on real-world industrial Internet datasets, and confirmed its effectiveness in detecting industrial Internet anomalies in the raw payloads.",https://ieeexplore.ieee.org/document/9345104/,2020 IEEE 6th International Conference on Computer and Communications (ICCC),11-14 Dec. 2020,ieeexplore
10.1109/IISWC47752.2019.9042000,Performance Aware Convolutional Neural Network Channel Pruning for Embedded GPUs,IEEE,Conferences,"Convolutional Neural Networks (CNN) are becoming a common presence in many applications and services, due to their superior recognition accuracy. They are increasingly being used on mobile devices, many times just by porting large models designed for server space, although several model compression techniques have been considered. One model compression technique intended to reduce computations is channel pruning. Mobile and embedded systems now have GPUs which are ideal for the parallel computations of neural networks and for their lower energy cost per operation. Specialized libraries perform these neural network computations through highly optimized routines. As we find in our experiments, these libraries are optimized for the most common network shapes, making uninstructed channel pruning inefficient. We evaluate higher level libraries, which analyze the input characteristics of a convolutional layer, based on which they produce optimized OpenCL (Arm Compute Library and TVM) and CUDA (cuDNN) code. However, in reality, these characteristics and subsequent choices intended for optimization can have the opposite effect. We show that a reduction in the number of convolutional channels, pruning 12% of the initial size, is in some cases detrimental to performance, leading to 2 slowdown. On the other hand, we also find examples where performance-aware pruning achieves the intended results, with performance speedups of 3 with cuDNN and above 10 with Arm Compute Library and TVM. Our findings expose the need for hardware-instructed neural network pruning.",https://ieeexplore.ieee.org/document/9042000/,2019 IEEE International Symposium on Workload Characterization (IISWC),3-5 Nov. 2019,ieeexplore
10.1109/ICUFN.2016.7537070,Performance evaluation of in-memory computing on scale-up and scale-out cluster,IEEE,Conferences,"Apache Spark framework, which is the implementation of Resilient Distributed Datasets(RDD), is used instead of MapReduce on recent data processing models of Hadoop ecosystem. In this paper, we evaluated the performance and resource usage of real world workloads on scale-up and scale-out clusters using the in-memory caching feature of Spark framework. In our experiments, scale-up processed data more efficiently than scaleout in write intensive workloads such as Sort and Scan, whereas scale-out had strength in those utilizing iterative algorithms such as Join, Pagerank and KMeans. Considering the efficiency in physical factors including performance per watt and the physical space each occupies, we show that it is more advantages to use scale up cluster than scale out.",https://ieeexplore.ieee.org/document/7537070/,2016 Eighth International Conference on Ubiquitous and Future Networks (ICUFN),5-8 July 2016,ieeexplore
10.1109/CVPR46437.2021.00216,Person30K: A Dual-Meta Generalization Network for Person Re-Identification,IEEE,Conferences,"Recently, person re-identification (ReID) has vastly benefited from the surging waves of data-driven methods. However, these methods are still not reliable enough for real-world deployments, due to the insufficient generalization capability of the models learned on existing benchmarks that have limitations in multiple aspects, including limited data scale, capture condition variations, and appearance diversities. To this end, we collect a new dataset named Person30K with the following distinct features: 1) a very large scale containing 1.38 million images of 30K identities, 2) a large capture system containing 6,497 cameras deployed at 89 different sites, 3) abundant sample diversities including varied backgrounds and diverse person poses. Furthermore, we propose a domain generalization ReID method, dual-meta generalization network (DMG-Net), to exploit the merits of meta-learning in both the training procedure and the metric space learning. Concretely, we design a ""learning then generalization evaluation"" metatraining procedure and a meta-discrimination loss to enhance model generalization and discrimination capabilities. Comprehensive experiments validate the effectiveness of our DMG-Net.",https://ieeexplore.ieee.org/document/9578872/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore
10.1109/BigData.2017.8258065,Personalized travel mode detection with smartphone sensors,IEEE,Conferences,"Detecting the travel modes such as walking and driving a car is an important task for user behavior understanding as well as transportation planning and management. Existing solutions for this task mainly train a generic classifier for all users although the walking or driving behaviors may differ greatly from one user to another. In this paper, we propose to build a personalized travel mode detection method. In particular, the proposed method can be divided into two stages. First, for a given target user, it applies user similarity computation to borrow data from a set of pre-collected data for transfer learning. Second, it estimates the data distribution in feature space, and uses it to reweight the borrowed data so as to minimize the model loss with respect to the target user. Experimental evaluations on real travel data show that the proposed method outperforms the generic method and the transfer learning method with kernel mean matching in terms of prediction accuracy.",https://ieeexplore.ieee.org/document/8258065/,2017 IEEE International Conference on Big Data (Big Data),11-14 Dec. 2017,ieeexplore
10.1109/ICOS.2017.8280269,Phishing classification models: Issues and perspectives,IEEE,Conferences,"Never-ending phishing threats on cyberspace motivate researchers to develop more proficient phishing classification models to survive a supreme cyber-security with safe web services. However, such achievements remain incompetent in their performance against novel phish attacks. This is attributed to the induction factors of the classification model itself such as hybrid feature space, inactive learning on up-to-date data flows, and limited adaptation to the evolving phish attacks. In this light, this paper surveys the current achievements, studies their limitations, restates what induction factors need to boost for a successful real-time application. Consequently, future outlooks are recommended on how to devote well-performed anti-phishing scheme.",https://ieeexplore.ieee.org/document/8280269/,2017 IEEE Conference on Open Systems (ICOS),13-14 Nov. 2017,ieeexplore
10.1109/CNSC.2014.6906671,Pixelwise object class segmentation based on synthetic data using an optimized training strategy,IEEE,Conferences,"In this paper we present an approach for low-level body part segmentation based on RGB-D data. The RGB-D sensor is thereby placed at the ceiling and observes a shared workspace for human-robot collaboration in the industrial domain. The pixelwise information about certain body parts of the human worker is used by a cognitive system for the optimization of interaction and collaboration processes. In this context, for rational decision making and planning, the pixelwise predictions must be reliable despite the high variability of the appearance of the human worker. In our approach we treat the problem as a pixelwise classification task, where we train a random decision forest classifier on the information contained in depth frames produced by a synthetic representation of the human body and the ceiling sensor, in a virtual environment. As shown in similar approaches, the samples used for training need to cover a broad spectrum of the geometrical characteristics of the human, and possible transformations of the body in the scene. In order to reduce the number of training samples and the complexity of the classifier training, we therefore apply an elaborated and coupled strategy for randomized training data sampling and feature extraction. This allows us to reduce the training set size and training time, by decreasing the dimensionality of the sampling parameter space. In order to keep the creation of synthetic training samples and real-world ground truth data simple, we use a highly reduced virtual representation of the human body, in combination with KINECT skeleton tracking data from a calibrated multi-sensor setup. The optimized training and simplified sample creation allows us to deploy standard hardware for the realization of the presented approach, while yielding a reliable segmentation in real-time, and high performance scores in the evaluation.",https://ieeexplore.ieee.org/document/6906671/,2014 First International Conference on Networks & Soft Computing (ICNSC2014),19-20 Aug. 2014,ieeexplore
10.1109/Indo-TaiwanICAN48429.2020.9181341,Planogram Design Analytics using Image Processing,IEEE,Conferences,"A planogram is a tool for visual merchandising of retail stores. It displays a detailed view of the retail store with the major intent for product placement. The planogram is highly useful for examining the point of sale as it demonstrates the exact positioning of products. Two major benefits are there for building planograms while planning the store layout such as maximizing sales and space of the retail store.An important part of Planogram management is to read planogram drawings and images and convert these to representational data in CSV or database formats.In this part, Contour analysis of image using OpenCV along with combination of text detection and recognition is performed using Connectionist Text Proposal Network and Convolutional Recurrent Neural Network deep learning models respectively. The proposed planogram design analytics system is implemented using real data. The customer had tested the system as per different cases. The feedback obtained from them confirms that the system meets the requirements to their satisfaction.",https://ieeexplore.ieee.org/document/9181341/,"2020 Indo  Taiwan 2nd International Conference on Computing, Analytics and Networks (Indo-Taiwan ICAN)",7-15 Feb. 2020,ieeexplore
10.1109/CIRA.2005.1554245,Plenary talk June 29; The 3<sup>rd</sup>Generation of Robotics: Ubiquitous Robot,IEEE,Conferences,"This talk shows its possibility of implementation in real life through demonstrations using a Sobot, Rity: i) continuous interface between physical and virtual worlds ii) seamless transmission of Sobot between a PC and a Mobot, and iii) omnipresence of Sobot. Rity, developed at the Robot Intelligence Technology (RIT) Laboratory, KAIST, is a Sobot implemented as a 12 DOF artificial creature in the virtual 3D world created in a PC. It has virtual sensors to survive in the virtual world and physical sensors attached to the PC to interact with the real world. Based on sensor information it can express its emotion, and interact with human beings through a web camera in the real world. It can generate behaviors autonomously and has its own IP. This means that it can be accessed through a network at anywhere and anytime using any device. With this technique omnipresence of Sobot can be realized in a ubiquitous space. The eventual goal of this research is to integrate Sobot, Embot, and Mobot to build up a Ubibot so that ubiquitous services through it can be available in a ubiquitous era",https://ieeexplore.ieee.org/document/1554245/,2005 International Symposium on Computational Intelligence in Robotics and Automation,27-30 June 2005,ieeexplore
10.1109/ICDE48307.2020.00021,PoisonRec: An Adaptive Data Poisoning Framework for Attacking Black-box Recommender Systems,IEEE,Conferences,"Data-driven recommender systems that can help to predict users' preferences are deployed in many real online service platforms. Several studies show that they are vulnerable to data poisoning attacks, and attackers have the ability to mislead the system to perform as their desires. Considering the realistic scenario, where the recommender system is usually a black-box for attackers and complex algorithms may be deployed in them, how to learn effective attack strategies on such recommender systems is still an under-explored problem. In this paper, we propose an adaptive data poisoning framework, PoisonRec, which can automatically learn effective attack strategies on various recommender systems with very limited knowledge. PoisonRec leverages the reinforcement learning architecture, in which an attack agent actively injects fake data (user behaviors) into the recommender system, and then can improve its attack strategies through reward signals that are available under the strict black-box setting. Specifically, we model the attack behavior trajectory as the Markov Decision Process (MDP) in reinforcement learning. We also design a Biased Complete Binary Tree (BCBT) to reformulate the action space for better attack performance. We adopt 8 widely-used representative recommendation algorithms as our testbeds, and make extensive experiments on 4 different real-world datasets. The results show that PoisonRec has the ability to achieve good attack performance on various recommender systems with limited knowledge.",https://ieeexplore.ieee.org/document/9101655/,2020 IEEE 36th International Conference on Data Engineering (ICDE),20-24 April 2020,ieeexplore
10.1109/DSR.2011.6026835,Portality - The portal between virtuality and reality,IEEE,Conferences,"This paper proposes an innovative concept of community, called Mirror Reality, which is formed by the seamless interaction between users of physical space and virtual world. The Mirror Reality is relying on a portal to achieve the data translation and communication between virtual and reality worlds. This study employs the Ontology technology to analyze unified representation of objects and events in both virtual world and physical space. Furthermore, the research issues of creating a Mirror Reality are also identified. The portal that is implemented from the analysis is called Portality to emphasize its role between virtuality and reality. In other words, Portality takes care all of the data conversion and processing to permit the interoperability between two totally different worlds. Finally, the prototype of Mirror Reality for the university campus with its Portality is illustrated at the end.",https://ieeexplore.ieee.org/document/6026835/,2011 Defense Science Research Conference and Expo (DSR),3-5 Aug. 2011,ieeexplore
10.1109/IJCNN.2017.7966215,Pose invariance through registration for hierarchical feature based pattern recognition systems,IEEE,Conferences,"One of the main challenges in pattern recognition is handling variations in pose, which has been addressed in the past using exhaustive training, increasingly complex neural network architectures, or state space transformations, but often with limits on pose variation. The solution presented here implements complete pose invariance by estimating affine transform parameters and then registering samples to archetypes. This solution is intended for hierarchical feature based systems that use global features to get into the vicinity of the solution and then use local features to achieve higher precision if required. Here the entire distribution of a set of local features represents a global feature, from which properties are extracted that describe its pose. The difference between these properties for a pair of distributions yields transform parameters. This solution was deployed as a stand-alone solution in a low-noise real-world system that automatically registers raster maps to GIS datasets, but to facilitate the reproducibility of experiments, the current paper presents results from randomly generated datasets.",https://ieeexplore.ieee.org/document/7966215/,2017 International Joint Conference on Neural Networks (IJCNN),14-19 May 2017,ieeexplore
10.1109/INFCOMW.2019.8845276,Poster Abstract: Deep Learning Workloads Scheduling with Reinforcement Learning on GPU Clusters,IEEE,Conferences,"With the recent widespread adoption of deep learning (DL) in academia and industry, more attention are attracted by DL platform, which can support research and development (R&amp;D) of AI firms, institutes and universities. Towards an off-the-shelf distributed GPU cluster, prior work propose prediction-based schedulers to allocate resources for diverse DL workloads. However, the prediction-based schedulers have disadvantages on prediction accuracy and offline-profiling costs. In this paper, we propose a learning-based scheduler, which models the scheduling problem as a reinforcement learning problem, achieving minimum average job completion time and maximum system utilization. The scheduler contains the designs of state space, action space, reward function and update scheme. Furthermore, we will evaluate our proposed scheduler implemented as a plugin of Tensorflow on real cluster and large-scale simulation.",https://ieeexplore.ieee.org/document/8845276/,IEEE INFOCOM 2019 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),29 April-2 May 2019,ieeexplore
10.1109/ISCE.2013.6570213,Power monitor v2: Novel power saving Android application,IEEE,Conferences,"This paper presents a novel scheme to derive power saving profiles based on the usage patterns of the Android devices. The entire architecture is developed as an Android app ""Power Monitor v2"" and is deployed to the smart devices. A monitoring module of the app periodically collects several data from the devices and stores them locally. A learning engine then operates on the raw data to generate multiple usage patterns over time and space, which characterizes the user contexts. The engine then processes the patterns to generate power saving profiles dynamically within the devices. The profiles contain several system settings of the smart devices and intelligently optimize power consumption. We also present a real life usage pattern and the power saving profile. The overall battery life for the device estimated to increase by 82%.",https://ieeexplore.ieee.org/document/6570213/,2013 IEEE International Symposium on Consumer Electronics (ISCE),3-6 June 2013,ieeexplore
10.1109/ICCDS.2017.8120453,Power optimization using Markov decision process based on multi-parameter constraint modeling,IEEE,Conferences,"Power optimization based on intelligent algorithm draws more and more attention. This article presents a novel low power optimization strategy based on the high level software power management employing Markov Process for charactering the real running workload. This article formulates workload characterization and selection with stochastic process method, and solves the formula using dynamic voltage frequency scaling base on microprocessor. Based on Markov process, the multi-parameter constraints has been employed to exploit the optimization space. Comparing with existing power optimization algorithm, our proposed power optimization algorithm doesn't need any prior data and maintains a value function representing expected reward. As many hardware events can be effectively captured and modeled, this optimization technique is capable to explore an ideal tradeoff in the constraint space.",https://ieeexplore.ieee.org/document/8120453/,"2017 International Conference on Circuits, Devices and Systems (ICCDS)",5-8 Sept. 2017,ieeexplore
10.1109/MASCOTS.2019.00045,Practical Design Space Exploration,IEEE,Conferences,"Multi-objective optimization is a crucial matter in computer systems design space exploration because real-world applications often rely on a trade-off between several objectives. Derivatives are usually not available or impractical to compute and the feasibility of an experiment can not always be determined in advance. These problems are particularly difficult when the feasible region is relatively small, and it may be prohibitive to even find a feasible experiment, let alone an optimal one. We introduce a new methodology and corresponding software framework, HyperMapper 2.0, which handles multi-objective optimization, unknown feasibility constraints, and categorical/ordinal variables. This new methodology also supports injection of the user prior knowledge in the search when available. All of these features are common requirements in computer systems but rarely exposed in existing design space exploration systems. The proposed methodology follows a white-box model which is simple to understand and interpret (unlike, for example, neural networks) and can be used by the user to better understand the results of the automatic search. We apply and evaluate the new methodology to the automatic static tuning of hardware accelerators within the recently introduced Spatial programming language, with minimization of design run-time and compute logic under the constraint of the design fitting in a target field-programmable gate array chip. Our results show that HyperMapper 2.0 provides better Pareto fronts compared to state-of-the-art baselines, with better or competitive hypervolume indicator and with 8x improvement in sampling budget for most of the benchmarks explored.",https://ieeexplore.ieee.org/document/8843094/,"2019 IEEE 27th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",21-25 Oct 2019,ieeexplore
10.1109/ICNC.2010.5584403,Practical travel time prediction algorithms based on neural network and data fusion for urban expressway,IEEE,Conferences,"Current travel time prediction algorithms often need large amount of travel time data to identify the algorithms parameters. However, it is highly costly and time-consuming to obtain many travel time data. In this paper, we proposed an algorithm structure to calculate the travel time which utilize neural network to dynamically predict future speed and employ data fusion to integrate the speed data of different detectors in a urban expressway link. Based on the algorithm structure, two practical travel time prediction algorithms, called as Space Discretization Travel Time Calculation Algorithm (SDTCM) and Speed Integral Travel Time Calculation Method (SITCM), were developed by the discretization of the space and integral of the predicted speed. Vehicle plate recognition technology was used to collect the real travel time data in the test section on Beijing Third-Ring urban expressway to evaluate the algorithms. The obtained results show that the average prediction error of two algorithms are both less than 10% which can meet the requirement of the field applications and the algorithms are easy to be implemented as no travel time data collection are needed in advance. The two algorithms have some advantages and disadvantages over each other in accuracy and smoothness. Although SDTCM is more accurate than SITCM in general, the fluctuation of error of the SITCM is a little smoother than SDTCM.",https://ieeexplore.ieee.org/document/5584403/,2010 Sixth International Conference on Natural Computation,10-12 Aug. 2010,ieeexplore
10.1145/2656045.2656064,Precise piecewise affine models from input-output data,IEEE,Conferences,"Formal design and analysis of embedded control software relies on mathematical models of dynamical systems, and such models can be hard to obtain. In this paper, we focus on automatic construction of piecewise affine models from input-output data. Given a set of examples, where each example consists of a d-dimensional real-valued input vector mapped to a real-valued output, we want to compute a set of affine functions that covers all the data points up to a specified degree of accuracy, along with a disjoint partitioning of the space of all inputs defined using a Boolean combination of affine inequalities with one region for each of the learnt functions. While traditional machine learning algorithms such as linear regression can be adapted to learn the set of affine functions, we develop new techniques based on automatic construction of interpolants to derive precise guards defining the desired partitioning corresponding to these functions. We report on a prototype tool, MOSAIC, implemented in Matlab. We evaluate its performance using some synthetic data, and compare it against known techniques using data-sets modeling electronic placement process in pick-and-place machines.",https://ieeexplore.ieee.org/document/6986111/,2014 International Conference on Embedded Software (EMSOFT),12-17 Oct. 2014,ieeexplore
10.1109/ICDMW.2016.0026,Predicting User Roles in Social Networks Using Transfer Learning with Feature Transformation,IEEE,Conferences,"How can we recognise social roles of people, given a completely unlabelled social network? We may train a role classification algorithm on another dataset, but then that dataset may have largely different values of its features, for instance, the degrees in the other network may be distributed in a completely different way than in the first network. Thus, a way to transfer the features of different networks to each other or to a common feature space is needed. This type of setting is called transfer learning. In this paper, we present a transfer learning approach to network role classification based on feature transformations from each network's local feature distribution to a global feature space. We implement our approach and show experiments on real-world networks of discussions on Wikipedia as well as online forums. We also show a concrete application of our approach to an enterprise use case, where we predict the user roles in ARIS Community, the online platform for customers of Software AG, the second-largest German software vendor. Evaluation results show that our approach is suitable for transferring knowledge of user roles across networks.",https://ieeexplore.ieee.org/document/7836657/,2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW),12-15 Dec. 2016,ieeexplore
10.1109/IJCNN.2018.8489412,Prediction of Spatial Spectrum in Cognitive Radio using Cellular Simultaneous Recurrent Networks,IEEE,Conferences,"In cognitive radio networks, it is desirable to determine radio spectrum usage in frequency, time, and spatial domains. Spectrum data improves cognitive radio network planning, sensing, routing, and security. Due to cost concerns, spectrum monitors are deployed sparsely in space, spectrum usage at nearby locations can be modeled for use in these applications. Previous work using neural networks for spatial spectrum prediction involved prior knowledge of transmitter locations as input to the models. In practical scenarios, the prior knowledge is not available. Hence, this work considers prediction of the spatial spectrum without knowing the transmitter location information. The prediction task is achieved by using a specialized recurrent neural network known as Cellular Simultaneous Recurrent Network (CSRN). Our investigation shows the proposed recurrent neural network operates in real-time and is generalized to offer spectrum estimations without further changes to the network, even when a transmitter location is changed. The experiments are conducted in a challenging indoor environment to assess the performance in a practical scenario. Our results suggest the CSRN can learn efficiently to predict signal across an indoor space while transmitters move to different locations. We perform a performance comparison of our proposed technique with an MLP based estimation method. Our analysis further suggests that the CSRN achieves comparable prediction accuracy to that of the MLP based method. The major advantage of the proposed CSRN based method is the ability to perform prediction from new radio configurations without retraining the network, and, hence is more suitable for a real-time practical environment.",https://ieeexplore.ieee.org/document/8489412/,2018 International Joint Conference on Neural Networks (IJCNN),8-13 July 2018,ieeexplore
10.1109/IJCNN.2003.1224017,Prediction of pitch and yaw head movements via recurrent neural networks,IEEE,Conferences,"In virtual-environment (VE) applications, where virtual objects are presented in a head-mounted display, virtual images must be continuously stabilized in space against the user's head motion. Latencies in head-motion compensation cause virtual objects to swim around instead of being stable in space. This results in an unnatural feel, disorientation, and simulation sickness in addition to errors in fitting/matching of virtual and real objects. Visual update delays are a critical technical obstacle for implementation of head-mounted displays in a wide variety of applications. To address this problem, we propose to use machine learning techniques to define a forward model of head movement based on angular velocity information. In particular, we utilize recurrent neural network to capture the temporal pattern of pitch and yaw motion. Our results demonstrate an ability to predict head motion up to 40 ms. ahead thus eliminating the main source of latencies. The accuracy of the system is tested for conditions akin to those encountered in virtual environments. These results demonstrate successful generalization by the learning system.",https://ieeexplore.ieee.org/document/1224017/,"Proceedings of the International Joint Conference on Neural Networks, 2003.",20-24 July 2003,ieeexplore
10.1109/EMBC.2017.8037453,Predictive local receptive fields based respiratory motion tracking for motion-adaptive radiotherapy,IEEE,Conferences,"Extracranial robotic radiotherapy employs external markers and a correlation model to trace the tumor motion caused by the respiration. The real-time tracking of tumor motion however requires a prediction model to compensate the latencies induced by the software (image data acquisition and processing) and hardware (mechanical and kinematic) limitations of the treatment system. A new prediction algorithm based on local receptive fields extreme learning machines (pLRF-ELM) is proposed for respiratory motion prediction. All the existing respiratory motion prediction methods model the non-stationary respiratory motion traces directly to predict the future values. Unlike these existing methods, the pLRF-ELM performs prediction by modeling the higher-level features obtained by mapping the raw respiratory motion into the random feature space of ELM instead of directly modeling the raw respiratory motion. The developed method is evaluated using the dataset acquired from 31 patients for two horizons in-line with the latencies of treatment systems like CyberKnife. Results showed that pLRF-ELM is superior to that of existing prediction methods. Results further highlight that the abstracted higher-level features are suitable to approximate the nonlinear and non-stationary characteristics of respiratory motion for accurate prediction.",https://ieeexplore.ieee.org/document/8037453/,2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),11-15 July 2017,ieeexplore
10.1109/ANZIIS.1994.396989,Primitive concept formation,IEEE,Conferences,"Our goal is to demonstrate the feasibility of an autonomous learning agent by developing means to learn and employ concepts in a primitive machine intelligence which must operate in a real-time, uncertain (noisy) environment. The paper reports on the first steps towards such an agent: the development of an agent, Alice, who starts out with only a primitive set of concepts-corresponding to perceptible attributes of objects in the environment and to her own utility function-and who generates a conceptual structure using cognitively plausible rules of concept formation and refinement, abstracting from the immediate attributes of the mushrooms she finds and their longer-term impact on her utility, in a goal-driven manner. The concept formation rules we have developed are more conservative than such standard methods of concept formation as version space methods and ID3. We suggest that this caution offers a competitive advantage in difficult environments.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/396989/,Proceedings of ANZIIS '94 - Australian New Zealnd Intelligent Information Systems Conference,29 Nov.-2 Dec. 1994,ieeexplore
10.1109/ICDCS51616.2021.00077,Privacy-Preserving Neural Network Inference Framework via Homomorphic Encryption and SGX,IEEE,Conferences,"Edge computing is a promising paradigm that pushes computing, storage, and energy to the networks' edge. It utilizes the data nearby the users to provide real-time, energy-efficient, and reliable services. Neural network inference in edge computing is a powerful tool for various applications. However, edge server will collect more personal sensitive information of users inevitably. It is the most basic requirement for users to ensure their security and privacy while obtaining accurate inference results. Homomorphic encryption (HE) technology is confidential computing that directly performs mathematical computing on encrypted data. But it only can carry out limited addition and multiplication operation with very low efficiency. Intel software guard extension (SGX) can provide a trusted isolation space in the CPU to ensure the confidentiality and integrity of code and data executed. But several defects are hard to overcome due to hardware design limitations when applying SGX in inference services. This paper proposes a hybrid framework utilizing SGX to accelerate the HE-based convolutional neural network (CNN) inference, eliminating the approximation operations in HE to improve inference accuracy in theory. Besides, SGX is also taken as a built-in trusted third party to distribute keys, thereby improving our framework's scalability and flexibility. We have quantified the various CNN operations in the respective cases of HE and SGX to provide the foresight practice. Taking the connected and autonomous vehicles as a case study in edge computing, we implemented this hybrid framework in CNN to verify its feasibility and advantage.",https://ieeexplore.ieee.org/document/9546527/,2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS),7-10 July 2021,ieeexplore
10.1109/ROBIO49542.2019.8961870,Probabilistic Inferences on Quadruped Robots: An Experimental Comparison,IEEE,Conferences,"Due to the reality gap, computer software cannot fully model the physical robot in its environment, with noise, ground friction, and energy consumption. Consequently, a limited number of researchers work on applying machine learning in real-world robots. In this paper, we use two intelligent black-box optimization algorithms, Bayesian Optimization (BO) and Covariance Matrix Adaptation Evolution Strategy (CMA-ES), to solve a quadruped robot gait's parametric search problem in 10 dimensions, and compare these two methods to find which one is more suitable for legged robots' controller parameters tuning. Our results show that both methods can find an optimal solution in 130 iterations. BO converges faster than CMA-ES within its constrained range, while CMA-ES finds the optimum in the continuous space. Compared with the specific controller parameters of two methods, we also find that for quadruped robot's oscillators, the angular amplitude is the most important parameter. Thus, it is very beneficial for the quick parametric search of legged robots' controllers and avoids time-consuming manual tuning.",https://ieeexplore.ieee.org/document/8961870/,2019 IEEE International Conference on Robotics and Biomimetics (ROBIO),6-8 Dec. 2019,ieeexplore
10.1109/CIG.2019.8848047,Project Thyia: A Forever Gameplayer,IEEE,Conferences,"The space of Artificial Intelligence entities is dominated by conversational bots. Some of them fit in our pockets and we take them everywhere we go, or allow them to be a part of human homes. Siri, Alexa, they are recognised as present in our world. But a lot of games research is restricted to existing in the separate realm of software. We enter different worlds when playing games, but those worlds cease to exist once we quit. Similarly, AI game-players are run once on a game (or maybe for longer periods of time, in the case of learning algorithms which need some, still limited, period for training), and they cease to exist once the game ends. But what if they didn't? What if there existed artificial game-players that continuously played games, learned from their experiences and kept getting better? What if they interacted with the real world and us, humans: live-streaming games, chatting with viewers, accepting suggestions for strategies or games to play, forming opinions on popular game titles? In this paper, we introduce the vision behind a new project called Thyia, which focuses around creating a present, continuous, `always-on', interactive game-player.",https://ieeexplore.ieee.org/document/8848047/,2019 IEEE Conference on Games (CoG),20-23 Aug. 2019,ieeexplore
10.1109/ICCW.2019.8757132,QoE Aware Transcoding for Live Streaming in SDN-Based Cloud-Aided HetNets: An Actor-Critic Approach,IEEE,Conferences,"With the advances in hand-held devices (smart-phones and tablets, etc.) and high speed wireless networks, users have an explosive growth demand for live streaming service. Due to the diversity of user equipments (UEs), the live streaming has to be transcoded as different versions. However, transcoding is a computationally expensive and time consuming process. Since the shortage of computational resources and unstable of wireless networks, providing strict delay requirement and high quality live videos for wireless UEs is a big challenge. In this paper, we investigate user scheduling, transcoding decision, computational and wireless spectrum resources allocation problem in software-defined networking (SDN) based cloud-aided of heterogeneous networks (HetNets). Our research focuses on improving UEs' quality of experience (QoE) while guaranteeing time-delay requirement for live streaming services. Different from existing literature, to approach the real wireless environment, the available computational and wireless spectrum resources are modeled as random processes in our research. Considering dynamic characteristics of wireless networks and the available resources, the above problem is modeled as a Markov decision problem (MDP). Since the action space of the MDP is multi-dimensional continuous variables mixed with discrete variables, traditional learning algorithms are powerless. Therefore, an online actor critic algorithm is proposed to resolve the problem. Simulation results show the proposed algorithm has superior performances compared with the policy gradient algorithm and deep Q-learning network (DQN).",https://ieeexplore.ieee.org/document/8757132/,2019 IEEE International Conference on Communications Workshops (ICC Workshops),20-24 May 2019,ieeexplore
10.1109/CCAAW.2019.8904903,Quantifying Degradations of Convolutional Neural Networks in Space Environments,IEEE,Conferences,"Advances in machine learning applications for image processing, natural language processing, and direct ingestion of radio frequency signals continue to accelerate. Less attention, however, has been paid to the resilience of these machine learning algorithms when implemented on real hardware and subjected to unintentional and/or malicious errors during execution, such as those occurring from space-based single event upsets (SEU). This paper presents a series of results quantifying the rate and level of performance degradation that occurs when convolutional neural nets (CNNs) are subjected to selected bit errors in single-precision number representations. This paper provides results that are conditioned upon ten different error case events to isolate the impacts showing that CNN performance can be gradually degraded or reduced to random guessing based on where errors arise. The degradations are then translated into expected operational lifetimes for each of four CNNs when deployed to space radiation environments. The discussion also provides a foundation for ongoing research that enhances the overall resilience of neural net architectures and implementations in space under both random and malicious error events, offering significant improvements over current implementations. Future work to extend these CNN resilience evaluations, conditioned upon architectural design elements and well-known error correction methods, is also introduced.",https://ieeexplore.ieee.org/document/8904903/,2019 IEEE Cognitive Communications for Aerospace Applications Workshop (CCAAW),25-26 June 2019,ieeexplore
10.1109/IJCNN.2012.6252422,Query based hybrid learning models for adaptively adjusting locality,IEEE,Conferences,"Local learning employs locality adjusting mechanisms to give local function estimation for each query, while global learning tries to capture the global distribution characteristics of the entire training set. When fitting well with local characteristics of each individual region, the locality parameter may help local learning to improve performance. However, the real data distribution is impossible to get for a real-world problem, and thus an optimal locality is hard to get for each query. In addition, it is quite time-consuming to build an independent local model for each query. To solve these problems, we present strategies for estimating and tuning locality according to local distribution. Based on local distribution estimation, global learning and local learning are combined to achieve a good compromise between capacity and locality. In addition, multi-objective learning principles for the combination are also given. In implementation, a unique global model is first built on the entire training set based on empirical minimization principle. For each query, it is measured that whether the global model can well fit the vicinity space of the query. When an uneven local distribution is found, the locality of the model is tuned, and a specific local model will be built on the local region. To investigate the performance of hybrid models, we apply them to a typical learning problem-spam filtering, in which data are always found to be unevenly distributed. Experiments were conducted on five real-world corpora, namely PU1, PU2, PU3, PUA, and TREC07. It is shown that the hybrid models can achieve a better compromise between capacity and locality, and hybrid models outperform both global learning and local learning.",https://ieeexplore.ieee.org/document/6252422/,The 2012 International Joint Conference on Neural Networks (IJCNN),10-15 June 2012,ieeexplore
10.1109/DS-RT.2016.9,RA2: Predicting Simulation Execution Time for Cloud-Based Design Space Explorations,IEEE,Conferences,"Design space exploration refers to the evaluation of implementation alternatives for many engineering and design problems. A popular exploration approach is to run a large number of simulations of the actual system with varying sets of configuration parameters to search for the optimal ones. Due to the potentially huge resource requirements, cloud-based simulation execution strategies should be considered in many cases. In this paper, we look at the issue of running large-scale simulation-based design space exploration problems on commercial Infrastructure-as-a-Service clouds, namely Amazon EC2, Microsoft Azure and Google Compute Engine. To efficiently manage cloud resources used for execution, the key problem would be to accurately predict the running time for each simulation instance in advance. This is not trivial due to the currently wide range of cloud resource types which offer varying levels of performance. In addition, the widespread use of virtualization techniques in most cloud providers often introduces unpredictable performance interference. In this paper, we propose a resource and application-aware (RA2) prediction approach to combat performance variability on clouds. In particular, we employ neural network based techniques coupled with non-intrusive monitoring of resource availability to obtain more accurate predictions. We conducted extensive experiments on commercial cloud platforms using an evacuation planning design problem over a month-long period. The results demonstrate that it is possible to predict simulation execution times in most cases with high accuracy. The experiments also provide some interesting insights on how we should run similar simulation problems on various commercially available clouds.",https://ieeexplore.ieee.org/document/7789881/,2016 IEEE/ACM 20th International Symposium on Distributed Simulation and Real Time Applications (DS-RT),21-23 Sept. 2016,ieeexplore
10.1109/ICMNN.1994.593723,RAN/sup 2/SOM: a reconfigurable neural network architecture based on bit stream arithmetic,IEEE,Conferences,"We introduce the RAN/sup 2/SOM (Reconfigurable Architecture Neural Networks with Serially Operating Multipliers) architecture, a neural net architecture with a reconfigurable interconnection scheme based on bit stream arithmetic. RAN/sup 2/SOM nets are implemented using field programmable gate array logic. By conducting the training phase in software and executing the actual application in hardware, conflicting demands can be met: training benefits from a fast edit-debug cycle, and once the design has stabilized a hardware implementation results in higher performance. While neural nets have been implemented in hardware in the past, larger digital nets have not been possible due to the real-estate requirements of single neutrons. We present a bit-serial encoding scheme and computation model, which allows space-efficient computation of the sum of weighted inputs, thereby facilitating the implementation of complex neural networks.",https://ieeexplore.ieee.org/document/593723/,Proceedings of the Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems,26-28 Sept. 1994,ieeexplore
10.1109/FPA.1994.636106,RCS: a reference model architecture for intelligent control,IEEE,Conferences,"The Real-time Control System (RCS) is a reference model architecture for intelligent real time control systems. It partitions the control problem into four basic elements: task decomposition, world modeling, sensory processing, and value judgment. It clusters these elements into computational nodes that have responsibility for specific subsystems and arranges these nodes in hierarchical layers such that each layer has characteristic functionality and timing. The RCS architecture has a systematic regularity, and recursive structure that suggests a canonical form. Systems based on the RCS architecture have been implemented more or less for a wide variety of applications that include loading and unloading of parts and tools in machine tools, controlling machining workstations, performing robotic deburring and chamfering, and controlling space station telerobots, multiple autonomous undersea vehicles, unmanned land vehicles, coal mining automation systems. postal service mail handling systems, and submarine operational automation systems. Software developers accustomed to using RCS for building control systems have found it provides a structured design approach that makes it possible to reuse a great deal of software.",https://ieeexplore.ieee.org/document/636106/,Proceedings of PerAc '94. From Perception to Action,7-9 Sept. 1994,ieeexplore
10.1109/CVPR.2019.00605,RL-GAN-Net: A Reinforcement Learning Agent Controlled GAN Network for Real-Time Point Cloud Shape Completion,IEEE,Conferences,"We present RL-GAN-Net, where a reinforcement learning (RL) agent provides fast and robust control of a generative adversarial network (GAN). Our framework is applied to point cloud shape completion that converts noisy, partial point cloud data into a high-fidelity completed shape by controlling the GAN. While a GAN is unstable and hard to train, we circumvent the problem by (1) training the GAN on the latent space representation whose dimension is reduced compared to the raw point cloud input and (2) using an RL agent to find the correct input to the GAN to generate the latent space representation of the shape that best fits the current input of incomplete point cloud. The suggested pipeline robustly completes point cloud with large missing regions. To the best of our knowledge, this is the first attempt to train an RL agent to control the GAN, which effectively learns the highly nonlinear mapping from the input noise of the GAN to the latent space of point cloud. The RL agent replaces the need for complex optimization and consequently makes our technique real time. Additionally, we demonstrate that our pipelines can be used to enhance the classification accuracy of point cloud with missing data.",https://ieeexplore.ieee.org/document/8953469/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore
10.1109/IJCNN.2019.8851689,RPR-BP: A Deep Reinforcement Learning Method for Automatic Hyperparameter Optimization,IEEE,Conferences,"We introduce a new deep reinforcement learning architecture - RPR-BP to optimize hyperparameter for any machine learning model on a given data set. In this method, an agent constructed by a Long Short-Term Memory Network aims at maximizing the expected accuracy of a machine learning model on a validation set. At each iteration, it selects a set of hyperparameters and uses the accuracy of the model on the validation set as the reward signal to update its internal parameters. After multiple iterations, the agent learns how to improve its decisions. However, the computation of the reward requires significant time and leads to low sample efficiency. To speed up training, we employ a neural network to predict the reward. The training process for the agent and the prediction network is divided into three phases: Real-Predictive-Real (RPR). First, the agent and the prediction network are trained by the real experience; then, the agent is trained by the reward generated from the prediction network; finally, the agent is trained again by the real experience. In this way, we can speed up training and make the agent achieve a high accuracy. Besides, to reduce the variance, we propose a Bootstrap Pool (BP) to guide the exploration in the search space. The experiment was carried out by optimizing hyperparameters of two widely used machine learning models: Random Forest and XGBoost. Experimental results show that the proposed method outperforms random search, Bayesian optimization and Tree-structured Parzen Estimator in terms of accuracy, time efficiency and stability.",https://ieeexplore.ieee.org/document/8851689/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore
10.1109/GCAIoT51063.2020.9345877,RSSI Based Real-Time and Secure Smart Parking Management System,IEEE,Conferences,"Discovering an available parking space in an unsupervised parking area is one of the critical issues that vehicle owners face which consumes a considerable time and effort. In this paper, a Received Signal Strength Indicator (RSSI) based approach is proposed for detecting available parking spaces. The Non-Linear Least Square method has been used to minimize the effects from external interferences. Further, itemploys a lightweight and scalable Message Queuing Telemetry Transport (MQTT) communication protocol. The proposed method does not require sensors to be deployed on each parking slot, which is more frequently used technique in the existing approaches, and it can provide a real-time representation of available parking slots. Moreover, the vehicle owners can discover available parking spaces remotely using the developed mobile application; thus, it saves a significant amount of time. Therefore, it exhibits a great promise as a real-time, cost effective, highly scalable and secure solution.",https://ieeexplore.ieee.org/document/9345877/,2020 IEEE Global Conference on Artificial Intelligence and Internet of Things (GCAIoT),12-16 Dec. 2020,ieeexplore
10.1109/SMC-IT.2009.40,Rapid Prototyping of Planning &amp; Scheduling Tools,IEEE,Conferences,"The Advanced Planning and Scheduling Initiative, or APSI, is an ESA programme to design and implement an Artificial Intelligence (AI) software infrastructure for planning and scheduling that can generically support different types and classes of space mission operations. The goal of the APSI is twofold: (1)~creating a software framework to improve the cost-effectiveness and flexibility of mission planning support tool development; (2)~bridging the gap between AI planning and scheduling technology and the world of space mission planning. A key aspect of the success of this project is the presence of a flexible timeline representation module that allows to exploit alternatives in the modeling of mission features. This paper shows an example of such a flexibility by using a real problem in the space realm - the HERSCHEL Science Long Term Planning process.",https://ieeexplore.ieee.org/document/5226820/,2009 Third IEEE International Conference on Space Mission Challenges for Information Technology,19-23 July 2009,ieeexplore
10.1109/CDC.2014.7039601,Reachability-based safe learning with Gaussian processes,IEEE,Conferences,"Reinforcement learning for robotic applications faces the challenge of constraint satisfaction, which currently impedes its application to safety critical systems. Recent approaches successfully introduce safety based on reachability analysis, determining a safe region of the state space where the system can operate. However, overly constraining the freedom of the system can negatively affect performance, while attempting to learn less conservative safety constraints might fail to preserve safety if the learned constraints are inaccurate. We propose a novel method that uses a principled approach to learn the system's unknown dynamics based on a Gaussian process model and iteratively approximates the maximal safe set. A modified control strategy based on real-time model validation preserves safety under weaker conditions than current approaches. Our framework further incorporates safety into the reinforcement learning performance metric, allowing a better integration of safety and learning. We demonstrate our algorithm on simulations of a cart-pole system and on an experimental quadrotor application and show how our proposed scheme succeeds in preserving safety where current approaches fail to avoid an unsafe condition.",https://ieeexplore.ieee.org/document/7039601/,53rd IEEE Conference on Decision and Control,15-17 Dec. 2014,ieeexplore
10.1109/ICIT.2006.372319,Real Time Classifier For Industrial Wireless Sensor Network Using Neural Networks with Wavelet Preprocessors,IEEE,Conferences,"Wireless sensor node is embedded of computation unit, sensing unit and a radio unit for communication. Amongst three units communication is the largest consumer of energy. Energy is the prime source for wireless sensor node to function. Hence every aspects of sensor node are designed with energy constraints. Neural Networks in particular the combination of ART1 and FuzzyART(FA) can be used very efficiently for developing Real time Classifier. Wireless sensor networks demand for the real time classification of sensor data. In this paper classification technique using ART1 and Fuzzy ART is discussed. ART1 and FA have very good architectural strategy, which makes it simple for VLSI implementation. The VLSI implementation of the proposed classifier can be a part of embedded microsensor. The paper discusses classification technique, which can reduce the energy need for communication and improves communications bandwidth. The proposed sensor clustering architecture can give distributed storage space for the sensor networks. Wavelet Transform is used as preprocessor for denoising the real word data from sensor node, this makes it much suitable for industrial environment. Many methods of wavelet transforms are available. Simplest Haar 1D transform is used for preprocessing and smoothing the sensor signals. The discrete wavelet transform implemented here helps to extract important feature in the sensor data like sudden changes at various scales.",https://ieeexplore.ieee.org/document/4237641/,2006 IEEE International Conference on Industrial Technology,15-17 Dec. 2006,ieeexplore
10.1109/ic-ETITE47903.2020.163,Real Time Object Detection in Surveillance Cameras with Distance Estimation using Parallel Implementation,IEEE,Conferences,"Object detection is not only shaping how Computers see and analyze things but it is also helping in the behavior of how an object reacts to the change in its environment. The main application of these object detection sensors or software is to find the location of an object in space or to track its movement. Object detection has infinitely many use cases and in this paper, we are introducing an application that will allow safety of users struck in a disaster and who need to be evacuated. In such cases the main thing to focus and to eradicate is camera noise, saturation and image compression. Our solution is to establish a connection between the person struck in a disaster with fire safety people. This works over a convolutional network that allows us to detect vulnerable things present inside a room that needs to be rescued and can also give an insight of any explosive inside the room. Our model uses Faster-RCNN and COCO which is a pretrained dataset. This allows real time object detection and classification on our network. Using this we were able to detect an object or a person and get him to rescue by providing them a shortest way out of that place. With this we were able to get an accuracy of more than 75% in our object detection model.",https://ieeexplore.ieee.org/document/9077855/,2020 International Conference on Emerging Trends in Information Technology and Engineering (ic-ETITE),24-25 Feb. 2020,ieeexplore
10.1109/ULTSYM.2009.5441886,Real time adaptive parametric equalization of Ultrasonic Transducers,IEEE,Conferences,"Parametric equalization is often used to achieve a desired response from an audio transmitter, but is rarely applied to ultrasonic transducer systems. The ability of a broadband ultrasonic transmission and reception system to adapt its frequency and time domain response to changing acoustic conditions would be a distinct advantage in certain applications. Ultrasonic remote monitoring systems would benefit significantly from this ability, as signal levels could be minimized and consequentially the transmitter power consumption decreased. This work presents a real-time adaptive ultrasonic parametric equalizer using optimization driven Matlab code to control the coefficients of a switched capacitor filter network implemented in a Cypress PSOC (Programmable System On a Chip). In this work, adaptive parametric magnitude equalization of a through-transmission ultrasonic system using CUTs (Capacitive Ultrasonic Transducers) has been achieved in real time by tracking a desired SNR (signal to noise ratio) across the operational frequency spectrum. A Matlab general radial basis function (GRBF) artificial neural network (ANN) was developed to control the equalization filter coefficients based on the received frequency response data. The adaptive parametric equaliser adjusts the magnitude of the driving signal to maintain the desired SNR as closely as possible. The neural network was trained using PSO (Particle Swarm Optimization) back-propagation, based on a state space model of the system developed from frequency response data. The developed equalization circuitry, which is switched capacitor based and was fully implemented on the PSOC, is also described.",https://ieeexplore.ieee.org/document/5441886/,2009 IEEE International Ultrasonics Symposium,20-23 Sept. 2009,ieeexplore
10.1109/ICDSP.2002.1028228,Real time nonlinear ARMA model structure identification,IEEE,Conferences,"This paper addresses the nonlinear autoregressive moving average (NARMA) identification problem in connection with the choice of the model structure (order) and computation of the time varying system coefficients. We introduce an intelligent method that is based on the reformulation of the problem in the standard state space form and the subsequent implementation of a bank of extended Kalman filters, each fitting a different order model. The problem is reduced then to selecting the true model, using the well known multi-model partitioning theory. Simulations illustrate that the proposed method is selecting the correct model order and identifies the time varying model parameters in real time, while it is insensitive to the noise variations.",https://ieeexplore.ieee.org/document/1028228/,2002 14th International Conference on Digital Signal Processing Proceedings. DSP 2002 (Cat. No.02TH8628),1-3 July 2002,ieeexplore
10.1109/CAC51589.2020.9327198,Real time production scheduling based on Asynchronous Advanced Actor Critic and composite dispatching rule,IEEE,Conferences,"In the era of smart manufacturing, the requirements of real-time, adaptability and long-term optimization of the semiconductor manufacturing system (SMS) are increased due to the further expanded, more complicated and unpredictable uncertainties. This paper addresses the real time production scheduling of SMS to maximize on productivity (PROD) and average daily movement (AvgMOV), and minimize mean cycle time (MCT). We propose an Asynchronous Advanced Actor Critic and composite dispatching rule based real time production scheduling (A3C-CR2) framework, which involves a scheduling knowledge training module and a deployment module. The action space is designed as a combination of the composite dispatching rule (CDR) based continuous scheduling actions. In terms of various performance indices over a long period, the proposed A3C-CR2 approach outperforms other dispatching rules.",https://ieeexplore.ieee.org/document/9327198/,2020 Chinese Automation Congress (CAC),6-8 Nov. 2020,ieeexplore
10.1109/TENCON.2018.8650524,Real-Time American Sign Language Recognition Using Skin Segmentation and Image Category Classification with Convolutional Neural Network and Deep Learning,IEEE,Conferences,A real-time sign language translator is an important milestone in facilitating communication between the deaf community and the general public. We hereby present the development and implementation of an American Sign Language (ASL) fingerspelling translator based on skin segmentation and machine learning algorithms. We present an automatic human skin segmentation algorithm based on color information. The YCbCr color space is employed because it is typically used in video coding and provides an effective use of chrominance information for modeling the human skin color. We model the skin-color distribution as a bivariate normal distribution in the CbCr plane. The performance of the algorithm is illustrated by simulations carried out on images depicting people of different ethnicity. Then Convolutional Neural Network (CNN) is used to extract features from the images and Deep Learning Method is used to train a classifier to recognize Sign Language.,https://ieeexplore.ieee.org/document/8650524/,TENCON 2018 - 2018 IEEE Region 10 Conference,28-31 Oct. 2018,ieeexplore
10.1109/ICASSP40776.2020.9053215,Real-Time Binaural Speech Separation with Preserved Spatial Cues,IEEE,Conferences,"Deep learning speech separation algorithms have achieved great success in improving the quality and intelligibility of separated speech from mixed audio. Most previous methods focused on generating a single-channel output for each of the target speakers, hence discarding the spatial cues needed for the localization of sound sources in space. However, preserving the spatial information is important in many applications that aim to accurately render the acoustic scene such as in hearing aids and augmented reality (AR). Here, we propose a speech separation algorithm that preserves the interaural cues of separated sound sources and can be implemented with low latency and high fidelity, therefore enabling a real-time modification of the acoustic scene. Based on the time-domain audio separation network (TasNet), a single-channel time-domain speech separation system that can be implemented in real-time, we propose a multi-input-multi-output (MIMO) end-to-end extension of TasNet that takes binaural mixed audio as input and simultaneously separates target speakers in both channels. Experimental results show that the proposed end-to-end MIMO system is able to significantly improve the separation performance and keep the perceived location of the modified sources intact in various acoustic scenes.",https://ieeexplore.ieee.org/document/9053215/,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",4-8 May 2020,ieeexplore
10.1109/CISIS.2012.213,Real-Time Classification of Sports Movement Using Adaptive Clustering,IEEE,Conferences,"Computer-based instructional systems provide an ideal setting for learning certain types of sports. In particular, the sports that require premium space could leverage the widely available computing and Internet facilities to teach individual users anywhere and anytime. An e-learning tennis instruction system is currently being designed and developed. The Nintendo Wii Remote is selected as the input device for its low cost and racket-handle like shape. After the data from motion sensors are captured, they have to be cleansed, normalised clustered and classified. Data of three common swings, backhand, forehand, and overhand, have been recorded from fifty people of various levels of tennis skill. Experiments are carried out to identify the most suitable techniques to classify a tennis swing. The adaptive nature of a prototype system is also introduced.",https://ieeexplore.ieee.org/document/6245591/,"2012 Sixth International Conference on Complex, Intelligent, and Software Intensive Systems",4-6 July 2012,ieeexplore
10.1109/ICTAI.2008.143,Real-Time Classification of Streaming Sensor Data,IEEE,Conferences,"The last decade has seen a huge interest in classification of time series. Most of this work assumes that the data resides in main memory and is processed offline. However, recent advances in sensor technologies require resource-efficient algorithms that can be implemented directly on the sensors as real-time algorithms. We show how a recently introduced framework for time series classification, time series bitmaps, can be implemented as efficient classifiers which can be updated in constant time and space in the face of very high data arrival rates. We describe results from a case study of an important entomological problem, and further demonstrate the generality of our ideas with an example from robotics.",https://ieeexplore.ieee.org/document/4669683/,2008 20th IEEE International Conference on Tools with Artificial Intelligence,3-5 Nov. 2008,ieeexplore
10.1109/CHASE.2016.72,Real-Time Tidal Volume Estimation Using Iso-surface Reconstruction,IEEE,Conferences,"Breathing volume measurement has long been an important physiological indication widely used for the diagnosis and treatment of pulmonary diseases. However, most of existing breathing volume monitoring techniques require either physical contact with the patient or are prohibitively expensive. In this paper we present an automated and inexpensive non-contact, vision-based method for monitoring an individual's tidal volume, which is extracted from a three-dimensional (3D) chest surface reconstruction from a single depth camera. In particular, formulating the respiration monitoring process as a 3D space-time volumetric representation, we introduce a real-time surface reconstruction algorithm to generate omni-direction deformation states of a patient's chest while breathing, which reflects the change in tidal volume over time. These deformation states are then used to estimate breathing volume through a per-patient correlation metric acquired through a Bayesian-network learning process. Through prototyping and implementation, our results indicate that we have achieved 92.2% to 94.19% accuracy in the tidal volume estimations through the experimentation based on the proposed vision-based method.",https://ieeexplore.ieee.org/document/7545835/,"2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)",27-29 June 2016,ieeexplore
10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.000-6,Real-Time Traffic Estimation of Unmonitored Roads,IEEE,Conferences,"Due to growing cities, the real-time knowledge about the state of traffic is a critical problem for urban mobility and the road-segments traffic densities estimation influences the efficiency of fundamental smart city services as smart routing, smart planning for evacuations, planning of civil works on the city, etc. Nevertheless, the traffic-related data from navigator Apps (e.g., TomTom, Google, Bing) are too expensive to be acquired. Also, the traditional sensors for the traffic flow detection are very expensive, and they are usually not dense enough for a correct traffic monitoring. In order to overcome such problems, there is a space for low cost and fast solutions for dense traffic flow reconstruction. We propose a real-time visual self-adaptive solution to reconstruct the traffic density at every location of a wide urban area leveraging the detections from a few fixed traffic sensors deployed within the area of interest. A such method is based on fluid dynamic models to simulate macroscopic phenomena as shocks formation and propagation of waves backwards along roads. Such physical constraints are applied to a detailed street graph which is enriched by specific parameters representing a weight in terms of traffic road capacity. The weight's assignment has been estimated by a stochastic learning approach at each time slot of the day. The accuracy of the proposed model comes from the error between the reconstructed traffic density and the measured values at the sensor position by excluding each sensor iteratively and reconstructing the flow without it. The proposed reconstruction model has been created by exploiting open and real-time data in the context of Sii-Mobility research project by using Km4City infrastructure in the area of Florence, Italy, for its corresponding Smart City solution.",https://ieeexplore.ieee.org/document/8512000/,"2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)",12-15 Aug. 2018,ieeexplore
10.1109/SoutheastCon42311.2019.9020591,Real-Time Traffic Incidence dataset,IEEE,Conferences,"This paper focuses on developing (i) a benchmark dataset for identification of traffic incidences, (ii) a congestion aware navigation application which uses this dataset for real-time detection and classification of traffic incidents, (iii) the System Level Software (or Middleware) required for Distributed Computing in such a system with Rapid Mobility, and (iv) a hardware prototype of the distributed computing and storage infrastructure. The video bandwidth requirement of 10-100 GigaBytes of data per minute per vehicular camera makes it a Big Data problem. With millions of smart vehicles predicted to be deployed within the next 5 years, BigData from a single vehicle, multiplied with the large number of vehicles, presents a Big-Squared-Data computing space which will easily overwhelm any Cloud infrastructure with its Real-Time or near Real-Time demands. Hence the need for a Fog tier between the Edge nodes and the Cloud to bring distributed computation (servers) and storage closer to the Edge nodes. Such a Fog consists of multiple Fog instances, each one of which services cells or Virtual Clusters of Edge nodes. Results show that Fog-Cloud computing framework outperforms a Cloud-only platform by 55.8% reduction in total latency or response time.",https://ieeexplore.ieee.org/document/9020591/,2019 SoutheastCon,11-14 April 2019,ieeexplore
10.1109/IGARSS47720.2021.9554141,"Real-Time, Deep Synthetic Aperture Sonar (SAS) Autofocus",IEEE,Conferences,"Synthetic aperture sonar (SAS) requires precise time-of-flight measurements of the transmitted/received waveform to produce well-focused imagery. It is not uncommon for errors in these measurements to be present resulting in image defocusing. To overcome this, an autofocus algorithm is employed as a post-processing step after image reconstruction to improve image focus. A particular class of these algorithms can be framed as a sharpness/contrast metric-based optimization. To improve convergence, a hand-crafted weighting function to remove bad areas of the image is sometimes applied to the image-under-test before the optimization procedure. Additionally, dozens of iterations are necessary for convergence which is a large compute burden for low size, weight, and power (SWaP) systems. We propose a deep learning technique to overcome these limitations and implicitly learn the weighting function in a data-driven manner. Our proposed method, which we call Deep Autofocus, uses features from the single-look-complex (SLC) to estimate the phase correction which is applied in k-space. Furthermore, we train our algorithm on batches of training imagery so that during deployment, only a single iteration of our method is sufficient to autofocus. We show results demonstrating the robustness of our technique by comparing our results to four commonly used image sharpness metrics. Our results demonstrate Deep Autofocus can produce imagery perceptually better than common iterative techniques but at a lower computational cost. We conclude that Deep Autofocus can provide a more favorable cost-quality tradeoff than alternatives with significant potential of future research.",https://ieeexplore.ieee.org/document/9554141/,2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS,11-16 July 2021,ieeexplore
10.1109/IJCNN.1992.227307,Real-time 2D analog motion detector VLSI circuit,IEEE,Conferences,"An analog VLSI implementation of real-time motion detection in early vision using a hybrid of the Reichardt and Ullman-Marr models has been fabricated. It has a 5*5 array of photoreceptors, with a receptor fill factor of 0.42. The majority of this circuit operates in strong inversion. The photosensors have squared logarithmic and linear responses for low and high intensity illumination, respectively. A linear passive resistive grid is used to implement the desired difference of Gaussian impulse response transfer function of the edge detection circuit whose dynamic range is 3 V. These outputs are used to determine the velocities of the translating objects. Velocities up to 2 m/s on chip corresponding to 6 m/s in space have been measured in 2D. The array has been fabricated in a 2 mu m n-well CMOS process.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/227307/,[Proceedings 1992] IJCNN International Joint Conference on Neural Networks,7-11 June 1992,ieeexplore
10.1109/ICISS49785.2020.9315906,Real-time Monitoring and Comprehensive Analysis Framework for Rainwater and Soil Pollution based on Big Data Feature Extraction and IoT,IEEE,Conferences,"Real-time monitoring and comprehensive analysis framework for rainwater and soil pollution based on big data feature extraction and Internet of Things (IoT) is discussed in this article. The procedural operation technology of the dispatch automation system is proposed, and introduces the overall structure and implementation plan of the dispatch automation system. The novelty is summarized into the following core aspects. (1) By representing the reconstructed training samples with low rank, a robust discriminant feature space constraint term was designed to enable the learning of feature subspace to improve the data analytic effectiveness. (2) For the forward propagation and backward propagation, the direction of the traversal and the operations that must be performed are different. Based on this, the novel real-time analysis model is designed. The proposed model is simulated on the hardware devices. The results have shown that the proposed method is efficient.",https://ieeexplore.ieee.org/document/9315906/,2020 3rd International Conference on Intelligent Sustainable Systems (ICISS),3-5 Dec. 2020,ieeexplore
10.1109/ICARM52023.2021.9536118,Real-time Monocular 3D People Localization and Tracking on Embedded System,IEEE,Conferences,"Localizing people in 3D space, rather than in original 2D image plane, provides a more comprehensive understanding of the scene and brings up more potential applications. However, inferring 3D locations usually requires stereo camera or additional sensors since deriving depth information from single image is regarded as an ill-posed problem. With recent progress in deep learning methods, depth estimation neural network can provide convincing depth map by a single RGB image. This work develops a people localization and tracking method based on a monocular camera. Specifically, an efficient self-supervised monocular depth estimation method is adopted to generate pseudo depth map. Afterwards, 2D object detection results are adopted for finding accurate people location. Finally, a filter based tracking method is adopted to fuse temporal information and improve the accuracy. Aiming to provide a real time solution for people tracking on embedded system, our methods are deployed and tested on a NVIDIA Jetson Xavier NX develop kit. The proposed efficient localization and tracking method is validated by a group of field tests. The overall performance reaches 12 fps with an acceptable accuracy compared to ground truth.",https://ieeexplore.ieee.org/document/9536118/,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),3-5 July 2021,ieeexplore
10.1109/IJCNN.2004.1380953,Real-time PCA (principal component analysis) implementation on DSP,IEEE,Conferences,PCA (principal component analysis) is a wellknown statistical technique used in many signal processing applications. An on-line temporal PCA learning algorithm is implemented on a floating-point DSP for real-time applications. This algorithm is coded in assembly language to optimize. The experimental results showed that the implemented on-line temporal PCA algorithm not only can accurately estimate the principal components from the input but also can track the principal components from the time varying input. And this algorithm can be applied in space easily by using spacial signals as its inputs instead of using the past inputs as in temporal PCA.,https://ieeexplore.ieee.org/document/1380953/,2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541),25-29 July 2004,ieeexplore
10.1109/ICPR48806.2021.9412741,Real-time Pedestrian Lane Detection for Assistive Navigation using Neural Architecture Search,IEEE,Conferences,"Pedestrian lane detection is a core component in many assistive and autonomous navigation systems. These systems are usually deployed in environments that require realtime processing. Many state-of-the-art deep neural networks only focus on detection accuracy but not inference speed. Without further modifications, they are not suitable for real-time applications. Furthermore, the task of designing a high-performing deep neural network is time-consuming and requires experience. To tackle these issues, we propose a neural architecture search algorithm that can find the best deep network for pedestrian lane detection automatically. The proposed method searches in a network-level space using the gradient descent algorithm. Evaluated on a dataset of 5,000 images, the deep network found by the proposed algorithm achieves comparable segmentation accuracy, while being significantly faster than other state-of-the-art methods. The proposed method has been successfully implemented as a real-time pedestrian lane detection tool.",https://ieeexplore.ieee.org/document/9412741/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore
10.1109/ICMLC.2003.1260131,Real-time face detection based on skin-color model and morphology filters,IEEE,Conferences,"This paper presents a new real-time face detection method that uses the skin color model in the YCrCb chrominance space to remove the non-skin-color pixels from the image from which we then extract candidate human face regions. We use the mathematical morphological filter to remove noisy regions and fill holes in the skin-color region. To locate the face region, we compute the similarity between the human face features and the candidate face regions in the image. We have implemented this algorithm in our smart media systems and found it effective in a real environment.",https://ieeexplore.ieee.org/document/1260131/,Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),5-5 Nov. 2003,ieeexplore
10.1109/ICNN.1996.549131,Real-time image restoration with an artificial neural network,IEEE,Conferences,"We present a neural network that can be applied to image correction in a preprocessing unit. Blur, geometric distortion and unequal brightness distribution are typical for many scanning techniques and can lead to difficulties during further processing of an image. These and other effects of image degradation, the space-variant can be considered simultaneously by this approach. In order to calibrate the correcting system the weights of a neural network are trained. Using suitable training patterns and an appropriate optimization criterion for the degraded images, the dimensioned network represents a space-variant filter with a behavior similar to the well-known Wiener filter. The restoration result can be easily altered by the scheme of the learning data generation. Theoretical considerations and examples for 1D, 2D and 3D implementations in both software and hardware are given.",https://ieeexplore.ieee.org/document/549131/,Proceedings of International Conference on Neural Networks (ICNN'96),3-6 June 1996,ieeexplore
10.1109/ROBOT.1993.291973,Real-time implementation of neural network learning control of a flexible Space manipulator,IEEE,Conferences,"A neural network approach to online learning control and real-time implementation for a flexible space robot manipulator is presented. An overview of the motivation and system development of the self-mobile space modulator (SM/sup 2/) is given. The neural network learns control by updating feedforward dynamics based on feedback control input. Implementation issues associated with online training strategies are addressed and a single stochastic training scheme is presented. A recurrent neural network architecture with improved performance is proposed. Using the proposed learning scheme, the manipulator tracking error is reduced by 85% compared to that of conventional proportional-integral-derivative (PID) control. The approach possesses a high degree of generality and adaptability to various applications. It will be a valuable learning control method for robots working in unconstructed environments.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/291973/,[1993] Proceedings IEEE International Conference on Robotics and Automation,2-6 May 1993,ieeexplore
10.1109/VR.2001.913773,Real-time input of 3D pose and gestures of a user's hand and its applications for HCI,IEEE,Conferences,"Introduces a method for tracking a user's hand in 3D and recognizing the hand's gesture in real time without the use of any invasive devices attached to the hand. Our method uses multiple cameras for determining the position and orientation of a user's hand moving freely in a 3D space. In addition, the method identifies pre-determined gestures in a fast and robust manner by using a neural network which has been properly trained beforehand. This paper also describes results of user study of our proposed method and several types of applications, including 3D object handling for a desktop system and a 3D walkthrough for a large immersive display system.",https://ieeexplore.ieee.org/document/913773/,Proceedings IEEE Virtual Reality 2001,13-17 March 2001,ieeexplore
10.1145/3061639.3062307,Real-time meets approximate computing: An elastic CNN inference accelerator with adaptive trade-off between QoS and QoR,IEEE,Conferences,"Due to the recent progress in deep learning and neural acceleration architectures, specialized deep neural network or convolutional neural network (CNNs) accelerators are expected to provide an energy-efficient solution for real-time vision/speech processing, recognition and a wide spectrum of approximate computing applications. In addition to their wide applicability scope, we also found that the fascinating feature of deterministic performance and high energy-efficiency, makes such deep learning (DL) accelerators ideal candidates as application-processor IPs in embedded SoCs concerned with real-time processing. However, unlike traditional accelerator designs, DL accelerators introduce a new aspect of design trade-off between real-time processing (QoS) and computation approximation (QoR) into embedded systems. This work proposes an elastic CNN acceleration architecture that automatically adapts to the hard QoS constraint by exploiting the error-resilience in typical approximate computing workloads. For the first time, the proposed design, including network tuning-and-mapping software and reconfigurable accelerator hardware, aims to reconcile the design constraint of QoS and Quality of Result (QoR), which are respectively the key concerns in real-time and approximate computing. It is shown in experiments that the proposed architecture enables the embedded system to work flexibly in an expanded operating space, significantly enhances its real-time ability, and maximizes the energy-efficiency of system within the user-specified QoS-QoR constraint through self-reconfiguration.",https://ieeexplore.ieee.org/document/8060406/,2017 54th ACM/EDAC/IEEE Design Automation Conference (DAC),18-22 June 2017,ieeexplore
10.1109/ICAR.1997.620222,Real-time navigation of a mobile robot using Kohonen's topology conserving neural network,IEEE,Conferences,"This paper proposes a real-time sensor based navigation method using Kohonen's topology conserving network for navigation of a mobile robot in any uncertain environment. The sensory information including target location with respect to current location of the mobile robot, have been discretely conserved using a two dimensional Kohonen lattice. Reinforcement learning based on a stochastic real valued technique have been implemented to compute the action space for this Kohonen lattice. The proposed scheme learns the input and output weight space of the Kohonen lattice which is generalized to any workspace. The effectiveness of the proposed scheme has been established by simulation where the complete domain of the input-space is quantized based on experience on sensory data encountered in real-time. The input-output mapping conserved by the Kohonen lattice during simulation was used to guide a mobile robot in a real-time environment. Successful navigation of the mobile robot without further training confirms the robustness of the proposed scheme.",https://ieeexplore.ieee.org/document/620222/,1997 8th International Conference on Advanced Robotics. Proceedings. ICAR'97,7-9 July 1997,ieeexplore
10.1109/PIC.2010.5687433,Real-time pedestrian tracking by visual attention and human knowledge learning,IEEE,Conferences,"In this paper, a novel model of pedestrian tracking by using object-based attention and human knowledge is presented. The selective units in the system are the objects and groupings which are space-driven as well as feature-driven. The factors of speed, motion direction and spatial location are used to cluster and form the groupings. Hierarchical selectivity of attention for objects in a grouping is implemented under the guide of human model knowledge with the help of a head detector. The motion cues are utilized to tackle the multi-person tracking through hierarchical selection of attention. The experimental results from outdoor environments are reported.",https://ieeexplore.ieee.org/document/5687433/,2010 IEEE International Conference on Progress in Informatics and Computing,10-12 Dec. 2010,ieeexplore
10.1109/ISCAS.2016.7539214,Real-time sensory information processing using the TrueNorth Neurosynaptic System,IEEE,Conferences,"Summary form only given. The IBM TrueNorth (TN) Neurosynaptic System, is a chip multi processor with a tightly coupled processor/memory architecture, that results in energy efficient neurocomputing and it is a significant milestone to over 30 years of neuromorphic engineering! It comprises of 4096 cores each core with 65K of local memory (6T SRAM)-synapses- and 256 arithmetic logic units - neurons-that operate on a unary number representation and compute by counting up to a maximum of 19 bits. The cores are event-driven using custom asynchronous and synchronous logic, and they are globally connected through an asynchronous packet switched mesh network on chip (NOC). The chip development board, includes a Zyng Xilinx FPGA that does the housekeeping and provides support for standard communication support through an Ethernet UDP interface. The asynchronous Addressed Event Representation (AER) in the NOC is al so exposed to the user for connection to AER based peripherals through a packet with bundled data full duplex interface. The unary data values represented on the system buses can take on a wide variety of spatial and temporal encoding schemes. Pulse density coding (the number of events Ne represents a number N), thermometer coding, time-slot encoding, and stochastic encoding are examples. Additional low level interfaces are available for communicating directly with the TrueNorth chip to aid programming and parameter setting. A hierarchical, compositional programming language, Corelet, is available to aid the development of TN applications. IBM provides support and a development system as well as Compass a scalable simulator. The software environment runs under standard Linux installations (Red Hat, CentOS and Ubuntu) and has standard interfaces to Matlab and to Caffe that is employed to train deep neural network models. The TN architecture can be interfaced using native AER to a number of bio-inspired sensory devices developed over many years of neuromorphic engineering (silicon retinas and silicon cochleas). In addition the architecture is well suited for implementing deep neural networks with many applications in computer vision, speech recognition and language processing. In a sensory information processing system architecture one desires both pattern processing in space and time to extract features in symbolic sub-spaces as well as natural language processing to provide contextual and semantic information in the form of priors. In this paper we discuss results from ongoing experimental work on real-time sensory information processing using the TN architecture in three different areas (i) spatial pattern processing -computer vision(ii) temporal pattern processing -speech processing and recognition(iii) natural language processing -word similarity-. A real-time demonstration will be done at ISCAS 2016 using the TN system and neuromorphic event based sensors for audition (silicon cochlea) and vision (silicon retina).",https://ieeexplore.ieee.org/document/7539214/,2016 IEEE International Symposium on Circuits and Systems (ISCAS),22-25 May 2016,ieeexplore
10.1109/ICUAS.2016.7502588,Real-time unmanned aerial vehicle 3D environment exploration in a mixed reality environment,IEEE,Conferences,"This paper presents a novel human robot interaction system that can be used for real-time 3D environment exploration with an unmanned aerial vehicle (UAV). The method creates a mixed reality environment, in which a user can interactively control a UAV and visualize the exploration data in real-time. The method uses a combination of affordable sensors, and transforms the control and viewing space from the UAV to the controller's perspective. Different hardware and software configurations are studied so that the system can be adjusted to meet different needs and environments. A prototype system is presented and test results are discussed.",https://ieeexplore.ieee.org/document/7502588/,2016 International Conference on Unmanned Aircraft Systems (ICUAS),7-10 June 2016,ieeexplore
10.1109/ICASSP.2008.4517901,Realtime detection of salient moving object: A multi-core solution,IEEE,Conferences,"Detection of salient moving object has great potentials in activity recognition, scene understanding, etc. However techniques to characterizing the object in fine granularity have not been well developed in real applications due to the computational intensity. The emerging multi-core technology in hardware design provides an opportunity for the compute intensive algorithms to boost speed in parallel. This paper proposed a scalable approach to detecting salient moving object which is designed inherently for parallelization. To characterize the object in fine granularity, we extract color-texture homogenous regions as the basic processing unit by image segmentation. To identify salient object, we generate probabilistic template by learning the space-time context. The parallel algorithm is implemented using OpenMP. Evaluations have been carried out on sports, news, and home video data. For the CIF size image, we get processing speed of 51.1 frames per second and near linear speed up on an eight-core machine. It indicates that the algorithm parallelization is a promising solution for practical applications in the multimedia field.",https://ieeexplore.ieee.org/document/4517901/,"2008 IEEE International Conference on Acoustics, Speech and Signal Processing",31 March-4 April 2008,ieeexplore
10.1109/AERO47225.2020.9172719,Recurrent Neural Network Based Prediction to Enhance Satellite Telemetry Compression,IEEE,Conferences,"Because of the gradually increasing number of remote measured low and/or high frequency sampled parameters in space applications, aerospace mission operators have to make hard choices on which parameters at which sampling rates should be downlinked. On-board aerospace applications are characterized by limited storage and communication budgets, while lossless data compression schemes should be sufficient enough to enhance transmission efficiency and hence the whole aerospace mission. In this paper, a proposed two-stage lossless compression method for telemetry data is presented. The proposed method consists of a decorrelation stage and an entropy coding one. The Long-Short-Term Memory (LSTM) Recurrent Neural Network (RNN) is implemented as a predictor in the decorrelation stage of the proposed method, and an illustrative method of applying LSTM network for telemetry data samples prediction is presented and figured out. In experiments, different entropy coders: Rice codes, arithmetic method and Huffman algorithm are separately implemented at the second stage. The proposed method is tested by different real telemetry data sets of FUNcube satellite in frames of data words of 8-,10-,16-bits widths. Experimental results show that the proposed method improved compression efficiency based on a single stage of entropy coder: Rice codes, arithmetic code, and Huffman algorithm by a ratio up to: 98%, 21%, and 1.6%, respectively.",https://ieeexplore.ieee.org/document/9172719/,2020 IEEE Aerospace Conference,7-14 March 2020,ieeexplore
10.1109/ICARM52023.2021.9536145,Reducing the Dimension of the Configuration Space with Self Organizing Neural Networks,IEEE,Conferences,"For robotics, especially industrial applications, it is crucial to reactively plan safe motions through efficient algorithms. Planning is more powerful in the configuration space than the task space. However, for robots with many degrees of freedom, this is challenging and computationally expensive. Sophisticated techniques for motion planning such as the Wavefront algorithm are limited by the high dimensionality of the configuration space, especially for robots with many degrees of freedom. For a neural implementation of the Wavefront algorithm in the configuration space, neurons represent discrete configurations and synapses are used for path planning. In order to decrease the complexity, we reduce the search space by pruning superfluous neurons and synapses. We present different models of self-organizing neural networks for this reduction. The approach takes real-life human motion data as input and creates a representation with reduced dimension. We compare six different neural network models and adapt the Wavefront algorithm to the different structures of the reduced output spaces. The method is backed up by an extensive evaluation of the reduced spaces, including their suitability for path planning by the Wavefront algorithm.",https://ieeexplore.ieee.org/document/9536145/,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),3-5 July 2021,ieeexplore
10.1109/VTCFall.2018.8690917,Refined Autoencoder-Based CSI Hidden Feature Extraction for Indoor Spot Localization,IEEE,Conferences,"Wireless indoor localization technique has attracted wide attention recently. Fingerprint (FP) based method with received signal strength indicator (RSSI) is a popular approach due to easy implementation and robustness. Nowadays, fine-grained indoor spot localization resort to channel state information (CSI) owing to rich information property of CSI. However, due to a higher dimension of CSI compared to RSSI, CSI-based FP method requires higher storage and communication overhead, which is not suitable for most scenarios. In this paper, we propose a novel refined autoencoder-based CSI hidden feature extraction for indoor spot localization (RACHEL). Based on the concept of FP, we first introduce an autoencoder (AE) for the dimension reduction and feature discrimination. A low dimensional hidden feature of trained AE model is saved as FP database in the off-line stage. For indoor spot localization problems, users position is assumed to be close to one of the reference points. Therefore, CSI transforms to hidden feature space and users location is estimated by the nearest-neighbor algorithm in the on-line stage. Furthermore, to enhance the performance, instinctive AE is modified by considering corruption from time-varying environment and sensitivity between the hidden layer and input layer. Performance evaluations demonstrate that the proposed RACHEL can achieve 97.8% spot classification accuracy and yield a spaced savings of 94.3%.",https://ieeexplore.ieee.org/document/8690917/,2018 IEEE 88th Vehicular Technology Conference (VTC-Fall),27-30 Aug. 2018,ieeexplore
10.1109/ICSME46990.2020.00074,Regression Testing of Massively Multiplayer Online Role-Playing Games,IEEE,Conferences,"Regression testing aims to check the functionality consistency during software evolution. Although general regression testing has been extensively studied, regression testing in the context of video games, especially Massively Multiplayer Online Role-Playing Games (MMORPGs), is largely untouched so far. One big challenge is that game testing requires a certain level of intelligence in generating suitable action sequences among the huge search space, to accomplish complex tasks in the MMORPG. Existing game testing mainly relies on either the manual playing or manual scripting, which are labor-intensive and time-consuming. Even worse, it is often unable to satisfy the frequent industrial game evolution. The recent process in machine learning brings new opportunities for automatic game playing and testing. In this paper, we propose a reinforcement learning-based regression testing technique that explores differential behaviors between multiple versions of an MMORPGs such that the potential regression bugs could be detected. The preliminary evaluation on real industrial MMORPGs demonstrates the promising of our technique.",https://ieeexplore.ieee.org/document/9240641/,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),28 Sept.-2 Oct. 2020,ieeexplore
10.1109/SSCI47803.2020.9308514,Regression learning on patches,IEEE,Conferences,"Neural networks often do poorly at representing dis-continuous functions, or even just functions with rapid transitions in the response surface between closely-spaced points in feature space. However, such `edges' in the data can be a useful way to partition the feature space in order to train specialised learners for individual regions. This is particularly beneficial where these regions are relatively simple, and hence low-complexity learners can be used successfully on them. Another benefit of such an approach is that it is easily parallelisable: the specialised learners use independent partitions of the data, and so they can be trained in parallel, while output prediction is based on the output of just one network, so there is no need to combine predictions. We introduce an algorithm to partition the data that is inspired by Finite Element Tearing and Interconnecting. Using an implementation based on a decision tree with neural networks at the leaves, we demonstrate our approach for regression learning on patches of the feature space. We use both artificial and real-world datasets to show that, in some use cases, this method can outperform conventional neural networks that see the entire feature set in the original training.",https://ieeexplore.ieee.org/document/9308514/,2020 IEEE Symposium Series on Computational Intelligence (SSCI),1-4 Dec. 2020,ieeexplore
10.1109/ISGTAsia49270.2021.9715603,Reinforcement Learning Based EV Charging Scheduling: A Novel Action Space Representation,IEEE,Conferences,"In recent years, several optimization techniques have been proposed for electric vehicle (EV) charging scheduling. A common approach to intelligent scheduling is day-ahead planning, assuming full arrival time, departure time and energy demand knowledge or having them forecasted. However, the result from the day-ahead scheduling is limitedly applicable due to the uncertainties from the charging behaviors. With the deployment of the EV charging communication protocol defined in ISO 15118, it is realistic to assume that the EV will publish the departure time and the energy demand upon arrival. Thus, real-time scheduling, making decisions at each decision timeslot, can adapt to the new information and increase scheduling performance. Traditional model-based approaches like model predictive control (MPC) still require models, for example, for the future arrival times to solve the scheduling problem. Reinforcement learning (RL), a model-free approach, has also been successfully applied to real-time scheduling. RL can learn how to make decisions without relying on any system knowledge. This paper proposes a new action space construction method for an RL as proposed in a preceding work. The resulting action space size is significantly reduced compared to the original approach. Further, we compare the performance of a novel prioritized RL method to the original method. A publicly available charging session dataset is used for performance comparison in contrast to the original method. It is shown, that the prioritized RL performs better.",https://ieeexplore.ieee.org/document/9715603/,2021 IEEE PES Innovative Smart Grid Technologies - Asia (ISGT Asia),5-8 Dec. 2021,ieeexplore
10.1109/CASE48305.2020.9249227,Reinforcement Learning with Converging Goal Space and Binary Reward Function,IEEE,Conferences,"Usage of a sparse and binary reward function remains one of the most challenging problems in reinforcement learning. In particular, when the environments wherein robotic agents learn are sufficiently vast, it is much more difficult to learn tasks because the probability of reaching the goal is minimal. A Hindsight Experience Replay algorithm was proposed to overcome these difficulties; however, problems persist that affect the learning speed and delay learning when a learning agent cannot receive proper rewards at the beginning of the learning process. In this paper, we present a simple method called Converging Goal Space and Binary Reward Function, which helps agents learn tasks easily and efficiently in large environments while providing a binary reward. At an early stage in training, a larger goal space margin facilitates the reward function for a more rapid policy learning. As the number of successes increases, the goal space is gradually reduced to the size used to the size used in the test. We apply this reward function to two different task experiments: sliding and throwing, which must be explored at a wider range than the reach of the robotic arms, and then compare the learning efficiency to that of experiments that only employ a sparse and binary reward function. We show that the proposed reward function performs better in large environments using physics simulation, and we demonstrate that the function is applicable to real world robotic arms.",https://ieeexplore.ieee.org/document/9249227/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/ISCID.2018.10171,Representation Learning for Knowledge Graph with Dynamic Margin,IEEE,Conferences,"Representation learning aims to embed knowledge graphs into a low-dimensional, dense, and real-valued vector space. Existing methods such as TransE and TransH make use of a global loss function based on margin to learn embedding representation, but the margin in the loss function is fixed during the training process. Since the margin is used as optimal distance to distinguish valid and invalid triples, it is reasonable to permit the margin to evolve during the training process. Based on this idea, a dynamic margin (DM) translation principle is proposed in this paper, and by introducing this principle into the classical TransE model, a knowledge graph embedding model named TransE-DM is presented. Compared with the TransE model, a feature of TransE-DM is that the loss function can evolve during the training process. Experimental results show that TransE-DM has a certain improvement on the link prediction task.",https://ieeexplore.ieee.org/document/8695607/,2018 11th International Symposium on Computational Intelligence and Design (ISCID),8-9 Dec. 2018,ieeexplore
10.1109/ICORR.2017.8009451,Representing high-dimensional data to intelligent prostheses and other wearable assistive robots: A first comparison of tile coding and selective Kanerva coding,IEEE,Conferences,"Prosthetic devices have advanced in their capabilities and in the number and type of sensors included in their design. As the space of sensorimotor data available to a conventional or machine learning prosthetic control system increases in dimensionality and complexity, it becomes increasingly important that this data be represented in a useful and computationally efficient way. Well structured sensory data allows prosthetic control systems to make informed, appropriate control decisions. In this study, we explore the impact that increased sensorimotor information has on current machine learning prosthetic control approaches. Specifically, we examine the effect that high-dimensional sensory data has on the computation time and prediction performance of a true-online temporal-difference learning prediction method as embedded within a resource-limited upper-limb prosthesis control system. We present results comparing tile coding, the dominant linear representation for real-time prosthetic machine learning, with a newly proposed modification to Kanerva coding that we call selective Kanerva coding. In addition to showing promising results for selective Kanerva coding, our results confirm potential limitations to tile coding as the number of sensory input dimensions increases. To our knowledge, this study is the first to explicitly examine representations for realtime machine learning prosthetic devices in general terms. This work therefore provides an important step towards forming an efficient prosthesis-eye view of the world, wherein prompt and accurate representations of high-dimensional data may be provided to machine learning control systems within artificial limbs and other assistive rehabilitation technologies.",https://ieeexplore.ieee.org/document/8009451/,2017 International Conference on Rehabilitation Robotics (ICORR),17-20 July 2017,ieeexplore
10.1109/ICIBA52610.2021.9688007,Research and Implementation of User Behavior Simulation Technology Based on Power Industry Cyber Range,IEEE,Conferences,"The current cyber range of the electric power industry has background traffic simulation problems, prospect behavior generation problems, and target simulation diversity problems in large-scale network environments. In response to these problems, this paper proposes a user behavior simulation technology suitable for power industry cyber range. We perform large-scale, high-fidelity network user behavior simulation by combining real traffic playback, inject user behavior traffic in the real network into the virtual target network, then model the user behavior in multiple scenarios in the power network space and generate it based on the method of sequence prediction, and finally realize the diversity simulation of user behavior.",https://ieeexplore.ieee.org/document/9688007/,"2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)",17-19 Dec. 2021,ieeexplore
10.1109/ICMLC.2003.1260027,Research and implementation of a real time approach to lip detection in video sequences,IEEE,Conferences,"Locating the lip in video sequences is one of the primary steps of the automatic lipreading system. In this paper a new approach to lip detection, which is based on Red Exclusion and Fisher transform, is presented. In this approach, firstly, we locate face region with skin-color model and motion correlation, then trisect the face image and take into account the lowest part, in which the lip lies, for the next processing. Secondly, we exclude R-component in RGB color space, then use G-component and B-component as the Fisher transform vector to enhance the lip image. Finally, in the enhanced image, we adaptively set the threshold to separate the lip color and the skin color in the light of the normal distribution of the gray value histogram. The experimental results showed that this fast approach is very efficient in detecting the whole lip and not affected by illuminant and different speakers.",https://ieeexplore.ieee.org/document/1260027/,Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),5-5 Nov. 2003,ieeexplore
10.1109/ICMLC.2003.1260018,"Research and implementation of real-time face detection, tracking and protection",IEEE,Conferences,"Privacy protection in video images is becoming one of the research focuses in the field of remote collaborative system. In this paper, a method for face detection, tracking and privacy protection is presented. According to skin-color distribution in the color space, we developed a statistical skin-color model through interactive sample training. Using this model we convert the color image to binary image and then segment face candidate region. Then we use a facial feature matching scheme for further detection. The presence or absence of a face in each region is verified by means of mouth detector. Real time detection and tracking can be achieved by using this method in video images. In order to speed up tracking, we improve the traditional method by adding motion prediction, which works better when several disturbing objects appear simultaneously. Finally we make the tracking region blurring and transmit the frames to the remote collaborative sites to obtain the privacy protection. The level of privacy protection can be dynamically adjusted according to collaborators' requests and credibility of remote sites. The experiment results show the proposed method not only has high speed and efficiency, but also is robust to head rotation to some extent.",https://ieeexplore.ieee.org/document/1260018/,Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),5-5 Nov. 2003,ieeexplore
10.1109/IEMBS.2007.4352483,Research on 3D Modeling for Head MRI Image Based on Immune Sphere-Shaped Support Vector Machine,IEEE,Conferences,"In head MRI image sequences, the boundary of each encephalic tissue is highly complicated and irregular. It is a real challenge to traditional 3D modeling algorithms. Support vector machine (SVM) based on statistical learning theory has solid theoretical foundation. sphere-shaped SVM (SSSVM) was originally developed for solving some special classification problems. In this paper, it is extended to image 3D modeling which tries to find the smallest hypersphere enclosing target data in high dimensional space by kernel function. However, selecting parameter is a complicated problem which directly affects modeling accuracy. Immune algorithm (IA), mainly applied to optimization, is used to search optimal parameter for SSSVM. So, immune SSSVM (ISSSVM) is proposed to construct the 3D models for encephalic tissues. As our experiment demonstrates, the models are constructed and reach satisfactory modeling accuracies. Theory and experiment indicate ISSSVM exhibits its great potential in image 3D modeling.",https://ieeexplore.ieee.org/document/4352483/,2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,22-26 Aug. 2007,ieeexplore
10.1109/ICDSCA53499.2021.9650252,Research on Behavior Mathematical Modeling of CGF Based on Reinforcement Learning under Complex Electromagnetic Environment,IEEE,Conferences,"This paper focuses on the modeling of agent behavior in complex electromagnetic environment. By using reinforcement learning technology, The traditional Markov decision process and dynamic programming algorithm are improved. The electromagnetic field intensity function is integrated with Markov decision process and dynamic programming algorithm, and high precision virtual battlefield environment is evaluated based on system simulation, in order to facilitate the modeling of the agents behavior, this paper meshes the complex electromagnetic battlefield environment and sets up the simulation task scene of agents maneuvering task using simulation and evaluation system. Several simulation tests were implemented to collect relative test data. The comparison was made with the traditional Markov decision algorithm from three dimensions, including time, space and working condition of electronic devices inside equipment. The results show that the improved Markov decision algorithm and dynamic programming algorithm proposed in this paper show better adaptability to adapt complex electromagnetic environment than traditional Markov algorithm, it will provide a strong support for efficient completion of combat tasks and improvement of combat effectiveness in complex electromagnetic simulation environment, and also lays a foundation for the subsequent research on the CGF behavior modeling method.",https://ieeexplore.ieee.org/document/9650252/,2021 IEEE International Conference on Data Science and Computer Application (ICDSCA),29-31 Oct. 2021,ieeexplore
10.1109/AUTEEE52864.2021.9668793,Research on Intelligent Acceleration Algorithm for Big Data Mining in Communication Network Based on Support Vector Machine,IEEE,Conferences,"As a budding technology, big datas technical implementation and commercial application are in the exploratory stage. With the increasing development of network and communication technology, a large amount of information is pouring in. How to effectively select the required information has become a more and more prominent problem. Data mining is a data processing technology developed to meet this need. Support vector machine is a new technology in data mining. It is a new tool to solve machine learning problems with the help of optimization methods. Among them, it focuses on the support vector machine, including the development history and present situation of support vector machine, the main basic concepts and research contents. On this basis, it studies various training algorithms of support vector machine which are relatively common at present, and compares their advantages and disadvantages. Big data has various data types, forming a data stream with various attributes. As we all know, data source classification based on batch processing can improve the query speed, but it still cant meet the demand of real-time query. Therefore, feature selection mechanism is usually introduced in the process of data mining modeling to reduce its load. However, when faced with the query of high-dimensional data, the query space grows exponentially, which is difficult to realize. Therefore, this paper proposes the efficiency of an intelligent acceleration algorithm for big data mining based on vector machine communication network.",https://ieeexplore.ieee.org/document/9668793/,"2021 IEEE 4th International Conference on Automation, Electronics and Electrical Engineering (AUTEEE)",19-21 Nov. 2021,ieeexplore
10.1109/ICSESS47205.2019.9040787,Research on Real-time Learning Prediction Method Based on Spark,IEEE,Conferences,"Based on the research of real-time prediction and big data processing platform, an effective solution is proposed to solve the shortcomings of current real-time learning prediction in engineering application. By analyzing learners' learning behaviors related to a certain course, learners' learning behaviors can be divided into three categories in terms of time and space: online learning behaviors, offline learning behaviors and performance of relevant basic courses. Based on the parallel computation and binary logistic regression algorithm in Spark framework, the off-line learning prediction model is created. In the real-time environment, large scale real-time learning prediction can be realized based on Spark Streaming and kafka. With the increase of learning behaviors data, the scalability problem of prediction scheme can be solved by expanding Spark cluster nodes. The advantages of the proposed scheme have been verified in the practical application of smart campus.",https://ieeexplore.ieee.org/document/9040787/,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),18-20 Oct. 2019,ieeexplore
10.1109/ITAIC.2019.8785778,Research on Security Protection of Network Based on Address Layout Randomization from the Perspective of Attackers,IEEE,Conferences,"At present, the network architecture is based on the TCP/IP protocol and node communications are achieved by the IP address and identifier of the node. The IP address in the network remains basically unchanged, so it is more likely to be attacked by network intruder. To this end, it is important to make periodic dynamic hopping in a specific address space possible, so that an intruder fails to obtain the internal network address and grid topological structure in real time and to continue to perform infiltration by the building of a new address space layout randomization system on the basis of SDN from the perspective of an attacker.",https://ieeexplore.ieee.org/document/8785778/,2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),24-26 May 2019,ieeexplore
10.1109/ICPCA.2008.4783569,Research on Software Architecture for Expert System in Practice,IEEE,Conferences,"Advanced applications in fields such as expert system, real-time process control, problem solving, machine learning require the construction, efficient access and management of large, shared knowledge bases. Such knowledge bases cannot be built using existed tools such as expert system shells, because these do not scale up, nor can be built in terms of existing database technology, because such technology does not support the rich representational structure and inference mechanisms required for knowledge-based systems. This paper proposes a generic architecture for a knowledge base management system intended for such applications. The software architecture assumes quotient space structure which analyses identic problem in different granularity space. A new software architecture model for knowledge base system based on quotient space is given. The atomic component, quotient component, connector and configuration are defined. As an example, a knowledge base system framework for agro-meteorology is described. Some components and a configuration about agro-meteorology are given. All above work sets up basic theory of software architecture for knowledge base system based on quotient space and provides new way for further research.",https://ieeexplore.ieee.org/document/4783569/,2008 Third International Conference on Pervasive Computing and Applications,6-8 Oct. 2008,ieeexplore
10.1109/CISE.2009.5363406,Research on Text Classification Algorithm by Combining Statistical and Ontology Methods,IEEE,Conferences,"Traditional statistics based text classification methods almost construct their characteristic vectors with some key terms, and they consider terms are independent of each other and there are no semantic relations among them. However, in the real world, words used to have semantic relationships, such as synonym, hyponymy and so on. Therefore, classification methods based on statistics do not conform to the fact and the classification results also do not satisfying. To draw this problem, there is a need to obtain characteristic semantic information by taking advantage of ontology. With the help of the features of ontology class hierarchical structure and property constraint, one can match the terms with domain ontology concepts and build up the concept vector space model. Using ontology method for text classification alone will lack scientific and stringency of the statistics. Taking all the above into consideration, this paper takes a combination of the two classification methods. Firstly, we choose the characteristics with statistics method and based on this, add in the ontology and form the concept vector space. Besides, we improve the KNN algorithm from two aspects. Finally, we implement a module for text classification of telecom domain. In the end, we make an analysis and comparison of the results of both statistics-only based (without improving the KNN algorithm) and the combination of two classification methods (with improved KNN).",https://ieeexplore.ieee.org/document/5363406/,2009 International Conference on Computational Intelligence and Software Engineering,11-13 Dec. 2009,ieeexplore
10.1109/ICBASE51474.2020.00054,Research on Unconstrained Face Recognition Based on Deep Learning,IEEE,Conferences,"The emergence of deep learning has greatly promoted the development of the field of face recognition. The accuracy of face recognition in real scenes is affected by many factors. Among them, the problem of multiple poses is still an external factor that is difficult to overcome in face recognition. For the identification process in the Central African people face extreme attitude with the state led to the problem of low recognition accuracy, this paper proposes a gesture of deep space based correction feature to improve the recognition accuracy of multi-pose, first proposed in 2019 to use Google's lightweight network MobileNet for attitude correction in deep space, additionally employed ResNet18 verify and compare recognition results. This paper uses the VggFace2 dataset to train the two models in an end-to-end manner, and then test them on the CFP dataset, IJB-A dataset, and LFW dataset. The results show that the two backbone models proposed in the article are not much different from ResNet50 on the CFP. The face recognition on the IJB-A dataset is around 96%. The average recognition on the public data set LFW is about 96%. From the results of the test set, the model in the article is better than other methods. In addition, MobileNetV3 has a better recognition accuracy than ResNet18, and the amount of calculation is smaller.",https://ieeexplore.ieee.org/document/9403774/,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),30 Oct.-1 Nov. 2020,ieeexplore
10.1109/ICMA.2019.8816557,Research on V-SLAM Methods,IEEE,Conferences,"With the development of intelligent mobile robots, SLAM, especially V-SLAM, as the basic technology of robot localization and navigation, has the advantages of strong adaptability, high precision and strong intelligence compared with the traditional localization technology. It is widely used in smart devices such as unmanned aerial vehicle, automatic driving and sweeping robots. According to different implementation methods, the visual SLAM is divided into: filter V-SLAM based on probability model, key frame BA-based V-SLAM using nonlinear optimization theory, direct tracking of V-SLAM under the assumption of luminosity invariance, space occupying V-SLAM that focuses on building three-dimensional dense maps. This paper focuses on representative systems of various V-SLAMs and gives their respective applicable scenarios and characteristics. Finally, this article forecasts the development of V-SLAM combining with multi-information fusion technology, semantic deep and learning technology.",https://ieeexplore.ieee.org/document/8816557/,2019 IEEE International Conference on Mechatronics and Automation (ICMA),4-7 Aug. 2019,ieeexplore
10.1109/ICAICA50127.2020.9182662,Research on feedback Cognitive Method of Insulator Self-blast State Based on Multi-scale Convolutional Network,IEEE,Conferences,"In view of the lack of the existing insulator self-blast state detection method and the scale defects of deep neural network structure, imitated the human cognitive model, and learn from the closed-loop control theory, this paper explores a feedback recognition method of insulator self-blast state with multi-scale convolutional neural network. Firstly, for the pre-processed insulator images, based on ResNet-18, branches with different network structure are added to improve the network ability to adapt to different resolutions. At the same time, the multi-scale information fusion module is added at the end of the network. Secondly, the multi-scale feature vector is sent to stochastic configuration networks (SCN) with universal approximation ability to establish the classification criterion of the self-blast state of insulator images with strong generalization ability. Finally, an imitation of human thinking patterns is employed that exhibits repeated deliberation and comparison. Consequently, based on generalized error and entropy theories, the performance index is defined to evaluate the uncertain results of the insulator self-blast states in real time. Then, the regulation mechanism is given to realize the self-optimizing of fixed feature space and the reconstruction of classification criteria, which renders the insulator self-blast states is re-recognized with feedback mechanism. Experimental results show that, compared with other open-loop and closed-loop algorithms, the proposed method enhances the generalization ability and improves the recognition accuracy of the model.",https://ieeexplore.ieee.org/document/9182662/,2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),27-29 June 2020,ieeexplore
10.1109/RCAR47638.2019.9044114,Research on omnidirectional mobile robot motion control based on integration of traction and steering wheel,IEEE,Conferences,"In order to solve the automatic transportation of heavy materials under the limited working space of production workshops and warehouses, two sets of heavy-duty omnidirectional mobile robot motion control systems with steering wheel drive units were designed. The steering wheel combination drive unit of the walking + steering set is used to build the mobile robot chassis, and the mechatronics servo system and mathematical model of multi-motor coordinated motion are constructed. The communication between the controller and the steering wheel combination drive unit is established through the CAN bus. The specific implementation is to capture and analyze the control signal through the controller to obtain the desired motion mode, to obtain the motion of each set of steering wheel unit through the mathematical model, and to realize the desired motion through the synthesis of each set of steering wheel unit motion. It has been verified by experiments that the two sets of steering wheel unit-driven mobile robot control system realizes the zero turning radius, 360-degree omnidirectional movement of the robot and rotation during the movement. It can be used for flexible work in tight spaces.",https://ieeexplore.ieee.org/document/9044114/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore
10.1109/ICIEA.2012.6360740,Research on resistor array non-uniformity correction with neural network,IEEE,Conferences,"In order to make the imagery respond just as the real-world scenes do, nonuniformity correction of resistor array is necessary. Most of all, this paper proposed a new neural network algorithm for nonuniformity correction. It has simple neural network model, at the same time, not only can well compensated nonuniformity, also can fit the characteristic curves. In addition, we adopted piecewise linear method to approach the correspondence curve between the actual input voltage and the theoretical input voltage for the first time. This approach greatly reduces the storage space and makes the algorithm easily realized in hardware. Theoretical analysis of the improved neural network algorithm, the algorithm's implementation steps, piecewise linear approach of the corresponding curve, simulation results and considerations are given in the paper.",https://ieeexplore.ieee.org/document/6360740/,2012 7th IEEE Conference on Industrial Electronics and Applications (ICIEA),18-20 July 2012,ieeexplore
10.1109/AICI.2009.91,Research on the Ontology-Based Complex Event Processing Engine of RFID Technology for Agricultural Products,IEEE,Conferences,"In order to improve the discriminating capacity of the RFID tags for agricultural products traceability data, a new handling engine ORFID-CEP for complex events of agricultural products RFID Tag based on ontology, is proposed by introducing the event ontology model into the field of the agricultural products quality safety administration. In the new engine, the ontology model of RFID event, semantic space and the rules of the event ontology are defined. In the RFID practical application instances, the number of the fringe real-time events is so large that those methods used in current systems can not handle them in time. Thus, many significant events are lost. Aiming to overcome the problem, the transform system of work flow for the complex event ontology and the optimized strategies for event exploration, event operation and restrictive conditions of event appearance are established. Experimental results show that the new engine can mine more complex events with semantic information than the conventional Esper-CEP engine. Additionally, the new engine is steadier than the Esper-CEP engine. With increasing number of the events, the growth rate of the new engine for mining complex events raises more quickly than Esper-CEP, which indicates that the engine has the good ability of information process.",https://ieeexplore.ieee.org/document/5376207/,2009 International Conference on Artificial Intelligence and Computational Intelligence,7-8 Nov. 2009,ieeexplore
10.1109/IEMBS.2007.4353720,Research on the Segmentation of MRI Image Based on Multi-Classification Support Vector Machine,IEEE,Conferences,"In head MRI image, the boundary of each encephalic tissue is highly complicated and irregular. It is a real challenge to traditional segmentation algorithms. As a new kind of machine learning, support vector machine (SVM) based on statistical learning theory (SLT) has high generalization ability, especially for dataset with small number of samples in high dimensional space. SVM was originally developed for two-class classification. It is extended to solve multi-class classification problem. In this paper, 57 dimensional feature vectors for MRI image are selected as input for SVM. The segmentation of MRI image based on the multi-classification SVM (MCSVM) is investigated. As our experiment demonstrates, the boundaries of 7 kinds of encephalic tissues are extracted successfully, and it can reach satisfactory generalization accuracy. Thus, SVM exhibits its great potential in image segmentation.",https://ieeexplore.ieee.org/document/4353720/,2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,22-26 Aug. 2007,ieeexplore
10.1109/DSAA.2016.32,Reserve Price Optimization at Scale,IEEE,Conferences,"Online advertising is a multi-billion dollar industry largely responsible for keeping most online content free and content creators (""publishers"") in business. In one aspect of advertising sales, impressions are auctioned off in second price auctions on an auction-by-auction basis through what is known as real-time bidding (RTB). An important mechanism through which publishers can influence how much revenue they earn is reserve pricing in RTB auctions. The optimal reserve price problem is well studied in both applied and academic literatures. However, few solutions are suited to RTB, where billions of auctions for ad space on millions of different sites and Internet users are conducted each day among bidders with heterogenous valuations. In particular, existing solutions are not robust to violations of assumptions common in auction theory and do not scale to processing terabytes of data each hour, a high dimensional feature space, and a fast changing demand landscape. In this paper, we describe a scalable, online, real-time, incrementally updated reserve price optimizer for RTB that is currently implemented as part of the AppNexus Publisher Suite. Our solution applies an online learning approach, maximizing a custom cost function suited to reserve price optimization. We demonstrate the scalability and feasibility with the results from the reserve price optimizer deployed in a production environment. In the production deployed optimizer, the average revenue lift was 34.4% with 95% confidence intervals (33.2%, 35.6%) from more than 8 billion auctions over 46 days, a substantial increase over non-optimized and often manually set rule based reserve prices.",https://ieeexplore.ieee.org/document/7796939/,2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA),17-19 Oct. 2016,ieeexplore
10.1109/ICSE43902.2021.00028,Resource-Guided Configuration Space Reduction for Deep Learning Models,IEEE,Conferences,"Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity. In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.",https://ieeexplore.ieee.org/document/9402095/,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),22-30 May 2021,ieeexplore
10.1109/ASE51524.2021.9678889,Restoring the Executability of Jupyter Notebooks by Automatic Upgrade of Deprecated APIs,IEEE,Conferences,"Data scientists typically practice exploratory programming using computational notebooks, to comprehend new data and extract insights. To do this they iteratively refine their code, actively trying to re-use and re-purpose solutions created by other data scientists, in real time. However, recent studies have shown that a vast majority of publicly available notebooks cannot be executed out of the box. One of the prominent reasons is the deprecation of data science APIs used in such notebooks, due to the rapid evolution of data science libraries. In this work we propose RELANCER, an automatic technique that restores the executability of broken Jupyter Notebooks, in near real time, by upgrading deprecated APIs. RELANCER employs an iterative runtime-error-driven approach to identify and fix one API issue at a time. This is supported by a machine-learned model which uses the runtime error message to predict the kind of API repair needed - an update in the API or package name, a parameter, or a parameter value. Then RELANCER creates a search space of candidate repairs by combining knowledge from API migration examples on GitHub as well as the API documentation and employs a second machine-learned model to rank this space of candidate mappings. An evaluation of RELANCER on a curated dataset of 255 un-executable Jupyter Notebooks from Kaggle shows that RELANCER can successfully restore the executability of 56% of the subjects, while baselines relying on just GitHub examples and just API documentation can only fix 38% and 36% of the subjects respectively. Further, pursuant to its real-time use case, RELANCER can restore execution to 49% of subjects, within a 5 minute time limit, while a baseline lacking its machine learning models can only fix 24%.",https://ieeexplore.ieee.org/document/9678889/,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),15-19 Nov. 2021,ieeexplore
10.1109/SA47457.2019.8938039,Restricted Boltzmann Machine as Image Pre-processing Method for Deep Neural Classifier,IEEE,Conferences,"The paper presents a novel approach to image preprocessing for feature extraction that is designed for reduction of dimensionality of the classifier which is in this case the convolutional neural network (CNN). The proposed method uses Restricted Boltzmann Machine(RBM) as an Aggregation Method (AM) for binary feature descriptors. The assumption of this technique is that the RBM is performing an dimension expansion of the feature space. Also the type of the data undergoes the transformation from binary to floating point. The conventional approach in convolutional neural networks uses as an input the image that consists of one (grayscale) or three channels (RGB). The method presented herein allows to have the number of channels configurable, as it depends on the size of the Restricted Boltzmann Machine (RBM). The size of the entire network and its parallel implementation makes the architecture usable in real-time systems with reduced memory size.",https://ieeexplore.ieee.org/document/8938039/,2019 First International Conference on Societal Automation (SA),4-6 Sept. 2019,ieeexplore
10.1109/CVPR46437.2021.01042,Retinex-inspired Unrolling with Cooperative Prior Architecture Search for Low-light Image Enhancement,IEEE,Conferences,"Low-light image enhancement plays very important roles in low-level vision areas. Recent works have built a great deal of deep learning models to address this task. However, these approaches mostly rely on significant architecture engineering and suffer from high computational burden. In this paper, we propose a new method, named Retinex-inspired Unrolling with Architecture Search (RUAS), to construct lightweight yet effective enhancement network for low-light images in real-world scenario. Specifically, building upon Retinex rule, RUAS first establishes models to characterize the intrinsic underexposed structure of low-light images and unroll their optimization processes to construct our holistic propagation structure. Then by designing a cooperative reference-free learning strategy to discover low-light prior architectures from a compact search space, RUAS is able to obtain a top-performing image enhancement network, which is with fast speed and requires few computational resources. Extensive experiments verify the superiority of our RUAS framework against recently proposed state-of-the-art methods. The project page is available at http://dutmedia.org/RUAS/.",https://ieeexplore.ieee.org/document/9577287/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore
10.1109/HPCA51647.2021.00028,Revisiting HyperDimensional Learning for FPGA and Low-Power Architectures,IEEE,Conferences,"Today's applications are using machine learning algorithms to analyze the data collected from a swarm of devices on the Internet of Things (IoT). However, most existing learning algorithms are overcomplex to enable real-time learning on IoT devices with limited resources and computing power. Recently, Hyperdimensional computing (HDC) is introduced as an alternative computing paradigm for enabling efficient and robust learning. HDC emulates the cognitive task by representing the values as patterns of neural activity in high-dimensional space. HDC first encodes all data points to high-dimensional vectors. It then efficiently performs the learning task using a well-defined set of operations. Existing HDC solutions have two main issues that hinder their deployments on low-power embedded devices: (i) the encoding module is costly, dominating 80% of the entire training performance, (ii) the HDC model size and the computation cost grow significantly with the number of classes in online inference.In this paper, we proposed a novel architecture, LookHD, which enables real-time HDC learning on low-power edge devices. LookHD exploits computation reuse to memorize the encoding module and simplify its computation with single memory access. LookHD also address the inference scalability by exploiting HDC governing mathematics that compresses the HDC trained model into a single hypervector. We present how the proposed architecture can be implemented on the existing low power architectures: ARM processor and FPGA design. We evaluate the efficiency of the proposed approach on a wide range of practical classification problems such as activity recognition, face recognition, and speech recognition. Our evaluations show that LookHD can achieve, on average, $ 28.3\times$ faster and $ 97.4\times$ more energy-efficient training as compared to the state-of-the-art HDC implemented on the FPGA. Similarly, in the inference, LookHD is $ 2.2\times$ faster, $ 4.1\times$ more energy-efficient, and has $ 6.3\times$ smaller model size than the same state-of-the-art algorithms.",https://ieeexplore.ieee.org/document/9407181/,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),27 Feb.-3 March 2021,ieeexplore
10.1109/SEFM.2009.36,Right Propositional Neighborhood Logic over Natural Numbers with Integer Constraints for Interval Lengths,IEEE,Conferences,"Interval temporal logics are based on interval structures over linearly (or partially) ordered domains, where time intervals, rather than time instants, are the primitive ontological entities. In this paper we introduce and study Right Propositional Neighborhood Logic over natural numbers with integer constraints for interval lengths, which is a propositional interval temporal logic featuring a modality for the 'right neighborhood' relation between intervals and explicit integer constraints for interval lengths. We prove that it has the bounded model property with respect to ultimately periodic models and is therefore decidable. In addition, we provide an EXP SPACE procedure for satisfiability checking and we prove EXPSPACE-hardness by a reduction from the exponential corridor tiling problem.",https://ieeexplore.ieee.org/document/5368086/,2009 Seventh IEEE International Conference on Software Engineering and Formal Methods,23-27 Nov. 2009,ieeexplore
10.1109/SAIS53221.2021.9483964,Robot First Aid: Autonomous Vehicles Could Help in Emergencies,IEEE,Conferences,"Safety is of critical importance in designing autonomous vehicles (AVs) that will be able to perform effectively in complex, mixed-traffic, real-world urban environments. Some prior research has looked at how to proactively avoid accidents with safe distancing and driver monitoring, but currently little research has explored strategies to recover afterwards from emergencies, from crime to natural disasters. The current short paper reports on our ongoing work using a speculative prototyping approach to explore this expansive design space, in the context of how a robot inside an AV could be deployed to support first aid. As a result, we present some proposals for how to detect emergencies, and examine and help victims, as well as lessons learned in prototyping. Thereby, our aim is to stimulate discussion and ideation that-by considering the prevalence of Murphy's law in our complex world, and the various technical, ethical, and practical concerns raised-could potentially lead to useful safety innovations.",https://ieeexplore.ieee.org/document/9483964/,2021 Swedish Artificial Intelligence Society Workshop (SAIS),14-15 June 2021,ieeexplore
10.1109/ASAMA.1999.805407,Robot media communication: an interactive real-world guide agent,IEEE,Conferences,"Describes a guide system and the software architecture for an autonomous, interactive robot based on a multi-agent system. A robot navigation system has been developed allowing the robot to guide people through halls in various types of exhibitions. Our approach uses an infrared location system in the hallway ceilings, making the environment part of a sensor-distributed robot system. The real-world guide agent is composed of a guide agent on a hand-held mobile computer and a robot agent on an autonomous mobile robot. The guide agent plays the role of ""robot media"" in order to integrate information in the information space of the mobile computer and the physical space of the exhibits in order to guide visitors through the physical space. This research aims to develop a cooperative adaptive system using two-way communication among spaces, media and human beings to construct transparent knowledge boundaries between the real space and the virtual space. The virtual space is generated from computer data using shared space technology and it creates a distributed intelligence in order to manage the communication and control the guide in a laboratory. We have experimented with and verified this software architecture using a prototype autonomous mobile robot equipped with a compass.",https://ieeexplore.ieee.org/document/805407/,"Proceedings. First and Third International Symposium on Agent Systems Applications, and Mobile Agents",6-6 Oct. 1999,ieeexplore
10.1109/ICSMC.2008.4811760,Robot navigation using KFLANN place field,IEEE,Conferences,"This paper presents an implementation of place cells for a robot navigation using the K-iterations fast learning artificial neural networks (KFLANN) clustering algorithm. The KFLANN possesses several desirable properties suitable for place cell robot navigation tasks. The technique proposed is able to autonomously adjust the resolution of cells according to the complexity of the environment. This is achieved through two parameters known as the tolerance and the vigilance of the network. In addition, a navigation system consisting of a topological map building and a place cell path planning strategy is presented. A physical implementation of the system was developed on an autonomous platform and actual results were obtained. The experimental results obtained indicate that the system was able to navigate successfully through the experimental space and also tolerate unexpected discrepancies arising from motor and sensor errors present in a real environment. Furthermore, despite abrupt changes in an environment due to the deliberate introduction of obstacles, the system was still able to cope without changes to the program. The experiment was also extended to include a kidnapped robot scenario and the results were favorable, indicating a positive use of allothetic cue recognition capabilities.",https://ieeexplore.ieee.org/document/4811760/,"2008 IEEE International Conference on Systems, Man and Cybernetics",12-15 Oct. 2008,ieeexplore
10.1109/C-CODE.2017.7918964,Robotic navigation based on logic-based planning,IEEE,Conferences,"Logic and Planning are interesting artificial intelligence problems in the context of robotic systems, i.e., robotic navigation. For such an autonomous system one of the requisites is that the goal has to be achieved without intervention of human being. We present a practical implementation of autonomous robotic navigation based on logic-based planning. We achieve this by using strength of PROLOG in order to generate plan to reach goal position from an initial. We utilize First Order Logic (FOL) that automatically asserts and retracts facts at runtime dynamically. All possible plans are computed using local search strategies (e.g., Depth and Breadth First) on state space representing a real, dynamic, and unpredictable environment. In order to navigate in the environment following optimized plan - one with fewest states, a balanced size 4-wheel differential drive robot has been carefully constructed. It can turn 90 and actuate forward by controlling linear (<inf>t</inf> = 0.25m/s) and angular (<inf>t</inf> = /8 rad/s) velocities of two rear motorized wheels. It is also equipped with an Ultrasonic sensor to avoid collision with obstacles. The system is evaluated in an environment comprising of corridors with adjacent rooms. Graphical User Interface (GUI) is developed in .Net (C#) to map situation in Prolog and transmit plan to hardware for execution. Average time calculated for a plan to generate is 0.065 seconds. The robot moves block by block where each block in the state space represents 2m<sup>2</sup> area. In addition to minors, our major contribution is that we offer a unified scheme for robotic navigation without calculating odometry data with the assumption the robot cannot be kidnapped nor slipped.",https://ieeexplore.ieee.org/document/7918964/,"2017 International Conference on Communication, Computing and Digital Systems (C-CODE)",8-9 March 2017,ieeexplore
10.1109/IITA.2008.146,Robust Fuzzy-Possibilistic C-Means Algorithm,IEEE,Conferences,"In allusion to the disadvantages that fuzzy c-means algorithm is sensitive to noise and possibilistic c-means is easy to generate superposition cluster center, a novel algorithm (FPCM) which simultaneously produces both memberships and possibilities was proposed in 1997. However, FPCM still uses a norm-induced distance, as a consequence, its performance on the noisy data is not strong enough. In this paper, a new algorithm using the ""kernel method"" based on the classical FPCM is presented and called as robust fuzzy-possibilistic algorithm (RFPCM). RFPCM adopts a new kernel-induced metric in the data space to replace the original Euclidean norm metric in FPCM. Experiments on the artificial and real datasets show that RFPCM has better clustering performance and is more robust to noise than FPCM and PCM.",https://ieeexplore.ieee.org/document/4739656/,2008 Second International Symposium on Intelligent Information Technology Application,20-22 Dec. 2008,ieeexplore
10.1109/ICDE.2019.00128,Robust High Dimensional Stream Classification with Novel Class Detection,IEEE,Conferences,"A primary challenge in label prediction over a data stream is the emergence of instances belonging to unknown or novel class over time. Traditionally, studies addressing this problem aim to detect such instances using cluster-based mechanisms. They typically assume that instances from the same class are closer to each other than those belonging to different classes in observed feature space. Unfortunately, this may not hold true in higher-dimensional feature space such as images. In recent years, Convolutional neural network (CNN) have emerged as a leading system to be employed in many real-world application. Yet, based on the assumption of closed world dataset with a fixed number of categories, CNN lacks robustness for novel class detection, so it is unclear on how such models can be used to deal with novel class instances along a high-dimensional image stream. In this paper, we focus on addressing this challenge by proposing an effective learning framework called CNN-based Prototype Ensemble (CPE) for novel class detection and correction. Our framework includes a prototype ensemble loss (PE) to improve the intra-class compactness and expand inter-class separateness in the output feature representation, thereby enabling the robustness of novel class detection. Moreover, we provide an incremental learning strategy which maintains a constant amount of exemplars to update the network, making it more practical for real-world application. We empirically demonstrate the effectiveness of our framework by comparing its performance over multiple realworld image benchmark data streams with existing state-of-theart data stream detection techniques. The implementation of CPE is on: https://github.com/Vitvicky/Convolutional-Net-PrototypeEnsemble",https://ieeexplore.ieee.org/document/8731449/,2019 IEEE 35th International Conference on Data Engineering (ICDE),8-11 April 2019,ieeexplore
10.1109/CVPR46437.2021.00844,Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments,IEEE,Conferences,"Localizing the camera in a known indoor environment is a key building block for scene mapping, robot navigation, AR, etc. Recent advances estimate the camera pose via optimization over the 2D/3D-3D correspondences established between the coordinates in 2D/3D camera space and 3D world space. Such a mapping is estimated with either a convolution neural network or a decision tree using only the static input image sequence, which makes these approaches vulnerable to dynamic indoor environments that are quite common yet challenging in the real world. To address the aforementioned issues, in this paper, we propose a novel outlier-aware neural tree which bridges the two worlds, deep learning and decision tree approaches. It builds on three important blocks: (a) a hierarchical space partition over the indoor scene to construct the decision tree; (b) a neural routing function, implemented as a deep classification network, employed for better 3D scene understanding; and (c) an outlier rejection module used to filter out dynamic points during the hierarchical routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark developed for camera relocalization in dynamic indoor environments. It achieves robust neural routing through space partitions and outperforms the state-of-the-art approaches by around 30% on camera pose accuracy, while running comparably fast for evaluation.",https://ieeexplore.ieee.org/document/9577932/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore
10.1109/ICRA48506.2021.9562105,Robust Place Recognition using an Imaging Lidar,IEEE,Conferences,"We propose a methodology for robust, real-time place recognition using an imaging lidar, which yields image-quality high-resolution 3D point clouds. Utilizing the intensity readings of an imaging lidar, we project the point cloud and obtain an intensity image. ORB feature descriptors are extracted from the image and encoded into a bag-of-words vector. The vector, used to identify the point cloud, is inserted into a database that is maintained by DBoW for fast place recognition queries. The returned candidate is further validated by matching visual feature descriptors. To reject matching outliers, we apply PnP, which minimizes the reprojection error of visual features positions in Euclidean space with their correspondences in 2D image space, using RANSAC. Combining the advantages from both camera and lidar-based place recognition approaches, our method is truly rotation-invariant, and can tackle reverse revisiting and upside down revisiting. The proposed method is evaluated on datasets gathered from a variety of platforms over different scales and environments. Our implementation and datasets are available at https://git.io/image-lidar.",https://ieeexplore.ieee.org/document/9562105/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore
10.1109/RATFG.1999.799239,Robust real-time human hand localization by self-organizing color segmentation,IEEE,Conferences,"This paper describes a robust tracking algorithm used to localize a human hand in video sequences. The localization system relies mainly on an automatic color-based segmentation scheme combined with the motion cue. An automatic self-organizing clustering algorithm, is proposed to learn the color clusters unsupervisedly in the HSI space without specifying the number of clusters in advance. The schemes of growing, pruning and merging of 1-D self-organizing map (SOM) are facilitated to find an appropriate number of clusters in the forming stage of SOM. The training and segmentation in our approach is fast enough to make possible real-time applications. This segmentation scheme is capable of tracking multiple objects of different colors simultaneously. A motion cue is employed to focus the attention of the tracking algorithm. This approach is also applied to other tasks such as human face tracking and color indexing. Our localization system implemented on a SGI O2 R10000 workstation is reliable and efficient at 20-30 Hz.",https://ieeexplore.ieee.org/document/799239/,"Proceedings International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems. In Conjunction with ICCV'99 (Cat. No.PR00378)",26-27 Sept. 1999,ieeexplore
10.1109/CVPR.2019.01204,Robustness of 3D Deep Learning in an Adversarial Setting,IEEE,Conferences,"Understanding the spatial arrangement and nature of real-world objects is of paramount importance to many complex engineering tasks, including autonomous navigation. Deep learning has revolutionized state-of-the-art performance for tasks in 3D environments; however, relatively little is known about the robustness of these approaches in an adversarial setting. The lack of comprehensive analysis makes it difficult to justify deployment of 3D deep learning models in real-world, safety-critical applications. In this work, we develop an algorithm for analysis of pointwise robustness of neural networks that operate on 3D data. We show that current approaches presented for understanding the resilience of state-of-the-art models vastly overestimate their robustness. We then use our algorithm to evaluate an array of state-of-the-art models in order to demonstrate their vulnerability to occlusion attacks. We show that, in the worst case, these networks can be reduced to 0% classification accuracy after the occlusion of at most 6.5% of the occupied input space.",https://ieeexplore.ieee.org/document/8953599/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore
10.1109/EEIS.1995.513798,Rotation-invariant MLP classifiers for automatic aerial image recognition,IEEE,Conferences,"This paper describes the application of Multi Layer Perceptron (MLP) neural networks to the problem of Automatic Aerial Image Recognition (AAIR). The classification of aerial images independent of their orientation is required for automatic tracking and target recognition. Rotation-invariance is achieved by using rotation invariant feature space in conjunction with feed forward neural networks. The performance of the neural network based classifiers in conjunction with 3 types of rotation-invariant AAIR global features: the Zernike moments, central moments, and polar transform are examined. The performance of the Zernike based classifier is compared with that of the classical central moments, and polar transform. The real part of the phase spectrum of the Fourier plane is employed in combination with the MLP for rotation and translation invariance. The advantages of these approaches are discussed. Although a large image data base would be necessary before this approach could be fully validated, the initial results are very promising.",https://ieeexplore.ieee.org/document/513798/,Eighteenth Convention of Electrical and Electronics Engineers in Israel,7-8 March 1995,ieeexplore
10.1109/CloudTech49835.2020.9365867,Run Time Optimization using a novel implementation of Parallel-PSO for real-world applications,IEEE,Conferences,"The majority of optimization algorithms and methods generally necessitate a considerable run time to reach their goal. Most of them are used mainly in real-world applications. This article concentrates on an efficient and well-known algorithm to solve optimization problems: the Particle Swarm Optimisation algorithm (PSO). This algorithm needs a considerable run time to solve an optimization problem with a high dimension space and data. The article also concentrates on OpenCL, which defines a common parallel programming language for various devices such as GPU, CPU, FPGA, etc. In order to minimize the run time of PSO, this paper introduces a new implementation of PSO in OpenCL. By decomposing the PSO code into two fragments, each one can run simultaneously. The experimental results covered both the sequential and parallel implementations. Furthermore, show that the PSO' OpenCL implementation is faster than the Sequential-PSO implementation. The OpenCL profiling results show the timing of each part of the executing of PSO in OpenCL.",https://ieeexplore.ieee.org/document/9365867/,2020 5th International Conference on Cloud Computing and Artificial Intelligence: Technologies and Applications (CloudTech),24-26 Nov. 2020,ieeexplore
10.1109/WACVW54805.2022.00011,SAPNet: Segmentation-Aware Progressive Network for Perceptual Contrastive Deraining,IEEE,Conferences,"Deep learning algorithms have recently achieved promising deraining performances on both the natural and synthetic rainy datasets. As an essential low-level preprocessing stage, a deraining network should clear the rain streaks and preserve the fine semantic details. However, most existing methods only consider low-level image restoration. That limits their performances at high-level tasks requiring precise semantic information. To address this issue, in this paper, we present a segmentation aware progressive network (SAPNet) based upon contrastive learning for single image deraining. We start our method with a lightweight derain network formed with progressive dilated units (PDU). The PDU can significantly expand the receptive field and characterize multiscale rain streaks without the heavy computation on multiscale images. A fundamental aspect of this work is an unsupervised background segmentation (UBS) network initialized with ImageNet and Gaussian weights. The UBS can faithfully preserve an image s semantic information and improve the generalization ability to unseen photos. Furthermore, we introduce a perceptual contrastive loss (PCL) and a learned perceptual image similarity loss (LPISL) to regulate model learning. By ex-ploiting the rainy image and ground-truth as the negative and the positive sample in the VGG-16 latent space, we bridge the fine semantic details between the derained image and the ground-truth in a fully constrained manner. Comprehensive experiments on synthetic and real-world rainy images show our model surpasses top-performing methods and aids object detection and semantic segmentation with considerable efficacy. A Pytorch Implementation is available at https://github.com/ShenZheng2000/SAPNetfor-image-deraining.",https://ieeexplore.ieee.org/document/9707514/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW),4-8 Jan. 2022,ieeexplore
10.1109/IGARSS.2018.8519369,SMOS Data Assimilation for Numerical Weather Prediction,IEEE,Conferences,"This paper presents the Soil Moisture and Ocean Salinity (SMOS) mission data assimilation activities conducted at the European Centre for Medium-Range Weather Forecasts (ECMWF) to analyse soil moisture for Numerical Weather Prediction (NWP) applications. Two different approaches are presented based on SMOS brightness temperature and SMOS neural network soil moisture data assimilation, respectively. For the first approach, SMOS brightness temperature data assimilation relies on forward modelling. Long term results, spanning the SMOS period, of SMOS forward modelling, monitoring and data assimilation are presented. They emphasize the relevance of SMOS data for monitoring and to support NWP model developments. For the second approach, a SMOS soil moisture product has been produced based on a Neural Network (NN) trained on ECMWF soil moisture. So, the SMOS-ECMWF NN soil moisture product captures the SMOS signal variability in time and space, while by design its climatology is consistent with that of the ECMWF soil moisture, which makes it suitable for data assimilation purpose. This approach, initially tested for 2012 in a global scale stand alone approach, shows that SMOS NN data assimilation slightly improves the two-metre air temperature forecast in the short range at regional scale. For NWP applications this approach has been further developed with a near real time production of the SMOS-ECMWF NN soil moisture product, with the implementation of the SMOS NN data assimilation in the ECMWF Integrated Forecasting System (IFS), and with high resolution (9km) global scale testing compatible with the current ECMWF NWP system.",https://ieeexplore.ieee.org/document/8519369/,IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium,22-27 July 2018,ieeexplore
10.1109/ICIP.2019.8803262,SSSDET: Simple Short and Shallow Network for Resource Efficient Vehicle Detection in Aerial Scenes,IEEE,Conferences,"Detection of small-sized targets is of paramount importance in many aerial vision-based applications. The commonly deployed low cost unmanned aerial vehicles (UAVs) for aerial scene analysis are highly resource constrained in nature. In this paper we propose a simple short and shallow network (SSSDet) to robustly detect and classify small-sized vehicles in aerial scenes. The proposed SSSDet is up to 4 faster, requires 4.4 less FLOPs, has 30 less parameters, requires 31 less memory space and provides better accuracy in comparison to existing state-of-the-art detectors. Thus, it is more suitable for hardware implementation in real-time applications. We also created a new airborne image dataset (ABD) by annotating 1396 new objects in 79 aerial images for our experiments. The effectiveness of the proposed method is validated on the existing VEDAI, DLR-3K, DOTA and Combined dataset. The SSSDet outperforms state-of-the-art detectors in term of accuracy, speed, compute and memory efficiency.",https://ieeexplore.ieee.org/document/8803262/,2019 IEEE International Conference on Image Processing (ICIP),22-25 Sept. 2019,ieeexplore
10.1109/IJCB52358.2021.9484360,STERLING: Towards Effective ECG Biometric Recognition,IEEE,Conferences,"Electrocardiogram (ECG) biometric recognition has recently attracted considerable attention and various promising approaches have been proposed. However, due to the real nonstationary ECG noise environment, it is still challenging to perform this technique robustly and precisely. In this paper, we propose a novel ECG biometrics framework named robuSt semanTic spacE leaRning with Local sImilarity preserviNG (STERLING) to learn a latent space where ECG signals can be robustly and discriminatively represented with semantic information and local structure being preserved. Specifically, in the proposed framework, a novel loss function is proposed to learn robust semantic representation by introducing l<inf>2,1</inf>-norm loss and making full use of the supervised information. In addition, a graph regularization is imposed to preserve the local structure information in each subject. Finally, in the learnt latent space, matching can be effectively done. The experimental results on three widely-used datasets indicate that the proposed framework can outperform the state-of-the-arts.",https://ieeexplore.ieee.org/document/9484360/,2021 IEEE International Joint Conference on Biometrics (IJCB),4-7 Aug. 2021,ieeexplore
10.1109/ICMLC.2002.1167432,SVM-based incremental active learning for user adaptation for online graphics recognition system,IEEE,Conferences,"User adaptation is critical in the future design of human-computer interaction systems. Many pattern recognition problems, such as handwriting/sketching recognition and speech recognition, are user dependent since different users' handwritings, drawing styles, and accents are different. Hence, the classifiers for solving these problems should provide the functionality of user adaptation so as to let all users experience better recognition results. However, the user adaptation functionality requires the classifiers have the incremental learning ability in order to learn fast. In this paper, an SVM-based incremental active learning algorithm is presented to solve this problem. By utilizing the support vectors and only a small portion of the nonsupport vectors as well, in addition to the new interrogative samples, in the iterative training and reclassification cycle, both the training time and the storage space are saved with only very little classification precision being lost. Theoretical analysis, experimentation, evaluation, and real application samples in our online graphics recognition system are presented to show the effectiveness of this algorithm.",https://ieeexplore.ieee.org/document/1167432/,Proceedings. International Conference on Machine Learning and Cybernetics,4-5 Nov. 2002,ieeexplore
10.1109/ITSC45102.2020.9294259,Safe Reinforcement Learning for Autonomous Lane Changing Using Set-Based Prediction,IEEE,Conferences,"Machine learning approaches often lack safety guarantees, which are often a key requirement in real-world tasks. This paper addresses the lack of safety guarantees by extending reinforcement learning with a safety layer that restricts the action space to the subspace of safe actions. We demonstrate the proposed approach using lane changing in autonomous driving. To distinguish safe actions from unsafe ones, we compare planned motions with the set of possible occupancies of traffic participants generated by set-based predictions. In situations where no safe action exists, a verified failsafe controller is executed. We used real-world highway traffic data to train and test the proposed approach. The evaluation result shows that the proposed approach trains agents that do not cause collisions during training and deployment.",https://ieeexplore.ieee.org/document/9294259/,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),20-23 Sept. 2020,ieeexplore
10.1109/CVPR42600.2020.01202,Scalable Uncertainty for Computer Vision With Functional Variational Inference,IEEE,Conferences,"As Deep Learning continues to yield successful applications in Computer Vision, the ability to quantify all forms of uncertainty is a paramount requirement for its safe and reliable deployment in the real-world. In this work, we leverage the formulation of variational inference in function space, where we associate Gaussian Processes (GPs) to both Bayesian CNN priors and variational family. Since GPs are fully determined by their mean and covariance functions, we are able to obtain predictive uncertainty estimates at the cost of a single forward pass through any chosen CNN architecture and for any supervised learning task. By leveraging the structure of the induced covariance matrices, we propose numerically efficient algorithms which enable fast training in the context of high-dimensional tasks such as depth estimation and semantic segmentation. Additionally, we provide sufficient conditions for constructing regression loss functions whose probabilistic counterparts are compatible with aleatoric uncertainty quantification.",https://ieeexplore.ieee.org/document/9156974/,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 2020,ieeexplore
10.1109/ISRCS.2013.6623773,Scalable machine learning framework for behavior-based access control,IEEE,Conferences,"Today's activities in cyber space are more connected than ever before, driven by the ability to dynamically interact and share information with a changing set of partners over a wide variety of networks. The success of approaches aimed at securing the infrastructure has changed the threat profile to point where the biggest threat to the US cyber infrastructure is posed by targeted cyber attacks. The Behavior-Based Access Control (BBAC) effort has been investigating means to increase resilience against these attacks. Using statistical machine learning, BBAC (a) analyzes behaviors of insiders pursuing targeted attacks and (b) assesses trustworthiness of information to support real-time decision making about information sharing. The scope of this paper is to describe the challenge of processing disparate cyber security information at scale, together with an architecture and work-in-progress prototype implementation for a cloud framework supporting a strategic combination of stream and batch processing.",https://ieeexplore.ieee.org/document/6623773/,2013 6th International Symposium on Resilient Control Systems (ISRCS),13-15 Aug. 2013,ieeexplore
10.1109/ICMLA.2007.110,Scalable optimal linear representation for face and object recognition,IEEE,Conferences,"Optimal component analysis (OCA) is a linear method for feature extraction and dimension reduction. It has been widely used in many applications such as face and object recognitions. The optimal basis of OCA is obtained through solving an optimization problem on a Grassmann manifold. However, one limitation of OCA is the computational cost becoming heavy when the number of training data is large, which prevents OCA from efficiently applying in many real applications. In this paper, a scalable OCA (S-OCA) that uses a two-stage strategy is developed to bridge this gap. In the first stage, we cluster the training data using K-means algorithm and the dimension of data is reduced into a low dimensional space. In the second stage, OCA search is performed in the reduced space and the gradient is updated using an numerical approximation. In the process of OCA gradient updating, instead of choosing the entire training data, S-OCA randomly chooses a small subset of the training images in each class to update the gradient. This achieves stochastic gradient updating and at the same time reduces the searching time of OCA in orders of magnitude. Experimental results on face and object datasets show efficiency of the S-OCA method, in term of both classification accuracy and computational complexity.",https://ieeexplore.ieee.org/document/4457247/,Sixth International Conference on Machine Learning and Applications (ICMLA 2007),13-15 Dec. 2007,ieeexplore
10.1109/IV48863.2021.9575822,Self-Supervised Action-Space Prediction for Automated Driving,IEEE,Conferences,"Making informed driving decisions requires reliable prediction of other vehicles' trajectories. In this paper, we present a novel learned multi-modal trajectory prediction architecture for automated driving. It achieves kinematically feasible predictions by casting the learning problem into the space of accelerations and steering angles - by performing action-space prediction, we can leverage valuable model knowledge. Additionally, the dimensionality of the action manifold is lower than that of the state manifold, whose intrinsically correlated states are more difficult to capture in a learned manner. For the purpose of action-space prediction, we present the simple Feed-Forward Action-Space Prediction (FFW-ASP) architecture. Then, we build on this notion and introduce the novel Self-Supervised Action-Space Prediction (SSP-ASP) architecture that outputs future environment context features in addition to trajectories. A key element in the self-supervised architecture is that, based on an observed action history and past context features, future context features are predicted prior to future trajectories. The proposed methods are evaluated on real-world datasets containing urban intersections and roundabouts, and show accurate predictions, outperforming state-of-the-art for kinematically feasible predictions in several prediction metrics.",https://ieeexplore.ieee.org/document/9575822/,2021 IEEE Intelligent Vehicles Symposium (IV),11-17 July 2021,ieeexplore
10.1109/CEC.2002.1006258,Self-adaptive systems using a massive multi-agent system,IEEE,Conferences,"We deal with systems using massive multi-agent organizations and expressing complex problems like the representation of the world sub-system managing the behavior of a robot. We propose an analysis and an operating representation of multi-agent organization in a geometric way, using specific multi-agent organization in a morphologic agent space. We propose also an architecture expressing the behavior of the massive multi-agent organization. So we open the way to the implementation of self-adaptive systems. We present an application for the behavior of an autonomous robot.",https://ieeexplore.ieee.org/document/1006258/,Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600),12-17 May 2002,ieeexplore
10.1109/IOLTS.2015.7229845,Self-awareness and self-learning for resiliency in real-time systems,IEEE,Conferences,"While the notion of self-awareness has a long history in biology, psychology, medicine, engineering and (more recently) computing, we are seeing the emerging need for self-awareness in the context of complex Systems-on-Chip that must address the often conflicting requirements of performance, resiliency, energy, cost, etc. in the face of highly dynamic operational behaviors coupled with process, environment, and workload variabilities. Unlike traditional Systems-on-Chip (SoCs), self-aware SoCs must deploy an intelligent co-design of the control, communication, and computing infrastructure that interacts with the physical environment in real-time in order to modify the systems behavior so as to adaptively achieve desired objectives and Quality-of-Service (QoS). Self-aware SoCs require a combination of ubiquitous sensing and actuation, health-monitoring, and self-learning to enable the SoCs adaptation over time and space. This special session targets self-learning and self-awareness in two domains. The first one is a self-learning runtime reliability prediction approach by reusing Design-for-Test (DfT) infrastructure. The other one discusses real-time systems and applications to wireless communication, signal processing and control.",https://ieeexplore.ieee.org/document/7229845/,2015 IEEE 21st International On-Line Testing Symposium (IOLTS),6-8 July 2015,ieeexplore
10.1109/ROBOT.2000.844830,Self-learning vision-guided robots for searching and grasping objects,IEEE,Conferences,"An approach to control vision-guided robots is introduced. It allows searching and grasping differently shaped objects that may be located anywhere in the robot's work space, even not visible in the initial fields of view of cameras. It eliminates the need for a calibration of the robot and of the vision system, it uses no world coordinates and no inverse perspective or kinematic transformations, and it comprises an automatic adaptation to changing parameters. The approach has been implemented on a calibration-free vision-guided manipulator with five degrees of freedom (DOF) and was evaluated in real-word experiments.",https://ieeexplore.ieee.org/document/844830/,Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),24-28 April 2000,ieeexplore
10.1109/INDICON.2016.7839069,Selfie continuous sign language recognition using neural network,IEEE,Conferences,"This works objective is to bring sign language closer to real time implementation on mobile platforms with a video database of Indian sign language created with a mobile front camera in selfie mode. Pre-filtering, segmentation and feature extraction on video frames creates a sign language feature space. Artificial Neural Network classifier on the sign feature space are trained with feed forward nets and tested. ASUS smart phone with 5M pixel front camera captures continuous sign videos containing on average of 220 frames for 18 single handed signs at a frame rate of 30fps. Sobel edge operator's power is enhanced with morphology and adaptive thresholding giving a near perfect segmentation of hand and head portions. Word matching score (WMS) gives the performance of the proposed method with an average WMS of around 90% for ANN with an execution time of 0.5221 seconds during classification. Fully novel method of implementing sign language to put sign language recognition systems on smart phones to make it a real time usage application.",https://ieeexplore.ieee.org/document/7839069/,2016 IEEE Annual India Conference (INDICON),16-18 Dec. 2016,ieeexplore
10.1109/SaCoNeT.2018.8585543,Semantic Networks Based Approach for SaaS Management in Cloud Computing,IEEE,Conferences,"Nowadays, Cloud Computing has emerged as a new model for hosting, managing and delivering services (IaaS, PaaS, SaaS) over the Internet. Enterprises and vendors are continuously migrating their services to the Cloud, this resulted an exponential amount of data, services and resources stored in data centers. Since there is no standard about Cloud service description or publication, therefore a key requirement that every Cloud provider needs to take into consideration is efficient management of resources and services by providing automated solutions, for a better service publication and discovery. The proposed approach aims to manage Cloud SaaS services for an efficient publication by classifying them into sets according to their domain to reduce the search space then interconnected the SaaS services of the same domain in a Semantic Network using the similarity measure (Input/Output similarity) between concepts. The proposed solution takes advantages from multidimensional index framework, WordNet Domain Ontology and semantic Web. A guided publication process and an implemented prototype are introduced validating the system using a real data set corpus.",https://ieeexplore.ieee.org/document/8585543/,2018 International Conference on Smart Communications in Network Technologies (SaCoNeT),27-31 Oct. 2018,ieeexplore
10.1109/AERO47225.2020.9172454,Semi-Supervised Machine Learning for Spacecraft Anomaly Detection &amp; Diagnosis,IEEE,Conferences,"This paper describes Anomaly Detection via Topological-feature Map (ADTM), a data-driven approach to Integrated System Health Management (ISHM) for monitoring the health of spacecraft and space habitats. Developed for NASA Ames Research Center, ADTM leverages proven artificial intelligence techniques for rapidly detecting and diagnosing anomalies in near real-time. ADTM combines Self-Organizing Maps (SOMs) as the basis for modeling system behavior with supervised machine learning techniques for localizing detected anomalies. A SOM is a two-layer artificial neural network (ANN) that produces a low-dimensional representation of the training samples. Once trained on normal system behavior, SOMs are adept at detecting behavior previously not encountered in the training data. Upon detecting anomalous behavior, ADTM uses a supervised classification approach to determine a subset of measurands that characterize the anomaly. This allows it to localize faults and thereby provide extra insight. We demonstrate the effectiveness of our approach on telemetry data collected from a lab-stationed CubeSat (the LabSat) connected to software that gave us the ability to trigger several real hardware faults. We include an analysis and discussion of ADTM's performance on several of these fault cases. We conclude with a brief discussion of future work, which contains investigation of a hierarchical SOM-architecture as well as a Case-Based Reasoning module for further assisting astronauts in diagnosis and remediation activities.",https://ieeexplore.ieee.org/document/9172454/,2020 IEEE Aerospace Conference,7-14 March 2020,ieeexplore
10.1109/ICIP.2019.8803816,Semi-Supervised Robust One-Class Classification in RKHS for Abnormality Detection in Medical Images,IEEE,Conferences,"Abnormality detection in medical images is a one-class classification problem for which typical methods use variants of kernel principal component analysis or one-class support vector machines. However, in practical deployment scenarios, many such methods are sensitive to the outliers present in the imperfectly-curated training sets. Current robust methods use heuristics for model fitting or lack formulations to leverage even a small amount of high-quality expert feedback. In contrast, we propose a novel method combining (i) robust statistical modeling, extending the multivariate generalized-Gaussian to a reproducing kernel Hilbert space, with (ii) semi-supervised learning to leverage a small expert-labeled outlier set. Results on simulated and real-world data, including endoscopy data, show that our method outperforms the state of the art in accurately detecting abnormalities.",https://ieeexplore.ieee.org/document/8803816/,2019 IEEE International Conference on Image Processing (ICIP),22-25 Sept. 2019,ieeexplore
10.1109/IJCNN52387.2021.9533507,Semi-supervised Anomaly Detection on Attributed Graphs,IEEE,Conferences,"We propose a simple yet effective method for detecting anomalous instances on an attribute graph with label information of a small number of instances. Although standard anomaly detection methods usually assume that instances are independent and identically distributed, in many real-world applications, instances are often explicitly connected, resulting in so-called attributed graphs. The proposed method embeds nodes (instances) on the attributed graph in a latent space by taking into account their attributes as well as the graph structure on the basis of graph convolutional networks (GCNs). To learn node embeddings specialized for anomaly detection, in which there is a class imbalance due to the rarity of anomalies, the parameters of a GCN are trained to minimize the volume of a hypersphere that encloses the node embeddings of normal instances while embedding anomalous ones outside the hypersphere. This enables us to detect anomalies by simply calculating the distances between the node embeddings and hypersphere center. The proposed method can effectively propagate label information on a small amount of nodes to unlabeled ones by taking into account the node's attributes, graph structure, and class imbalance. In experiments with five real-world attributed graph datasets, we demonstrate that the proposed method outperforms various existing anomaly detection methods.",https://ieeexplore.ieee.org/document/9533507/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore
10.1109/ICGEC.2010.33,Semi-supervised Kernel Based Progressive SVM,IEEE,Conferences,"Most existing semi-supervised methods implemented either the cluster assumption or the manifold assumption. The performance will degrade if the assumption was not proper for the data. A method was proposed by combining both the cluster assumption and the manifold assumption. A semi-supervised kernel which reflected geometric information of the samples was constructed through warping the Reproducing Kernel Hilbert Space. Then the semi-supervised kernel was used in SVM which was based on cluster assumption, and a progressive learning procedure was used in the proposed method. Experiments had been took on synthetic and real data sets, and the results showed that, compared with the progressive SVM with common kernel and the standard SVM with semi supervised kernel, the proposed method using semi-supervised kernel in progressive SVM had competitive performance.",https://ieeexplore.ieee.org/document/5715381/,2010 Fourth International Conference on Genetic and Evolutionary Computing,13-15 Dec. 2010,ieeexplore
10.1109/SYNASC54541.2021.00037,Severity Prediction of Software Vulnerabilities based on their Text Description,IEEE,Conferences,"Software vulnerabilities represent a real challenge nowadays, often resulting in disruption of vital systems and data loss. Due to the multitude of software applications used within a company, system administrators often end up in the situation of facing multiple vulnerabilities at the same time, having no choice but to prioritize the most critical ones. Administrators commonly use vulnerability databases and metric systems to rank vulnerabilities; however, it usually takes from days to weeks for the metrics to be published since these metrics are established by human security analysts and the number of daily discovered exploits is constantly increasing. Therefore, newly discovered vulnerabilities, especially those without an available patch, represent the largest problem. In this paper, we propose a deep learning approach to predict the severity score and other metrics of a vulnerability using only its text description, which is available on discovery. We use a Multi-Task Learning architecture with a pre-trained BERT model for computing vector-space representations of words. Our best configuration achieves a mean absolute error of 0.86 for the severity score and an accuracy of 71.55% for the severity level.",https://ieeexplore.ieee.org/document/9700266/,2021 23rd International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC),7-10 Dec. 2021,ieeexplore
10.1109/COMPSAC51774.2021.00240,Shared-latent Variable Network Alignment,IEEE,Conferences,"The increasing popularity and diversity of social media sites, has encouraged many people to participate in different online social networks to enjoy a variety of services. Linking the same users across different social networks, also known as social network alignment, is a critical task of great research challenges. Many existing works usually focus on finding a projection function from one subspace to another for network alignment, however, the projection functions proposed in their papers are independent and updated individually, which could not effectively exploit the non-parallel data, and yield inferior alignment performance. In this paper, we propose a Shared-latent Variable Network Alignment (SVNA) architecture to effectively exploit the non-parallel data for network alignment, and jointly train projection functions and decoders in a unified framework with the shared latent variable z. Specifically, SVNA first employs the graph convolutional networks to preserve the structural information of the network. By introducing the shared latent variable z, SVNA simultaneously integrates two projection functions and two decoders for jointly training. Both projection functions and decoders share the same latent space, therefore both projection directions can learn from the non-parallel data more effectively. Thereafter, SVNA utilizes the Generative Adversarial Networks (GANs) framework to further train the projection functions, and adopts a probability-based semi-supervised method to achieve the network alignment. Experiments on three real-world datasets show that SVNA generally outperforms the state-of-the-art methods in network alignment task.",https://ieeexplore.ieee.org/document/9529755/,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",12-16 July 2021,ieeexplore
10.1109/KIMAS.2003.1245110,Sharing learning policies between multiple mobile robots,IEEE,Conferences,"Learning of a complex task usually requires a long learning period. In order to reduce the time of learning, the task is divided into several subtasks. Multiple agents can be used to serve a complex task by learning these subtasks concurrently. With a good knowledge sharing mechanism, the learning policy can be shared or exchanged among these agents and can enhance their learning efficiency. The learning policy is a mapping from system states to actions. The mechanism of sharing or exchanging learning knowledge among multiagent system is proposed. An index of expertise, which indicates the skill level of each learning agent, is presented. This index is used to select the best preferable advice among multiple advices, which can increase the probability of finding solution in the search space. The experiment in which the learning knowledge is exchanged between a mobile robot and a computer simulated agent is implemented in order to verify the validity of the proposed algorithm. The experimental results show that the learning efficiency of the advisor agent is increased and the advisee robot can use the given advice for avoiding collision with obstacle successfully in the real world implementation.",https://ieeexplore.ieee.org/document/1245110/,IEMC '03 Proceedings. Managing Technologically Driven Organizations: The Human Side of Innovation and Change (IEEE Cat. No.03CH37502),30 Sept.-4 Oct. 2003,ieeexplore
10.1109/SSPD.2017.8233263,Short Codes and Entanglement-Based Quantum Key Distribution via Satellite,IEEE,Conferences,"Quantum key distribution (QKD) provides the opportunity to deliver unconditional communication security. The most robust version of QKD relies on quantum entanglement. Very recently, ubiquitous deployment of such entanglement-based QKD over large distances has moved closer to reality, as verified by quantum entanglement distribution from a low Earth orbit satellite. We will demonstrate that this robust form of QKD via space will require a renewed focus on short-block length error-correcting codes in order to facilitate the reconciliation phase of the key distribution. Focusing on discrete variable QKD and adopting the low data rates consistent with measured entanglement distribution from space, we quantify the benefits of state-of-the-art short- block length codes in the context of device- independent QKD. Our results highlight the trade- off between the attainable key throughput vs the communication latency encountered in space-based implementations of this ultra-secure technology.",https://ieeexplore.ieee.org/document/8233263/,2017 Sensor Signal Processing for Defence Conference (SSPD),6-7 Dec. 2017,ieeexplore
10.1109/LARS-SBR-WRE48964.2019.00060,Sim-to-Real in Reinforcement Learning for Everyone,IEEE,Conferences,"In reinforcement learning (RL), it remains a challenge to have a robotic agent perform a task in the real world for which it was trained in simulation. In this paper, we present our work training a low-cost robotic arm in simulation to move towards a predefined target in space, represented by a red ball in an RGB image, and transferring the capability to the real arm. We exercised the entire end-to-end flow including the 3D modeling of the arm, training of a state-of-the-art RL policy in simulation with multiple actors in a distributed fashion, domain randomization in order to close the sim-to-real gap, and finally the execution of the trained model in the real robot. We also implemented a mechanism to edit the image captured from the camera before sending it to the model for inference, which allowed us to automate reward computation in the physical world. Our work highlights important challenges of training RL agents and moving them to the real world, validating important aspects shown by other works as well as detailing steps not explained by some of them (e.g. how to compute the reward in the real world). The conducted experiments show the improvements observed as the techniques were added to the final solution.",https://ieeexplore.ieee.org/document/9018558/,"2019 Latin American Robotics Symposium (LARS), 2019 Brazilian Symposium on Robotics (SBR) and 2019 Workshop on Robotics in Education (WRE)",23-25 Oct. 2019,ieeexplore
10.1109/CISP.2009.5303973,Simple Ensemble of Extreme Learning Machine,IEEE,Conferences,"In this paper, a novel approach for neural network ensemble called Simple Ensemble of Extreme Learning Machine (SE-ELM) is proposed. It is proved theoretically in this study that the generalization ability of an ensemble is determined by the diversity of its components' output space. Therefore SE-ELM regards the diversity of components' output space as a target during the training process. In the first phase, SE-ELM initializes each component with different input weights and analytically determines the output weights through generalized inverse operation of the hidden layer output matrices. The difference among components' input weights forces those components to have different output space thus increasing the diversity of the ensemble. Experiments carried on four real world problems show that SE-ELM not only runs much faster but also presents better generalization performance than some classic ensemble algorithms.",https://ieeexplore.ieee.org/document/5303973/,2009 2nd International Congress on Image and Signal Processing,17-19 Oct. 2009,ieeexplore
10.1109/ICAIIT.2019.8834627,Simple Implementation of Fuzzy Controller for Low Cost Microcontroller,IEEE,Conferences,"Fuzzy logic inference system requires a long enough computational process, starting from fuzzification, matching rule base, drawing conclusions and defuzzification. By using C programming, fuzzy logic systems can be carried out by a low capability microcontrollers, for example PIC16F877A that has a flash memory capacity of 8KB and the maximum clock speed is 20 MHz. But if it will be applied to a microcontroller that has a smaller size of flash memory, there is a possibility that it cannot be realized, especially if it uses the gaussian membership function and each membership function inputs was devided in to more than three memberships. To overcome this problem we need a method to accelerate the fuzzy logic computation process. One of that method is using a linear interpolation from the table of input-output fuzzy system. In this paper, the linear interpolation was illustrated by using bilinear fuzzy interpolation. Based on the GNU Octave simulation, it was obtained data that the time being needed to complete the simulation for fuzzy bilinear Look Up Table has 7 times faster than the fuzzy system computation, whereas from the real implementation, it was obtained that fuzzy bilinear Look Up Table gives 2 times faster and can save program memory space around 25% compared to ordinary fuzzy system.",https://ieeexplore.ieee.org/document/8834627/,2019 International Conference of Artificial Intelligence and Information Technology (ICAIIT),13-15 March 2019,ieeexplore
10.1109/IJCNN.2000.859462,"Simulating the evolution of 2D pattern recognition on the CAM-Brain Machine, an evolvable hardware tool for building a 75 million neuron artificial brain",IEEE,Conferences,"This paper presents some simulation results of the evolution of 2D visual pattern recognizers to be implemented very shortly on real hardware, namely the ""CAM-Brain Machine"" (CBM), an FPGA based piece of evolvable hardware which implements a genetic algorithm (GA) to evolve a 3D cellular automata (CA) based neural network circuit module, of approximately 1,000 neurons, in about a second, i.e. a complete run of a GA, with tens of thousands of circuit growths and performance evaluations. Up to 65,000 of these modules, each of which is evolved with a humanly specified function, can be downloaded into a large RAM space, and interconnected according to humanly specified artificial brain architectures. This RAM, containing an artificial brain with up to 75 million neurons, is then updated by the CBM at a rate of 130 billion CA cells per second. Such speeds will enable real time control of robots and hopefully the birth of a new research field that we call ""brain building"". The first such artificial brain, to be built at STARLAB in 2000 and beyond, will be used to control the behaviors of a life sized kitten robot called ""Robokitty"". This kitten robot will need 2D pattern recognizers in the visual section of its artificial brain. This paper presents simulation results on the evolvability and generalization properties of such recognizers.",https://ieeexplore.ieee.org/document/859462/,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,27-27 July 2000,ieeexplore
10.1109/ITAIC49862.2020.9338925,Simulation Modeling for Inertial Work-pattern of UAV Optoelectronic Gimbals,IEEE,Conferences,"At present, the simulation of photoelectric gimbals is only semi physical simulation, which can not meet the training requirements of full digital simulation. Based on working principle of the optoelectronic gimbals, a two-freedom-degrees optoelectronic gimbals model composed of inner ring, outer ring and bracket was built. Based on direction cosine theory and quasi Newton numerical iterative technique, for the actual equipment of the optoelectronic gimbals, the inertial work-pattern simulating &amp; modeling algorithm (IWPSMA) were proposed in this projection, which to keep the axis stable in the inertial space. The model and IWPSMA were simulated, with the UAV moving in a circle, the azimuth changes periodically in 360, and the elevation angle change nonlinearly in 0-35. The turntable model and algorithm are loaded into the 3D simulation scene. The experimental results show that the fidelity of the model and the real equipment are high, the boresight axis of the model keeps relatively stable in the inertial space, the computer resource occupation rate is low, and the 3D picture is smooth and stable.",https://ieeexplore.ieee.org/document/9338925/,2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),11-13 Dec. 2020,ieeexplore
10.1109/CCECE.2003.1226239,Simulation of three-rate neural network control for stochastic model of a fighter aircraft,IEEE,Conferences,"The nature of the multirate dynamics of the process makes it very attractive for applications, since the multirate phenomena are complex. This paper presents a research methodology for describing a three-rate continuous-time stochastic control system as state-space (SS) type decentralized models of multiinput/multioutput (MIMO) stochastic control subsystems with the three neural networks (NNs) of different structure. The block diagram of the decentralized control subsystems with the radial basis control NNs for the ""fast"" and ""intermediate"" subsystems, and with one linear layer control NN for the ""slow"" subsystem is designed. An illustrative design example - NN control of an experimental fighter aircraft model in the short approach to landing task - was carried out using the proposed three-rate SS decentralization technique. The simulation results with use of software package Simulink demonstrate that this research technique would work for real-time MIMO stochastic systems.",https://ieeexplore.ieee.org/document/1226239/,CCECE 2003 - Canadian Conference on Electrical and Computer Engineering. Toward a Caring and Humane Technology (Cat. No.03CH37436),4-7 May 2003,ieeexplore
10.1109/WCICA.2004.1342067,Simulation of two-rate adaptive hybrid control with neural and neuro-fuzzy networks for stochastic model of missile autopilot,IEEE,Conferences,"This paper describes a two-rate stochastic control system as state-space (SS) type decomposed and discretized models of multi-input/multi-output (MIMO) stochastic subsystems with the ""fast"" and ""slow"" control neural networks (NNs) and with the ""fast"" and ""slow"" neuro-fuzzy networks (NFNs). The block diagrams both the original system with linear-quadratic-Gaussian (LQG) regular and decomposed subsystems with two-rate NNs and NFNs hybrid adaptive control were designed. An illustrative example - two-rate NN and NFN hybrid control of decomposed stochastic model of a rigid guided missile over different operating conditions was carried out using the proposed two-rate SS decomposition technique. This example demonstrates that this research technique results in simplified low-order autonomous control subsystems with various discretization periods and with various speeds of actuation, and shows the quality of the proposed technique. The obtained results show that the control tasks for the autonomous subsystems can be solved more qualitatively than for the original system. This simulation and animation results with use of software package Simulink demonstrate that this research technique would work for real-time stochastic systems.",https://ieeexplore.ieee.org/document/1342067/,Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788),15-19 June 2004,ieeexplore
10.1109/IJCNN.2019.8851808,Skip-GANomaly: Skip Connected and Adversarially Trained Encoder-Decoder Anomaly Detection,IEEE,Conferences,"Despite inherent ill-definition, anomaly detection is a research endeavour of great interest within machine learning and visual scene understanding alike. Most commonly, anomaly detection is considered as the detection of outliers within a given data distribution based on some measure of normality. The most significant challenge in real-world anomaly detection problems is that available data is highly imbalanced towards normality (i.e. non-anomalous) and contains at most a sub-set of all possible anomalous samples - hence limiting the use of well-established supervised learning methods. By contrast, we introduce an unsupervised anomaly detection model, trained only on the normal (non-anomalous, plentiful) samples in order to learn the normality distribution of the domain, and hence detect abnormality based on deviation from this model. Our proposed approach employs an encoder-decoder convolutional neural network with skip connections to thoroughly capture the multi-scale distribution of the normal data distribution in image space. Furthermore, utilizing an adversarial training scheme for this chosen architecture provides superior reconstruction both within image space and a lower-dimensional embedding vector space encoding. Minimizing the reconstruction error metric within both the image and hidden vector spaces during training aids the model to learn the distribution of normality as required. Higher reconstruction metrics during subsequent test and deployment are thus indicative of a deviation from this normal distribution, hence indicative of an anomaly. Experimentation over established anomaly detection benchmarks and challenging real-world datasets, within the context of X-ray security screening, shows the unique promise of such a proposed approach.",https://ieeexplore.ieee.org/document/8851808/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore
10.1109/ICITECH.2017.8079996,Sliding Recursive Hierarchical Adaptive PCA for 3D image processing,IEEE,Conferences,"Principal Components Analysis (PCA) is the basic approach for processing of 3D tensor images (for example, multi- and hyper-spectral, multi-view, computer tomography, video, etc.). As a result of their processing, the information redundancy is significantly reduced. This is of high importance for their efficient compression and for the reduction of the features space needed, when object recognition is performed. The basic obstacle for the wide application of PCA, is the high computational complexity. One of the approaches, used to overcome the problem, is to use algorithms, based on the recursive PCA. The well-known methods for recursive PCA are aimed at the processing of sequences of images, represented as non-overlapping groups of vectors. In the last several years, the interest towards the recursive PCA for 3D tensor images executed in sliding temporal window, was significantly increased. Such analysis is needed, for example for the convolution tensor decomposition in multi-layer neural networks, in the systems for 3D noise filtration, video compression, etc. In this work is proposed new method, called Sliding Recursive Hierarchical Adaptive PCA (SR-HAPCA), based on the HAPCA algorithm. The method SR-HAPCA retains all advantages of HAPCA towards the PCA: it decreases the number of calculations needed, and permits parallel implementation. Besides, the lower computational complexity of SR-HAPCA makes easier its application in real-time processing of 3D tensor images.",https://ieeexplore.ieee.org/document/8079996/,2017 8th International Conference on Information Technology (ICIT),17-18 May 2017,ieeexplore
10.1109/ICWAPR.2007.4421653,Small-shaped space target recognition based on wavelet decomposition and support vector machine,IEEE,Conferences,"A kind of method for small-shaped space target recognition was proposed in this paper based on feature extraction with wavelet decomposition and formative support vector machine (FSVM) with sequential minimal optimization (SMO) algorithm. Firstly, the significance and characteristics of space target recognition were discussed and a two-stage recognition strategy was designed. And then aiming at the characteristics of small-shaped space target recognition, a new method was implemented based on feature extraction with wavelet decomposition and FSVM with SMO algorithm. Simulation results show the good performance of the algorithm proposed in this paper: the correct rate is more than 97% within 1360 simulation samples of ten classes of small shaped space targets; meanwhile the algorithm is characterized with high speed of near real time in both implementation of training and testing.",https://ieeexplore.ieee.org/document/4421653/,2007 International Conference on Wavelet Analysis and Pattern Recognition,2-4 Nov. 2007,ieeexplore
10.1109/AERO47225.2020.9172439,"Smart &amp; Integrated Management System - Smart Cities, Epidemiological Control Tool Using Drones",IEEE,Conferences,"This paper describes the development of a real application using Drones over urban regions to help the authorities at epidemiological control through a disruptive solutions based on a customizable Smart &amp; Integrated Management System (SIGI), devices and software based on the Enterprise Resource Planning (ERP) concept. Compound by management software, Drones and specific IoT devices, both referred to as sensors, the sensors collect the data of the interest areas in real time, creating a specified database. Based on the data collected from the interest areas, SIGI software has the ability to show real-time situational analysis of these areas and allows that the administrator can optimize resources (material and human) improving the efficiency of resource allocation in these areas. In addition to the development of the management software, the development of sensors to collect the information in the field and update these information to the database of the management software, are considered. The sensors will be recognized as IoT devices for the collection of meteorological data, images and command / control Drones. Initially the system will be customized, using an Artificial Intelligence tool, to collect data and identify the outbreaks of the dengue mosquito, zika and Chikungunya, nominee by risk areas. After the definition of the potential risk areas, in a complementary way, a totally customized Drone will be used to map these areas of interest, generating aerial photographs, identifying and geotagging the potential targets, which will allow the agents to identify potential mosquito breeding sites. After the identification of breeding areas, the next step will be the effective combat of the vectors, using the Drones to fly over the areas of interest, where biological defenses will be dropped over the targets to combat mosquitoes. Due some Drone flight restrictions over the cities, the whole process will be monitored by a situation room, that will be able to control the Drone remotely, access the air space controller, reads the sensors installed in the city (field), that will measure, for example, rainfall through weather stations installed in risk areas and subsequently processed by Intelligent System Integrated Management (SIGI), which will result to the information public official reflecting the situational analysis of the areas, which will enable a better management of available resources, helping the public agent, preventively in the decision making.",https://ieeexplore.ieee.org/document/9172439/,2020 IEEE Aerospace Conference,7-14 March 2020,ieeexplore
10.1109/UEMCON.2018.8796749,Smart City Software Revolution - Blackboard Systems for Smart City Solutions,IEEE,Conferences,"This paper examines the possibilities and requirements of using the blackboard software system in the context of a smart city. Using Unreal Engine 4, a virtual smart city was developed to test the capabilities and difficulties which would accompany the use of the blackboard. A number of simulations were run to generate insight regarding the viability of implementing the blackboard in a real-world setting. The tasks that the blackboard attempted addressed the following goals: a weather reaction system; a parking space availability system; and a daylight reaction system. An autonomous car running on preset routes was implemented in two separate ways to compare the performance between a blackboard system and blueprints.",https://ieeexplore.ieee.org/document/8796749/,"2018 9th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",8-10 Nov. 2018,ieeexplore
10.1109/BigComp51126.2021.00051,Smart Energy Management System based on Reconfigurable AI Chip and Electrical Vehicles,IEEE,Conferences,"Almost every larger city in Europe has ambitious smart city projects. This is particularly true for Hamburg, a Hanseatic city in the north of Germany. Hamburg is the smartest city in Germany according to a Federal Association for Information Technology. Although there are no megacities in the European Union (the largest city in the European Union is Berlin with 3.7 million inhabitants), the increasing urbanization is apparent and produces problems to be solved. At the same time rural depopulation creates conjugated problems.One category of these problems is mobility. Mobility can be regarded as the need to move persons and freight. In densely populated cities an increasing amount of transport users have to share a decreasing amount of space with conflicting needs. At the same time in rural areas, a dwindling supply of local public transport makes the mobility of the remaining residents more difficult. The same applies to parcel delivery or the supply of goods. Autonomous systems have great potential to create a sustainable and livable environment. The author has initiated a publicly funded project to investigate technologies of autonomous mobile systems which interact with a smart city. The test area intelligent urban mobility (Testfeld intelligente Quartiersmobilitat) at the campus of Hamburgs University of Applied Sciences is created to do research on connected and autonomous mobile systems like multipurpose robots and other mobility users like pedestrians with a smartphone. A particular focus is on neighborhood mobility. This means that distances of less than 3 kilometers usually have to be covered. The special type of needs in neighborhood mobility has two important aspects that affect development of autonomous mobile systems: It is slow mobility and the transport users are especially vulnerable. The acceptance of the residents of autonomous systems is equally important, as is the protection of privacy when collecting environmental data. They are expected to make decisions on their own in complex environments. The real world usually differs from a simulation or an experimental setup in a laboratory - a problem commonly referred to as Sim-2-Real gap. Active and non-destructive exploration is expected from an autonomous system to solve unexpected problems. Machine learning methods come into play which in turn have their own pitfalls. The author has built a specialized laboratory to investigate machine learning technology applied to autonomous systems. In this laboratory miniature autonomous vehicles are developed. The general idea of this experimental setup allows research on new methodologies for autonomous systems in a very small scale",https://ieeexplore.ieee.org/document/9373129/,2021 IEEE International Conference on Big Data and Smart Computing (BigComp),17-20 Jan. 2021,ieeexplore
10.1109/ECICE50847.2020.9302010,Smart Lock Security System Based on Artificial Internet of Things,IEEE,Conferences,"Artificial intelligence and Internet of Things (AIoT) technology were applied to develop an integrated system for remote space authentification and power management. The system is applied to various commercial uses such as the control of mobile or cloud authorization in buildings and the management of shared offices or hospitals. To develop the proposed system, AIoT smart lock with low power consumption was designed to control the access and manage the power consumption of other IoT devices effectively in the venue. AIoT communication firmware and devices were developed to enable smart control and management. Finally, an energy management system helps enterprises monitor the power consumption of devices in real-time and grant authorization for device monitoring and management. The proposed system sends monitoring data to a cloud server and enables the managing of commercial activities and unmanned venues. Through hardware and software integration, this study created the AIoT smart lock, promoted the use of IoT devices in relevant industries, and enhanced the competitiveness of product research and development.",https://ieeexplore.ieee.org/document/9302010/,"2020 IEEE Eurasia Conference on IOT, Communication and Engineering (ECICE)",23-25 Oct. 2020,ieeexplore
10.1109/IROS51168.2021.9636018,Smart Pointers and Shared Memory Synchronisation for Efficient Inter-process Communication in ROS on an Autonomous Vehicle,IEEE,Conferences,"Despite the stringent requirements of a real-time system, the reliance of the Robot Operating System (ROS) on the loopback network interface imposes a considerable overhead on the transport of high bandwidth data, while the nodelet package, which is an efficient mechanism for intra-process communication, does not address the problem of efficient local inter-process communication (IPC). To remedy this, we propose a novel integration into ROS of smart pointers and synchronisation primitives stored in shared memory. These obey the same semantics and, more importantly, exhibit the same performance as their C++ standard library counterparts, making them preferable to other local IPC mechanisms. We present a series of benchmarks for our mechanism - which we call LOT (Low Overhead Transport) - and use them to assess its performance on realistic data loads based on Fives Autonomous Vehicle (AV) system, and extend our analysis to the case where multiple ROS nodes are running in Docker containers. We find that our mechanism performs up to two orders of magnitude better than the standard IPC via local loopback. Finally, we apply industry-standard profiling techniques to explore the hotspots of code running in both user and kernel space, comparing our implementation against alternatives.",https://ieeexplore.ieee.org/document/9636018/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore
10.23919/FRUCT.2012.8253108,Smart space logistic service for real-time ridesharing,IEEE,Conferences,The paper describes a logistic service-based approach to real-time ridesharing based on smartspace concept. Smart-M3 information platform is used as smart space infrastructure for presented approach. The service is based on Smart-M3 RDF ontology which is formed by ontology slices of participants' mobile devices. The paper presents an algorithm for finding appropriate fellow- travelers for drivers as well as definition of acceptable pick-up and drop-off points for them.,https://ieeexplore.ieee.org/document/8253108/,2012 11th Conference of Open Innovations Association (FRUCT),23-27 April 2012,ieeexplore
10.1109/ICCD.1995.528945,Smart-pixel array processors based on optimal cellular neural networks for space sensor applications,IEEE,Conferences,"A smart-pixel cellular neural network with hardware annealing capability, digitally programmable synaptic weights, and multisensor parallel interface has been under development for advanced space sensor applications. The smart-pixel CNN architecture is a programmable multi-dimensional array of optoelectronic neurons which are locally connected with their local neurons and associated active-pixel sensors. Integration of the neuroprocessor in each processor node of a scalable multiprocessor system offers orders-of-magnitude computing performance enhancements for on-board real-time intelligent multisensor processing and control tasks of advanced small satellites. The smart-pixel CNN operation theory, architecture, design and implementation, and system applications are investigated in detail. The VLSI implementation feasibility was illustrated by a prototype smart-pixel 5/spl times/5-neuroprocessor array chip of active dimensions 1380 /spl mu/m/spl times/746 /spl mu/m in a 2-/spl mu/m CMOS technology.",https://ieeexplore.ieee.org/document/528945/,Proceedings of ICCD '95 International Conference on Computer Design. VLSI in Computers and Processors,2-4 Oct. 1995,ieeexplore
10.1109/IJCNN.2017.7965906,Social recommendation using Euclidean embedding,IEEE,Conferences,"Traditional recommender systems assume that all the users are independent, and they usually face the cold start and data sparse problems. To alleviate these problems, social recommender systems use social relations as an additional input to improve recommendation accuracy. Social recommendation follows the intuition that people with social relationships share some kinds of preference towards items. Current social recommendation methods commonly apply the Matrix Factorization (MF) model to incorporate social information into the recommendation process. As an alternative model to MF, we propose a novel social recommendation approach based on Euclidean Embedding (SREE) in this paper. The idea is to embed users and items in a unified Euclidean space, where users are close to both their desired items and social friends. Experimental results conducted on two real-world data sets illustrate that our proposed approach outperforms the state-of-the-art methods in terms of recommendation accuracy.",https://ieeexplore.ieee.org/document/7965906/,2017 International Joint Conference on Neural Networks (IJCNN),14-19 May 2017,ieeexplore
10.1109/SERE-C.2014.24,Software Reliability Virtual Testing for Reliability Assessment,IEEE,Conferences,"The basic condition of software reliability assessment is failure time, which must be acquired during a test based on operational profile or on real usage. Failure data from software development or other non-software reliability testing (SRT) cannot be used for reliability evaluation because such data do not include usage information and failure time. This paper presents a software reliability virtual test (SRVT), which constructs the software input space model and the known failure input space model through which possible failure time can be determined by matching the randomly generate inputs. An experiment comparing SRT and SRVT with different thresholds is introduced to verify SRVT. Results indicate that SRVT saves a large amount of testing time while providing reliability assessment with acceptable accuracy.",https://ieeexplore.ieee.org/document/6901643/,2014 IEEE Eighth International Conference on Software Security and Reliability-Companion,30 June-2 July 2014,ieeexplore
10.1109/WICT.2012.6409060,Software effort prediction using unsupervised learning (clustering) and functional link artificial neural networks,IEEE,Conferences,"Software cost estimation continues to be an area of concern for managing of software development industry. We use unsupervised learning (e.g., clustering algorithms) combined with functional link artificial neural networks for software effort prediction. The unsupervised learning (clustering) indigenously divide the input space into the required number of partitions thus eliminating the need of ad-hoc selection of number of clusters. Functional link artificial neural networks (FLANNs), on the other hand is a powerful computational model. Chebyshev polynomial has been used in the FLANN as a choice for functional expansion to exhaustively study the performance. Three real life datasets related to software cost estimation have been considered for empirical evaluation of this proposed method. The experimental results show that our method could significantly improve prediction accuracy of conventional FLANN and has the potential to become an effective method for software cost estimation.",https://ieeexplore.ieee.org/document/6409060/,2012 World Congress on Information and Communication Technologies,30 Oct.-2 Nov. 2012,ieeexplore
10.1109/GrC.2010.13,Solving Course Timetabling Problem Using Interrelated Approach,IEEE,Conferences,"University timetabling is very hectic resources allocation job against tough constraints. The problem is broadly recognized on account of its crucial significance for curriculum activities. Its intensive complexity has challenged the researchers from diverse disciplines for several decades. In the research paper, a novel interrelated approach is employed that primarily depends on Genetic Algorithm supported by Local Search algorithm. Local Search systematizes the events in each timetabling chromosome up to certain degree. Later on GA is likely to obtain more feasible solution available on the search space. The approach has been applied on real dataset and the research direction is validated by promising outcome. The bottom line is minimizing computational time for GA by initializing the set of partial solutions. In addition, exploitation of the resources usage and effective events deployment are key objectives.",https://ieeexplore.ieee.org/document/5576024/,2010 IEEE International Conference on Granular Computing,14-16 Aug. 2010,ieeexplore
10.1109/ICTAI.2017.00154,Some Neighbourhood Approaches for the Antenna Positioning Problem,IEEE,Conferences,"The problem of positioning antennas in cellular networks is a known problem in the field of telecommunications. It consists of selecting from a set of candidate sites, the best locations to install the base stations in order to maximize the network coverage while minimizing the number of the used stations. In theory, the problem is NP-hard. To solve it in practice, we propose in this work the adaptation of two metaheuristics based on the local search that are the Iterated Local Search (ILS) and the Breakout Local Search (BLS) and provide a new algorithm inspired from both the ILS and the BLS algorithms. The latter is based on a local search process and a perturbation in the exploration of the search space. It is distinguished by its mechanism of reinitialization of the search and by its way of generating a new starting solution. To validate our approach, we have implemented, tested and compared these algorithms to several other methods on a real instance of the problem. The experimental results obtained show that the proposed approach improves the performance of these methods in most of the cases.",https://ieeexplore.ieee.org/document/8372057/,2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI),6-8 Nov. 2017,ieeexplore
10.1109/SCT.1993.336521,Some structural complexity aspects of neural computation,IEEE,Conferences,"Recent work by H.T. Siegelmann and E.D. Sontag (1992) has demonstrated that polynomial time on linear saturated recurrent neural networks equals polynomial time on standard computational models: Turing machines if the weights of the net are rationals, and nonuniform circuits if the weights are real. Here, further connections between the languages recognized by such neural nets and other complexity classes are developed. Connections to space-bounded classes, simulation of parallel computational models such as Vector Machines, and a discussion of the characterizations of various nonuniform classes in terms of Kolmogorov complexity are presented.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/336521/,[1993] Proceedings of the Eigth Annual Structure in Complexity Theory Conference,18-21 May 1993,ieeexplore
10.1109/ICDE.2007.367865,Source-aware Entity Matching: A Compositional Approach,IEEE,Conferences,"Entity matching (a.k.a. record linkage) plays a crucial role in integrating multiple data sources, and numerous matching solutions have been developed. However, the solutions have largely exploited only information available in the mentions and employed a single matching technique. We show how to exploit information about data sources to significantly improve matching accuracy. In particular, we observe that different sources often vary substantially in their level of semantic ambiguity, thus requiring different matching techniques. In addition, it is often beneficial to group and match mentions in related sources first, before considering other sources. These observations lead to a large space of matching strategies, analogous to the space of query evaluation plans considered by a relational optimizer. We propose viewing entity matching as a composition of basic steps into a ""match execution plan"". We analyze formal properties of the plan space, and show how to find a good match plan. To do so, we employ ideas from social network analysis to infer the ambiguity and related-ness of data sources. We conducted extensive experiments on several real-world data sets on the Web and in the domain of personal information management (PIM). The results show that our solution significantly outperforms current best matching methods.",https://ieeexplore.ieee.org/document/4221668/,2007 IEEE 23rd International Conference on Data Engineering,15-20 April 2007,ieeexplore
10.1109/SSCI50451.2021.9660133,Space and Time Efficiency Analysis of Data-Driven Methods Applied to Embedded Systems,IEEE,Conferences,"One of the applications of data-driven methods in the industry is the creation of real-time, embedded measurements, whether to monitor or replace sensor signals. As the number of embedded systems in products raises over time, the energy efficiency of such systems must be considered in the design. The time (processor) efficiency of the embedded software is directly related to the energy efficiency of the embedded system. Therefore, when considering some embedded software solutions, such as data-driven methods, time efficiency must be taken into account to improve energy efficiency. In this work, the energy efficiency of three data-driven methods: the Sparse Identification of Nonlinear Dynamics (SINDy), the Extreme Learning Machine (ELM), and the Random-Vector Functional Link (RVFL) network were assessed by using the creation of a real-time in-cylinder pressure sensor for diesel engines as a task. The three methods were kept with equivalent performances, whereas their relative execution time was tested and classified by their statistical rankings. Additionally, the space (memory) efficiency of the methods was assessed. The contribution of this work is to provide a guide to choose the best data-driven method to be used in an embedded system in terms of efficiency.",https://ieeexplore.ieee.org/document/9660133/,2021 IEEE Symposium Series on Computational Intelligence (SSCI),5-7 Dec. 2021,ieeexplore
10.1109/ICACI.2018.8377475,Space-time constrained optimization for deep locomotion controller,IEEE,Conferences,"Physics-based methods synthesize motion for virtual characters following physics principles in real world. Given the high dimensions and continuity of joint actuation, designing a controller for virtual characters is a challenging task. Existing methods normally construct the controller based on Finite State Machine, which is a manual process and requires expert knowledge. This paper uses deep neural network, as the locomotion controller, to control the motion of virtual characters. The network learning is conducted with the Deep Deterministic Policy Gradient. We propose to integrate space-time constraints, as part of the reward function, during the learning process. The experiment results confirm that the introduction of the space-time constraints avoids the problem of generating awkward gait (as observed in existing methods).",https://ieeexplore.ieee.org/document/8377475/,2018 Tenth International Conference on Advanced Computational Intelligence (ICACI),29-31 March 2018,ieeexplore
10.1109/INFOCOMWKSHPS50562.2020.9162585,Sparser: Secure Nearest Neighbor Search with Space-filling Curves,IEEE,Conferences,"Nearest neighbor search, a classic way of identifying similar data, can be applied to various areas, including database, machine learning, natural language processing, software engineering, etc. Secure nearest neighbor search aims to find nearest neighbors to a given query point over encrypted data without accessing data in plaintext. It provides privacy protection to datasets when nearest neighbor queries need to be operated by an untrusted party (e.g., a public server). While different solutions have been proposed to support nearest neighbor queries on encrypted data, these existing solutions still encounter critical drawbacks either in efficiency or privacy. In light of the limitations in the current literature, we propose a novel approximate nearest neighbor search solution, referred to as Sparser, by leveraging a combination of space-filling curves, perturbation, and Order-Preserving Encryption. The advantages of Sparser are twofold, strengthening privacy and improving efficiency. Specifically, Sparser pre-processes plaintext data with space-filling curves and perturbation, such that data is sparse, which mitigates leakage abuse attacks and renders stronger privacy. In addition to privacy enhancement, Sparser can efficiently find approximate nearest neighbors over encrypted data with logarithmic time. Through extensive experiments over real-world datasets, we demonstrate that Sparser can achieve strong privacy protection under leakage abuse attacks and minimize search time.",https://ieeexplore.ieee.org/document/9162585/,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),6-9 July 2020,ieeexplore
10.1109/RAICS.2013.6745450,Sparsity-based representation for categorical data,IEEE,Conferences,"Over the past few decades, many algorithms have been continuously evolving in the area of machine learning. This is an era of big data which is generated by different applications related to various fields like medicine, the World Wide Web, E-learning networking etc. So, we are still in need for more efficient algorithms which are computationally cost effective and thereby producing faster results. Sparse representation of data is one giant leap toward the search for a solution for big data analysis. The focus of our paper is on algorithms for sparsity-based representation of categorical data. For this, we adopt a concept from the image and signal processing domain called dictionary learning. We have successfully implemented its sparse coding stage which gives the sparse representation of data using Orthogonal Matching Pursuit (OMP) algorithms (both Batch and Cholesky based) and its dictionary update stage using the Singular Value Decomposition (SVD). We have also used a preprocessing stage where we represent the categorical dataset using a vector space model based on the TF-IDF weighting scheme. Our paper demonstrates how input data can be decomposed and approximated as a linear combination of minimum number of elementary columns of a dictionary which so formed will be a compact representation of data. Classification or clustering algorithms can now be easily performed based on the generated sparse coded coefficient matrix or based on the dictionary. We also give a comparison of the dictionary learning algorithm when applying different OMP algorithms. The algorithms are analysed and results are demonstrated by synthetic tests and on real data.",https://ieeexplore.ieee.org/document/6745450/,2013 IEEE Recent Advances in Intelligent Computational Systems (RAICS),19-21 Dec. 2013,ieeexplore
10.1109/GRC.2005.1547267,Spatio-temporal granular logic and its applications to dynamic information systems,IEEE,Conferences,"The main research content of the paper is a granular logic with meaning of space and time. Temporal operators Until (U) and Since (S) are studied. U and S are introduced into the granular logic, to have a temporal granular logic. Subsequently, a spatio-temporal changing function /spl pi/: X/spl times/T/spl rarr/X is proposed, which are functions of two arguments mapping a point in space and a point in time, to another a point in space. Where X is the state space, T is a time-lines. Temporal granular logic is defined in the state space, thus the spatio-temporal granular logic is constructed. Further, the deductive systems of the logic are discussed. Finally, the applications of the logic are illustrated with real examples.",https://ieeexplore.ieee.org/document/1547267/,2005 IEEE International Conference on Granular Computing,25-27 July 2005,ieeexplore
10.1109/IROS.2014.6943162,Spatio-temporal motion features for laser-based moving objects detection and tracking,IEEE,Conferences,"This paper proposes a spatio-temporal motion feature detection and tracking method using range sensors working on a moving platform. The proposed spatio-temporal motion features are similar to optical flow but are extended on a moving platform with fusion of odometry and show much better classification accuracy with consideration of different uncertainties. In the proposal, the ego motion is compensated by odometry sensors and the laser scan points are accumulated and represented as space-time point clouds, from which the velocities and moving directions can be extracted. Based on these spatio-temporal features, a supervised learning technique is applied to classify the points as static or moving and Kalman filters are implemented to track the moving objects. A real experiment is performed during day and night on an autonomous vehicle platform and shows promising results in a crowded and dynamic environment.",https://ieeexplore.ieee.org/document/6943162/,2014 IEEE/RSJ International Conference on Intelligent Robots and Systems,14-18 Sept. 2014,ieeexplore
10.1109/ICNN.1994.374437,Spatiotemporal computation with a general purpose analog neural computer: real-time visual motion estimation,IEEE,Conferences,"An analog neural network implementation of spatiotemporal feature extraction for real-time visual motion estimation is presented. Visual motion can be represented as an orientation in the space-time domain. Thus, motion estimation translates into orientation detection. The spatiotemporal orientation detector discussed is based on Adelson and Bergen's model with modifications to accommodate the computational limitations of hardware analog neural networks. The analog neural computer used here has the unique property of offering temporal computational capabilities through synaptic time-constants. These time-constants are crucial for implementing the spatiotemporal filters. Analysis, implementation and performance of the motion filters are discussed. The performance of the neural motion filters is found to be consistent with theoretical predictions and the real stimulus motion.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/374437/,Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94),28 June-2 July 1994,ieeexplore
10.1109/VLSID.2018.26,Special Session: Design of Energy-Efficient and Reliable VLSI Systems: A Data-Driven Perspective,IEEE,Conferences,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. The amount of data generated and collected across computing platforms every day is not only enormous, but growing at an exponential rate. Advanced data analytics and machine learning techniques have become increasingly essential to analyze and extract meaning from such Big Data. These techniques can be very useful to detect patterns and trends to improve the operational behavior of computing systems, but they also introduce a number of outstanding challenges: (1) How can we design and deploy data analytics mechanisms to improve energy-efficiency and reliability in IoT and mobile devices, without introducing significant software overheads? (2) How to leverage emerging technologies (e.g.,3D integration) to design energy-efficient and reliable manycore systems for big data computing? (3) How to use machine learning and data mining techniques for effective design space exploration of computing systems, and enable adaptive control to improve energy-efficiency? (4) How can data analytics detect anomalies and increase robustness in the network backbone of emerging large-scale networking systems? To address these outstanding challenges, out-of-the-box approaches need to be explored. In this special session, we will discuss these outstanding problems and describe far-reaching solutions applicable across the interconnected ecosystem of IoT and mobile devices, manycore chips, datacenters, and networks. The special session brings together speakers with unique insights on applying data analytics and machine learning to real-world problems to achieve the most sought after features on multi-scale computing platforms, viz. intelligent data mining, energyefficiency, and robustness. By integrating data analytics and machine learning algorithms, statistical modeling, embedded hardware and software design, and cloud computing content, this session will engage a broad section of Embedded and VLSI Design conference attendees. This special session is targeted towards university researchers/professors, students, industry professionals, and embedded/VLSI system designers. This session will attract newcomers who want to learn how to apply data analytics to solve problems in computing systems, as well as experienced researchers looking for exciting new directions in embedded systems, VLSI design, EDA algorithms, and multi-scale computing.",https://ieeexplore.ieee.org/document/8326889/,2018 31st International Conference on VLSI Design and 2018 17th International Conference on Embedded Systems (VLSID),6-10 Jan. 2018,ieeexplore
10.1109/ISDA.2008.125,Spider Search: An Efficient and Non-Frontier-Based Real-Time Search Algorithm,IEEE,Conferences,"Real-time search algorithms are limited to constant-bounded search at each time step. We do not see much difference between standard search algorithms and good real-time search algorithms when problem sizes are small. However, having a good real-time search algorithm becomes important when problem sizes are large. In this paper we introduce a simple yet efficient algorithm, Spider search, which uses very low constant time and space to solve problems when agents need deep (but not exhaustive) path analysis at each step. We expect that Spider search is the first in a new class of tree-based rather than frontier-based search algorithms.",https://ieeexplore.ieee.org/document/4696381/,2008 Eighth International Conference on Intelligent Systems Design and Applications,26-28 Nov. 2008,ieeexplore
10.1109/HPCS48598.2019.9188104,Staged deployment of interactive multi-application HPC workflows,IEEE,Conferences,"Running scientific workflows on a supercomputer can be a daunting task for a scientific domain specialist. Workflow management solutions (WMS) are a standard method for reducing the complexity of application deployment on high performance computing (HPC) infrastructure. We introduce the design for a middleware system that extends and combines the functionality from existing solutions in order to create a high-level, staged usercentric operation/deployment model. This design addresses the requirements of several use cases in the life sciences, with a focus on neuroscience. In this manuscript we focus on two use cases: 1) three coupled neuronal simulators (for three different space/time scales) with in-transit visualization and 2) a closed-loop workflow optimized by machine learning, coupling a robot with a neural network simulation. We provide a detailed overview of the application-integrated monitoring in relationship with the HPC job. We present here a novel usage model for large scale interactive multi-application workflows running on HPC systems which aims at reducing the complexity of deployment and execution, thus enabling new science.",https://ieeexplore.ieee.org/document/9188104/,2019 International Conference on High Performance Computing & Simulation (HPCS),15-19 July 2019,ieeexplore
10.23919/EUSIPCO.2019.8902815,"State Space Models with Dynamical and Sparse Variances, and Inference by EM Message Passing",IEEE,Conferences,"Sparse Bayesian learning (SBL) is a probabilistic approach to estimation problems based on representing sparsity-promoting priors by Normals with Unknown Variances. This representation blends well with linear Gaussian state space models (SSMs). However, in classical SBL the unknown variances are a priori independent, which is not suited for modeling group sparse signals, or signals whose variances have structure. To model signals with, e.g., exponentially decaying or piecewise-constant (in particular block-sparse) variances, we propose SSMs with dynamical and sparse variances (SSM-DSV). These are two-layer SSMs, where the bottom layer models physical signals, and the top layer models dynamical variances that are subject to abrupt changes. Inference and learning in these hierarchical models is performed with a message passing version of the expectation maximization (EM) algorithm, which is a special instance of the more general class of variational message passing algorithms. We validated the proposed model and estimation algorithm with two applications, using both simulated and real data. First, we implemented a block-outlier insensitive Kalman smoother by modeling the disturbance process with a SSM-DSV. Second, we used SSM-DSV to model the oculomotor system and employed EM-message passing for estimating neural controller signals from eye position data.",https://ieeexplore.ieee.org/document/8902815/,2019 27th European Signal Processing Conference (EUSIPCO),2-6 Sept. 2019,ieeexplore
10.1109/ICMLA.2019.00022,State Summarization of Video Streams for Spatiotemporal Query Matching in Complex Event Processing,IEEE,Conferences,"Modelling complex events in unstructured data like videos not only requires detecting objects but also the spatiotemporal relationships among objects. Complex Event Processing (CEP) systems discretize continuous streams into fixed batches using windows and apply operators over these batches to detect patterns in real-time. To this end, we apply CEP techniques over video streams to identify spatiotemporal patterns by capturing window state. This work introduces a novel problem where an input video stream is converted to a stream of graphs which are aggregated to a single graph over a given state. Incoming video frames are converted to a timestamped Video Event Knowledge Graph (VEKG) [1] that maps objects to nodes and captures spatiotemporal relationships among object nodes. Objects coexist across multiple frames which leads to the creation of redundant nodes and edges at different time instances that results in high memory usage. There is a need for expressive and storage efficient graph model which can summarize graph streams in a single view. We propose Event Aggregated Graph (EAG), a summarized graph representation of VEKG streams over a given state. EAG captures different spatiotemporal relationships among objects using an Event Adjacency Matrix without replicating the nodes and edges across time instances. These enable the CEP system to process multiple continuous queries and perform frequent spatiotemporal pattern matching computations over a single summarised graph. Initial experiments show EAG takes 68.35% and 28.9% less space compared to baseline and state of the art graph summarization method respectively. EAG takes 5X less search time to detect pattern as compare to VEKG stream.",https://ieeexplore.ieee.org/document/8999043/,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),16-19 Dec. 2019,ieeexplore
10.1109/ICDM.2003.1250930,Statistical relational learning for document mining,IEEE,Conferences,"A major obstacle to fully integrated deployment of many data mining algorithms is the assumption that data sits in a single table, even though most real-world databases have complex relational structures. We propose an integrated approach to statistical modelling from relational databases. We structure the search space based on ""refinement graphs"", which are widely used in inductive logic programming for learning logic descriptions. The use of statistics allows us to extend the search space to include richer set of features, including many which are not Boolean. Search and model selection are integrated into a single process, allowing information criteria native to the statistical model, for example logistic regression, to make feature selection decisions in a step-wise manner. We present experimental results for the task of predicting where scientific papers will be published based on relational data taken from CiteSeer. Our approach results in classification accuracies superior to those achieved when using classical ""flat"" features. The resulting classifier can be used to recommend where to publish articles.",https://ieeexplore.ieee.org/document/1250930/,Third IEEE International Conference on Data Mining,22-22 Nov. 2003,ieeexplore
10.1109/ITSC.2015.83,Steady-State Signal Control for Urban Traffic Networks,IEEE,Conferences,"A typical reason of local traffic congestion is that the distribution of traffic flows in the network is unbalanced, i.e., traffic congestion occurs in some links, but there is still enough space not being sufficiently utilized in other links. Therefore, the consensus notion, extensively investigated in multi-agent networks, is introduced in traffic control, and the steady-state signal control approach is presented, the idea behind which is that traffic states in network links asymptotically converge to given steady-state values, implying balanced distribution of traffic flows is realized. A signal control model of the network is first proposed, which is a linear time-varying system with discharging proportions of internal network links as control variables. Furthermore, analytic relation between steady-state values and inputs of the network is established based on matrix theory, based on which the steady-state signal control law is provided, balancing time-varying traffic flows in the network. At last, simulation investigations are conducted in VISSIM software, compared with fixed-time signal control approach in a real-world topological network in Beijing of China, which shows that the proposed signal control approach can improve performance indices of the network and then reduce local traffic congestion.",https://ieeexplore.ieee.org/document/7313175/,2015 IEEE 18th International Conference on Intelligent Transportation Systems,15-18 Sept. 2015,ieeexplore
10.1109/ROBOT.1996.506888,Stereo sketch: stereo vision-based target reaching behavior acquisition with occlusion detection and avoidance,IEEE,Conferences,"In this paper, we proposed a method by which a stereo vision-based mobile robot learns to reach a target by detecting and avoiding occlusions. We call the internal representation that describes the learning behavior ""stereo sketch"". First, an input scene is segmented into homogeneous regions by the enhanced ISODATA algorithm with minimum description length principle in terms of image coordinates and disparity information obtained from the fast stereo matching unit based on the coarse-to-fine control method. Then, in terms of the segmented regions including the target area and their occlusion status identified during the stereo and motion disparity estimation process, we construct a state space for the reinforcement learning method to obtain a target reaching behavior. As a result the robot can avoid obstacles without explicitly describing them. We give the computer simulation results and real robot implementation to show the validity of our method.",https://ieeexplore.ieee.org/document/506888/,Proceedings of IEEE International Conference on Robotics and Automation,22-28 April 1996,ieeexplore
10.1109/ICTAI.2016.0014,Strategy Inference in Multi-Agent Multi-Team Scenarios,IEEE,Conferences,"Creating simulations for multi-agent multi-team interactions is a daunting task. It is non-trivial to compose a situation where each individual agent maintains their own 'personality' while still following the assigned policy dictated by a team's central command. Further, the complexity is inflated by ensuring that each of these agent policies is coordinated into a cohesive team strategy. Finally, peaking the complexity, is evaluating the performance of the team's strategy against other teams' strategies in real-time. This is the work of this paper, proposing SIMAMT, the simulation space for multi-agent multi-team engagements, and testing it. We will first cover the system and how well it models the virtual environment for strategic interaction. Second, we will deliver results from a practical test of strategy inference within such an environment using the SIE (Strategy Inference Engine).",https://ieeexplore.ieee.org/document/7814574/,2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI),6-8 Nov. 2016,ieeexplore
10.1109/ICDM51629.2021.00171,Streaming Dynamic Graph Neural Networks for Continuous-Time Temporal Graph Modeling,IEEE,Conferences,"Dynamic graphs are suitable for modeling structured data that evolve over time and have been widely used in many application scenarios such as social networks, financial transaction networks, and recommendation systems. Recently, many dynamic graph methods are proposed to deal with temporal networks. However, due to the limitations of storage space and computational efficiency, most approaches evolve node representations by aggregating the latest state information of neighbor nodes, thus losing a lot of information about neighbor nodes state changes. Besides, high computational complexity makes it challenging to deploy dynamic graph algorithms in real-time. To tackle these challenges, we propose a novel streaming dynamic graph neural network (SDGNN) for modeling continuous-time temporal graphs, which can fully capture the state changes of neighbors and reduce the computational complexity of inference. Under SDGNN, an incremental update component is designed to incrementally update node representation based on the interaction sequence, an inference component is utilized for specific downstream tasks, and a message propagation component is employed to propagate interactive information to the influenced nodes by considering the update time interval, position distance, and influence strengths. Extensive experiments demonstrated that the proposed approach significantly outperforms state-of-the-art methods by capturing more state change information and efficient parallelization.",https://ieeexplore.ieee.org/document/9679075/,2021 IEEE International Conference on Data Mining (ICDM),7-10 Dec. 2021,ieeexplore
10.1109/ICSE-NIER.2019.00031,Structural Coverage Criteria for Neural Networks Could Be Misleading,IEEE,Conferences,"There is a dramatically increasing interest in the quality assurance for DNN-based systems in the software engineering community. An emerging hot topic in this direction is structural coverage criteria for testing neural networks, which are inspired by coverage metrics used in conventional software testing. In this short paper, we argue that these criteria could be misleading because of the fundamental differences between neural networks and human written programs. Our preliminary exploration shows that (1) adversarial examples are pervasively distributed in the finely divided space defined by such coverage criteria, while available natural samples are very sparse, and as a consequence, (2) previously reported fault-detection ""capabilities"" conjectured from high coverage testing are more likely due to the adversary-oriented search but not the real ""high"" coverage.",https://ieeexplore.ieee.org/document/8805667/,2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER),25-31 May 2019,ieeexplore
10.1109/ICASSP.2003.1201643,Structural risk minimization using nearest neighbor rule,IEEE,Conferences,"We present a novel nearest neighbor rule-based implementation of the structural risk minimization principle to address a generic classification problem. We propose a fast reference set thinning algorithm on the training data set similar to a support vector machine approach. We then show that the nearest neighbor rule based on the reduced set implements the structural risk minimization principle, in a manner which does not involve selection of a convenient feature space. Simulation results on real data indicate that this method significantly reduces the computational cost of the conventional support vector machines, and achieves a nearly comparable test error performance.",https://ieeexplore.ieee.org/document/1201643/,"2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03).",6-10 April 2003,ieeexplore
10.1109/ICME.2003.1221046,Structural risk minimization using nearest neighbor rule,IEEE,Conferences,"We present a novel nearest neighbor rule-based implementation of the structural risk minimization principle to address a generic classification problem. We propose a fast reference set thinning algorithm on the training data set similar to a support vector machine approach. We then show that the nearest neighbor rule based on the reduced set implements the structural risk minimization principle, in a manner, which does not involve selection of a convenient feature space. Simulation results on real data indicate that this method significantly reduces the computational cost of the conventional support vector machines, and achieves a nearly comparable test error performance.",https://ieeexplore.ieee.org/document/1221046/,2003 International Conference on Multimedia and Expo. ICME '03. Proceedings (Cat. No.03TH8698),6-9 July 2003,ieeexplore
10.1109/SNPD51163.2021.9705000,Structure-Preserving Deep Autoencoder-based Dimensionality Reduction for Data Visualization,IEEE,Conferences,"Here, we propose a structure-preserving deep autoencoder-based dimensionality reduction scheme for data visualization. For this, we introduce two regularizers for regularizing autoencoders. The proposed regularizers help the encoded feature space preserve the local and global structures present in the original feature space. A chosen reduced dimensionality of two or three for the encoded feature space enables us to visualize the extracted latent representations of the data using scatterplots. The proposed method has two variants, depending on which regularizer it uses. The proposed approach, moreover, is unsupervised and has predictability. We use three synthetic datasets and one real-world dataset to illustrate the effectiveness of the proposed method. We also visually compare it with three state-of-the-art data visualization schemes and discuss several future research directions.",https://ieeexplore.ieee.org/document/9705000/,"2021 IEEE/ACIS 22nd International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",24-26 Nov. 2021,ieeexplore
10.1109/CONIELECOMP.2017.7891831,Study of direct local search operators influence in memetic differential evolution for constrained numerical optimization problems,IEEE,Conferences,"This paper analyzes the influence of the depth of direct local search methods in constrained numerical optimization problems in order to use as a local search operator (LSO) within a memetic algorithm. To perform this study, five direct local search methods (Random Walk, Simulated Annealing, Nelder-Mead, Hooke-Jeeves, and Hill Climber) are implemented separately to analyze their behavior within constrained search spaces by using a proposed measure named proximity rate, which measures the closeness of the solutions found by the LSO and the known optimal solution. Finally, all methods are used as LSO, separately, in a memetic algorithm based on Differential Evolution (MDE) structure, where the best solution in the population is used to exploit promising areas in the search space by the aforementioned LSOs. The comparative analysis has been performed on twenty-four benchmark problems used in the special session on Single Objective Constrained Real-Parameter Optimization in CEC'2006. Numerical results show that there is not a negative influence of LSO's depth within MDE approach; since regardless of the number of fitness evaluations allowed during the LSO search process, the MDE approach obtains competitive results.",https://ieeexplore.ieee.org/document/7891831/,"2017 International Conference on Electronics, Communications and Computers (CONIELECOMP)",22-24 Feb. 2017,ieeexplore
10.1109/EIConRus49466.2020.9039234,Study of the Algorithm for Assessing a Persons Emotional State by a Face Image Using Actions Units,IEEE,Conferences,"The paper is devoted to the study of algorithm for basic emotions (fear, happiness, anger, sadness, disgust, surprise) recognition from a single face image in the feature space of facial action units. For classification k-nearest neighbors (k-NN) algorithm was applied as well as our own knowledge-based algorithm, which uses emotions description, presented in facial action coding system. Testing was performed on three publicly available databases and on our own database. The results for k-NN algorithm demonstrate comparatively high recognition level (98.2 % and 97.1 % for 6 and 7 classes respectively on Cohn-Kanade database, 10-fold cross-validation), but also its sensitivity to the homogeneity of training and test sets, which was shown during cross-database testing. In contrast, the proposed algorithm is not based on machine learning, demonstrates slightly weaker but stable results, does not require much space for storing data, and is suitable for real-time implementation.",https://ieeexplore.ieee.org/document/9039234/,2020 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus),27-30 Jan. 2020,ieeexplore
10.1109/CSSE.2008.246,Study on Adaptive Intrusion Detection Engine Based on Gene Expression Programming Rules,IEEE,Conferences,"High false alarm rate and time-space cost of rule extraction and detection limit the application of machine learning in real intrusion detection system (IDS), and IDS cannot satisfy most system performance requirements simultaneously. In this paper, a Constraint-based gene expression programming rule extraction algorithm (CGREA) is proposed which guarantees the validity of rules and reduces the evolution time through grammar constraint and probability restriction. Additionally, an adaptive intrusion detection engine (AIDE) is applied to automatically renew the detected order of rules according to the performance metric. The KDD CUPpsila99 DATA is used for evaluation and results show that the rules, which are extracted by the CGREA algorithm within a few evolution generations, can not only achieve high detection rate but also detect unknown attacks. Moreover, the AIDE based on CGREA increases the attack detection rate, and adapts itself to different performance requirements with different sequences of rule detection.",https://ieeexplore.ieee.org/document/4722502/,2008 International Conference on Computer Science and Software Engineering,12-14 Dec. 2008,ieeexplore
10.1109/CSIE.2009.234,Study on Multi-agent Simulation System Based on Reinforcement Learning Algorithm,IEEE,Conferences,"Multi-agent simulation system based on reinforcement learning algorithm is a micro-individual acts of modeling and simulation methods, which have wide applicability, distribution, intelligent and interactive features etc. Firstly, studying on reinforcement learning algorithm, and then analysis and design the multi-agent simulation system structure, multi-agent system main modules, the implementation of the definition and finally, carefully design the multi-agent simulation system software, and multi-agent simulation collective system simulation and surrounded the location gathered from the space simulation experiment, the results showed that: Construct a multi-agent simulation system based on reinforcement learning algorithm, achieve real-time simulation of multi-agene, and multi-agent to get effect quickly, and to quickly construct surrounded conduct by mobile groups, the conduct of the system to achieve the global optimum effect.",https://ieeexplore.ieee.org/document/5170590/,2009 WRI World Congress on Computer Science and Information Engineering,31 March-2 April 2009,ieeexplore
10.1109/ICMLC.2002.1167430,Study on discretization based on rough set theory,IEEE,Conferences,"Discretization of attributes with real values is an important problem in data mining based on rough set. And discretization based on rough set has some particular characteristics. The method of discretization based on rough set and Boolean reasoning is discussed. Determination of. candidate cuts is discussed in detail. A theorem is proposed. to show that all bound cuts can discern the same objects pairs as the whole initial cuts. A strategy to select candidate cuts is proposed based on the theorem. Under the strategy, the space complexity and time complexity of improved algorithm decline obviously. The experiments results also confirm that.",https://ieeexplore.ieee.org/document/1167430/,Proceedings. International Conference on Machine Learning and Cybernetics,4-5 Nov. 2002,ieeexplore
10.1109/INFCOM.2012.6195649,SubFlow: Towards practical flow-level traffic classification,IEEE,Conferences,"Many research efforts propose the use of flow-level features (e.g., packet sizes and inter-arrival times) and machine learning algorithms to solve the traffic classification problem. However, these statistical methods have not made the anticipated impact in the real world. We attribute this to two main reasons: (a) training the classifiers and bootstrapping the system is cumbersome, (b) the resulting classifiers have limited ability to adapt gracefully as the traffic behavior changes. In this paper, we propose an approach that is easy to bootstrap and deploy, as well as robust to changes in the traffic, such as the emergence of new applications. The key novelty of our classifier is that it learns to identify the traffic of each application in isolation, instead of trying to distinguish one application from another. This is a very challenging task that hides many caveats and subtleties. To make this possible, we adapt and use subspace clustering, a powerful technique that has not been used before in this context. Subspace clustering allows the profiling of applications to be more precise by automatically eliminating irrelevant features. We show that our approach exhibits very high accuracy in classifying each application on five traces from different ISPs captured between 2005 and 2011. This new way of looking at application classification could generate powerful and practical solutions in the space of traffic monitoring and network management.",https://ieeexplore.ieee.org/document/6195649/,2012 Proceedings IEEE INFOCOM,25-30 March 2012,ieeexplore
10.1109/ICCICA52458.2021.9697222,Supervised Classification for Analysis and Detection of Potentially Hazardous Asteroid,IEEE,Conferences,"The use of Artificial Intelligence (AI) in solving real- time problems are increasing day by day with the increase in the availability of data and computation power. It is now substantial to use AI-based tools and techniques in space science. Asteroids, rocky objects that orbit around the sun, often produce an array of effects that cause harm to humans and biodiversity on earth. Such effects can cause wind blast, overpressure shock, thermal radiation, cratering, seismic shaking, ejecta deposition, tsunami, and many more. With the availability of data on asteroid parameters and nature, it provides an opportunity to use Machine Learning (ML) to address this problem and reduce the risk. This paper presents a thorough study on the impact of Potentially Hazardous Asteroids (PHAs) and proposes a supervised machine learning method to detect whether an asteroid with specific parameters is hazardous or not. We compare manifold classification algorithms that were implemented on the data. Random forest gave the best performance in terms of accuracy (99.99%) and average F1- score (99.22%).",https://ieeexplore.ieee.org/document/9697222/,2021 International Conference on Computational Intelligence and Computing Applications (ICCICA),26-27 Nov. 2021,ieeexplore
10.1109/ICASSP40776.2020.9053766,Supervised Deep Hashing for Efficient Audio Event Retrieval,IEEE,Conferences,"Efficient retrieval of audio events can facilitate real-time implementation of numerous query and search-based systems. This work investigates the potency of different hashing techniques for efficient audio event retrieval. Multiple state-of-the-art weak audio embeddings are employed for this purpose. The performance of four classical unsupervised hashing algorithms is explored as part of off-the-shelf analysis. Then, we propose a partially supervised deep hashing framework that transforms the weak embeddings into a low-dimensional space while optimizing for efficient hash codes. The model uses only a fraction of the available labels and is shown here to significantly improve the retrieval accuracy on two widely employed audio event datasets. The extensive analysis and comparison between supervised and unsupervised hashing methods presented here, give insights on the quantizability of audio embeddings. This work provides a first look in efficient audio event retrieval systems and hopes to set baselines for future research.",https://ieeexplore.ieee.org/document/9053766/,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",4-8 May 2020,ieeexplore
10.1109/ICSMC.2006.384813,Supervised Learning of Motion Style for Real-time Synthesis of 3D Character Animations,IEEE,Conferences,"In this paper, we present a supervised learning framework to learn a probabilistic mapping from values of a low-dimensional <i>style variable</i>, which defines the characteristics of a certain kind of 3D human motion such as walking or boxing, to high-dimensional vectors defining 3D poses. All possible values of the style variable span an Euclidean space called style space. The supervised learning framework guarantees that each dimension of style space corresponds to a certain aspect of the motion characteristics, such as body height and pace length, so the user can precisely define a 3D pose by locating a point in the style space. Moreover, every curve in the Euclidean style space corresponds to a smooth motion sequence. We developed a graphical user interface program, with which, users simply points mouse cursor in the style space to define a 3D pose and drags mouse cursor to synthesis 3D animations in real-time.",https://ieeexplore.ieee.org/document/4274578/,"2006 IEEE International Conference on Systems, Man and Cybernetics",8-11 Oct. 2006,ieeexplore
10.1109/ROMAN.1994.365922,Surface display: a force feedback system simulating the surface of an object,IEEE,Conferences,"Force feedback is an interface based on the phenomenon of contact. In the implementation of a virtual force feedback environment, the object is defined in a computer and the user is in the real world. Therefore, an interface device is required to transmit touch sensation to the user's finger when he or she touches a virtual object in virtual space. In this paper, methodology to realize force feedback is categorized from this view point, and the idea of ""surface display"" is presented as a force feedback method in a virtual environment.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/365922/,Proceedings of 1994 3rd IEEE International Workshop on Robot and Human Communication,18-20 July 1994,ieeexplore
10.1109/TENCON.2010.5685846,Symbol-based soft relaying strategy for cooperative wireless networks,IEEE,Conferences,"We propose an improved Soft Forwarding (SF) protocol which is based on the symbol-by-symbol detection at the relay node, unlike in the conventional relaying strategy which implements a bit-by-bit signal analysis at the relay node. A relay node which is typically acts like a repeater can avoid unnecessary computation complexity and above all, is resource efficient if compared to the baseline SF. The proposed strategy is implemented based on Maximum Likelihood Detector (MLD) criterion. We simplify the MLD rule to one-dimensional real space such that all metric values are compared in a straight line making the ML performance analysis tractable. Simulations have shown that the proposed schemes outperform the conventional schemes in error rate performance.",https://ieeexplore.ieee.org/document/5685846/,TENCON 2010 - 2010 IEEE Region 10 Conference,21-24 Nov. 2010,ieeexplore
10.1109/ICDE.1987.7272393,Symbolic processing: Issues and opportunties,IEEE,Conferences,"Symbolic processing is distinguished from other types of computation, e.g. numeric, commercial, or real-time, by the fact that its metadata is both complex and complicated. That is, the data objects manipulated by symbolic programs have descriptions which are not simple and not always predictable. The metadata is complex in the sense that the data objects are multi-faceted, having a variety of attributes and relationships to other objects. Processing this multidimensional data is done in slices, involving just a few types of attributes at a time, so different programs must deal with different projections (or views) of the data. The metadata is complicated in that it is recursive, meaning that the interpretation of a data object requires processing other instances of the same object class, to an unpredictable extent. Thus complexity and complication in metadata translate directly into data management and resource (space and time) management problems in processing.",https://ieeexplore.ieee.org/document/7272393/,1987 IEEE Third International Conference on Data Engineering,3-5 Feb. 1987,ieeexplore
10.1109/SCC.2017.68,SynAdapt: Automated Synthesis of Adaptive Agents,IEEE,Conferences,"Distributed autonomous multi-agent reasoning and classification systems have been thought of to be the basis of intelligence and have wide applications in the space of operational intelligence in closing the loop between sensing, analytics, and actions. This paper targets multi-agent systems that employ rulebased logics (i.e., rules that determine the output/response of an agent depending on the range of the input values) with pre-defined rules to accurately perceive the environment, and provide associated reactions. Such rule-based systems do not perform well in scenarios, where human generated rules cannot adapt to dynamic variations in the data distribution arising due to dynamic changes in the environment, especially if data dimensionality is very high. Examples of such scenarios exist wherever the sensed data arrives from the physical world - such as weather data, physical sensor data, human behaviour controlled data, etc. Clearly, to meet the adaptivity requirements of such scenarios we require the agents to possess adaptive reasoning capability such that they can adapt the underlying rules with respect to the changing environment. Developing such adaptive agents requires the developer to additionally possess considerable expertise of state-of-the-art machine learning techniques, apart from possessing knowledge of the agent's target domain. To address the above issues, we automate the process of development and deployment of adaptive agents. We present the fundamental design concepts behind the development of SynAdapt: a new adaptive meta-learning based multi-agent synthesis framework, that automates the synthesis of adaptive multi-agent systems from high-level user specifications. SynAdapt provides the following key features: a) Automated synthesis and deployment of adaptive agents from high-level user specification, b) Agents synthesised by SynAdapt can select a learning strategy that is particularly suited for given user specifications and input dataset, and c) Agents synthesised by SynAdapt can leverage adaptive ensemble learning techniques to deal with concept drift.",https://ieeexplore.ieee.org/document/8035021/,2017 IEEE International Conference on Services Computing (SCC),25-30 June 2017,ieeexplore
10.1109/GLOBECOM46510.2021.9685850,TTDeep: Time-Triggered Scheduling for Real-Time Ethernet via Deep Reinforcement Learning,IEEE,Conferences,"Schedule scheme is essential for real-time Ethernet. Due to the inevitable change of network configurations, the solution requires to be incrementally scheduled in a timely manner. Solver-based methods are time-consuming, while handcrafted scheduling heuristics require domain knowledge and professional expertise, and their application scenarios are usually limited. Instead of designing heuristic strategy manually, we propose TTDeep, a deep reinforcement learning schedule framework, to incrementally schedule Time-Triggered (TT) flows and adapt to various topologies. Our novel framework includes 3 key designs: a period layer to capture the periodical transmission nature of TT flows, the graph neural network to extract and represent topology features, and a 3-step selection paradigm to alleviate the huge action search space issue. Comprehensive experiments show that TTDeep can schedule TT flows much faster than solver-based methods and schedule nearly twice more TT flows on average compared to handcrafted heuristics.",https://ieeexplore.ieee.org/document/9685850/,2021 IEEE Global Communications Conference (GLOBECOM),7-11 Dec. 2021,ieeexplore
10.1109/BGC.Geomatics.2017.6,Table of contents,IEEE,Conferences,"The following topics are dealt with: 4D reconstruction and visualisation of Krakow Fortress; a comparison of housing prices calculated through floor premium and housing prices calculated through valuation reports; a novel approach of laser scanning point cloud quality assessment using wavelet analysis; a novel method of chromatic aberration detection and correction using wavelet analysis; accuracy study of close range 3D object reconstruction based on point clouds; accuracy tests of vertical component of RTK GNSS measurements corrected by a laser system; airborne laser scanning point cloud update by used of the terrestrial laser scanning and the low-level aerial photogrammetry; an analysis of the influence of planning conditions on property value; an analysis of the performance and coordinates time series of CORS Network LitPOS; analysis of factors influencing private land market in Lithuania; analysis of geodetical measurements' accuracy for forest block network in Tytuvnai state forest enterprise; analysis of reconnaissance imagery acquired in different spectral ranges of the electromagnetic spectrum; analysis of the impact of Galileo observations on the tropospheric delays estimation; analysis of the use of GNSS systems in road construction; analysis of variability of the motorway impact on agricultural land as an example a4 motorway section; application of BN in risk diagnostics arising from the degree of urban regeneration area degradation; application of robust estimation methods to displacements determination in geodetic control network of dam; application of the fuzzy set theory to determine the partial factor of safety; automation of dot maps production supported by BDOT10k database; cartographic visualization in the real estate market investigation with the use of GIS tools; combined method of surface flow measurement using terrestrial laser scanning and synchronous photogrammetry; comparative analysis of 3D models made with various technologies on the example of altar in the wang temple in Karpacz; comparing the effectiveness of ANNs and SVMs in forecasting the impact of traffic-induced vibrations on building; comparison of quality of metric photos relative orientation in Micmac and PhotoScan; complex monitoring of the coastal cliffs on the example of cliff in Jastrzebia Gora, Poland; design of geodetic network using controlled computer simulation; design of slender reinforced concrete columns of annular cross-section according to the nominal curvature method and the exact method; detecting cantilever beam vibration with accelerometers and GNSS; determination of elastic constants in Brazilian tests using digital image correlation; determination of land degradation for sustainable development of municipal territories diagnostic of geometry distortion of a wooden structure based on a point cloud; digital photogrammetry in the analysis of the ventricles' shape and size; digital zenith camera's results and its use in DFHRS v.4.3 Software for quasi-geoid determination; emergency management for infrastructure and transportation systems in the US; establishment of the right of electricity transmission easement through court proceedings; evaluation of inventory surveying of Faade scaffolding conducted during ORKWIZ project; fuzzy sets in the GIS environment in the location of objects on the surface of water bodies; geometry design and structural analysis of steel single-layer geodesic domes; GIS tools for assessing wine tourism potential in Slovak Republic; hydrographic survey data model for the purpose of inter-system exchange; identifying real transport networks in rural areas on the basis of cadastral data; impact of Galileo observations on the position and ambiguities estimation of GNSS reference stations; integration of thermal digital 3D model and a MASW (multichannel analysis of surface wave) as a means of improving monitoring of spoil tip stability; leptokurtosis of error distribution and its influence on estimation accuracy: the case of three estimates applied in adjustment of geodetic measurements; load tests of the movable footbridge over the Port Canal in Ustka; MEMS gyro in the context of inertial positionning; methodology of an assessment of building construction variants with the use of expert systems; national height system testing in baltic countries using gnss measurements; on the quality parameters of the modern gravity survey of the Lithuanian territory; online geocoding services: a benchmarking analysis to some European cities; problems using foam concrete substructure building foundations in irrigated soils; regional ionosphere modeling based on Multi-GNSS data and TPS interpolation; remote sensing methods in the study of the impact of long-term process of sulphur mining on environmental changes of the Carpathian foreland; safety assessment of the regional Warmia and Mazury road network using time-series analysis; sensors in river information services of the Odra River in Poland: current state and planned extension; single frequency RTK positioning using Schreiber's differencing scheme; spatial approach to risk premium determination for residential market classification; stability of permanent EPN stations near the Baltic Sea shoreline; structural health monitoring system for suspension footbridge; study of the flow dynamics of surface water masses in the area of the Coastal Gulf of Gdansk; technical condition of buildings managed by the agricultural property agency - diagnosis and assessment; technology of rapid and ultrarapid static GPS/GLONASS surveying in urban environments; technology roadmapping for multi-functional composite platform application in space vehicles; testing of geodetic laser instruments under conditions of air turbulence; the analysis of resource use plans based on selected municipalities - towards the realisation of good governance concept; the cokriging method in the process of developing land value maps; the digital image correlation system accuracy direct testing using strain gauges; the influence of elevation data generalization on the accuracy of the RUSLE model; the integration of image and nonimage data to obtain underwater situation refinement; the use of multi-criteria GIS analysis in the revitalisation of space; Timber frame houses with different insulation materials - seismic analysis; towards precise visual navigation and direct georeferencing for MAV using ORB-SLAM2; use of a model of potential to analyse population distribution structure; use of optical method for improvement of soil dynamic tests in torsional shear apparatus; video imagery orientation acquired using a low cost mobile mapping system; and tropospheric delay estimates using absolute and relative approaches to GNSS data processing - preliminary results.",https://ieeexplore.ieee.org/document/8071432/,2017 Baltic Geodetic Congress (BGC Geomatics),22-25 June 2017,ieeexplore
10.1109/WI-IAT.2009.201,Tank War Using Online Reinforcement Learning,IEEE,Conferences,"Real-Time Strategy(RTS) games provide a challenging platform to implement online reinforcement learning(RL) techniques in a real application. Computer as one player monitors opponents'(human or other computers) strategies and then updates its own policy using RL methods. In this paper, we propose a multi-layer framework for implementing the online RL in a RTS game. The framework significantly reduces the RL computational complexity by decomposing the state space in a hierarchical manner. We implement the RTS game - Tank General, and perform a thorough test on the proposed framework. The results show the effectiveness of our proposed framework and shed light on relevant issues on using the RL in RTS games.",https://ieeexplore.ieee.org/document/5285131/,2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology,15-18 Sept. 2009,ieeexplore
10.1109/QRS-C.2018.00071,Task Resource Planning and Verification Method Based on Intelligent Planning,IEEE,Conferences,"In order to meet the demands of rapid resource scheduling and capability recombination in a rapidly changing environment, real-time and dynamic resource planning and flexible scheduling are carried out in a relatively short time within a wide space. The paper proposes a fast generation method of task system resource guarantee plan based on intelligent planning, allocation mechanism and verification method. In this paper, we use an ontology-based semantic representation model and Hierarchical Task Network (HTN) to get the corresponding sequences of the resource type, breaking through the hierarchical task network planning combined with business process driven technology, to implement the task decomposition and resource guarantee scheme generation technique. Then we use Multidimensional Dynamic List Scheduling (MDLS) resource allocation algorithm to complete screening and the operation of the specific distribution of resources. During the resource allocation, we use conflict resolution algorithm to avoid resource conflict, to ensure resource combination optimization.",https://ieeexplore.ieee.org/document/8431999/,"2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)",16-20 July 2018,ieeexplore
10.1109/PADSW.2000.884672,Teleoperation system for real world robots-adaptive robot navigation based on sensor fusion,IEEE,Conferences,"The authors propose a teleoperation system with an autonomous robot which is able to solve tasks even without a large load for the operator and the system. Most teleoperation systems require skilled operators and expensive interfaces to solve tasks because they assume that the operator controls a robot completely. For these problems, we propose a teleoperation system which consists of an operation system and an autonomous robot. The operation system has a man-machine interface and allows a user to specify the working space and the tasks to be done. The autonomous robot follows the instruction from the operation system to solve the specific tasks. The paper focuses on navigation problems of the autonomous robot as an essential part of the proposed system. Namely, the autonomous robot should keep on the instructed paths in the real world to achieve a goal of the tasks. Our approach is based on a sensor fusion method based on two learning schemes: self-organizing map (SOM) and reinforcement learning. These learning schemes allow the system to be able to solve the tasks in an unreliable environment such as outdoors. Computational simulations reveal the effectiveness and robustness of the proposed method in the navigation problem.",https://ieeexplore.ieee.org/document/884672/,Proceedings Seventh International Conference on Parallel and Distributed Systems: Workshops,4-7 July 2000,ieeexplore
10.1145/3238147.3238227,Template-Guided Concolic Testing via Online Learning,IEEE,Conferences,"We present template-guided concolic testing, a new technique for effectively reducing the search space in concolic testing. Addressing the path-explosion problem has been a significant challenge in concolic testing. Diverse search heuristics have been proposed to mitigate this problem but using search heuristics alone is not sufficient to substantially improve code coverage for real-world programs. The goal of this paper is to complement existing techniques and achieve higher coverage by exploiting templates in concolic testing. In our approach, a template is a partially symbolized input vector whose job is to reduce the search space. However, choosing a right set of templates is nontrivial and significantly affects the final performance of our approach. We present an algorithm that automatically learns useful templates online, based on data collected from previous runs of concolic testing. The experimental results with open-source programs show that our technique achieves greater branch coverage and finds bugs more effectively than conventional concolic testing.",https://ieeexplore.ieee.org/document/8999998/,2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE),3-7 Sept. 2018,ieeexplore
10.1109/ICDM.2006.157,Temporal Data Mining in Dynamic Feature Spaces,IEEE,Conferences,"Many interesting real-world applications for temporal data mining are hindered by concept drift. One particular form of concept drift is characterized by changes to the underlying feature space. Seemingly little has been done in this area. This paper presents FAE, an incremental ensemble approach to mining data subject to such concept drift. Empirical results on large data streams demonstrate promise.",https://ieeexplore.ieee.org/document/4053168/,Sixth International Conference on Data Mining (ICDM'06),18-22 Dec. 2006,ieeexplore
10.1109/SNPD.2008.105,Testing Component-Based Real Time Systems,IEEE,Conferences,"This paper focuses on studying efficient solutions for modeling and deriving compositional tests for component-based real-time systems. In this work, we propose a coherent framework that does not require the computation of the synchronous product (composition) of components, and therefore avoids a major bottleneck in this class of test. For this framework, we introduce an approach and associated algorithm. In our approach, the overall behavior of the system is obtained by restricting free runs of components to those involving interactions between them. This restriction is achieved through the use of a particular component called assembly controller. For the generation algorithm, compositional test cases are derived from the assembly controller model using symbolic analysis. This reduces the state space size (a practical size) and enables the generation of sequences which cover all critical interaction scenarios.",https://ieeexplore.ieee.org/document/4617482/,"2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing",6-8 Aug. 2008,ieeexplore
10.1109/CITISIA50690.2020.9371771,Text Analysis for Honeypot Misuse Inference,IEEE,Conferences,"Transformation of raw text is required for computational text analysis using Natural Language Processing methods. Computational text analysis leverage on human brain limitations to automatically index documents for retrieval and topic generation for topic distribution correlations in corpus of voluminous documents. Natural language non-parametric and parametric Topic modeling with Expectancy Maximization and Gibbs sampling render technique to build Machine Learning models for evaluation with log-likelihood, topic coherence and coefficient of determination of held-out document. This research extends the concept of Natural Language Processing to automate analysis of High interaction honeypot system call documents to deduce system resources misuse by malcode during real-time engagement with the user-space applications of the deployed honeypot.",https://ieeexplore.ieee.org/document/9371771/,2020 5th International Conference on Innovative Technologies in Intelligent Systems and Industrial Applications (CITISIA),25-27 Nov. 2020,ieeexplore
10.1109/THS.2013.6699006,The CERT assessment tool: Increasing a security incident responder's ability to assess risk,IEEE,Conferences,"We set out to create an assessment and situational awareness tool for incident response. Extracting the risk assessment expertise and creating a systemic step-by-step workflow that could be followed by non-experts was challenging; however, what proved to be even more difficult was the mapping of that workflow to a common, natural language used by non-experts while still supporting the incident response. We at the Digital Intelligence and Investigation Directorate (DIID) have developed a way to maintain the velocity of incident response through the creation of a feed-forward decision support system to assist a security responder deal with the scale and challenges of assessing risk in critical information systems. Unfortunately, many applications fall short of expectations because the technology is used inappropriately: the wrong tool applied in the wrong way. Taking interaction techniques combined with a decision support system and applying them to one particularly demanding area - security incident response - leads to the conclusion that there is a proper and formal way to approach maintaining situational awareness in this complex domain. The CERT Assessment Tool increases a security incident responder's ability to assess risk and identify the incident response plan of critical information systems. The interface has four primary affordances to the user: (1) digital storage of the collected interview data with tagging of the information to create meta data of the objects as well as standardize terminology by reusing objects, (2) structured data that enables situational awareness of all systems on site and flexibility and recursion of system attributes, (3) guidance questions that provide runtime support for the system currently being assessed and a general direction to better assess each system based on historical data, and (4) real-time rules that make recommendations to the user through `push' notifications, which enables a user to identify and mitigate risk in information systems security affecting the safety of a system or the implementation of the security plan. The creation of a security decision support system framework to represent a series of steps to view the entire space of a security incident allows us to use techniques specifically designed or selected to align with one of the three identified stages of incident response - pre-incident (perception), during the event (comprehension), or after the event (projection). This combination of rules based on machine learning and push notifications are a first step in how computers will be able to support and advance the decision support technologies that are the backbone of this system.",https://ieeexplore.ieee.org/document/6699006/,2013 IEEE International Conference on Technologies for Homeland Security (HST),12-14 Nov. 2013,ieeexplore
10.1109/C5.2012.11,The Development of a Programming-Project Sharing Environment on Virtual Space,IEEE,Conferences,"Recently, there are classes using GUI programming environment such as Squeak Etoys or Scratch in primary education. When children study in such environment, children do not have enough opportunity to view the other's works and show their own work to parents and it is difficult to perform mutual evaluation and review lesson. In this paper, we propose an environment where children or parents can view the children's works on virtual space. It can provide opportunities for children to do mutual evaluation or review the past lesson. We have deployed this system to classes using GUI-programming environment and conducted a questionnaire for children. As a result, seven out of eleven children could view the other's works well. Moreover, eight out of ten children could find a scene in which other children watch the work together, and among these children, six of them viewed this attracting work.",https://ieeexplore.ieee.org/document/6195219/,"2012 10th International Conference on Creating, Connecting and Collaborating through Computing",18-20 Jan. 2012,ieeexplore
10.1109/CRC52766.2021.9620166,The Development of an Omnidirectional Mobile Robot Based on Hub Motor,IEEE,Conferences,"To enable the ability of moving in a narrow space, the robots are required to move in all directions. However, traditional robots with omnidirectional mobile function are easily wearing, with poor bearing capacity, and with complex structure. We designed and proposed an omnidirectional mobile robot (OMR) with the active split offset caster (ASoC) wheelset as the driving wheelset. Specifically, we first established the mathematical model of the robot based on the differential drive principle. Then the mathematical model is verified with synchronized real and simulated movement of the designed robot. A motion capture system is used to track the actual motion trajectory. Finally, a road test experiment is applied to test the stability of the designed robot in a real environment. The tiny differences between real and simulated trajectory show that the designed robot has the ability to move omnidirectionally in a narrow space. The road test experiment proves the advanced adaptability in real environment. Therefore, the designed robot can assist in a narrow space.",https://ieeexplore.ieee.org/document/9620166/,"2021 6th International Conference on Control, Robotics and Cybernetics (CRC)",9-11 Oct. 2021,ieeexplore
10.1109/CIE.2002.1185854,The HyperClass: education in a broadband Internet environment,IEEE,Conferences,"The 1980s saw the advent of the PC in education, the 1990s saw the coming of narrow band Internet to education, the first decade of the new Millennium seems set to see the spread of broadband Internet in education. What will it mean for education when students and teachers can access Pentium 4 computers with 214 meg bandwidth from wherever they are? The author with Lalita Rajasingham and Nobuyoshi Terashima in collaboration with colleagues around the world, have been studying this question. In particular they have been looking at the application on the Internet of a technology called HyperReality. The idea of the technology is that it provides a space where physical reality and virtual reality and human intelligence and artificial intelligence can interact in a manner that becomes increasingly seamless. The space could be a class-hence the idea of a HyperClass. The paper looks at the experimental work already carried out between Japan and New Zealand to develop such a system. It addresses the implications for education of real students, teachers and objects freely interacting with virtual students, teachers and objects and with artificial intelligence in a class. Finally it draws on the work of Tiffin and Rajasingham in designing a global virtual university to look at the possibilities for developing HyperUniversities, HyperColleges and HyperSchools which would allow the intersection of global and local dimensions in education.",https://ieeexplore.ieee.org/document/1185854/,"International Conference on Computers in Education, 2002. Proceedings.",3-6 Dec. 2002,ieeexplore
10.1109/CAIA.1988.196126,The NASA systems autonomy program: applying AI in space,IEEE,Conferences,"Summary form only given. The National Aeronautics and Space Administration (NASA) has initiated an aggressive program to develop, integrate, and implement autonomous systems technologies starting with knowledge-based (expert) systems and evolving towards 'autonomous', intelligent systems. Research thrusts to achieve these capabilities are centered around machine learning; coordinated real-time decision-making by multiple, cooperating intelligent agents; management, maintenance, and real-time control of distributed databases; software verification and validation; and fault-tolerant, multiprocessor architectures capable of operating in a heterogeneous environment. A summary of the research plans and the progress made in each of these areas is discussed.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/196126/,[1988] Proceedings. The Fourth Conference on Artificial Intelligence Applications,14-18 March 1988,ieeexplore
10.1109/ICMLA.2009.15,The Neuro Slot Car Racer: Reinforcement Learning in a Real World Setting,IEEE,Conferences,"This paper describes a novel real-world reinforcement learning application: The Neuro Slot Car Racer. In addition to presenting the system and first results based on Neural Fitted Q-Iteration, a standard batch reinforcement learning technique, an extension is proposed that is capable of improving training times and results by allowing for a reduction of samples required for successful training. The Neuralgic Pattern Selection approach achieves this by applying a failure-probability function which emphasizes neuralgic parts of the state space during sampling.",https://ieeexplore.ieee.org/document/5381535/,2009 International Conference on Machine Learning and Applications,13-15 Dec. 2009,ieeexplore
10.1109/SysCon47679.2020.9275860,The Role of Attribute Ranker using classification for Software Defect-Prone Data-sets Model: An Empirical Comparative Study,IEEE,Conferences,"Feature selection, is an issue firmly identified with size decrease of data-sets model. The target of feature selection is to recognize features in the data-set as significant, and dispose of some other feature as unimportant and repetitive data. Since feature selection diminishes the dimensionality of the data-sets model, it holds out the probability of increasingly successful and quick activity of data mining algorithm which can be worked quicker and all the more adequately by utilizing feature selection. In this research paper we will investigate feature extraction Principal Component Analysis (PCA) with attribute ranker search technique. In practice, Principal Component Analysis with attribute ranker search strategy isnt just used to improve extra storage space or the computational accuracy and efficiency of the classification algorithm, however can likewise enhance the prescient presentation by diminishing the scourge of dimensionality  particularly on the off chance that we are working with software defect-prone or non-defected data-sets models. We have used 10 datasets models, these datasets models basically REPOSITORY model of NASA which contain binary class defected and non-defected datasets models. We have also used 6 classifiers for comparatively analysis between objective model and real datasets model. We illustrated the comparatively analysis between PCA Ranker Search method with No-PCA. We have also compared classifiers efficiency with each other. The most efficient and useful classifier are the Bagging and Multilayer Perceptron at all attribute ranking search method. But the comparatively analysis between the classifiers that Nave bayes and MultiLayer Perceptron have well increased the correctly classified instances percentage in overall software fault forecast. One more comparison between the PCA Ranker Search Method and No-PCA that is attribute ranker search method is really good for increasing accuracy and efficiency for software fault forecast dataset model as compare the no-PCA method.",https://ieeexplore.ieee.org/document/9275860/,2020 IEEE International Systems Conference (SysCon),24 Aug.-20 Sept. 2020,ieeexplore
10.1109/ICRMS.2009.5270085,The analysis and modeling for the input space of real-time embedded software,IEEE,Conferences,"Software reliability testing is one of the important tasks in software reliability engineering, in which the failure data can be used to evaluate and validate the software reliability. In this paper, the input space of real-time embedded software is analyzed first. And the model of input space constructed with usage space and input value space is presented, with which a formal modeling method, the usage profile in network graph form, is presented. With the usage profile, the constraint conditions of operations and the dynamic actions of software users can be expressed closely to actual situation. The software reliability testing cases can be generated with random sampling according to the software usage profile.",https://ieeexplore.ieee.org/document/5270085/,"2009 8th International Conference on Reliability, Maintainability and Safety",20-24 July 2009,ieeexplore
10.1109/ICIT.2002.1189350,The application of a reinforcement learning agent to a multi-product manufacturing facility,IEEE,Conferences,"An intelligent agent-based scheduling system, consisting of a reinforcement learning agent and a simulation model has been developed and tested on a classic scheduling problem. The production facility studied is a multiproduct serial line subject to stochastic failure. The agent goal is to minimise total production costs, through selection of job sequence and batch size. To explore state space the agent used reinforcement learning. By applying an independent inventory control policy for each product, the agent successfully identified optimal operating policies for a real production facility.",https://ieeexplore.ieee.org/document/1189350/,"2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.",11-14 Dec. 2002,ieeexplore
10.1109/ChiCC.2014.6896010,The design and optimization method of near space intelligent target generator,IEEE,Conferences,"This paper presents the design and optimization method of near space intelligent target generator to simulate the physical characteristics of the near space vehicle. Combined with High Level Architecture distributed simulation technology, a common, repeatable and verified platform for the near space vehicle has been provided. This method used 3D modeling software Creator and 3D visual rendering software Vega, two-dimensional map and three-dimensional vision were constructed to form a simulation environment, which enhanced the authenticity of the simulation. Based on particle swarm optimization, the intelligent path planning study of near space vehicle was conducted in this environment to make up for the inadequate intelligence of traditional target generators.",https://ieeexplore.ieee.org/document/6896010/,Proceedings of the 33rd Chinese Control Conference,28-30 July 2014,ieeexplore
10.1109/FIE.2001.963683,The design of pedagogical agent for distance virtual experiment,IEEE,Conferences,"Experimental learning about electric-machinery is high risk and time-consuming, students do not always enjoy the laboratory, the teacher also needs to pay full attention to all students and equipment, besides it is hard to provide enough machine sets and space for learning. So it is our motivation to design a virtual laboratory for the teaching of electric-machinery experiment by Internet under the more real environment without concern about danger or limitations. It is taught by an experienced teacher using a pedagogical agent which is the most important key factor. Taking the above features into consideration, an interactive virtual laboratory based on an expert system has been designed and implemented to improve the learning, operation and control in electric-machinery experiments. The system set up is a highly intelligent and interactive mechanism. Through the system, students could have a more complete environment for distance learning.",https://ieeexplore.ieee.org/document/963683/,31st Annual Frontiers in Education Conference. Impact on Engineering and Science Education. Conference Proceedings (Cat. No.01CH37193),10-13 Oct. 2001,ieeexplore
10.1109/POWERI.2006.1632564,The development of artificial neural network space vector PWM and diagnostic controller for voltage source inverter,IEEE,Conferences,"This paper presents the development of neural-network-based controller of space vector modulation (ANN-SVPWM) for voltage-source inverters (VSI). This ANN-SVPWM controller completely covers the undermodulation and overmodulation modes with operation extended linearly and smoothly up to square wave (six-step) by using theory of modulation between the limit trajectories. The ANN controller has the advantage of the very fast implementation of an SVM algorithm that can increase the switching frequency of power switches of the static converter. Furthermore, a ANN diagnosis method for real-time fault detection of power switches is proposed in this paper. The ANN controller uses the individual training strategy with the fixed weight and supervised models. The complete ANN-SVPWM and diagnostic controller can be used in power applications such as APF, STATCOM, UPFC and motor drives. A computer simulation program is developed using Matlab/Simulink together with the neural network toolbox for training the ANN-controller.",https://ieeexplore.ieee.org/document/1632564/,2006 IEEE Power India Conference,10-12 April 2006,ieeexplore
10.1109/ICIT.2009.4939636,The investigation of ANN space vector PWM and diagnostic controller for four switch three phase inverter fed induction motor drive,IEEE,Conferences,"This paper is to present the investigation of neural-network-based controller of space vector modulation (ANN-SVPWM) for four switch three phase inverter (FSTPI) fed induction motor drive. This ANN-SVPWM controller completely covers the under modulation and over modulation modes with operation extended linearly and smoothly up to square wave (six-step). The ANN controller uses the individual training strategy with the fixed weight and supervised models. Furthermore, ANN diagnosis method for real-time fault detection of power switches is proposed in this paper. The complete ANN-SVPWM and diagnostic controller can be used in power applications such as motor drives. A computer simulation program is developed using Matlab/Simulink together with the neural network toolbox for training the ANN-controller.This method has been validated experimentally using kit ACE 1104 (DSPACE) .",https://ieeexplore.ieee.org/document/4939636/,2009 IEEE International Conference on Industrial Technology,10-13 Feb. 2009,ieeexplore
10.1109/ICACITE51222.2021.9404738,The learning approaches using Augmented Reality in learning environments: Meta-Analysis,IEEE,Conferences,"With the emergence of Industrial Revolution 4.0, the educational settings are changing quickly. Augmented Reality (AR) is one of the upcoming technologies. AR enhances the real world by overlaying/augmenting the virtual/digital information over it. It provides the user with the ability to interact with the created virtual world in real space. The aim of this study is to classify the learning approaches implemented through AR technology. The technique used for the analysis is derived from systematic search of online literature databases like Taylor Francis, Web of Science, Springer, ScienceDirect and Scopus. The keywords used for the search include learning approaches, AR, AR in education, AR in learning and teaching and integration approaches. The findings of this research work highlights 4 categories of educational learning approaches that highlight AR. The approaches are experimental learning, game-based, interactive and collaborative learning. The research findings can be referred by other researchers and educators to identify the potential of AR in education and the learning approaches currently used with AR for their further research on how these approaches can be effectively and efficiently implemented in educational settings.",https://ieeexplore.ieee.org/document/9404738/,2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),4-5 March 2021,ieeexplore
10.1109/IJCNN.2015.7280318,The on-line curvilinear component analysis (onCCA) for real-time data reduction,IEEE,Conferences,"Real time pattern recognition applications often deal with high dimensional data, which require a data reduction step which is only performed offline. However, this loses the possibility of adaption to a changing environment. This is also true for other applications different from pattern recognition, like data visualization for input inspection. Only linear projections, like the principal component analysis, can work in real time by using iterative algorithms while all known nonlinear techniques cannot be implemented in such a way and actually always work on the whole database at each epoch. Among these nonlinear tools, the Curvilinear Component Analysis (CCA), which is a non-convex technique based on the preservation of the local distances into the lower dimensional space, plays an important role. This paper presents the online version of CCA. It inherits the same features of CCA, is adaptive in real time and tracks non-stationary high dimensional distributions. It is composed of neurons with two weights: one, pointing to the input space, quantizes the data distribution, and the other, pointing to the output space, represents the projection of the first weight. This on-line CCA has been conceived not only for the previously cited applications, but also as a basic tool for more complex supervised neural networks for modelling very complex high dimensional data. This algorithm is tested on 2-D and 3-D synthetic data and on an experimental database concerning the bearing faults of an electrical motor, with the goal of novelty (fault) detection.",https://ieeexplore.ieee.org/document/7280318/,2015 International Joint Conference on Neural Networks (IJCNN),12-17 July 2015,ieeexplore
10.1109/ACSSC.2003.1292255,The use of CNN models and vertical rectification for a direct trigonometric recovery of 3D scene geometry from a stream of images,IEEE,Conferences,"The exploration of unknown environment autonomously and intelligently by autonomous mobile robot is one of the main problems that still have to be solved. From the biological systems it appears that it is not only a matter of computational power but also the right sensors and methods of implementation. In this work we understood that the invention of the powerful and practically realizable, emerging paradigm called cellular nonlinear network (CNN) fully realized the concept of real-time handling of time signals coining from space distribution sources. On the other hand inertial gyro sensor can increase the robustness and computing efficiency of vision system, which is subject to signal degradation and high computational expense, by providing a frame-to-frame prediction of camera orientation and position. Since most of the earlier or recent studies have tackled the artificial vision with two or three views in account, in this work we present a direct trigonometric recovery of 3D scene geometry from a stream of images. These images are vertically rectified using gyroscopes' output to minimize the token relative displacements between each frame. To match this vertically rectified stream of images in real-tune we borrow the strength of CNN, and the matching results are trigonometrically processed for 3D range estimation.",https://ieeexplore.ieee.org/document/1292255/,"The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003",9-12 Nov. 2003,ieeexplore
10.1109/COGINF.2002.1039310,Theoretical and practical aspects of computer speech processing,IEEE,Conferences,"The design process of computer speech systems is presented. Going over the solutions of neural networks, the systolic structure gives the biggest possibility for smart realization of a speech processor. A speech processing system, built generally as software surroundings, occupies an immense memory space. For real-time application it is necessary to combine the existing algorithms. Further increasing of the speed can be also achieved by using hardware-software co-design. The approach presented in this paper, permits to design such effective systems of human voice synthesis and speech identification. Since the spoken language can be very often ambiguous, then it was useful to include a fuzzy logic approach and also artificial neural networks, to obtain more effective methods of speech processing. Mixed analog-digital realizations are also discussed. Practical realizations of the current mode building blocks ensure the cheapest realization of CMOS integrated circuits.",https://ieeexplore.ieee.org/document/1039310/,Proceedings First IEEE International Conference on Cognitive Informatics,19-20 Aug. 2002,ieeexplore
10.1109/MMRP.2019.00013,Three-Dimensional Mapping of High-Level Music Features for Music Browsing,IEEE,Conferences,"The increased availability of musical content comes with the need of novel paradigms for recommendation, browsing and retrieval from large music libraries. Most music players and streaming services propose a paradigm based on content listing of meta-data information, which provides little insight on the music content. In services with huge catalogs of songs, a more informative paradigm is needed. In this work we propose a framework for music browsing based on the navigation into a three-dimensional (3-D) space, where musical items are placed as a 3-D mapping of their high-level semantic descriptors. We conducted a survey to guide the design of the framework and the implementation choices. We rely on state-of-the-art techniques from Music Information Retrieval to automatically extract the high-level descriptors from a low-level representation of the musical signal. The framework is validated by means of a subjective evaluation from 33 users, who give positive feedbacks and highlight promising future developments especially in virtual reality field.",https://ieeexplore.ieee.org/document/8665368/,2019 International Workshop on Multilayer Music Representation and Processing (MMRP),23-24 Jan. 2019,ieeexplore
10.1109/ICASSP.1998.675393,Time-first search for large vocabulary speech recognition,IEEE,Conferences,"This paper describes a new search technique for large vocabulary speech recognition based on a stack decoder. Considerable memory savings are achieved with the combination of a tree based lexicon and a new search technique. The search proceeds time-first, that is partial path hypotheses are extended into the future in the inner loop and a tree walk over the lexicon is performed as an outer loop. Partial word hypotheses are grouped based on language model state. The stack maintains information about groups of hypotheses and whole groups are extended by one word to form new stack entries. An implementation is described of a one-pass decoder employing a 65000 word lexicon and a disk-based trigram language model. Real time operation is achieved with a small search error, a search space of about 5 Mbyte and a total memory usage of about 35 Mbyte.",https://ieeexplore.ieee.org/document/675393/,"Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)",15-15 May 1998,ieeexplore
10.1109/SEAMS.2012.6224393,"Timing constraints for runtime adaptation in real-time, networked embedded systems",IEEE,Conferences,"In this work, we consider runtime adaptation in networked embedded systems with tight real-time constraints. For such systems, we aim to adapt the placement of software components on networked hardware components at runtime without violating real-time constraints. We develop constraints for such an adaptation process and show the applicability to networked embedded systems like automotive in-vehicle networks. Then, we analyze two approaches for finding solutions in the resulting search space for adaptations, one based on planning algorithms and the other based on constraint solving. While planning approaches start from the current configuration and aim to find a migration sequence and a valid configuration, constraint solving approaches first find solutions and then check for a possible migration sequence. Based on simulations for the automotive domain, we show that approaches based on planning algorithms scale poorly, while constraint solving approaches can find solutions effectively.",https://ieeexplore.ieee.org/document/6224393/,2012 7th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),4-5 June 2012,ieeexplore
10.1109/RTAS48715.2020.000-1,Timing of Autonomous Driving Software: Problem Analysis and Prospects for Future Solutions,IEEE,Conferences,"The software used to implement advanced functionalities in critical domains (e.g. autonomous operation) impairs software timing. This is not only due to the complexity of the underlying high-performance hardware deployed to provide the required levels of computing performance, but also due to the complexity, non-deterministic nature, and huge input space of the artificial intelligence (AI) algorithms used. In this paper, we focus on Apollo, an industrial-quality Autonomous Driving (AD) software framework: we statistically characterize its observed execution time variability and reason on the sources behind it. We discuss the main challenges and limitations in finding a satisfactory software timing analysis solution for Apollo and also show the main traits for the acceptability of statistical timing analysis techniques as a feasible path. While providing a consolidated solution for the software timing analysis of Apollo is a huge effort far beyond the scope of a single research paper, our work aims to set the basis for future and more elaborated techniques for the timing analysis of AD software.",https://ieeexplore.ieee.org/document/9113112/,2020 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),21-24 April 2020,ieeexplore
10.1109/ICTAI.2015.130,Toward Efficient Agreements in Real-Time Multilateral Agent-Based Negotiations,IEEE,Conferences,"Negotiations among autonomous agents have gained a mass of attention from a variety of communities in the past decade. This paper deals with a prominent type of automated negotiations, namely, multilateral multi-issue negotiation that runs under real-time constraints and in which the negotiating agents have no prior knowledge about their opponents' preferences over the space of negotiation outcomes. We propose a novel negotiation approach which enables an agent to reach an efficient agreement with multiple opponents. The proposed approach achieves that goal by, 1) employing sparse pseudo-input Gaussian processes to model the behavior of opponents, 2) learning fuzzy opponent preferences to increase the satisfaction of other parties, and 3) adopting an adaptive decision-making mechanism to handle uncertainty in negotiation. The experimental results show, both from the standard mean-score perspective and the perspective of empirical game theory, that the agent applying the proposed approach outperforms the state-of-the-art negotiation agents from the recent Automated Negotiating Agents Competition (ANAC) in a variety of negotiation domains.",https://ieeexplore.ieee.org/document/7372227/,2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI),9-11 Nov. 2015,ieeexplore
10.1109/WI-IAT.2013.63,Towards Adaptive Multi-agent Planning in Cyber Physical Space,IEEE,Conferences,"Cyber physical space is potentially hosting innumerable spatio-temporal data streams due to increasing use of social networking platforms as real-time information dissemination system and world-wide deployment of sensors for continuous monitoring of physical phenomena. In this paper we address the problem of how cyber physical space can be used for sensing and responding to global calamities such as earthquake. The paper proposes a novel approach of generating online adaptive response in assisting search-and-rescue operations using situation awareness built from real-time heterogeneous spatio-temporal data streams. Online adaptive response is achieved by using agent-based cooperative task sharing and modeling agent decision making as self-organized emergent behavior based on concepts of complex adaptive system. An implemented simulation platform use concepts of situation modeling, domain task network, contract net protocol based negotiation and complex adaptive system to generate adaptive plans. Preliminary simulation results are promising as we have been able to demonstrate a repertoire of self-organized emergent behaviors.",https://ieeexplore.ieee.org/document/6690049/,2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT),17-20 Nov. 2013,ieeexplore
10.1109/IEMBS.2007.4353479,Towards Asynchronous Brain-computer Interfaces: A P300-based Approach with Statistical Models,IEEE,Conferences,"Asynchronous control is a critical issue in developing brain-computer interfaces for real-life applications, where the machine should be able to detect the occurrence of a mental command. In this paper we propose a computational approach for robust asynchronous control using the P300 signal, in a variant of oddball paradigm. First, we use Gaussian models in the support vector margin space to describe various types of EEG signals that are present in an asynchronous P300-based BCI. This allows us to derive a probability measure of control state given EEG observations. Second, we devise a recursive algorithm to detect and locate control states in ongoing EEG. Experimental results indicate that our system allows information transfer at approx. 20 bit/min at low false alarm rate (1/min).",https://ieeexplore.ieee.org/document/4353479/,2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,22-26 Aug. 2007,ieeexplore
10.1109/ICWS.2014.79,Towards Effectively Identifying RESTful Web Services,IEEE,Conferences,"In recent years, RESTful Web services have been rapidly developed and deployed, because of the advantages of lightweight, flexibility and extensibility, etc. However, most RESTful services are described in heterogeneous and ordinary HTML pages, which makes them really difficult to be identified and crawled automatically from the Internet. In this paper we propose a hybrid classifier framework called co-NV for automatic identification of RESTful services on the Web. In our framework, web pages are analyzed and filtered according to the contents and structure characteristics of HTML documents, with Nave Bayes classifier and Vector Space Model (VSM) respectively. Experiments with real RESTful services prove that our framework works effectively with high precision and recall rate, and is very practical.",https://ieeexplore.ieee.org/document/6928939/,2014 IEEE International Conference on Web Services,27 June-2 July 2014,ieeexplore
10.1109/ASAP.2018.8445099,Towards Hardware Accelerated Reinforcement Learning for Application-Specific Robotic Control,IEEE,Conferences,"Reinforcement Learning (RL) is an area of machine learning in which an agent interacts with the environment by making sequential decisions. The agent receives reward from the environment based on how good the decisions are and tries to find an optimal decision-making policy that maximises its longterm cumulative reward. This paper presents a novel approach which has showon promise in applying accelerated simulation of RL policy training to automating the control of a real robot arm for specific applications. The approach has two steps. First, design space exploration techniques are developed to enhance performance of an FPGA accelerator for RL policy training based on Trust Region Policy Optimisation (TRPO), which results in a 43% speed improvement over a previous FPGA implementation, while achieving 4.65 times speed up against deep learning libraries running on GPU and 19.29 times speed up against CPU. Second, the trained RL policy is transferred to a real robot arm. Our experiments show that the trained arm can successfully reach to and pick up predefined objects, demonstrating the feasibility of our approach.",https://ieeexplore.ieee.org/document/8445099/,"2018 IEEE 29th International Conference on Application-specific Systems, Architectures and Processors (ASAP)",10-12 July 2018,ieeexplore
10.1109/ICSE-SEIP52600.2021.00027,Towards Inclusive Software Engineering Through A/B Testing: A Case-Study at Windows,IEEE,Conferences,"Engineering software to be inclusive of all those that might/could/should use the software is important. However, today, data used to engineer software can have inherent biases (e.g. gender identity) with inclusiveness concerns. While much attention has been given to this topic in the AI/ML space, in this paper, we examine another data-centric software engineering process, A/B testing, for which we have a dearth of understanding today. Using real-world data from the Windows out of box experience (OOBE) feature, we provide a case-study of how inclusiveness concerns can manifest in A/B testing, practical adjustments to A/B testing towards inclusive software engineering, and insights into ongoing challenges. We discuss implications for research and practice.",https://ieeexplore.ieee.org/document/9402142/,2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),25-28 May 2021,ieeexplore
10.1109/ICCV.2013.129,Towards Motion Aware Light Field Video for Dynamic Scenes,IEEE,Conferences,"Current Light Field (LF) cameras offer fixed resolution in space, time and angle which is decided a-priori and is independent of the scene. These cameras either trade-off spatial resolution to capture single-shot LF or tradeoff temporal resolution by assuming a static scene to capture high spatial resolution LF. Thus, capturing high spatial resolution LF video for dynamic scenes remains an open and challenging problem. We present the concept, design and implementation of a LF video camera that allows capturing high resolution LF video. The spatial, angular and temporal resolution are not fixed a-priori and we exploit the scene-specific redundancy in space, time and angle. Our reconstruction is motion-aware and offers a continuum of resolution tradeoff with increasing motion in the scene. The key idea is (a) to design efficient multiplexing matrices that allow resolution tradeoffs, (b) use dictionary learning and sparse representations for robust reconstruction, and (c) perform local motion-aware adaptive reconstruction. We perform extensive analysis and characterize the performance of our motion-aware reconstruction algorithm. We show realistic simulations using a graphics simulator as well as real results using a LCoS based programmable camera. We demonstrate novel results such as high resolution digital refocusing for dynamic moving objects.",https://ieeexplore.ieee.org/document/6751235/,2013 IEEE International Conference on Computer Vision,1-8 Dec. 2013,ieeexplore
10.1109/ICDCSW53096.2021.00009,Towards Understanding the Adaptation Space of AI-Assisted Data Protection for Video Analytics at the Edge,IEEE,Conferences,"Edge computing facilitates the deployment of distributed AI applications, capable of processing video data in real time. AI-assisted video analytics can provide valuable information and benefits in various domains. Face recognition, object detection, or movement tracing are prominent examples enabled by this technology. However, such mechanisms also entail threats regarding privacy and security, for example if the video contains identifiable persons. Therefore, adequate data protection is an increasing concern in video analytics. AI-assisted data protection mechanisms, such as face blurring, can help, but are often computationally expensive. Additionally, the heterogeneous hardware of end devices and the time-varying load on edge services need to be considered. Therefore, such systems need to adapt to react to changes during their operation, ensuring that conflicting requirements on data protection, performance, and accuracy are addressed in the best possible way. Sound adaptation decisions require an understanding of the adaptation options and their impact on different quality attributes. In this paper, we identify factors that can be adapted in AI-assisted data protection for video analytics using the example of a face blurring pipeline. We measure the impact of these factors using a heterogeneous edge computing hardware testbed. The results show a large and complex adaptation space, with varied impacts on data protection, performance, and accuracy.",https://ieeexplore.ieee.org/document/9545916/,2021 IEEE 41st International Conference on Distributed Computing Systems Workshops (ICDCSW),7-10 July 2021,ieeexplore
10.1109/ICC.2010.5501969,Towards a Theory of Generalizing System Call Representation for In-Execution Malware Detection,IEEE,Conferences,"The major contribution of this paper is two-folds: (1) we present our novel variable-length system call representation scheme compared to existing fixed- length sequence schemes, and (2) using this representation, we present our in-execution malware detector that can not only identify zero-day malware without any a priori knowledge but can also detect a malicious process while it is executing. Our representation scheme - a more generalized version of n-gram - can be visualized in a k-dimensional hyperspace in which processes move depending upon their sequence of system calls. The process marks its impact in space by generating hyper-grams that are later used to evaluate an unknown process according to their profile. The proposed technique is evaluated on a real world dataset extracted from a Linux System. The results of our analysis show that our in-execution malware detector with hyper- gram representation achieves low processing overheads and improved detection accuracies as compared to conventional n-grams.",https://ieeexplore.ieee.org/document/5501969/,2010 IEEE International Conference on Communications,23-27 May 2010,ieeexplore
10.1109/IC2E.2018.00038,"Towards an Adaptive, Fully Automated Performance Modeling Methodology for Cloud Applications",IEEE,Conferences,"The advent of the Cloud computing era along with the wide adoption of the distributed paradigm has enabled applications to increase their performance standards and greatly extend their scalability limits. Nevertheless, the ability of modern applications to be deployed in numerous different ways has complicated their structure and enormously increased the difficulty of extracting accurate performance models for them. This capability is crucial for many operations, such as the identification of the most appropriate execution setups for an anticipated workload, finding bottlenecks, etc. In this work, we propose a fully automated performance modeling methodology that aims at the creation of highly accurate performance models for a given maximum number of deployments. The main idea of the proposed methodology lies on the ""smart"" exploration of the application configuration space, the selection and deployment of a set of representative application configurations and the construction of the performance model through the adoption of Machine Learning techniques. Moreover, taking into consideration the often unstable and error-prone nature of the cloud, the proposed system attempts to overcome transient cloud failures that occur during the application deployment phase through the re-execution of the parts that failed. Our evaluation, conducted for a set of real-world applications frequently deployed over cloud infrastructures, indicates that our system is capable of both constructing performance models of high accuracy and doing so in an efficient manner, fixing application deployments that present errors without requiring human intervention.",https://ieeexplore.ieee.org/document/8360323/,2018 IEEE International Conference on Cloud Engineering (IC2E),17-20 April 2018,ieeexplore
10.1109/IV.2014.39,Towards the Identification of Consumer Trajectories in Geo-Located Search Data,IEEE,Conferences,"Modern geo-positioning system (GPS) enabled smart phones are generating an increasing volume of information about their users, including geo-located search, movement, and transaction data. While this kind of data is increasingly rich and offers many grand opportunities to identify patterns and predict behaviour of groups and individuals, it is not immediately obvious how to develop a framework for extracting plausible inferences from these data. In our case, we have access to a large volume of real user data from the Point smart phone application, and we have developed a generic and layered system architecture to incrementally find aggregate items of interest within that data. This includes time and space correlations, e.g., are people searching for dinner and a movie, distributions of usage patterns and platforms, e.g., geographic distribution of Android, Apple, and BlackBerry users, and clustering to identify relatively complex search and movement patterns we call ""consumer trajectories."" Our pursuit of these kinds of patterns has helped guide our development of information extraction, machine learning, and visualization methods that provide systematic tools for investigating the geo-located data, and for the development of both conceptual tools and visualization tools in aid of finding both interesting and useful patterns in that data. Included in our system architecture is the ability to consider the difference between exploratory and explanatory hypotheses on data patterns, as well as the deployment of multiple visualization methods that can provide alternatives to help expose interesting patterns. In our introduction to our framework here, we provide examples of formulating hypotheses on geo-located behaviour, and how a variety of methods including those from machine learning and visualization, can help confirm or deny the value of such hypotheses as they emerge. In this particular case, we provide an initial basis for identifying semantically motivated data artifacts we call geo-located consumer trajectories. We investigate their plausibility with a variety of time and space series clustering and visualization models.",https://ieeexplore.ieee.org/document/6902904/,2014 18th International Conference on Information Visualisation,16-18 July 2014,ieeexplore
10.1109/ROMAN.2017.8172430,"Towards the use of consumer-grade electromyographic armbands for interactive, artistic robotics performances",IEEE,Conferences,"In recent years, gesture-based interfaces have been explored in order to control robots in non-traditional ways. These require the use of systems that are able to track human body movements in 3D space. Deploying Mo-cap or camera systems to perform this tracking tend to be costly, intrusive, or require a clear line of sight, making them ill-adapted for artistic performances. In this paper, we explore the use of consumer-grade armbands (Myo armband) which capture orientation information (via an inertial measurement unit) and muscle activity (via electromyography) to ultimately guide a robotic device during live performances. To compensate for the drop in information quality, our approach rely heavily on machine learning and leverage the multimodality of the sensors. In order to speed-up classification, dimensionality reduction was performed automatically via a method based on Random Forests (RF). Online classification results achieved 88% accuracy over nine movements created by a dancer during a live performance, demonstrating the viability of our approach. The nine movements are then grouped into three semantically-meaningful moods by the dancer for the purpose of an artistic performance achieving 94% accuracy in real-time. We believe that our technique opens the door to aesthetically-pleasing sequences of body motions as gestural interface, instead of traditional static arm poses.",https://ieeexplore.ieee.org/document/8172430/,2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),28 Aug.-1 Sept. 2017,ieeexplore
10.1109/ICIVC47709.2019.8980828,Traffic Lights Detection and Recognition Algorithm Based on Multi-feature Fusion,IEEE,Conferences,"Detection and recognition of traffic lights is important for intelligent assisted driving. Traditional color space based traffic lights detection algorithms could be easily affected by other objects (such as buildings, car taillights) in the surrounding environment, and the detection accuracy and real-time performance are not ideal enough. Generally, the deep learning based methods have better advantages of real-time and accuracy performance for the normal scene with obvious traffic lights targets. However, the small traffic lights targets detection rate and accuracy in night-time of these methods are still can't be satisfactory. To solve this problem, this paper proposed a novel traffic lights detection and recognition algorithm based on multi-feature fusion, which can be implemented in two steps (detection and recognition). For the first step, the SLIC (simple linear iterative clustering) super-pixel segmentation algorithm is used for purposes reducing the image data processing complexity and improving the real-time performance. The mean-shift algorithm was used to cluster the HSV (Hue, Saturation, Value) color space components respectively for enhancing the target data and reducing the interference from other targets. For the second step, the feature information extracted by CNN (Convolutional Neural Network) and HOG(Histogram of Oriented Gradient) feature are fused. The SVM (Support Vector Machine) classifier is trained on a data set of traffic lights established by our own. To verify the proposed algorithm in this paper, amount of experiments were carried out in real traffic scenes. Experimental results show that this algorithm almost has the same real-time performance with YOLO_V3 neural network and a better accuracy.",https://ieeexplore.ieee.org/document/8980828/,"2019 IEEE 4th International Conference on Image, Vision and Computing (ICIVC)",5-7 July 2019,ieeexplore
10.1109/GLOBECOM38437.2019.9014245,Traffic Offloading and Power Allocation for Green HetNets Using Reinforcement Learning Method,IEEE,Conferences,"In order to satisfy the boosting mobile traffic demand, the deployment of small cells has been regarded as a feasible solution. But the growth of network infrastructure leads to a tremendous increase of energy consumption. Using renewable energy harvested from the environment to power the small cell base station, forming green heterogeneous networks (HetNets), can help reduce the conventional energy consumption. However, the stochastic user demand and random renewable energy harvesting amount have brought new challenges for the network operation. Based on reinforcement learning, this paper proposes a decentralized and a centralized base station operation scheme. Assuming each base station operates individually, the energy efficiency maximization problem is modeled as a general-sum game. After defining the state, action and reward of each base station, the problem can be solved by multi-agent reinforcement learning. Assuming there is a centralized controller, then the whole network can be seen as a huge agent, thus, the problem can be solved using deep reinforcement learning since the state space is too complicated. Simulation results show the centralized scheme shows higher performance but need more signaling overheads. The energy efficiency is lower for the decentralized scheme but it can be realized easier in real life. Both our proposed scheme can achieve significant energy efficiency improvement compared to the greedy scheme.",https://ieeexplore.ieee.org/document/9014245/,2019 IEEE Global Communications Conference (GLOBECOM),9-13 Dec. 2019,ieeexplore
10.1109/ICSGEA51094.2020.00079,Traffic-DesNet: A Robust Deep Learning-based Flow Prediction Model By The Using of DesNet,IEEE,Conferences,"Real-time and accurate short-term traffic flow forecasting has become a critical problem in intelligent transportation systems (ITS). Short-term traffic flow is not only affected by time, but also by space. Ordinary short-term traffic prediction methods only solve the impact of time but not the impact of space. Convolutional neural networks can use convolution kernels to resolve spatial correlations and dependencies between time and space, so they can more accurately predict short-term traffic flows. We introduce DenseNet for short-term traffic flow prediction, use convolution kernels to extract features between time and space, and superimpose the features extracted from the upper layer to extract new features. It can effectively use high-level information to discover new features at the bottom again. To better capture the implicit correlation in some areas. It can be found through experiments that the model is not only superior in accuracy to traditional methods but also has a significant improvement in efficiency.",https://ieeexplore.ieee.org/document/9260368/,2020 5th International Conference on Smart Grid and Electrical Automation (ICSGEA),13-14 June 2020,ieeexplore
10.1109/IJCNN.2000.861451,Training neural networks with threshold activation functions and constrained integer weights,IEEE,Conferences,"Evolutionary neural network training algorithms are presented. These algorithms are applied to train neural networks with weight values confined to a narrow band of integers. We constrain the weights and biases in the range [-2/sup k+1/+1, 2/sup k-1/-1], for k=3, 4, 5, thus they can be represented by just k bits. Such neural networks are better suited for hardware implementation than the real weight ones. Mathematical operations that are easy to implement in software might often be very burdensome in the hardware and therefore more costly. Hardware-friendly algorithms are essential to ensure the functionality and cost effectiveness of the hardware implementation. To this end, in addition to the integer weights, the trained neural networks use threshold activation functions only, so hardware implementation is even easier. These algorithms have been designed keeping in mind that the resulting integer weights require less bits to be stored and the digital arithmetic operations between them are easier to be implemented in hardware. Obviously, if the network is trained in a constrained weight space, smaller weights are found and less memory is required. On the other hand, as we have found here, the network training procedure can be more effective and efficient when larger weights are allowed. Thus, for a given application a trade off between effectiveness and memory consumption has to be considered. Our intention is to present results of evolutionary algorithms on this difficult task. Based on the application of the proposed class of methods on classical neural network benchmarks, our experience is that these methods are effective and reliable.",https://ieeexplore.ieee.org/document/861451/,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,27-27 July 2000,ieeexplore
10.1109/ROMOCO.2005.201413,Trajectory realization with collision avoidance algorithm,IEEE,Conferences,"This paper describes the approach to collision avoidance problem for 3-DOF anthropomorphic robot manipulators. The novelty of the approach is the decomposition of 3D space to two 2D spaces. Resulting is the computationally efficient algorithm, suitable for implementation in the real-time systems. Simulation of the anthropomorphic manipulator operating in three dimensional space with obstacles is also presented.",https://ieeexplore.ieee.org/document/1554392/,"Proceedings of the Fifth International Workshop on Robot Motion and Control, 2005. RoMoCo '05.",23-25 June 2005,ieeexplore
10.1109/IROS51168.2021.9636842,Trajectory-based Split Hindsight Reverse Curriculum Learning,IEEE,Conferences,"Grasping is one of the most fundamental problems in robotic manipulation. In recent years, with the development of data-driven methods, reinforcement learning has been used in solving robotic grasping problems. However, grasping is a long-horizon and sparse reward task, whose natural reward only appears when the task is successfully achieved. Therefore, it brings great challenges to the deployment of reinforcement learning methods. To tackle this difficulty, we propose a new method called Trajectory-based Split Hindsight Reverse Curriculum Learning. This method of reverse learning from the goal can greatly improve the learning efficiency and the final performance of the tasks. Specifically, based on referred trajectories, the agent starts to learn in a small state space near the goal and then gradually in larger state spaces until covering the entire state space. Through split hindsight experience replay, the sampled trajectory is divided into segments that match the current subspace's size; then, they are modified to successful trajectories to enable more efficient learning. In both simulation and real-world experiments, our method surpasses the existing methods and achieves the goal-oriented grasping tasks with higher success rates and better data efficiencies. The detailed experimental results can be viewed at https://youtu.be/7uNRzmRZhDk.",https://ieeexplore.ieee.org/document/9636842/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore
10.1109/ICIP40778.2020.9191037,Transductive Prototypical Network For Few-Shot Classification,IEEE,Conferences,"Few-shot learning is the key step towards human-level intelligence. Prototypical Network is a promising approach to address the key issue of over-fitting for few-shot learning. Nevertheless, the original Prototypical Network only uses one or few labeled instances to represent the corresponding class, which easily deviates from the real class distribution leading to the imprecise classification results. In this paper, we propose Transductive Prototypical Network (Td-PN), a universal transductive approach that refines the class representations by merging scarce labeled samples and high-confidence ones of target set. Our proposed Td-PN first maps the samples to a classifying-friendly (discriminative) embedding space by redesigning a weighted contrastive loss, then utilizes the transductive inference to obtain the powerful prototype representation for each class. Experiments demonstrate that our approach outperforms the state-of-the-art algorithms.",https://ieeexplore.ieee.org/document/9191037/,2020 IEEE International Conference on Image Processing (ICIP),25-28 Oct. 2020,ieeexplore
10.1109/SEAMS.2017.11,Transfer Learning for Improving Model Predictions in Highly Configurable Software,IEEE,Conferences,"Modern software systems are built to be used in dynamic environments using configuration capabilities to adapt to changes and external uncertainties. In a self-adaptation context, we are often interested in reasoning about the performance of the systems under different configurations. Usually, we learn a black-box model based on real measurements to predict the performance of the system given a specific configuration. However, as modern systems become more complex, there are many configuration parameters that may interact and we end up learning an exponentially large configuration space. Naturally, this does not scale when relying on real measurements in the actual changing environment. We propose a different solution: Instead of taking the measurements from the real system, we learn the model using samples from other sources, such as simulators that approximate performance of the real system at low cost. We define a cost model that transform the traditional view of model learning into a multi-objective problem that not only takes into account model accuracy but also measurements effort as well. We evaluate our cost-aware transfer learning solution using real-world configurable software including (i) a robotic system, (ii) 3 different stream processing applications, and (iii) a NoSQL database system. The experimental results demonstrate that our approach can achieve (a) a high prediction accuracy, as well as (b) a high model reliability.",https://ieeexplore.ieee.org/document/7968130/,2017 IEEE/ACM 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),22-23 May 2017,ieeexplore
10.1109/ICDM.2010.146,Transfer Learning via Cluster Correspondence Inference,IEEE,Conferences,"Transfer learning targets to leverage knowledge from one domain for tasks in a new domain. It finds abundant applications, such as text/sentiment classification. Many previous works are based on cluster analysis, which assume some common clusters shared by both domains. They mainly focus on the one-to-one cluster correspondence to bridge different domains. However, such a correspondence scheme might be too strong for real applications where each cluster in one domain corresponds to many clusters in the other domain. In this paper, we propose a Cluster Correspondence Inference (CCI) method to iteratively infer many-to-many correspondence among clusters from different domains. Specifically, word clusters and document clusters are exploited for each domain using nonnegative matrix factorization, then the word clusters from different domains are corresponded in a many-to-many scheme, with the help of shared word space as a bridge. These two steps are run iteratively and label information is transferred from source domain to target domain through the inferred cluster correspondence. Experiments on various real data sets demonstrate that our method outperforms several state-of-the-art approaches for cross-domain text classification.",https://ieeexplore.ieee.org/document/5694061/,2010 IEEE International Conference on Data Mining,13-17 Dec. 2010,ieeexplore
10.1109/WCNC.2018.8377359,Transferring knowledge for tilt-dependent radio map prediction,IEEE,Conferences,"Fifth generation wireless networks (5G) will face key challenges caused by diverse patterns of traffic demands and massive deployment of heterogeneous access points. In order to handle this complexity, machine learning techniques are expected to play a major role. However, due to the large space of parameters related to network optimization, collecting data to train models for all possible network configurations can be prohibitive. In this paper, we analyze the possibility of performing a knowledge transfer, in which a machine learning model trained on a particular network configuration is used to predict a quantity of interest in a new, unknown setting. We focus on the tilt-dependent received signal strength maps as quantities of interest and we analyze two cases where the knowledge acquired for a particular antenna tilt setting is transferred to (i) a different tilt configuration of the same antenna or (ii) a different antenna with the same tilt configuration. Promising results supporting knowledge transfer are obtained through extensive experiments conducted using different machine learning models on a real dataset.",https://ieeexplore.ieee.org/document/8377359/,2018 IEEE Wireless Communications and Networking Conference (WCNC),15-18 April 2018,ieeexplore
10.1109/ICTAI.2014.37,Triangulation Versus Graph Partitioning for Tackling Large Real World Qualitative Spatial Networks,IEEE,Conferences,"There has been interest in recent literature in tackling very large real world qualitative spatial networks, primarily because of the real datasets that have been, and are to be, offered by the Semantic Web community and scale up to millions of nodes. The proposed techniques for tackling such large networks employ the following two approaches for retaining the sparseness of their underlying graphs and reasoning with them: (i) graph triangulation and sparse matrix implementation, and (ii) graph partitioning and parallelization. Regarding the latter approach, an implementation has been offered recently, presented in [AAAI, 2014]. However, although the implementation looks promising and with space for improvement, an improper use of competing solvers in the evaluation process resulted in the wrong conclusion that it is able to provide fast consistency for very large qualitative spatial networks with respect to the state-of-the-art. In this paper, we review the two aforementioned approaches and provide new results that are different to the results presented in [AAAI, 2014] by properly re-evaluating them with the benchmark dataset of that paper. Thus, we establish a clear view on the state-of-the-art solutions for reasoning with large real world qualitative spatial networks efficiently, which is the main result of this paper.",https://ieeexplore.ieee.org/document/6984473/,2014 IEEE 26th International Conference on Tools with Artificial Intelligence,10-12 Nov. 2014,ieeexplore
10.23919/FUSION49465.2021.9626895,Tuning Multi Object Tracking Systems using Bayesian Optimization,IEEE,Conferences,"Tracking-by-Detection has become the major paradigm in Multi Object Tracking (MOT) for a large variety of sensors. Regardless of the type of tracking system, hyper parameters are often chosen manually instead of doing a structured search to reveal the full potential of the system.In this work we tackle this problem by utilizing Bayesian Optimization (BO) to tune tracking systems, enabling to find the best combination of hyper parameters for Gaussian Mixture Probability Hypothesis Density Trackers (GM-PHD) in two different tracking applications. We use the Tree-structured Parzen Estimator (TPE) algorithm [1] [2] with an Expected Improvement (EI) acquisition function as a blackbox optimizer. TPE supports to conveniently incorporate domain expert knowledge by modeling prior probability distributions of the search space. In our experiments we use the popular MOTA metric as optimization objective.Evaluation is performed in a simulation scenario with an in depth discussion of the found parameters and a real world example that uses the MOT-20 challenge dataset [3] demonstrates the unconditional applicability of the approach. We finish with a conclusion on Bayesian Optimization for MOT systems and future research.",https://ieeexplore.ieee.org/document/9626895/,2021 IEEE 24th International Conference on Information Fusion (FUSION),1-4 Nov. 2021,ieeexplore
10.1109/SPAWC51858.2021.9593170,Turning Channel Noise into an Accelerator for Over-the-Air Principal Component Analysis,IEEE,Conferences,"T In recent years, the attempts on distilling mobile data into useful knowledge have led to the deployment of machine learning algorithms at the network edge. Principal component analysis (PCA) is a classic technique for extracting the linear structure of a dataset, which is useful for feature extraction and data compression. In this work, we propose the deployment of distributed PCA over a multi-access channel based on the algorithm of stochastic gradient descent to learn the dominant feature space of a distributed dataset at multiple devices. Over- the-air aggregation is adopted to reduce the multi-access latency, giving the name over-the-air PCA. The novelty of this design lies in exploiting channel noise to accelerate the descent in the region around each saddle point encountered by gradient descent, thereby increasing the convergence speed of over-the-air PCA. The idea is materialized by proposing a power-control scheme controlling the level of channel noise accordingly. The scheme is proved to achieve faster convergence than in the case without power control by experiments on real datasets.",https://ieeexplore.ieee.org/document/9593170/,2021 IEEE 22nd International Workshop on Signal Processing Advances in Wireless Communications (SPAWC),27-30 Sept. 2021,ieeexplore
10.1109/VRW52623.2021.00262,Turning a Messy Room into a Fully Immersive VR Playground,IEEE,Conferences,"In this study, to enable a VR experience with an HMD even in a space with obstacles, we developed a real-time construction system of a reality-based VR space that does not impair the atmosphere of the virtual world. In addition, we aim to construct a VR space that is easier to recognize its structure by classifying ""objects that are boundaries of space"" and ""ordinary obstacles"" using a deep learning network and superimposing virtual objects corresponding to each type of real object. We implemented a real-time construction system for a VR space that reflects the distribution of objects in a real space using two depth cameras mounted on HMD, and created guidelines and applications for using the proposed system.",https://ieeexplore.ieee.org/document/9419119/,2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),27 March-1 April 2021,ieeexplore
10.1109/IEMBS.1997.757650,Two dimensional interleaved differential k-space sampling for fluoroscopic triggering of contrast-enhanced three dimensional MR angiography,IEEE,Conferences,"The purpose of this work is to demonstrate how 2D differential k-space sampling can be implemented with interleaving of the view (phase encoding) order, providing smoother temporal behavior and no spatial resolution penalty when compared to standard sequential k-space ordering. Implementation in a clinical setting provides a method for real-time bolus tracking and timing for contrast-enhanced 3D MR angiography.",https://ieeexplore.ieee.org/document/757650/,Proceedings of the 19th Annual International Conference of the IEEE Engineering in Medicine and Biology Society. 'Magnificent Milestones and Emerging Opportunities in Medical Engineering' (Cat. No.97CH36136),30 Oct.-2 Nov. 1997,ieeexplore
10.1109/ISIC.2000.882942,Two suggestions for efficient implementation of CMAC's,IEEE,Conferences,"The CMAC (Cerebellar Model Articulation Controller) suffers from two important problems: the huge amount of memory needed for its implementation in many common situations, and the lack of a systematic way for selecting appropriate values for its parameters, particularly number of quantization intervals. This paper presents two proposals for addressing these difficulties: 1) a dynamic implementation that requires memory only for those weights needed to represent the training data set, and that performs linear interpolation when a query using other weights is requested; and 2) consists of the definition of an index of correlation from which the optimal number of quantization intervals that should be assigned to each dimension of the input space that can be found. Experiments are performed for two synthetic cases and for one set of real data. These are used to model the dynamic behaviour of a real sensor-based car. Figures are given to show the memory savings and mean squared error obtained.",https://ieeexplore.ieee.org/document/882942/,Proceedings of the 2000 IEEE International Symposium on Intelligent Control. Held jointly with the 8th IEEE Mediterranean Conference on Control and Automation (Cat. No.00CH37147),19-19 July 2000,ieeexplore
10.1109/CEC.2014.6900322,Type-II opposition-based differential evolution,IEEE,Conferences,"The concept of opposition-based learning (OBL) can be categorized into Type-I and Type-II OBL methodologies. The Type-I OBL is based on the opposite points in the variable space while the Type-II OBL considers the opposite of function value on the landscape. In the past few years, many research works have been conducted on development of Type-I OBL-based approaches with application in science and engineering, such as opposition-based differential evolution (ODE). However, compared to Type-I OBL, which cannot address a real sense of opposition in term of objective value, the Type-II OBL is capable to discover more meaningful knowledge about problem's landscape. Due to natural difficulty of proposing a Type-II-based approach, very limited research has been reported in that direction. In this paper, for the first time, the concept of Type-II OBL has been investigated in detail in optimization; also it is applied on the DE algorithm as a case study. The proposed algorithm is called opposition-based differential evolution Type-II (ODE-II) algorithm; it is validated on the testbed proposed for the IEEE Congress on Evolutionary Computation 2013 (IEEE CEC-2013) contest with 28 benchmark functions. Simulation results on the benchmark functions demonstrate the effectiveness of the proposed method as the first step for further developments in Type-II OBL-based schemes.",https://ieeexplore.ieee.org/document/6900322/,2014 IEEE Congress on Evolutionary Computation (CEC),6-11 July 2014,ieeexplore
10.1109/IEIT53597.2021.00077,UAV Control in Smart City Based on Space-Air-Ground Integrated Network,IEEE,Conferences,"Unmanned Aerial Vehicle (UAV) is an important part of the wireless network system of the future smart city. As a difficult point in the large-scale application of UAV, UAV control gradually attracts people's attention. Aiming at the problems of UAV control in smart city application, a near real time online learning architecture of UAV control based on the software-defined space-air-ground integrated network (SSAG) was proposed. This architecture uses the two-layer software defined network (SDN) controller architecture of SSAG framework to separate UAV control. The upper-tier SDN controller is responsible for the scheduling of UAV configuration, while the lower-tier SDN controller is responsible for regional coordination of UAV. The upper-tier SDN controller updates the tendency of network states by acquiring network states information in time interval. By simulating the network state in the next time interval, the optimal strategy of UAV scheduling of the next time interval is obtained by using the strategy iteration algorithm. Finally, an example is given to verify that the near real-time online learning architecture can accurately predict the UAV requirement, and increase the throughput of the network system compared with the traditional approach.",https://ieeexplore.ieee.org/document/9526159/,"2021 International Conference on Internet, Education and Information Technology (IEIT)",16-18 April 2021,ieeexplore
10.1109/RCAR47638.2019.9043987,UAV Path Planning Based on Biological Excitation Neural Network and Visual Odometer,IEEE,Conferences,"Unmanned aerial vehicle(UAV) have been widely used in military and civil fields due to their compact structure, flexible mobility, low cost and other advantages. With the development of artificial intelligence in recent years, more intelligent and advanced algorithms have appeared, in which machine vision, as an important branch in the field of artificial intelligence, has also been greatly developed. The limitation of space, load, endurance and computing capacity hinders the application of intelligent algorithms on UAV. In the paper a semi-autonomous control platform of the quadrotor UAV was developed and the upper and lower dual control core architecture is implemented. Based on the hardware platform, the improved visual inertia odometer (VIO) and the biological excitation neural network are used to improve the flight performance and the ability of autonomy. To solve the problem of the synchronization for VIO, a cubic spline interpolation function was employed. A biological excitation neural network was extended to solve UAV on-line path planning. It provides an on-board path planning approach for UAV in the 3D world considering the dynamic obstacles. Finally, the feasibility and stability of the designed system were verified by flight experiments.",https://ieeexplore.ieee.org/document/9043987/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore
10.1109/AERO.2017.7943775,UAV tracking and following a ground target under motion and localisation uncertainty,IEEE,Conferences,"Unmanned Aerial Vehicles (UAVs) are increasingly being used in numerous applications, such as remote sensing, environmental monitoring, ecology and search and rescue missions. Effective use of UAVs depends on the ability of the system to navigate in the mission scenario, especially if the UAV is required to navigate autonomously. There are particular scenarios in which UAV navigation faces challenges and risks. This creates the need for robust motion planning capable of overcoming different sources of uncertainty. One example is a UAV flying to search, track and follow a mobile ground target in GPS-denied space, such as below canopy or in between buildings, while avoiding obstacles. A UAV navigating under these conditions can be affected by uncertainties in its localization and motion due to occlusion of GPS signals and the use of low cost sensors. Additionally, the presence of strong winds in the airspace can disturb the motion of the UAV. In this paper, we describe and flight test a novel formulation of a UAV mission for searching, tracking and following a mobile ground target. This mission is formulated as a Partially Observable Markov Decision Process (POMDP) and implemented in real flight using a modular framework. We modelled the UAV dynamic system, the uncertainties in motion and localization of both the UAV and the target, and the wind disturbances. The framework computes a motion plan online for executing motion commands instead of flying to way-points to accomplish the mission. The system enables the UAV to plan its motion allowing it to execute information gathering actions to reduce uncertainty by detecting landmarks in the scenario, while making predictions of the mobile target trajectory and the wind speed based on observations. Results indicate that the system overcomes uncertainties in localization of both the aircraft and the target, and avoids collisions into obstacles despite the presence of wind. This research has the potential of use particularly for remote monitoring in the fields of biodiversity and ecology.",https://ieeexplore.ieee.org/document/7943775/,2017 IEEE Aerospace Conference,4-11 March 2017,ieeexplore
10.1109/SCC.2016.75,UCLAO* and BHUC: Two Novel Planning Algorithms for Uncertain Web Service Composition,IEEE,Conferences,"The inherent uncertainty of Web service is the most important characteristic due to its deployment and invocation within a real and highly dynamic Internet environment. Web service composition with uncertainty (U-WSC) has become an important research issue in service computing. Although some research has been done on U-WSC via non-deterministic planning in Artificial Intelligence, they cannot handle the situation that uncertain Web services with the same functionality exist in a service repository and could not get all of possible solution plans that constitute an uncertain composition solution for a given request. To solve above research challenges, this paper models a U-WSC problem into a U-WSC planning problem. Accordingly, two novel uncertain planning algorithms using heuristic search called UCLAO* and BHUC, are presented to solve the U-WSC planning problem with state space reduction, which leads to high efficiency of finding a service composition solution. We have conducted empirical experiments based on a running example in e-commerce application as well as our large-scale simulated datasets. The experimental results demonstrate that our proposed algorithms outperform the state-of-the-art non-deterministic planning algorithms in terms of effectiveness, efficiency and scalability.",https://ieeexplore.ieee.org/document/7557495/,2016 IEEE International Conference on Services Computing (SCC),27 June-2 July 2016,ieeexplore
10.1109/COMSWA.2007.382562,Ubiquitous Semantic Space: A context-aware and coordination middleware for Ubiquitous Computing,IEEE,Conferences,"Ubiquitous computing poses the challenge of increased communication, context-awareness and functionality. In a highly dynamic and weekly connected ubiquitous environment, continuous access to the network (synchronous communication) is very difficult. So it's necessary to go for tuple space which provides asynchronous communication without any loss in data. Tuple space offers a coordination infrastructure for communication between autonomous entities by providing a logically shared memory along with data persistence, transactional security as well as temporal and spatial decoupling properties that make it desirable for distributed systems [2] such as ubiquitous computing. In order to automate the task and the system to be intelligent, context awareness is a must. This can be achieved by using semantic web technology. Existing middleware's for ubiquitous computing concentrates on RPC communication paradigm and deals with context-awareness separately. In our approach of constructing the middleware we provide common solution to both communication and context-awareness using ubiquitous semantic space. Ubiquitous semantic space [5] brings together tuple space, semantic web technologies and ubiquitous computing. Hence in this paper, we introduce a context-aware and co-ordination middleware framework for ubiquitous environment using ubiquitous semantic space. Ubiquitous semantic space uses ontologies to define the semantics of various concepts. Using ontologies facilitates different agents in the environments to have a common semantic understanding of different contexts. Ontology is represented using ontology web language, OWL [6]. We have modeled a ubiquitous semantic space ontology structure suitable for communicating conceptual information among the agents. Our model also incorporates context-triggered action which is more useful for real-time ubiquitous application having reactive behavior. For enabling context-triggered action, our model has reactive space into ubiquitous semantic space. Reactive space, stores rules written in SWRL [22], semantic web rule language and fired using JESS [3] reasoner at the appropriate time. The middleware could easily adapt to changes in the environment. The structure of the ubiquitous semantic space is designed in a fashion to have privacy among the communicating devices and the agents. Hence our middleware uses a decentralized architecture which supports asynchronous communication, context-awareness, context-sensitive communication, Privacy sensitive, adaptive to context-changes and reactive to emergency situation.",https://ieeexplore.ieee.org/document/4267986/,2007 2nd International Conference on Communication Systems Software and Middleware,7-12 Jan. 2007,ieeexplore
10.1109/AIVR46125.2019.00072,Ubiquitous Virtual Humans: A Multi-platform Framework for Embodied AI Agents in XR,IEEE,Conferences,"We present an architecture and framework for the development of virtual humans for a range of computing platforms, including mobile, web, Virtual Reality (VR) and Augmented Reality (AR). The framework uses a mix of in-house and commodity technologies to support audio-visual sensing, speech recognition, natural language processing, nonverbal behavior generation and realization, text-to-speech generation, and rendering. This work builds on the Virtual Human Toolkit, which has been extended to support computing platforms beyond Windows. The resulting framework maintains the modularity of the underlying architecture, allows re-use of both logic and content through cloud services, and is extensible by porting lightweight clients. We present the current state of the framework, discuss how we model and animate our characters, and offer lessons learned through several use cases, including expressive character animation in seated VR, shared space and navigation in room-scale VR, autonomous AI in mobile AR, and real-time user performance feedback based on mobile sensors in headset AR.",https://ieeexplore.ieee.org/document/8942321/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore
10.1109/PerComWorkshops51409.2021.9431061,Ultra-fast Machine Learning Classifier Execution on IoT Devices without SRAM Consumption,IEEE,Conferences,"With the introduction of edge analytics, IoT devices are becoming smart and ready for AI applications. A few modern ML frameworks are focusing on the generation of small-size ML models (often in kBs) that can directly be flashed and executed on tiny IoT devices, particularly the embedded systems. Edge analytics eliminates expensive device-to-cloud communications, thereby producing intelligent devices that can perform energy-efficient real-time offline analytics. Any increase in the training data results in a linear increase in the size and space complexity of the trained ML models, making them unable to be deployed on IoT devices with limited memory. To alleviate the memory issue, a few studies have focused on optimizing and fine-tuning existing ML algorithms to reduce their complexity and size. However, such optimization is usually dependent on the nature of IoT data being trained. In this paper, we presented an approach that protects model quality without requiring any alteration to the existing ML algorithms. We propose SRAM-optimized implementation and efficient deployment of widely used standard/stable ML-frameworks classifier versions (e.g., from Python scikit-learn). Our initial evaluation results have demonstrated that ours is the most resource-friendly approach, having a very limited memory footprint while executing large and complex ML models on MCU-based IoT devices, and can perform ultra-fast classifications while consuming 0 bytes of SRAM. When we tested our approach by executing it on a variety of MCU-based devices, the majority of models ported and executed produced 1-4x times faster inference results in comparison with the models ported by the sklearn-porter, m2cgen, and emlearn libraries.",https://ieeexplore.ieee.org/document/9431061/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore
10.1109/CCWC47524.2020.9031228,Ultra-thin MobileNet,IEEE,Conferences,"Convolutional Neural Networks (CNNs) are deep learning architectures which play an important role in object detection, image classification, face recognition, autonomous driving applications, etc. MobileNet is a light CNN model which was developed especially for embedded vision applications. But still, it is quite challenging to deploy the baseline model into memory constrained micro-controller units. Design Space Exploration of the above-mentioned model can make it less memory and computationally intensive. This paper proposes some modifications to the existing baseline MobileNet architecture to make it more efficient and suitable to be deployed on real-time embedded platforms. The intent behind developing such an architecture is to reduce the size, the number of parameters, computation time per epoch and the overfitting problem considerably without letting the accuracy drop below the baseline accuracy level. We achieve good accuracy levels by using the Swish activation function instead of the standard activation function ReLU and introducing a regularization method called random erasing instead of Drop out into the network. We decrease the model size by using Separable Convolutions in place of Depthwise Separable Convolutions, changing the channel depth, choosing an optimum width multiplier value and eliminating some layers with the same output shape, without much drop in the accuracy levels. We train the model with the above-mentioned modifications from scratch on the CIFAR-10 dataset and obtain a much lighter architecture as compared to the baseline MobileNet V1. We name the new DNN architecture as Ultra-thin MobileNet having a size of 3.9 MB only which is deployable in real-time embedded processors with limited memory and power.",https://ieeexplore.ieee.org/document/9031228/,2020 10th Annual Computing and Communication Workshop and Conference (CCWC),6-8 Jan. 2020,ieeexplore
10.1109/IoTDI.2018.00014,UnTran: Recognizing Unseen Activities with Unlabeled Data Using Transfer Learning,IEEE,Conferences,"The success and impact of activity recognition algorithms largely depends on the availability of the labeled training samples and adaptability of activity recognition models across various domains. In a new environment, the pre-trained activity recognition models face challenges in presence of sensing bias- ness, device heterogeneities, and inherent variabilities in human behaviors and activities. Activity Recognition (AR) system built in one environment does not scale well in another environment, if it has to learn new activities and the annotated activity samples are scarce. Indeed building a new activity recognition model and training the model with large annotated samples often help overcome this challenging problem. However, collecting annotated samples is cost-sensitive and learning activity model at wild is computationally expensive. In this work, we propose an activity recognition framework, UnTran that utilizes source domains' pre-trained autoencoder enabled activity model that transfers two layers of this network to generate a common feature space for both source and target domain activities. We postulate a hybrid AR framework that helps fuse the decisions from a trained model in source domain and two activity models (raw and deep-feature based activity model) in target domain reducing the demand of annotated activity samples to help recognize unseen activities. We evaluated our framework with three real-world data traces consisting of 41 users and 26 activities in total. Our proposed UnTran AR framework achieves  75% F1 score in recognizing unseen new activities using only 10% labeled activity data in the target domain. UnTran attains  98% F1 score while recognizing seen activities in presence of only 2-3% of labeled activity samples.",https://ieeexplore.ieee.org/document/8366975/,2018 IEEE/ACM Third International Conference on Internet-of-Things Design and Implementation (IoTDI),17-20 April 2018,ieeexplore
10.1109/IV47402.2020.9304826,Uncertainty-aware Energy Management of Extended Range Electric Delivery Vehicles with Bayesian Ensemble,IEEE,Conferences,"In recent years, deep reinforcement learning (DRL) algorithms have been widely studied and utilized in the area of Intelligent Transportation Systems (ITS). DRL agents are mostly trained with transition pairs and interaction trajectories generated from simulation, and they can achieve satisfying or near optimal performances under familiar input states. However, for relative rare visited or even unvisited regions in the state space, there is no guarantee that the agent could perform well. Unfortunately, novel conditions are inevitable in real-world problems and there is always a gap between the real data and simulated data. Therefore, to implement DRL algorithms in real-world transportation systems, we should not only train the agent learn a policy that maps states to actions, but also the model uncertainty associated with each action. In this study, we adapt the method of Bayesian ensemble to train a group of agents with imposed diversity for an energy management system of a delivery vehicle. The agents in the ensemble agree well on familiar states but show diverse results on unfamiliar or novel states. This uncertainty estimation facilitates the implementation of interpretable postprocessing modules which can ensure robust and safe operations under high uncertainty conditions.",https://ieeexplore.ieee.org/document/9304826/,2020 IEEE Intelligent Vehicles Symposium (IV),19 Oct.-13 Nov. 2020,ieeexplore
10.23919/ACC45564.2020.9147911,Unmanned Aerial Vehicle Angular Velocity Control via Reinforcement Learning in Dimension Reduced Search Spaces,IEEE,Conferences,"Search space dimension reduction strategies are studied for reinforcement learning based angular velocity control of multirotor unmanned aerial vehicles. Reinforcement learning approximates the value function iteratively over the state-action space, which is 6-dimensional in the case of multirotor angular velocity control. An inverse-dynamics approach is applied to reduce the 6-dimensional state-action space to a 3-dimensional state-only search space while estimating the uncertain model parameters. The search space dimension is further reduced when the state variables are only allowed to vary following either a motion camouflage strategy or a hyperbolic tangent path. Simulation results show that the modified reinforcement learning algorithms can be implemented in real time for multirotor angular velocity control.",https://ieeexplore.ieee.org/document/9147911/,2020 American Control Conference (ACC),1-3 July 2020,ieeexplore
10.1109/ICPR48806.2021.9412661,Unsupervised Domain Adaptation for Object Detection in Cultural Sites,IEEE,Conferences,"The ability to detect objects in cultural sites from the egocentric point of view of the user can enable interesting applications for both the visitors and the manager of the site. Unfortunately, current object detection algorithms have to be trained on large amounts of labeled data, the collection of which is costly and time-consuming. While synthetic data generated from the 3D model of the cultural site can be used to train object detection algorithms, a significant drop in performance is generally observed when such algorithms are deployed to work with real images. In this paper, we consider the problem of unsupervised domain adaptation for object detection in cultural sites. Specifically, we assume the availability of synthetic labeled images and real unlabeled images for training. To study the problem, we propose a dataset containing 75244 synthetic and 2190 real images with annotations for 16 different artworks. We hence investigate different domain adaptation techniques based on image-to-image translation and feature alignment. Our analysis points out that such techniques can be useful to address the domain adaptation issue, while there is still plenty of space for improvement on the proposed dataset. We release the dataset at our web page to encourage research on this challenging topic: https://iplab.dmi.unict.it/EGO-CH-OBJ-ADAPT/.",https://ieeexplore.ieee.org/document/9412661/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore
10.1109/CCDC49329.2020.9164102,Unsupervised Feature Transfer for Batch Process Based on Geodesic Flow Kernel,IEEE,Conferences,"The problem of misalignment of the original measurement model is caused by nonlinear, time-varying characteristic of the batch process. In this paper, a method based on geodesic flow kernel (GFK) for feature transfer is proposed. By mapping data into the manifold space, the feature transfer from source domain to target domain is implemented. Distribution adaptation of real-time data and modeling data is performed to reduce the distribution difference between them. The historical data through distribution adaptation is used to establish a regression model to predict the real-time data, by which the unsupervised batch process soft sensor modeling is realized. The application of predicting the concentration of penicillin between different batches during the fermentation of penicillin demonstrated that the prediction accuracy of the model can be improved more effectively than the traditional soft sensor method.",https://ieeexplore.ieee.org/document/9164102/,2020 Chinese Control And Decision Conference (CCDC),22-24 Aug. 2020,ieeexplore
10.1109/CCNC49033.2022.9700501,Unsupervised root-cause identification of software bugs in 5G RAN,IEEE,Conferences,"Developers of complex system like 5G Radio Access Networks (RAN) need algorithms that can automatically locate the root causes of software bugs. Existing methods mainly use supervised learning to track down root causes and only a few of these provide enough information to identify the function in which a software bug occurs. Supervised learning methods work well when scenarios can be repeated, and the normal behavior is somewhat similar. In RAN, where thousands of different configurations are used, software is updated frequently, and each node has its own traffic intensity, using unsupervised learning that does not require any pre-training can be more suitable. The few existing methods that use unsupervised learning to locate the root cause of software bugs can only detect delays or software hangs, and are not able to identify the many types of bugs that occur in RAN. We propose a multi-step method that uses unsupervised learning to analyze kernel and user space traces in system logs. The methods can guide developers by suggesting top-k candidate functions that are likely to contain a software bug. Our methods, MultiSpace and CallGraph were evaluated using an advanced 5G testbed in which many different software bugs that are common in RAN, were injected. The results shows that MultiSpace and CallGraph, can detect a wider range of software bugs than previous methods and only adds an average CPU load of 1.3% on the testbed. An important aspect is also that our methods scale well with large amount of data produced by real time systems, like RAN, and can analyze the data much faster.",https://ieeexplore.ieee.org/document/9700501/,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),8-11 Jan. 2022,ieeexplore
10.1109/COGINF.2009.5250777,User-oriented healthcare support system based on symbiotic computing,IEEE,Conferences,"We propose a multi-agent-based healthcare support system in ubiquitous computing environment. By utilizing knowledge about healthcare and various information including vital sign, physical location, and video data of a user under observation from real space, the system provides useful information regarding health condition effectively and in user-oriented manner. This paper describes a user-oriented healthcare support system based on concept of symbiotic computing, focusing on design and initial prototype implementation of the system.",https://ieeexplore.ieee.org/document/5250777/,2009 8th IEEE International Conference on Cognitive Informatics,15-17 June 2009,ieeexplore
10.1109/ITT51279.2020.9320781,Using Deep Learning And Machine Learning In Space Network,IEEE,Conferences,"The UAE have achieved the goal to be a country to host its potential satellites in the space i.e. Dubaiset1, Khalifa set 1 and others by using the technologies like deep space navigation, autonomous satellite and planetary spectrum generator. But, these technologies are currently dependent on the command center which are manually operated. Thus, increasing the chance of error. Due to this, the demand of software which integrate deep learning and machine learning in the commanding centers are coming under high demand. This will make the probability of an error to a very minute percentage. The Artificial Intelligence which will be used in the Planetary Spectrum Generator and autonomous satellite to create a safe passage of communication directly from the rovers on different planets and satellites in the orbit to the command center without any human interference. The software will work with deep learning which currently all the space stations work with and will combine it with machine learning to make it into a complete network of different data collected from different satellites into the planetary spectrum generator(PSG) and thus giving the correct information with less time and perfect efficiency.",https://ieeexplore.ieee.org/document/9320781/,2020 Seventh International Conference on Information Technology Trends (ITT),25-26 Nov. 2020,ieeexplore
10.1109/ICCW.2019.8756759,Using Deep Q-Learning to Prolong the Lifetime of Correlated Internet of Things Devices,IEEE,Conferences,"Battery-powered sensors deployed in the Internet of Things (IoT) require energy-efficient solutions to prolong their lifetime. When these sensors observe a physical phenomenon distributed in space and evolving in time, the collected observations are expected to be correlated. In this paper, we propose an updating mechanism leveraging Reinforcement Learning (RL) to take advantage of the exhibited correlation in the information collected. We implement the proposed updating mechanism employing deep Q-learning. Our mechanism is capable of learning the correlation in the information collected and determine the frequency with which sensors should transmit their updates, while taking into consideration a highly dynamic environment. We evaluate our solution using environmental observations, namely temperature and humidity, obtained in a real deployment. We demonstrate that our mechanism is capable of adapting the transmission frequency of sensors' updates according to the ever-changing environment. We show that our proposed mechanism is capable of significantly extending battery-powered sensors' lifetime without compromising the accuracy of the observations provided to the IoT service.",https://ieeexplore.ieee.org/document/8756759/,2019 IEEE International Conference on Communications Workshops (ICC Workshops),20-24 May 2019,ieeexplore
10.1109/IPDPSW.2014.52,Using Physical Stigmergy in Decentralized Optimization under Multiple Non-separable Constraints: Formal Methods and an Intelligent Lighting Example,IEEE,Conferences,"In this paper, a distributed asynchronous algorithm for intelligent lighting is presented that minimizes collective power use while meeting multiple user lighting constraints simultaneously and requires very little communication among agents participating in the distributed computation. Consequently, the approach is arbitrarily scalable, adapts to exogenous disturbances, and is robust to failures of individual agents. This algorithm is an example of a decentralized primal-space algorithm for constrained non-linear optimization that achieves coordination between agents using stigmergic memory cues present in the physical system as opposed to explicit communication and synchronization. Not only does this work make of stigmergy, a property first used to describe decentralized decision making in eusocial insects, but details of the algorithm are inspired by classic social foraging theory and more recent results in eusocial-insect macronutrient regulation. This theoretical analysis in this paper guarantees that the decentralized stigmergically coupled system converges to within a finite neighborhood of the optimal resource allocation. These results are validated using a hardware implementation of the algorithm in a small-scale intelligent lighting scenario. There are other real-time distributed resource allocation applications that are amenable to these methods, like distributed power generation, in general, this paper means to provide proof of concept that physical variables in cyberphysical systems can be leveraged to reduce the communication burden of algorithms.",https://ieeexplore.ieee.org/document/6969416/,2014 IEEE International Parallel & Distributed Processing Symposium Workshops,19-23 May 2014,ieeexplore
10.1109/Indo-TaiwanICAN48429.2020.9181327,Using Quantization-Aware Training Technique with Post-Training Fine-Tuning Quantization to Implement a MobileNet Hardware Accelerator,IEEE,Conferences,"In recent years, the internet of things (IoT) has been developed near the public's life circle. At the edge device, for real-time data analysis of data, a lightweight deep learning neural network (DNN) is required. In this paper, the lightweight model MobileNet is used to design an energy efficiency hardware accelerator at the edge device. In the software framework (Tensorflow), the quantization-aware training technique with post-training fine-tuning quantization is applied to quantize the model to improve training convergence speed and parameter minimization. In hardware design considerations, fixed-point operations can reduce computational complexity and memory storage space as compared to floating-point operations, which directly affects the power consumption of the circuit. The proposed MobileNet hardware accelerator can achieve low power consumption and is suitable for the edge devices.",https://ieeexplore.ieee.org/document/9181327/,"2020 Indo  Taiwan 2nd International Conference on Computing, Analytics and Networks (Indo-Taiwan ICAN)",7-15 Feb. 2020,ieeexplore
10.1109/ICST.2017.20,Using Semantic Similarity in Crawling-Based Web Application Testing,IEEE,Conferences,"To automatically test web applications, crawling-based techniques are usually adopted to mine the behavior models, explore the state spaces or detect the violated invariants of the applications. However, their broad use is limited by the required manual configurations for input value selection, GUI state comparison and clickable detection. In existing crawlers, the configurations are usually string-matching based rules looking for tags or attributes of DOM elements, and often application-specific. Moreover, in input topic identification, it can be difficult to determine which rule suggests a better match when several rules match an input field to more than one topic. This paper presents a natural-language approach based on semantic similarity to address the above issues. The proposed approach represents DOM elements as vectors in a vector space formed by the words used in the elements. The topics of encountered input fields during crawling can then be inferred by their similarities with ones in a labeled corpus. Semantic similarity can also be applied to suggest if a GUI state is newly discovered and a DOM element is clickable under an unsupervised learning paradigm. We evaluated the proposed approach in input topic identification with 100 real-world forms and GUI state comparison with real data from industry. Our evaluation shows that the proposed approach has comparable or better performance to the conventional techniques. Experiments in input topic identification also show that the accuracy of the rule-based approach can be improved by up to 22% when integrated with our approach.",https://ieeexplore.ieee.org/document/7927970/,"2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)",13-17 March 2017,ieeexplore
10.1109/IOTSMS52051.2020.9340189,Using Siamese Networks to Detect Shading on the Edge of Solar Farms,IEEE,Conferences,"Solar power is one of the most promising sources of green power for future cities. However, real-time anomaly detection remains a challenge. Internet of Things (IoT) is an effective platform for real-time monitoring of large-scale solar farms. Using low-cost edge devices such as the Raspberry Pi (RPI), it is possible to not only read power and irradiance values from in-situ sensors, but to also apply machine learning and deep learning algorithms for real-time analysis and for detecting anomalous behaviors. This paper presents the design and implementation of an edge analytics application that uses RPI as an edge device. The Isolation Forest algorithm was first used to detect shading anomalies. A Siamese neural network was then trained to create a latent-space mapping. An anomaly detection model based on the latent space and a neural network and kNN was developed. These models could detect shading anomalies with an F1-Score of 0.94. Embedded variants of the model based on TensorFlow Lite and TensorRT were evaluated to service a large number of solar panels at 1Hz. The results are that a single RPI could do parallel anomaly detection of 512 solar panels at 1 Hz with 0% failures. The TensorRT variant consumed more resources than the TensorFlow Lite implementation, but the maximum CPU utilization remained below 75%.",https://ieeexplore.ieee.org/document/9340189/,"2020 7th International Conference on Internet of Things: Systems, Management and Security (IOTSMS)",14-16 Dec. 2020,ieeexplore
10.1109/CEC.2002.1007018,Using cultural algorithms to evolve strategies in agent-based models,IEEE,Conferences,"Cultural algorithms are self-adaptive models that support the collective evolution process through the employment of a population and a belief space. The cultural approach is applied to derive a generalized set of beliefs from successive populations of parameter configurations from an agent-based simulation of transactions within a durable goods market. The maintenance of this information allows for the guided evolution of the agent-based system over successive simulations. In order to more effectively evaluate parameter configurations, software engineering techniques of white and black box testing are applied. In this paper, a methodology for the use of cultural algorithms to optimize strategies in agent-based models is presented. This approach is demonstrated in an application used to model pricing strategies in the context of an agent-based model under a simulated real-world market scenario and a heterogeneous population.",https://ieeexplore.ieee.org/document/1007018/,Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600),12-17 May 2002,ieeexplore
10.23919/MIPRO.2019.8756928,Utilizing Apples ARKit 2.0 for Augmented Reality Application Development,IEEE,Conferences,"When it comes to practical augmented reality applications, mobile platform tools are the most deserving. Thanks to the nature of mobile devices and their everyday usage, the ideal basis for this kind of content has inadvertently formed itself. Consequently, within the iOS development environment, Apple's Xcode program enables application development using the ARKit library which delivers a host of benefits. Amongst the plethora of advantages, this paper focuses on utilizing features such as the ability to measure distances between two points in space, horizontal and vertical plane detection, the ability to detect three-dimensional objects and utilize them as triggers, and the consolidated implementation of ARKit and MapKit libraries in conjunction with the Google Places API intended for displaying superimposed computer-generated content on iOS 11 and later iterations of Apple's mobile operating system.",https://ieeexplore.ieee.org/document/8756928/,"2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",20-24 May 2019,ieeexplore
10.1109/AERO.2015.7119180,Utilizing Artificial Intelligence to achieve a robust architecture for future robotic spacecraft,IEEE,Conferences,"This paper presents a novel failure-tolerant architecture for future robotic spacecraft. It is based on the Time and Space Partitioning (TSP) principle as well as a combination of Artificial Intelligence (AI) and traditional concepts for system failure detection, isolation and recovery (FDIR). Contrary to classic payload that is separated from the platform, robotic devices attached onto a satellite become an integral part of the spacecraft itself. Hence, the robot needs to be integrated into the overall satellite FDIR concept in order to prevent fatal damage upon hardware or software failure. In addition, complex dexterous manipulators as required for onorbit servicing (OOS) tasks may reach unexpected failure states, where classic FDIR methods reach the edge of their capabilities with respect to successfully detecting and resolving them. Combining, and partly replacing traditional methods with flexible AI approaches aims to yield a control environment that features increased robustness, safety and reliability for space robots. The developed architecture is based on a modular on-board operational framework that features deterministic partition scheduling, an OS abstraction layer and a middleware for standardized inter-component and external communication. The supervisor (SUV) concept is utilized for exception and health management as well as deterministic system control and error management. In addition, a Kohonen self-organizing map (SOM) approach was implemented yielding a real-time robot sensor confidence analysis and failure detection. The SOM features nonsupervized training given a typical set of defined world states. By compiling a set of reviewable three-dimensional maps, alternative strategies in case of a failure can be found, increasing operational robustness. As demonstrator, a satellite simulator was set up featuring a client satellite that is to be captured by a servicing satellite with a 7-DoF dexterous manipulator. The avionics and robot control were integrated on an embedded, space-qualified Airbus e.Cube on-board computer. The experiments showed that the integration of SOM for robot failure detection positively complemented the capabilities of traditional FDIR methods.",https://ieeexplore.ieee.org/document/7119180/,2015 IEEE Aerospace Conference,7-14 March 2015,ieeexplore
10.1109/AERO47225.2020.9172453,Utilizing Reinforcement Learning to Autonomously Mange Buffers in a Delay Tolerant Network Node,IEEE,Conferences,"In order to effectively communicate with Earth from deep space there is a need for network automation similar to that of the Internet. The existing automated network protocols, such as TCP and IP, cannot work in deep space due to the assumptions under which they were designed. Specifically, protocols assume the existence of an end-to-end path between the source and destination for the entirety of a communication session and the path being traversable in a negligible amount of time. In contrast, a Delay Tolerant Network is a set of protocols that allows networking in environments where links suffer from high-delay or disruptions (e.g. Deep Space). These protocols rely on different assumptions such as time synchronization and suitable memory allocation. In this paper, we consider the problem of autonomously avoiding memory overflows in a Delay Tolerant Node. To that end, we propose using Reinforcement Learning to automate buffer management given that we can easily measure the relative rates of data coming in and out of the DTN node. In the case of detecting overflow, we let the autonomous agent choose between three actions: slowing down the client, requesting more resources from the Deep Space Network, or selectively dropping packets once the buffer nears capacity. Furthermore, we show that all of these actions can be realistically implemented in real-life operations given current and planned capabilities of Delay Tolerant Networking and the Deep Space Network. Similarly, we also show that using Reinforcement Learning for this problem is well suited to this application due to the number of possible states and variables, as well as the fact that large distances between deep space spacecraft and Earth prevent human-in-the-loop intervention.",https://ieeexplore.ieee.org/document/9172453/,2020 IEEE Aerospace Conference,7-14 March 2020,ieeexplore
10.1109/ICCV48922.2021.00239,V-DESIRR: Very Fast Deep Embedded Single Image Reflection Removal,IEEE,Conferences,"Real world images often gets corrupted due to unwanted reflections and their removal is highly desirable. A major share of such images originate from smart phone cameras capable of very high resolution captures. Most of the existing methods either focus on restoration quality by compromising on processing speed and memory requirements or, focus on removing reflections at very low resolutions, there by limiting their practical deploy-ability. We propose a light weight deep learning model for reflection removal using a novel scale space architecture. Our method processes the corrupted image in two stages, a Low Scale Sub-network (LSSNet) to process the lowest scale and a Progressive Inference (PI) stage to process all the higher scales. In order to reduce the computational complexity, the sub-networks in PI stage are designed to be much shallower than LSSNet. Moreover, we employ weight sharing between various scales within the PI stage to limit the model size. This also allows our method to generalize to very high resolutions without explicit retraining. Our method is superior both qualitatively and quantitatively compared to the state of the art methods and at the same time 20 faster with 50 less number of parameters compared to the most recent state-of-the-art algorithm RAGNet. We implemented our method on an android smart phone, where a high resolution 12 MP image is restored in under 5 seconds.",https://ieeexplore.ieee.org/document/9710425/,2021 IEEE/CVF International Conference on Computer Vision (ICCV),10-17 Oct. 2021,ieeexplore
10.1109/ICRA.2019.8793556,VPE: Variational Policy Embedding for Transfer Reinforcement Learning,IEEE,Conferences,"Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffer from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments.We consider the problem of transferring knowledge within a family of similar Markov decision processes. We assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.",https://ieeexplore.ieee.org/document/8793556/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore
10.1109/ICPR48806.2021.9412896,Variational Inference with Latent Space Quantization for Adversarial Resilience,IEEE,Conferences,"Despite their tremendous success in modelling high-dimensional data manifolds, deep neural networks suffer from the threat of adversarial attacks - Existence of perceptually valid input-like samples obtained through careful perturbation that lead to degradation in the performance of the underlying model. Major concerns with existing defense mechanisms include non-generalizability across different attacks, models and large inference time. In this paper, we propose a generalized defense mechanism capitalizing on the expressive power of regularized latent space based generative models. We design an adversarial filter, devoid of access to classifier and adversaries, which makes it usable in tandem with any classifier. The basic idea is to learn a Lipschitz constrained mapping from the data manifold, incorporating adversarial perturbations, to a quantized latent space and re-map it to the true data manifold. Specifically, we simultaneously auto-encode the data manifold and its perturbations implicitly through the perturbations of the regularized and quantized generative latent space, realized using variational inference. We demonstrate the efficacy of the proposed formulation in providing resilience against multiple attack types (black and white box) and methods, while being almost real-time. Our experiments show that the proposed method surpasses the state-of-the-art techniques in several cases. The implementation code is available at - https://github.com/mayank31398/lqvae.",https://ieeexplore.ieee.org/document/9412896/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore
10.1109/BigData52589.2021.9671727,Variational Open Set Recognition (VOSR),IEEE,Conferences,"Open set recognition models address the real-world scenario where classes of data unobserved during training are encountered in testing after deployment. Closed set classifiers wrongly attempt to classify instances from an unknown class as belonging to one of the known classes from the training set, which reduces the models accuracy. Ideally, these unknown instances should be recognized as such, while known instances should continue to be accurately classified. Unfortunately, state-of-the-art open set methods solve this problem by making restrictive assumptions on the variance and/or boundedness of the distributions of known classes. In this paper, we propose a novel method, Variational Open-Set Recognition (VOSR) that eliminates these assumptions. VOSR incorporates a closed set classifier, an unknown detector, and a novel Structured Gaussian Mixture Variational Autoencoder (SGM-VAE) that guarantees separable class distributions with known variances in its la-tent space. Further, by encouraging a large distance between class-specific distributions, VOSR increases the likelihood that instances from unknown classes lie in low-probability regions and thus are more readily identifiable. In rigorous evaluation, we demonstrate that VOSR outperforms state-of-the-art open set classifiers with up to a 14% F1 score increase in identifying instances from unknown classes in multiple image classification and human activity recognition datasets.",https://ieeexplore.ieee.org/document/9671727/,2021 IEEE International Conference on Big Data (Big Data),15-18 Dec. 2021,ieeexplore
10.1109/ICCV.2005.246,Vector boosting for rotation invariant multi-view face detection,IEEE,Conferences,"In this paper, we propose a novel tree-structured multiview face detector (MVFD), which adopts the coarse-to-fine strategy to divide the entire face space into smaller and smaller subspaces. For this purpose, a newly extended boosting algorithm named vector boosting is developed to train the predictors for the branching nodes of the tree that have multicomponents outputs as vectors. Our MVFD covers a large range of the face space, say, +/-45/spl deg/ rotation in plane (RIP) and +/-90/spl deg/ rotation off plane (ROP), and achieves high accuracy and amazing speed (about 40 ms per frame on a 320 /spl times/ 240 video sequence) compared with previous published works. As a result, by simply rotating the detector 90/spl deg/, 180/spl deg/ and 270/spl deg/, a rotation invariant (360/spl deg/ RIP) MVFD is implemented that achieves real time performance (11 fps on a 320 /spl times/ 240 video sequence) with high accuracy.",https://ieeexplore.ieee.org/document/1541289/,Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1,17-21 Oct. 2005,ieeexplore
10.1109/SISY50555.2020.9217076,Vehicle Control in Highway Traffic by Using Reinforcement Learning and Microscopic Traffic Simulation,IEEE,Conferences,"The paper presents a simple yet powerful and intelligent driver agent, designed to operate in a preset highway situation using Policy Gradient Reinforcement Learning (RL) agent. The goal is to navigate safely in dense highway traffic and proceed through the defined length with the shortest time possible. The algorithm uses a dense neural network as a function approximator for the agent with discrete action space on the control level, e.g., acceleration and steering. The developed simulation environment uses the open-source traffic simulator called Simulation of Urban MObility (SUMO), integrated with an interface, to interact with the agent in real-time. With this tool, numerous driving and highway situations can be created and fed to the agent from which it can learn. The environment opens the opportunity to randomize and customize the other road users' behavior. Thus the experience can be more diverse, and thus the representation becomes more general. The article describes the modeling environment, the details on the learning agent, and the rewarding scheme. After evaluating the experiences gained from the training, some ideas for optimization and further development goals are also proposed.",https://ieeexplore.ieee.org/document/9217076/,2020 IEEE 18th International Symposium on Intelligent Systems and Informatics (SISY),17-19 Sept. 2020,ieeexplore
10.1109/SAMI.2016.7422985,Vehicle navigation by fuzzy cognitive maps using sonar and RFID technologies,IEEE,Conferences,"Emerging concept of the so-called intelligent space (IS) offers means for use of mobile autonomous devices like vehicles or robots in a very broad area without necessity for these devices to own all necessary sensors. From this reason also new navigation methods are developing, which utilize IS means, with the aim to offer maybe not so accurate but first of all cheep and reliable solutions for a wide variety of devices. Our paper deals with the examination of possibility to interconnect sparsely deployed RFID tags with sonars. As signals produced by these two technologies are often affected by uncertainty and incompleteness we use fuzzy logic for their processing as well as control of the entire navigation process. For this purpose a special type of a fuzzy cognitive map was proposed. The paper describes real navigation experiments with a simple vehicle and evaluates them by selected criteria. Based on obtained results their explanations and conclusions for potential future research are sketched.",https://ieeexplore.ieee.org/document/7422985/,2016 IEEE 14th International Symposium on Applied Machine Intelligence and Informatics (SAMI),21-23 Jan. 2016,ieeexplore
10.1109/ICNNSP.2008.4590383,Video object matching based on SIFT algorithm,IEEE,Conferences,"SIFT (Scale Invariant Feature Transform) is used to solve visual tracking problem, where the appearances of the tracked object and scene background change during tracking. The implementation of this algorithm has five major stages: scale-space extrema detection; keypoint localization; orientation assignment; keypoint descriptor; keypoint matching. From the beginning frame, object is selected as the template, its SIFT features are computed. Then in the following frames, the SIFT features are computed. Euclidean distance between the objects SIFT features and the frames SIFT features can be used to compute the accurate position of the matched object. The experimental results on real video sequences demonstrate the effectiveness of this approach and show this algorithm is of higher robustness and real-time performance. It can solve the matching problem with translation, rotation and affine distortion between images. It plays an important role in video object tracking and video object retrieval.",https://ieeexplore.ieee.org/document/4590383/,2008 International Conference on Neural Networks and Signal Processing,7-11 June 2008,ieeexplore
10.1109/CIMCA.2005.1631331,Virtual Hand: An Interface for Interactive Sketching in Virtual Reality,IEEE,Conferences,"This paper describes a virtual environment in which a designer can define the contour of a sketch by controlling a pointer using a pair of data gloves in 3D space. Most standard input devises like joysticks, mice, keyboards, trackballs, and light pens do not imitate natural hand motions such as drawing and sketching. The methodology used is to construct an interactive 3D model from a combination of sketches. The sketches are drawn in 3D space using the user's hands as dynamic input devices. The above-mentioned approach is mainly consists of two parts: hand gesture recognition and implementation of a virtual hand. Our focus is to examine the sketching behaviours using hand gestures, where a virtual hand generates 3D models through the exploration of a number of hand drawn sketches",https://ieeexplore.ieee.org/document/1631331/,"International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)",28-30 Nov. 2005,ieeexplore
10.1109/ACC.2006.1657291,Virtual air-fuel ratio sensors for engine control and diagnostics,IEEE,Conferences,"Virtual air-fuel ratio sensors for an internal combustion engine using recurrent neural and wavelet networks have been developed. A nonlinear state-space modeling strategy is proposed for the architecture of the stated recurrent neural network which is trained using some variants of real time recurrent learning (RTRL) algorithm. A two-stage training approach is proposed for improving the accuracy of the RNN topology. Additionally, wavelets as activation functions have been employed to construct a single-layer network called wavenet. The wavenet is used to model the exhaust air-fuel ratio that has proved a more challenging task in a purely neural net-based architecture using sigmoid activation functions. The methodology has been implemented in a V8 spark ignition engine through rapid prototyping tools for the real time generalization and performance evaluation. Observations and comments are made on the test patterns used for the training. Some of the limitations of such a data driven approach are highlighted. Representative experimental results for the 8-cylinder engine test data are listed. The virtual sensor may be used for more precise average air-fuel ratio control and enhanced reliability engendered through the diagnostic capabilities of the sensor",https://ieeexplore.ieee.org/document/1657291/,2006 American Control Conference,14-16 June 2006,ieeexplore
10.1109/ROBOT.1995.525277,Vision-based reinforcement learning for purposive behavior acquisition,IEEE,Conferences,"This paper presents a method of vision-based reinforcement learning by which a robot learns to shoot a ball into a goal, and discusses several issues in applying the reinforcement learning method to a real robot with vision sensor. First, a ""state-action deviation"" problem is found as a form of perceptual aliasing in constructing the state and action spaces that reflect the outputs from physical sensors and actuators, respectively. To cope with this, an action set is constructed in such a way that one action consists of a series of the same action primitive which is successively executed until the current state changes. Next, to speed up the learning time, a mechanism of learning form easy missions (or LEM) which is a similar technique to ""shaping"" in animal learning is implemented. LEM reduces the learning time from the exponential order in the size of the state space to about the linear order in the size of the state space. The results of computer simulations and real robot experiments are given.",https://ieeexplore.ieee.org/document/525277/,Proceedings of 1995 IEEE International Conference on Robotics and Automation,21-27 May 1995,ieeexplore
10.1109/FSKD.2017.8393254,Visual control system design of wheeled inverted pendulum robot based on Beaglebone Black,IEEE,Conferences,"The wheeled inverted pendulum robot has broad prospects of applications in real life. It can use two coaxial wheels to achieve the body self-balancing, forward moving and turning. But the general wheeled inverted pendulum robot seldom has vision function to perceive enviromental change. In order to realize the robust visual control, a wheeled inverted-pendulum vision robot with attitude sensors, photoelectric encoders, ultrasonic sensors and so on is designed based on Beaglebone Black board. The moving object is separated in the space domain by obtaining the image sequence which is sent by a robot-mounted camera, and the modeling, identification and tracking of target sequence are implemented in the time domain. The balance PD, speed PI and steering PD controllers are designed to realize the dynamic balance, forward and steering function of the robot. To satisfy the functional requirements of the visual tracking system, an improved tracking-learning-detection algorithm based on kernelized correlation filtering is used, and a tracking anomaly based on spatial context is detected to determine the tracking state and reduce the error rate. Experimental results show that the robot reaches the requirement of design and achieves better visual control effectiveness.",https://ieeexplore.ieee.org/document/8393254/,"2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)",29-31 July 2017,ieeexplore
10.1109/ICIAP.2003.1234077,Visual self-localisation using automatic topology construction,IEEE,Conferences,"The paper proposes a machine learning method for self-localising a mobile agent, using the images supplied by a single omni-directional camera. The images acquired by the camera may be viewed as an implicit topological representation of the environment. The environment is a priori unknown and the topological representation is derived by unsupervised neural network architecture. The architecture includes a self-organising neural network, and is constituted by a growing neural gas, which is well known for its topology preserving quality. The growth depends on the topology that is not a priori defined, and on the need of discovering it, by the neural network, during the learning. The implemented system is able to recognise correctly the input frames and to reconstruct a topological map of the environment. Each node of the neural network identifies a single zone of the environment and the connections between the nodes correspond to the real space connections in the environment.",https://ieeexplore.ieee.org/document/1234077/,"12th International Conference on Image Analysis and Processing, 2003.Proceedings.",17-19 Sept. 2003,ieeexplore
10.1109/INFOCOM41043.2020.9155483,Voiceprint Mimicry Attack Towards Speaker Verification System in Smart Home,IEEE,Conferences,"The advancement of voice controllable systems (VC-Ses) has dramatically affected our daily lifestyle and catalyzed the smart home's deployment. Currently, most VCSes exploit automatic speaker verification (ASV) to prevent various voice attacks (e.g., replay attack). In this study, we present VMask, a novel and practical voiceprint mimicry attack that could fool ASV in smart home and inject the malicious voice command disguised as a legitimate user. The key observation behind VMask is that the deep learning models utilized by ASV are vulnerable to the subtle perturbations in the voice input space. To generate these subtle perturbations, VMask leverages the idea of adversarial examples. Then by adding the subtle perturbations to the recordings from an arbitrary speaker, VMask can mislead the ASV into classifying the crafted speech samples, which mirror the former speaker for human, as the targeted victim. Moreover, psychoacoustic masking is employed to manipulate the adversarial perturbations under human perception threshold, thus making victim unaware of ongoing attacks. We validate the effectiveness of VMask by performing comprehensive experiments on both grey box (VGGVox) and black box (Microsoft Azure Speaker Verification) ASVs. Additionally, a real-world case study on Apple HomeKit proves the VMask's practicability on smart home platforms.",https://ieeexplore.ieee.org/document/9155483/,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications,6-9 July 2020,ieeexplore
10.1109/CIG.2018.8490413,Wall Building in the Game of StarCraft with Terrain Considerations,IEEE,Conferences,"StarCraft is a Real-Time Strategy game, which has a large state-space, is played in real-time, and commonly features two opposing players, capable of acting simultaneously. One of the aspects of the game is building walls. In this paper, we present an algorithm that can be used for wall building for an agent playing the game of StarCraft: Brood War.",https://ieeexplore.ieee.org/document/8490413/,2018 IEEE Conference on Computational Intelligence and Games (CIG),14-17 Aug. 2018,ieeexplore
10.1109/CVPR.2011.5995702,What you saw is not what you get: Domain adaptation using asymmetric kernel transforms,IEEE,Conferences,"In real-world applications, what you saw during training is often not what you get during deployment: the distribution and even the type and dimensionality of features can change from one dataset to the next. In this paper, we address the problem of visual domain adaptation for transferring object models from one dataset or visual domain to another. We introduce ARC-t, a flexible model for supervised learning of non-linear transformations between domains. Our method is based on a novel theoretical result demonstrating that such transformations can be learned in kernel space. Unlike existing work, our model is not restricted to symmetric transformations, nor to features of the same type and dimensionality, making it applicable to a significantly wider set of adaptation scenarios than previous methods. Furthermore, the method can be applied to categories that were not available during training. We demonstrate the ability of our method to adapt object recognition models under a variety of situations, such as differing imaging conditions, feature types and codebooks.",https://ieeexplore.ieee.org/document/5995702/,CVPR 2011,20-25 June 2011,ieeexplore
10.1109/LCNW.2014.6927697,Where is that car parked? A wireless sensor network-based approach to detect car positions,IEEE,Conferences,"The global trend of increased urbanization makes space rare in city environments in general and for parking in particular. In addition, cars become bigger and often use more than one parking space. As a result neighboring parking spaces can be affected by a parked car. So, a basically free parking space might be too narrow for an arriving car depending on the arriving car's size. Therefore, means to detect car positions on parking spaces in a fine granular way are required to detect such situations and avoid inefficient parking space searches. Wireless sensor networks provide the possibility to sense the exact occupation of a parking space and potential influences on neighboring parking spaces. However, current solutions focus only on the detection if a parking space is occupied or not. In our work, we present a sensor deployment and a machine learning-based approach able to provide the mentioned more fine-granular detection level. We have conducted an extensive real-world evaluation of our solution, in particular considering different characteristics of today's car bodies. In our tests, our approach achieved an accuracy of more than 98%.",https://ieeexplore.ieee.org/document/6927697/,39th Annual IEEE Conference on Local Computer Networks Workshops,8-11 Sept. 2014,ieeexplore
10.1109/ICC.2018.8422973,WiParkFind: Finding Empty Parking Slots Using WiFi,IEEE,Conferences,"With ever increasing number of vehicles, shortage of parking space is becoming a serious problem. Going to shopping, school, and workplace can be a headache as finding an available parking spot is getting harder causing wasted time and gas. In this paper, we present WiParkFind: a low-cost, non-intrusive, and real- time parking occupancy monitoring system based on WiFi signals. The channel state information (CSI) of received WiFi signals is analyzed by using a machine learning technique to capture distinctive characteristics of CSI data that are strongly correlated with the number of empty parking slots in order to detect whether there is an empty slot, and how many empty slots are available. Compared with contemporary approaches based on magnetic sensors deployed on individual parking slots, WiParkFind utilizes low-cost off- the-shelf WiFi devices, dramatically reducing the cost for purchasing, installing, and maintaining a large number of sensors, and backend server systems. A proof-of-concept system of WiParkFind was developed and deployed in a department parking lot. The results demonstrate that the average classification accuracy of WiParkFind over a week of data collection is 78.2%, and the accuracy is improved to 90.8% with a tolerance of one empty slot.",https://ieeexplore.ieee.org/document/8422973/,2018 IEEE International Conference on Communications (ICC),20-24 May 2018,ieeexplore
10.1145/3125503.3125568,Work-in-progress: testing autonomous cyber-physical systems using fuzzing features from convolutional neural networks,IEEE,Conferences,Autonomous cyber-physical systems rely on modern machine learning methods such as deep neural networks to control their interactions with the physical world. Testing of such intelligent cyberphysical systems is a challenge due to the huge state space associated with high-resolution visual sensory inputs. We demonstrate how fuzzing the input using patterns obtained from the convolutional filters of an unrelated convolutional neural network can be used to test computer vision algorithms implemented in intelligent cyber-physical systems. Our method discovers interesting counterexamples to a pedestrian detection algorithm implemented in the popular OpenCV library. Our approach also unearths counterexamples to the correct behavior of an autonomous car similar to NVIDIA's end-to-end self-driving deep neural net running on the Udacity open-source simulator.,https://ieeexplore.ieee.org/document/8094374/,2017 International Conference on Embedded Software (EMSOFT),15-20 Oct. 2017,ieeexplore
10.1109/ASE.2019.00077,Wuji: Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning,IEEE,Conferences,"Game testing has been long recognized as a notoriously challenging task, which mainly relies on manual playing and scripting based testing in game industry. Even until recently, automated game testing still remains to be largely untouched niche. A key challenge is that game testing often requires to play the game as a sequential decision process. A bug may only be triggered until completing certain difficult intermediate tasks, which requires a certain level of intelligence. The recent success of deep reinforcement learning (DRL) sheds light on advancing automated game testing, without human competitive intelligent support. However, the existing DRLs mostly focus on winning the game rather than game testing. To bridge the gap, in this paper, we first perform an in-depth analysis of 1349 real bugs from four real-world commercial game products. Based on this, we propose four oracles to support automated game testing, and further propose Wuji, an on-the-fly game testing framework, which leverages evolutionary algorithms, DRL and multi-objective optimization to perform automatic game testing. Wuji balances between winning the game and exploring the space of the game. Winning the game allows the agent to make progress in the game, while space exploration increases the possibility of discovering bugs. We conduct a large-scale evaluation on a simple game and two popular commercial games. The results demonstrate the effectiveness of Wuji in exploring space and detecting bugs. Moreover, Wuji found 3 previously unknown bugs, which have been confirmed by the developers, in the commercial games.",https://ieeexplore.ieee.org/document/8952543/,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),11-15 Nov. 2019,ieeexplore
10.1109/CSCWD49262.2021.9437611,Zero-Shot Learning Based on Knowledge Sharing,IEEE,Conferences,"Zero-Shot Learning (ZSL) is an emerging research that aims to solve the classification problems with very few training data. The present works on ZSL mainly focus on the mapping of learning semantic space to visual space. It encounters many challenges that obstruct the progress of ZSL research. First, the representation of the semantic feature is inadequate to represent all features of the categories. Second, the domain shift problem still exists during the transfer from semantic space to visual space. In this paper, we introduce knowledge sharing (KS) to enrich the representation of semantic features. Based on KS, we apply a generative adversarial network to generate pseudo visual features from semantic features that are very close to the real visual features. Abundant experimental results from two benchmark datasets of ZSL show that the proposed approach has a consistent improvement.",https://ieeexplore.ieee.org/document/9437611/,2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD),5-7 May 2021,ieeexplore
10.1109/AI-CSP52968.2021.9671151,i-DERASSA: e-learning Platform based on Augmented and Virtual Reality interaction for Education and Training,IEEE,Conferences,"With the Coronavirus 2019 disease (COVID-19) spread, causing a world pandemic, e-learning can provide an optimal education and training solution. COVID-19 has wholly disrupted the education system. Switching to e-learning could be the enabler to create a new, more effective method of educating students when correctly applied. This paper proposes an elearning platform based on 3D interaction using augmented reality (AR) and virtual reality (VR) designed to meet particular learning objectives divided into levels and subjects, which aims to facilitate the teaching process and administrative workload in schools. We provide details on the platform, VR and AR courses/exercises conception and implementation. Many concepts may be hard to explain in a classroom or visualise in a textbook, e.g., anatomical concepts, molecular structures, space phenomena, or complex abstract topics. The AR and VR technologies make it much easier to achieve, creating a rich, interactive experience that combines real and virtual worlds. VR that provides information through realistic 3D models in immersive environments can better present concepts and skills. Our main contribution is the integration of AR and VR interaction in the web; we develop courses/ exercises selected from the Algerian Ministry of Education and Teaching learning program based on 3D interaction in virtual and augmented scenes. We hope by the integration of VR and AR into education attempts to increase the level of participation and maybe even the understanding of abstract and complex concepts.",https://ieeexplore.ieee.org/document/9671151/,2021 International Conference on Artificial Intelligence for Cyber Security Systems and Privacy (AI-CSP),20-21 Nov. 2021,ieeexplore
10.1109/ICDCS.2008.40,iSky: Efficient and Progressive Skyline Computing in a Structured P2P Network,IEEE,Conferences,"An interesting problem in peer-based data management is efficient support for skyline queries within a multiattribute space. A skyline query retrieves from a set of multidimensional data points a subset of interesting points, compared to which no other points are better. Skyline queries play an important role in multi-criteria decision making and user preference applications. In this paper, we address the skyline computing problem in a structured P2P network. We exploit the iMinMax(thetas) transformation to map high-dimensional data points to 1-dimensional values. All transformed data points are then distributed on a structured P2P network called BATON, where all peers are virtually organized as a balanced binary search tree. Subsequently, a progressive algorithm is proposed to compute skyline in the distributed P2P network. Further, we propose an adaptive skyline filtering technique to reduce both processing cost and communication cost during distributed skyline computing. Our performance study, with both synthetic and real datasets, shows that the proposed approach can dramatically reduce transferred data volume and gain quick response time.",https://ieeexplore.ieee.org/document/4595880/,2008 The 28th International Conference on Distributed Computing Systems,17-20 June 2008,ieeexplore
10.1109/RCAR52367.2021.9517479,sEMG-based Gesture Recognition by Rotation Forest-Based Extreme Learning Machine,IEEE,Conferences,"The motion information contained in surface electromyography (sEMG) signals contributes significantly to the prosthetic hand control. However, the accuracy and speed of gesture recognition from sEMG signals are still insufficient for natural control. In order to alleviate this problem, this paper propose a rotation forest-based extreme learning machine method (RoF-ELM) to improve the recognition performance based on sEMG signals. Firstly, the active motion segments were picked out and pre-processed by sliding window. Then, 104 features were extracted from each sample, and SVM-RFE method was used to reduce the feature space dimension. Finally, the RoF-ELM classification model was constructed and tested based on the EMG data for gestures Data Set containing a total of 72 recordings, each of which consists of six basic gestures. 100 trials were implemented to validate the performance of the proposed method, the results show that the RoF-ELM method have the highest accuracy (91.11%) across different subjects with relatively short runtime compared with decision tree (DT), ELM, random forest (RF), and RoF methods.",https://ieeexplore.ieee.org/document/9517479/,2021 IEEE International Conference on Real-time Computing and Robotics (RCAR),15-19 July 2021,ieeexplore
10.1109/IDEA49133.2020.9170686,Error back propagation based Recurrent Neural Networks for Intrusion Detection system,IEEE,Conferences,"Internet usage is increasing daily, which adds to the system's vulnerability. Network security is always a major problem for network administrators and is constantly changing due to the additional application space and requirements of a smart and efficient network. Simple and more efficient software tools include victims on the security side of the protocol that hackers use to perform various types of attacks on the network. The purpose of this review is to establish and coordinate processes to prevent one member against new and known attacks, and to act as a separate security system or an independent network. The neural network connects the body's immune system to receive memory, errors, and synchronized learning. This paper discusses interrelated processes that are largely based on the application of artificial intelligence for intrusion detection and learning processes.Proposed approach finds the type of session i.e. either normal or intrusion where if intrusion found than class of intrusion was detected. Here artificial neural network was used for finding the patterns in the input data. In this work Back propagation is used for the ANN in recursive manner. Proposed algorithm gives an effective framework which has been a promising one for distinguishing interruption of various kind where, one can get the detail of the class of attack also. Experiment has been conduced on real data set where various set of testing data were pass for comparison on different evaluation parameters. Proposed approach detects all sort of attacks applied on the network such as DoS (Denial of service), (R2L) Remote to local, (U2R) User to remote, Probe etc. In this work a Random Forest Tree is used for the detection of intrusion in network. Proposed approach improved 6.87% accuracy, 12.06% Precision and 1.15% recall.",https://ieeexplore.ieee.org/document/9170686/,"2nd International Conference on Data, Engineering and Applications (IDEA)",28-29 Feb. 2020,ieeexplore
10.1109/FUZZY.2008.4630539,Tell me the important stuff - fuzzy ontologies and personal assessments for interaction with the semantic web,IEEE,Conferences,"The semantic Web attempts to make the Web an universal medium of data exchange. To achieve this it needs to both make data sources accessible to machine-based systems, such as software agents and allow humans to easily create, understand and search for these data sources. Crisp ontologies are extremely useful for improving data extraction from structured data. However many ontological approaches require an unnatural level of precision and rigidity when dealing with real user queries or data sources. Previous work has suggested that fuzzification of ontologies may increase their utility. A major area of activity in the search and ubiquitous computing space is the development of location aware services. This paper suggests that in the mobile and context-aware semantic Web environment, fuzzification needs to extend to the representation of the importance of particular. Additionally, mobile devices tend to require simple interfaces and work with low bandwidth, this implies that obtaining small numbers of relevant results is extremely important. The work previously done in representing uncertainty in geographical information systems may assist in this development. This paper suggest that the combination of the use of fuzzy ontologies and fuzzy spatial relations may be effective in increasing the usefulness of the mobile semantic Web.",https://ieeexplore.ieee.org/document/4630539/,2008 IEEE International Conference on Fuzzy Systems (IEEE World Congress on Computational Intelligence),1-6 June 2008,ieeexplore
10.1109/TII.2018.2884951,3-D Deployment Optimization for Heterogeneous Wireless Directional Sensor Networks on Smart City,IEEE,Journals,"The development of smart cities and the emergence of three-dimensional (3-D) urban terrain data have introduced new requirements and issues to the research on the 3-D deployment of wireless sensor networks. We study the deployment issue of heterogeneous wireless directional sensor networks in 3-D smart cities. Traditionally, studies on the deployment problem of WSNs focus on omnidirectional sensors on a 2-D plane or in full 3-D space. Based on 3-D urban terrain data, we transform the deployment problem into a multiobjective optimization problem, in which objectives of Coverage, Connectivity Quality, and Lifetime, as well as the Connectivity and Reliability constraints, are simultaneously considered. A graph-based 3-D signal propagation model employing the line-of-sight concept is used to calculate the signal path loss. Novel distributed parallel multiobjective evolutionary algorithms (MOEAs) are also proposed. For verification, real-world and artificial urban terrains are utilized. In comparison with other state-of-the-art MOEAs, the novel algorithms could more effectively and more efficiently address the deployment problem in terms of optimization performance and operation time.",https://ieeexplore.ieee.org/document/8558102/,IEEE Transactions on Industrial Informatics,March 2019,ieeexplore
10.1109/OJCOMS.2020.3010270,"6G Wireless Communication Systems: Applications, Requirements, Technologies, Challenges, and Research Directions",IEEE,Journals,"The demand for wireless connectivity has grown exponentially over the last few decades. Fifth-generation (5G) communications, with far more features than fourth-generation communications, will soon be deployed worldwide. A new paradigm of wireless communication, the sixth-generation (6G) system, with the full support of artificial intelligence, is expected to be implemented between 2027 and 2030. Beyond 5G, some fundamental issues that need to be addressed are higher system capacity, higher data rate, lower latency, higher security, and improved quality of service (QoS) compared to the 5G system. This paper presents the vision of future 6G wireless communication and its network architecture. This article describes emerging technologies such as artificial intelligence, terahertz communications, wireless optical technology, free-space optical network, blockchain, three-dimensional networking, quantum communications, unmanned aerial vehicles, cell-free communications, integration of wireless information and energy transfer, integrated sensing and communication, integrated access-backhaul networks, dynamic network slicing, holographic beamforming, backscatter communication, intelligent reflecting surface, proactive caching, and big data analytics that can assist the 6G architecture development in guaranteeing the QoS. Besides, expected applications with 6G communication requirements and possible technologies are presented. We also describe potential challenges and research directions for achieving this goal.",https://ieeexplore.ieee.org/document/9144301/,IEEE Open Journal of the Communications Society,2020,ieeexplore
10.1109/TNSRE.2017.2697415,A 128-Channel FPGA-Based Real-Time Spike-Sorting Bidirectional Closed-Loop Neural Interface System,IEEE,Journals,"A multichannel neural interface system is an important tool for various types of neuroscientific studies. For the electrical interface with a biological system, high-precision high-speed data recording and various types of stimulation capability are required. In addition, real-time signal processing is an important feature in the implementation of a real-time closed-loop system without unwanted substantial delay for feedback stimulation. Online spike sorting, the process of assigning neural spikes to an identified group of neurons or clusters, is a necessary step to make a closed-loop path in real time, but massive memory-space requirements commonly limit hardware implementations. Here, we present a 128-channel field-programmable gate array (FPGA)-based real-time closed-loop bidirectional neural interface system. The system supports 128 channels for simultaneous signal recording and eight selectable channels for stimulation. A modular 64-channel analog front-end (AFE) provides scalability and a parameterized specification of the AFE supports the recording of various electrophysiological signal types with 1.59  0.76 $\mu {V}$ root-mean-square noise. The stimulator supports both voltage-controlled and current-controlled arbitrarily shaped waveforms with the programmable amplitude and duration of pulse. An empirical algorithm for online real-time spike sorting is implemented in an FPGA. The spike-sorting is performed by template matching, and templates are created by an online real-time unsupervised learning process. A memory saving technique, called dynamic cache organizing, is proposed to reduce the memory requirement down to 6 kbit per channel and modular implementation improves the scalability for further extensions.",https://ieeexplore.ieee.org/document/7908970/,IEEE Transactions on Neural Systems and Rehabilitation Engineering,Dec. 2017,ieeexplore
10.1109/TNS.2021.3123381,A 3-D Neutron Distribution Reconstruction Method Based on the Off-Situ Measurement for Reactor,IEEE,Journals,"Understanding the core neutron distribution is an indispensable condition to ensure the safe operation and effective burn-up of reactor fuel. It has become one of the new ways for reactor monitoring to use ex-core monitoring signals to calculate the in-core information recently. In this article, a fully convolutional network (FCN)-based architecture was implemented, where the 2-D and 3-D fully convolutional neural networks were used to reconstruct the space- and energy-dependent 3-D neutron distribution of the reactor core through the off-situ measurement directly. The new method has been validated in the H. B. Robinson reactor. The differences between reconstruction results of neutron flux distribution and Monte Carlo (MC) calculation were mostly less than 2%. The average relative deviation of reconstructing the neutron density was kept at 5%. It was demonstrated that the provided method was applicable to monitor the reactor core theoretically and would provide strong support for the subsequent research on the real-time monitoring for an operating reactor, safety, and operational testing for new reactor designs, and evaluation of source term information in a nuclear emergency.",https://ieeexplore.ieee.org/document/9591247/,IEEE Transactions on Nuclear Science,Dec. 2021,ieeexplore
10.1109/LSSC.2020.3016924,A Bio-Inspired Reservoir-Computer for Real-Time Stress Detection From ECG Signal,IEEE,Journals,"This letter presents the first on-chip bio-inspired reservoir computer (RC) prototype implemented in a 65-nm CMOS. The RC comprises 50 time-multiplexed neurons, and each neuron embeds a strong nonlinearity in a feedback loop. The RC applies a nonlinear transformation to the input and projects it to high-dimensional space, thus allowing linear separation by a simple logistic-regression (LR) layer implemented off-chip. We demonstrate real-time stress detection from electrocardiogram (ECG) signals using the RC. The RC achieves 93% classification accuracy which is 6% better than the state-of-the-art digital classifiers. Operating at 40 kHz, the prototype consumes 27.5 nJ/classification which is $7\times $ lower than the state-of-the-art ECG processors performing similar complexity classification task.",https://ieeexplore.ieee.org/document/9169659/,IEEE Solid-State Circuits Letters,2020,ieeexplore
10.1109/ACCESS.2020.2966017,A Classification Algorithm Based on Complex Number Feature,IEEE,Journals,"In this study, a classification algorithm based on complex number feature is proposed. Specifically, the SVM framework is reformulated, so each example would be classified in the unitary space. The cost function is redefined by considering the maximum margin of real and imaginary units of the complex number feature at the same time. The cost function is based on the expectation of the hinge loss, and its derivatives can be calculated in closed forms. Using a stochastic gradient descent (SGD) algorithm, this method allows for efficient implementation. For complex number feature, the example uncertainty is modeled by a sample preprocessing method based on within-class Euclidean distance Gaussian distribution sample (DGS). In addition, a complex number feature selection method based on improved hybrid discrimination analysis (HDA) is proposed by considering the correlation between real and imaginary units of complex number feature. The proposed classification algorithm is tested on synthetic data and three publicly available and popular datasets, namely, MNIST, WDBC, and Voc2012. Experimental results verify the effectiveness of the proposed method. The codes are available: https://github.com/luckysomebody/paper-code.",https://ieeexplore.ieee.org/document/8957151/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3061477,A Cloud-Based Platform for Big Data-Driven CPS Modeling of Robots,IEEE,Journals,"This paper proposes an improved cyber-physical systems (CPS) architecture for a smart robotic factory based on an industrial cloud platform driven by big data based on the traditional CPS architecture. This paper uses the architecture analysis and design language to model and design a total of three scales for the underlying cell-level robot, the system-level robot shop, and the overall robotic smart factory CPS, respectively, to complete the conceptual scheme for building a robotic smart factory from a local to an overall CPS system. Using the advantages of cloud computing and combining robotic CPS with cloud computing, an architecture for an industrial management system for CPS cloud computing is proposed. Base based distributed storage architecture with Storm based distributed real-time processing architecture. In terms of modeling, the advantages and disadvantages of using AADL, structural analysis, and design language, and modelers, a physical device modeling language, are combined to analyze the advantages and disadvantages of architecture analysis &amp; design language (AADL) for modeling CPS and propose a CPS analysis and design based on AADL and applicable to it. The paper also investigates the use of LeNet models for state identification in the HSV color space. The algorithm was verified on a self-built power equipment indicator dataset with a 100% detection rate and 99.8% state recognition accuracy after four consecutive frames of fusion detection. Simulink simulation of the trolley was carried out in terms of a cell-level robotic trolley CPS system to demonstrate the effectiveness of the design of a robotic CPS system driven by soaring data based on the industrial cloud platform proposed in this paper.",https://ieeexplore.ieee.org/document/9360827/,IEEE Access,2021,ieeexplore
10.1109/TSMCB.2012.2188509,A Comparison Study of Validity Indices on Swarm-Intelligence-Based Clustering,IEEE,Journals,"Swarm intelligence has emerged as a worthwhile class of clustering methods due to its convenient implementation, parallel capability, ability to avoid local minima, and other advantages. In such applications, clustering validity indices usually operate as fitness functions to evaluate the qualities of the obtained clusters. However, as the validity indices are usually data dependent and are designed to address certain types of data, the selection of different indices as the fitness functions may critically affect cluster quality. Here, we compare the performances of eight well-known and widely used clustering validity indices, namely, the CaliskiHarabasz index, the <formula formulatype=""inline""><tex Notation=""TeX"">$CS$</tex></formula> index, the DaviesBouldin index, the Dunn index with two of its generalized versions, the <formula formulatype=""inline""> <tex Notation=""TeX"">$I$</tex></formula> index, and the silhouette statistic index, on both synthetic and real data sets in the framework of differential-evolutionparticle-swarm-optimization (DEPSO)-based clustering. DEPSO is a hybrid evolutionary algorithm of the stochastic optimization approach (differential evolution) and the swarm intelligence method (particle swarm optimization) that further increases the search capability and achieves higher flexibility in exploring the problem space. According to the experimental results, we find that the silhouette statistic index stands out in most of the data sets that we examined. Meanwhile, we suggest that users reach their conclusions not just based on only one index, but after considering the results of several indices to achieve reliable clustering structures.",https://ieeexplore.ieee.org/document/6170593/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",Aug. 2012,ieeexplore
10.1109/TKDE.2019.2961657,A Data-Driven Sequential Localization Framework for Big Telco Data,IEEE,Journals,"The proliferation of telco networks and mobile terminals brings the accumulation of tremendous amounts of measure report(MR) data at a rapid pace. The MR data is generated by mobile objects while connecting to data services and is stored in backend data centers. To geo-tag or localize such MR data is believed to have a profound effect on the analytics and optimizations of telco and traffic networks. However, MR records are of noisy and partial observations regarding to mobile objects' geo-locations and hence pose challenges to accurate telco data localization. There have been quite a few attempts. Single-point localization methods map a MR record to a location, but come out with limited accuracies due to the ignorance of spatiotemporal coherence of successive MR records. Recent efforts on sequential localization techniques alleviate this by mapping a sequence of MR records to a trajectory. However, existing solutions are often with assumptions on specific models, e.g., mobility and signal strength distributions, or priori knowledge on topology space, e.g., road networks, limiting the deployment in practice. To this end, we propose a data-driven framework to tackle the challenges in sequential telco localization. We solely use raw MR records and a public third-party GPS dataset for the learning of the correlations between mobile objects' locations and MR records, requiring no model assumptions and priori knowledge. To handle the data-intensive workloads during the learning process, we use materialized views for efficient online localization and light-weighted indexing techniques for periodical parameters tuning, in order to improve the efficiency and scalability. Results on real data show that our solution achieves 58.8 percent improvement in median localization errors compared with state-of-art sequential localization techniques that require hypothesis models and priori knowledge, making our solution superior in terms of effectiveness, efficiency, and employability.",https://ieeexplore.ieee.org/document/8939387/,IEEE Transactions on Knowledge and Data Engineering,1 Aug. 2021,ieeexplore
10.1109/TPAMI.2007.24,A Fast Biologically Inspired Algorithm for Recurrent Motion Estimation,IEEE,Journals,"We have previously developed a neurodynamical model of motion segregation in cortical visual area V1 and MT of the dorsal stream. The model explains how motion ambiguities caused by the motion aperture problem can be solved for coherently moving objects of arbitrary size by means of cortical mechanisms. The major bottleneck in the development of a reliable biologically inspired technical system with real-time motion analysis capabilities based on this neural model is the amount of memory necessary for the representation of neural activation in velocity space. We propose a sparse coding framework for neural motion activity patterns and suggest a means by which initial activities are detected efficiently. We realize neural mechanisms such as shunting inhibition and feedback modulation in the sparse framework to implement an efficient algorithmic version of our neural model of cortical motion segregation. We demonstrate that the algorithm behaves similarly to the original neural model and is able to extract image motion from real world image sequences. Our investigation transfers a neuroscience model of cortical motion computation to achieve technologically demanding constraints such as real-time performance and hardware implementation. In addition, the proposed biologically inspired algorithm provides a tool for modeling investigations to achieve acceptable simulation time",https://ieeexplore.ieee.org/document/4042700/,IEEE Transactions on Pattern Analysis and Machine Intelligence,Feb. 2007,ieeexplore
10.1162/NECO_a_00065,A Framework for Simulating and Estimating the State and Functional Topology of Complex Dynamic Geometric Networks,MIT Press,Journals,"We introduce a framework for simulating signal propagation in geometric networks (networks that can be mapped to geometric graphs in some space) and developing algorithms that estimate (i.e., map) the state and functional topology of complex dynamic geometric networks. Within the framework, we define the key features typically present in such networks and of particular relevance to biological cellular neural networks: dynamics, signaling, observation, and control. The framework is particularly well suited for estimating functional connectivity in cellular neural networks from experimentally observable data and has been implemented using graphics processing unit high-performance computing. Computationally, the framework can simulate cellular network signaling close to or faster than real time. We further propose a standard test set of networks to measure performance and compare different mapping algorithms.",https://ieeexplore.ieee.org/document/6796295/,Neural Computation,Jan. 2011,ieeexplore
10.1109/ACCESS.2021.3084236,A Generic Clustering-Based Algorithm for Approximating IOHMM Topology and Parameters,IEEE,Journals,"In this paper, a novel generic clustering-based algorithm for approximating the topology and the parameters of discrete state space Input/Output Hidden Markov Models (IOHMMs) with continuous observation spaces is introduced. The algorithm can accommodate any continuous space clustering method, whether incremental or not; it can easily be extended to Input/Output Hidden Semi-Markov Models (IOHSMMs) as well as standard HMMs and HSMMs. In this paper, the proposed algorithm is implemented with the Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN*) clustering algorithm. This algorithm brings numerous benefits such as the capability to learn the topology and the parameters of the model with more or less conservatism and the capability to define distributions from several frameworks of the uncertainty theory such as the probabilities, the possibilities or the imprecise probabilities. The algorithm is validated on synthetic and real-world datasets.",https://ieeexplore.ieee.org/document/9442778/,IEEE Access,2021,ieeexplore
10.1109/TNNLS.2019.2892157,A Greedy Assist-as-Needed Controller for Upper Limb Rehabilitation,IEEE,Journals,"Previous studies on robotic rehabilitation have shown that subjects' active participation and effort involved in rehabilitation training can promote the performance of therapies. In order to improve the voluntary effort of participants during the rehabilitation training, assist-as-needed (AAN) control strategies regulating the robotic assistance according to subjects' performance and conditions have been developed. Unfortunately, the heterogeneity of patients' motor function capability in task space is not taken into account during the implementation of these controllers. In this paper, a new scheme called greedy AAN (GAAN) controller is designed for the upper limb rehabilitation training of neurologically impaired subjects. The proposed GAAN control paradigm includes a baseline controller and a Gaussian RBF network that is utilized to model the functional capability of subjects and to provide corresponding a task challenge for them. In order to avoid subjects' slacking and encourage their active engagement, the weight vectors of RBF networks evaluating subjects' impairment level are updated based on a greedy strategy that makes the networks progressively learn the maximum forces over time provided by subjects. Simultaneously, a challenge level modification algorithm is employed to adjust the task challenge according to the task performance of subjects. Experiments on 12 subjects with neurological impairment are conducted to validate the performance and feasibility of the GAAN controller. The results show that the proposed GAAN controller has significant potential to promote the subjects' voluntary engagement during training exercises.",https://ieeexplore.ieee.org/document/8635557/,IEEE Transactions on Neural Networks and Learning Systems,Nov. 2019,ieeexplore
10.1109/TCSVT.2017.2726564,A Hardware Architecture for Cell-Based Feature-Extraction and Classification Using Dual-Feature Space,IEEE,Journals,"Many computer-vision and machine-learning applications in robotics, mobile, wearable devices, and automotive domains are constrained by their real-time performance requirements. This paper reports a dual-feature-based object recognition coprocessor that exploits both histogram of oriented gradient (HOG) and Haar-like descriptors with a cell-based parallel sliding-window recognition mechanism. The feature extraction circuitry for HOG and Haar-like descriptors is implemented by a pixel-based pipelined architecture, which synchronizes to the pixel frequency from the image sensor. After extracting each cell feature vector, a cell-based sliding window scheme enables parallelized recognition for all windows, which contain this cell. The nearest neighbor search classifier is, respectively, applied to the HOG and Haar-like feature space. The complementary aspects of the two feature domains enable a hardware-friendly implementation of the binary classification for pedestrian detection with improved accuracy. A proof-of-concept prototype chip fabricated in a 65-nm SOI CMOS, having thin gate oxide and buried oxide layers (SOTB CMOS), with 3.22-mm<sup>2</sup> core area achieves an energy efficiency of 1.52 nJ/pixel and a processing speed of 30 fps for 1024  1616-pixel image frames at 200-MHz recognition working frequency and 1-V supply voltage. Furthermore, multiple chips can implement image scaling, since the designed chip has image-size flexibility attributable to the pixel-based architecture.",https://ieeexplore.ieee.org/document/7979565/,IEEE Transactions on Circuits and Systems for Video Technology,Oct. 2018,ieeexplore
10.1109/TIP.2008.2002837,A Hybrid Color and Frequency Features Method for Face Recognition,IEEE,Journals,"This correspondence presents a novel hybrid Color and Frequency Features (CFF) method for face recognition. The CFF method, which applies an Enhanced Fisher Model (EFM), extracts the complementary frequency features in a new hybrid color space for improving face recognition performance. The new color space, the <i>RIQ</i> color space, which combines the <i>R</i> component image of the <i>RGB</i> color space and the chromatic components <i>I</i> and <i>Q</i> of the <i>YIQ</i> color space, displays prominent capability for improving face recognition performance due to the complementary characteristics of its component images. The EFM then extracts the complementary features from the real part, the imaginary part, and the magnitude of the <i>R</i> image in the frequency domain. The complementary features are then fused by means of concatenation at the feature level to derive similarity scores for classification. The complementary feature extraction and feature level fusion procedure applies to the <i>I</i> and <i>Q</i> component images as well. Experiments on the Face Recognition Grand Challenge (FRGC) version 2 Experiment 4 show that i) the hybrid color space improves face recognition performance significantly, and ii) the complementary color and frequency features further improve face recognition performance.",https://ieeexplore.ieee.org/document/4623246/,IEEE Transactions on Image Processing,Oct. 2008,ieeexplore
10.1109/TCYB.2019.2928174,A Knee-Guided Evolutionary Algorithm for Compressing Deep Neural Networks,IEEE,Journals,"Deep neural networks (DNNs) have been regarded as fundamental tools for many disciplines. Meanwhile, they are known for their large-scale parameters, high redundancy in weights, and extensive computing resource consumptions, which pose a tremendous challenge to the deployment in real-time applications or on resource-constrained devices. To cope with this issue, compressing DNNs for accelerating its inference has drawn extensive interest recently. The basic idea is to prune parameters with little performance degradation. However, the overparameterized nature and the conflict between parameters reduction and performance maintenance make it prohibitive to manually search the pruning parameter space. In this paper, we formally establish filter pruning as a multiobjective optimization problem, and propose a knee-guided evolutionary algorithm (KGEA) that can automatically search for the solution with quality tradeoff between the scale of parameters and performance, in which both conflicting objectives can be optimized simultaneously. In particular, by incorporating a minimum Manhattan distance approach, the search effort in the proposed KGEA is explicitly guided toward the knee area, which greatly facilitates the manual search for a good tradeoff solution. Moreover, the parameter importance is directly estimated on the criterion of performance loss, which can robustly identify the redundancy. In addition to the knee solution, a performance-improved model can also be found in a fine-tuning-free fashion. The experiments on compressing fully convolutional LeNet and VGG-19 networks validate the superiority of the proposed algorithm over the state-of-the-art competing methods.",https://ieeexplore.ieee.org/document/8781874/,IEEE Transactions on Cybernetics,March 2021,ieeexplore
10.1109/ACCESS.2021.3087630,A Machine Learning Method to Synthesize Channel State Information Data in Millimeter Wave Networks,IEEE,Journals,"In millimeter-wave (MMW) networks, the channel state information (CSI) carries essential information from the user to the base station (BS). The CSI values depend highly on the geometrical and physical features of the environment. Therefore, it is impossible to generate CSI data for computer simulations or analysis through mathematical models. The CSI in MMW networks can only be acquired through physical measurement(s) or with the help of expensive and complicated ray-tracing software. For many users, both these options are infeasible. This work aims to propose a simple and fast method that can generate artificial samples from the real data samples while ensuring that the artificial samples look similar to the real ones. The proposed method helps increase the size of existing CSI datasets and likely to benefit the evolution of deep learning models that need a large amount of training/testing data. The proposed method comprises two parts. (i) The first part applies data clustering and transformations such as principal component analysis (PCA)-based dimensionality reduction and probability integral transform (PIT) to convert the real data into a multivariate normal distribution of a smaller number of variables, and (ii) The second part synthesizes artificial data by learning from the multivariate normal distribution of the first part. The last step in the second part is to apply PIT and inverse PCA transformations to transform the artificial data into the same space as the input data. We compared the proposed method's performance with the well-known Kernel density estimation (KDE)-based methods that use Scott's rule and Silverman's rule to choose the bandwidth parameter value. The results show that the artificial samples generated by the proposed method exhibit very high similarity with the real ones as compared to the KDE-based methods.",https://ieeexplore.ieee.org/document/9448268/,IEEE Access,2021,ieeexplore
10.1109/TVT.2020.3027352,A Nash Q-Learning Based Motion Decision Algorithm With Considering Interaction to Traffic Participants,IEEE,Journals,"In order to improve the efficiency and comfort of autonomous vehicles while ensuring safety, the decision algorithm needs to interact with human drivers, infer the most probable behavior and then makes advantageous decision. This paper proposes a Nash-Q learning based motion decision algorithm to consider the interaction. First, the local trajectory of surrounding vehicle is predicted by kinematic constraints, which can reflect the short-term motion trend. Then, the future action space is built based the predicted local trajectory that consists of five basis actions. With that, the Nash-Q learning process can be implemented by the game between these basis actions. By elimination of strictly dominated actions and the Lemke-Howson method, the autonomous vehicle can decide the optimal action and infer the behavior of surrounding vehicle. Finally, the lane merging scenario is built to test the performance contrast to the existing methods. The driver in loop experiment is further designed to verify the interaction performance in multi-vehicle traffic. The results show that the Nash-Q learning based algorithm can improve the efficiency and comfort by 15.75% and 20.71% to the Stackelberg game and the no-interaction method respectively while the safety is ensured. It can also make real-time interaction with human drivers in multi-vehicle traffic.",https://ieeexplore.ieee.org/document/9207975/,IEEE Transactions on Vehicular Technology,Nov. 2020,ieeexplore
10.1109/ACCESS.2020.2998108,A Neural-Network-Based Model of Charge Prediction via the Judicial Interpretation of Crimes,IEEE,Journals,"The neural-network-based charge prediction, which is to predict the defendants' charges from the criminal case documents via neural network, has been a development-potential affair in artificial intelligence (AI) based legal assistant system and made some achievements. Neural network is playing important role to capture deep information in current work. However, charge prediction suffers from serious data imbalance in real-world situation. Only high-frequency charges are easy to be predicted whereas plenty of low-frequency ones are hard to be hold. Furthermore, the presence of confusing charges makes prediction worse. Here, we propose a novel model of charge prediction via the judicial interpretation of crimes (CPJIC) to provide more accurate charge prediction. The concept of crime interpretation is introduced into CPJIC, which alleviates the problems resulted from data imbalance and confusing charges. With the technique of embedding, both fact description and crime interpretation are embedded into a low-dimensional vector space as well as a neural network, delivering implemented computable charge prediction. The experimental results demonstrate that CPJIC can identify the low-frequency and confusing charges better than previous work.",https://ieeexplore.ieee.org/document/9103029/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2019.2905875,A Novel Definition of Equivalent Uniform Dose Based on Volume Dose Curve,IEEE,Journals,"With the improvement of mobile device performance, the requirement of equivalent dose description in intensity-modulated radiation therapy is increasing in mobile multimedia for healthcare. The emergence of mobile cloud computing will provide cloud servers and storage for intensity-modulated radiotherapy (IMRT) mobile applications, thus realizing visualized radiotherapy in a real sense. Equivalent uniform dose (EUD) is a biomedical indicator based on the dose measure. In this paper, the dose volume histogram is used to describe the dose distribution of different tissues in target and nontarget regions. The traditional definition of EUD, such as the exponential form and the linear form, has only a few parameters in the model for fast calculation. However, there is no close relationship between this traditional definition and the dose volume histogram. In order to establish the consistency between the EUD and the dose volume histogram, this paper proposes a novel definition of EUD based on the volume dose curve, called VD-EUD. By using a unique organic volume weight curve, it is easy to calculate VD-EUD for different dose distributions. In definition, different weight curves are used to represent the biological effects of different organs. For the target area, we should be more careful about those voxels with a low dose (cold point); thus, the weight curve is monotonically decreasing. While for the nontarget area, the curve is monotonically increasing. Furthermore, we present the curves for parallel, serial, and mixed organs of nontarget areas separately, and we define the weight curve form with only two parameters. Medical doctors can adjust the curve interactively according to different patients and organs. We also propose a fluence map optimization model with the VD-EUD constraint, which means that the proposed EUD constraint will lead to a large feasible solution space. We compare the generalized EUD (gEUD) and the proposed VD-EUD by experiments, which show that the VD-EUD has a closer relationship with the dose volume histogram. If the biological survival probability is equivalent to the VD-EUD, the feasible solution space would be large, and the target areas can be covered. By establishing a personalized organic weight curve, medical doctors can have a unique VD-EUD for each patient. By using the flexible and adjustable EUD definition, we can establish the VD-EUD-based fluence map optimization model, which will lead to a larger solution space than the traditional dose volume constraint-based model. The VD-EUD is a new definition; thus, we need more clinical testing and verification.",https://ieeexplore.ieee.org/document/8673563/,IEEE Access,2019,ieeexplore
10.1109/TETC.2019.2901272,A Novel Quantum-Inspired Fuzzy Based Neural Network for Data Classification,IEEE,Journals,"The performance of the neural network (NN) depends on the various parameters such as structure, initial weight, number of hidden layer neurons, and learning rate. The improvement in classification performance of NN without changing its structure is a challenging issue. This paper proposes a novel learning model called Quantum-inspired Fuzzy Based Neural Network (Q-FNN) to solve two-class classification problems. In the proposed model, NN architecture is formed constructively by adding neurons in the hidden layer and learning is performed using the concept of Fuzzy c-Means (FCM) clustering, where the fuzziness parameter (<i>m</i>) is evolved using the quantum computing concept. The quantum computing concept provides a large search space for a selection of <i>m</i>, which helps in finding the optimal weights and also optimizes the network architecture. This paper also proposes a modified step activation function for the formation of hidden layer neurons, which handles the overlapping samples belong to different class regions. The performance of the proposed Q-FNN model is superior and competitive with the state-of-the-art methods in terms of accuracy, sensitivity, and specificity on 15 real-world benchmark datasets.",https://ieeexplore.ieee.org/document/8651334/,IEEE Transactions on Emerging Topics in Computing,1 April-June 2021,ieeexplore
10.1109/ACCESS.2019.2901347,A Parallel Community Detection in Multi-Modal Social Network With Apache Spark,IEEE,Journals,"A constrained latent space model (CLSM) infers community membership based on two modalities in multi-modal social network data: network topology and node attributes. In this paper, we extend our previous model, CLSM in two ways. First, we introduce the Spark implementation of CLSM for parallel computation by fitting the inference algorithm into the map-reduce framework. Second, we consider user reputation besides the network homophily, which also affects social interactions between users. We test CLSM and its extension on two real-world problems: understanding link and user attributes in the location-based social network, and a review-trust network. Our proposed models in Spark can be easily deployed on commercial cloud services, such as Google cloud or Amazon web service and find latent community membership in large-scale datasets. We perform extensive experiments on real-world datasets and show how CLSM extension improves our previous model. We also share meaningful insights we discovered with the datasets.",https://ieeexplore.ieee.org/document/8651275/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2020.2973411,A Parallel Multi-Verse Optimizer for Application in Multilevel Image Segmentation,IEEE,Journals,"Multi-version optimizer (MVO) inspired by the multi-verse theory is a new optimization algorithm for challenging multiple parameter optimization problems in the real world. In this paper, a novel parallel multi-verse optimizer (PMVO) with the communication strategy is proposed. The parallel mechanism is implemented to randomly divide the initial solutions into several groups, and share the information of different groups after each fixed iteration. This can significantly promote the cooperation individual of MVO algorithm, and reduce the deficiencies that the original MVO is premature convergence, search stagnation and easily trap into local optimal search space. To confirm the performance of the proposed scheme, the PMVO algorithm was compared with the other well-known optimization algorithms, such as gray wolf optimizer (GWO), particle swarm optimization (PSO), multi-version optimizer (MVO), and parallel particle swarm optimization (PPSO) under CEC2013 test suite. The experimental results prove that the PMVO is superior to the other compared algorithms. In addition, PMVO is also applied to solve complex multilevel image segmentation problems based on minimum cross entropy thresholding. The application results appear that the proposed PMVO algorithm can achieve higher quality image segmentation compared to other similar algorithms.",https://ieeexplore.ieee.org/document/8995472/,IEEE Access,2020,ieeexplore
10.1109/TCYB.2018.2849442,A Pareto-Based Sparse Subspace Learning Framework,IEEE,Journals,"High-dimensionality is a common characteristic of real-world data, which often results in high time and space complexity or poor performance of ensuing methods. Subspace learning, as one kind of dimension reduction method, provides a way to overcome the aforementioned problem. In this paper, we introduce multiobjective evolutionary optimization into subspace learning, and propose a Pareto-based sparse subspace learning algorithm for classification tasks. The proposed algorithm aims at minimizing two conflicting objective functions, the reconstruction error and the sparsity. A kernel trick derived from Gaussian kernel is implemented to the sparse subspace learning for the nonlinear phenomena of nature. In order to speed up the convergence, an entropy-driven initialization scheme and a gradient-descent mutation scheme are designed specifically. At last, a knee point is selected from the Pareto front to guarantee that we can obtain a solution with good classification performance, and yet as sparse as possible. The experiments and detailed analysis on real-life datasets and the hyperspectral images demonstrated that the proposed model achieves comparable results with the existing conventional subspace learning and evolutionary feature selection algorithms. Hence, this paper provides a more flexible and efficient approach for sparse subspace learning.",https://ieeexplore.ieee.org/document/8417943/,IEEE Transactions on Cybernetics,Nov. 2019,ieeexplore
10.1109/TCAD.2018.2877010,A QoS-QoR Aware CNN Accelerator Design Approach,IEEE,Journals,"Recently powerful convolutional neural network (CNN) accelerators are emerging as energy-efficient solutions for real-time vision/speech processing, recognition and a wide spectrum of approximate computing applications. In addition to the broad applicability scope of such deep learning (DL) accelerators, we found that the fascinating feature of deterministic performance makes them ideal candidates as application-processors in embedded SoCs concerned with real-time processing. However, unlike traditional accelerator designs, DL accelerators introduce the new aspect of design tradeoff between real-time processing [quality of service (QoS)] and computation approximation [quality of result (QoR)] into embedded systems. This paper proposes an elastic CNN acceleration architecture that automatically adapts to the user-specified QoS constraint by exploiting the error-resilience in typical approximate computing workloads. For the first time, the proposed design, including the network tuning-and-mapping software and reconfigurable accelerator hardware, aims to reconcile the design constraint of QoS and QoR, which are respectively, the critical concerns in real-time and approximate computing. It is shown in experiments the proposed architecture enables the embedded system to work flexibly in an expanded operating space, significantly enhances its real-time ability, and maximizes the system energy-efficiency within the user-specified QoS-QoR constraint through self-reconfiguration. Also, we showcase the application of the proposed design approach to lower power image recognition challenge (LPIRC) and how it is employed to forge an energy-efficient solution to the LPIRC contest.",https://ieeexplore.ieee.org/document/8500296/,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,Nov. 2019,ieeexplore
10.1109/TCSVT.2018.2864321,A Real-Time Convolutional Neural Network for Super-Resolution on FPGA With Applications to 4K UHD 60 fps Video Services,IEEE,Journals,"In this paper, we present a novel hardware-friendly super-resolution (SR) method based on a convolutional neural network (CNN) and its dedicated hardware (HW) on field programmable gate array (FPGA). Although CNN-based SR methods have shown very promising results for SR, their computational complexities are prohibitive for hardware implementation. To the best of our knowledge, we are the first to implement a real-time CNN-based SR HW that upscales 2K full high-definition video to 4K ultra high-definition (UHD) video at 60 frames per second (fps). In our dedicated CNN-based SR HW, low-resolution input frames are processed line-by-line, and the number of convolutional filter parameters is reduced significantly by incorporating depth-wise separable convolutions with a residual connection. Our CNN-based SR HW incorporates a cascade of 1D convolutions having large receptive fields along horizontal lines while keeping vertical receptive fields minimal, which allows us to save required line memory space in achieving comparable SR performance against full 2D convolution operations. For efficient HW implementation, we use a simple and effective quantization method with little peak signal-to-noise ratio (PSNR) degradation. Also, we propose a compression method to efficiently store intermediate feature map data to reduce the number of line memories used in HW. Our HW implementation on the FPGA generates 4K UHD frames of higher PSNR values at 60 fps and shows better visual quality, compared with conventional CNN-based SR methods that are trained and tested in software.",https://ieeexplore.ieee.org/document/8429522/,IEEE Transactions on Circuits and Systems for Video Technology,Aug. 2019,ieeexplore
10.1109/TASLP.2014.2375572,A Recursive Dialogue Game for Personalized Computer-Aided Pronunciation Training,IEEE,Journals,"Learning languages in addition to the native language is very important for all people in the globalized world today, and computer-aided pronunciation training (CAPT) is attractive since the software can be used anywhere at any time, and repeated as many times as desired. In this paper, we introduce the immersive interaction scenario offered by spoken dialogues to CAPT by proposing a recursive dialogue game to make CAPT personalized. A number of tree-structured sub-dialogues are linked sequentially and recursively as the script for the game. The system policy at each dialogue turn is to select in real-time along the dialogue the best training sentence for each specific individual learner within the dialogue script, considering the learner's learning status and the future possible dialogue paths in the script, such that the learner can have the scores for all pronunciation units considered reaching a predefined standard in a minimum number of turns. The purpose here is that those pronunciation units poorly produced by the specific learner can be offered with more practice opportunities in the future sentences along the dialogue, which enables the learner to improve the pronunciation without having to repeat the same training sentences many times. This makes the learning process for each learner completely personalized. The dialogue policy is modeled by Markov decision process (MDP) with high-dimensional continuous state space, and trained with fitted value iteration using a huge number of simulated learners. These simulated leaners have the behavior similar to real learners, and were generated from a corpus of real learner data. The experiments demonstrated very promising results and a real cloud-based system is also successfully implemented.",https://ieeexplore.ieee.org/document/6971195/,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",Jan. 2015,ieeexplore
10.1109/TNNLS.2013.2280126,A Robust and Scalable Neuromorphic Communication System by Combining Synaptic Time Multiplexing and MIMO-OFDM,IEEE,Journals,"This paper describes a novel architecture for enabling robust and efficient neuromorphic communication. The architecture combines two concepts: 1) synaptic time multiplexing (STM) that trades space for speed of processing to create an intragroup communication approach that is firing rate independent and offers more flexibility in connectivity than cross-bar architectures and 2) a wired multiple input multiple output (MIMO) communication with orthogonal frequency division multiplexing (OFDM) techniques to enable a robust and efficient intergroup communication for neuromorphic systems. The MIMO-OFDM concept for the proposed architecture was analyzed by simulating large-scale spiking neural network architecture. Analysis shows that the neuromorphic system with MIMO-OFDM exhibits robust and efficient communication while operating in real time with a high bit rate. Through combining STM with MIMO-OFDM techniques, the resulting system offers a flexible and scalable connectivity as well as a power and area efficient solution for the implementation of very large-scale spiking neural architectures in hardware.",https://ieeexplore.ieee.org/document/6740875/,IEEE Transactions on Neural Networks and Learning Systems,March 2014,ieeexplore
10.1109/TPDS.2021.3049450,A Scalable Platform for Distributed Object Tracking Across a Many-Camera Network,IEEE,Journals,"Advances in deep neural networks (DNN) and computer vision (CV) algorithms have made it feasible to extract meaningful insights from large-scale deployments of urban cameras. Tracking an object of interest across the camera network in near real-time is a canonical problem. However, current tracking platforms have two key limitations: 1) They are monolithic, proprietary and lack the ability to rapidly incorporate sophisticated tracking models, and 2) They are less responsive to dynamism across wide-area computing resources that include edge, fog, and cloud abstractions. We address these gaps using Anveshak, a runtime platform for composing and coordinating distributed tracking applications. It provides a domain-specific dataflow programming model to intuitively compose a tracking application, supporting contemporary CV advances like query fusion and re-identification, and enabling dynamic scoping of the camera network's search space to avoid wasted computation. We also offer tunable batching and data-dropping strategies for dataflow blocks deployed on distributed resources to respond to network and compute variability. These balance the tracking accuracy, its real-time performance, and the active camera-set size. We illustrate the concise expressiveness of the programming model for four tracking applications. Our detailed experiments for a network of 1000 camera-feeds on modest resources exhibit the tunable scalability, performance, and quality trade-offs enabled by our dynamic tracking, batching, and dropping strategies.",https://ieeexplore.ieee.org/document/9314091/,IEEE Transactions on Parallel and Distributed Systems,1 June 2021,ieeexplore
10.1109/JSEN.2020.3042665,A Searching Space Constrained Partial to Full Registration Approach With Applications in Airport Trolley Deployment Robot,IEEE,Journals,"For airports with high passenger and luggage flows, a large number of staff members have to be hired to deploy the scattered passenger luggage trolleys. To release humans from the repetitive and laborious job, we develop an autonomous trolley deployment robot to detect, transport and collect the scattered idle trolleys to recycling points. This paper will firstly illustrate the entire collection pipeline of the deployment robot system and then address the key challenge: partial to full point set registration. With the perception framework, the robot can detect the idle trolleys and acquire the pose of the trolleys on the ground, and then capture the trolley from behind, along the same direction for subsequent grasping and manipulation. With RGB-D camera and a segmentation Convolutional Neural Network, the robot can generate a partial surface point cloud of the detected trolley. The resulting point cloud, data and a pre-scanned full trolley point cloud, model, are matched by an implicit pose. To tackle the low accuracy and long computation time issues, a novel searching space-constrained point set registration algorithm is proposed to register the two overlapping point sets. Based on Branch-and-Bound (BnB) mechanism, the error between data and model is iteratively optimized. The constraint of searching space speeds up the global searching of the optimal pose, by pruning the candidate spaces which is impossible to contain the optimal result. To evaluate the performance, an airport trolley segmentation dataset and a point cloud dataset for registration are constructed. Experimental results on the datasets and synthetic dataset show that our method achieves higher accuracy and success rate than the previous methods. The experiments demonstrated in video clips validate the developed system works in real-world applications.",https://ieeexplore.ieee.org/document/9281085/,IEEE Sensors Journal,"15 May15, 2021",ieeexplore
10.1109/TDSC.2020.2971477,A Security Analysis of Captchas With Large Character Sets,IEEE,Journals,"Captcha, which can prevent computer programs from attacking websites, has been the most important security technology for many years. The most popularly deployed Captcha is the text-based scheme. The vast majority of the existing text Captchas are designed with English letters and Arabic numerals. Recently, text Captchas with large character sets are being increasingly popular. From the perspective of attackers, larger character set means greater solution space and better theoretical security. However, the security of Captchas with large character sets in real world has never been studied comprehensively. In this article, we introduce a simple, fast, and effective deep learning method to attack these newly emerging Captchas. Taking 11 Chinese Captchas as representatives, we ran our experimental attack on each of them. Our attack achieved high success rates, ranging from 34.7 to 86.9 percent at an average speed of 0.175 seconds on these schemes. All of the results show that the Chinese text Captcha can be easily broken, demonstrating that text Captchas with large character sets are also insecure in existing forms. As a substitute, we proposed a 3D image-based scheme combining semantic comprehension and dragging action. The preliminary experimental results show that it is more robust than current text-based schemes.",https://ieeexplore.ieee.org/document/8979440/,IEEE Transactions on Dependable and Secure Computing,1 Nov.-Dec. 2021,ieeexplore
10.1109/TSMCB.2011.2171946,A Self-Learning Particle Swarm Optimizer for Global Optimization Problems,IEEE,Journals,"Particle swarm optimization (PSO) has been shown as an effective tool for solving global optimization problems. So far, most PSO algorithms use a single learning pattern for all particles, which means that all particles in a swarm use the same strategy. This monotonic learning pattern may cause the lack of intelligence for a particular particle, which makes it unable to deal with different complex situations. This paper presents a novel algorithm, called self-learning particle swarm optimizer (SLPSO), for global optimization problems. In SLPSO, each particle has a set of four strategies to cope with different situations in the search space. The cooperation of the four strategies is implemented by an adaptive learning framework at the individual level, which can enable a particle to choose the optimal strategy according to its own local fitness landscape. The experimental study on a set of 45 test functions and two real-world problems show that SLPSO has a superior performance in comparison with several other peer algorithms.",https://ieeexplore.ieee.org/document/6069879/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",June 2012,ieeexplore
10.1109/ACCESS.2021.3065872,A Survey of Software Clone Detection From Security Perspective,IEEE,Journals,"For software engineering, if two code fragments are closely similar with minor modifications or even identical due to a copy-paste behavior, that is called software/code clone. Code clones can cause trouble in software maintenance and debugging process because identifying all copied compromised code fragments in other locations is time-consuming. Researchers have been working on code clone detection issues for a long time, and the discussion mainly focuses on software engineering management and system maintenance. Another considerable issue is that code cloning provides an easy way to attackers for malicious code injection. A thorough survey work of code clone identification/detection from the security perspective is indispensable for providing a comprehensive review of existing related works and proposing future potential research directions. This paper can satisfy above requirements. We review and introduce existing security-related works following three different classifications and various comparison criteria. We then discuss three further research directions, (i) deep learning-based code clone vulnerability detection, (ii) vulnerable code clone detection for 5G-Internet of Things devices, and (iii) real-time detection methods for more efficiently detecting clone attacks. These methods are more advanced and adaptive to technological development than current technologies, and still have enough research space for future studies.",https://ieeexplore.ieee.org/document/9378511/,IEEE Access,2021,ieeexplore
10.1109/TIE.2011.2159949,A TS Fuzzy System Learned Through a Support Vector Machine in Principal Component Space for Real-Time Object Detection,IEEE,Journals,"This paper proposes a Takagi-Sugeno (TS) fuzzy system learned through a support vector machine (SVM) in principal component space (TFS-SVMPC) for real-time object detection. The antecedent part of the TFS-SVMPC classifier is generated using an algorithm that is similar to fuzzy clustering. The dimension of the free parameter vector in the TS consequent part of the TFS-SVMPC is first reduced by principal component analysis (PCA). A linear SVM is then used to tune the subsequent parameters in the principal component space to give the system better generalization performance. The TFS-SVMPC is used as a classifier in a camera-based real-time object detection system. The object detection system consists of two stages. The first stage uses a color histogram of the global color appearance of an object as a detection feature for a TFS-SVMPC classifier. In particular, an efficient method for histogram extraction during the image scanning process is proposed for real-time implementation. The second stage uses the geometry-dependent local color appearance as a color feature for another TFS-SVMPC classifier. Comparisons with other types of classifiers and detection methods for the detection of different objects verify the performance of the proposed TFS-SVMPC-based detection method.",https://ieeexplore.ieee.org/document/5892887/,IEEE Transactions on Industrial Electronics,Aug. 2012,ieeexplore
10.1109/TITS.2018.2873137,A Unified Spatio-Temporal Model for Short-Term Traffic Flow Prediction,IEEE,Journals,"This paper proposes a unified spatio-temporal model for short-term road traffic prediction. The contributions of this paper are as follows. First, we develop a physically intuitive approach to traffic prediction that captures the time-varying spatio-temporal correlation between traffic at different measurement points. The spatio-temporal correlation is affected by the road network topology, time-varying speed, and time-varying trip distribution. Distinctly different from previous black-box approaches to road traffic modeling and prediction, parameters of the proposed approach have physically intuitive meanings which make them readily amendable to suit changing road and traffic conditions. Second, unlike some existing techniques that capture the variation of spatio-temporal correlation by a complete re-design and calibration of the model, the proposed approach uses a unified model that incorporates the physical factors potentially affecting the variation of spatio-temporal correlation into a series of parameters. These parameters are relatively easy to control and adjust when road and traffic conditions change, thereby greatly reducing the computational complexity. Experiments using two sets of real traffic traces demonstrate that the proposed approach has superior accuracy compared with the widely used space-time autoregressive integrated moving average (STARIMA) and the back propagation neural network approaches, and is only marginally inferior to that obtained by constructing multiple STARIMA models for different times of the day, however, with a much reduced computational and implementation complexity.",https://ieeexplore.ieee.org/document/8525272/,IEEE Transactions on Intelligent Transportation Systems,Sept. 2019,ieeexplore
10.1109/TASE.2020.3032075,A Virtual Mechanism Approach for Exploiting Functional Redundancy in Finishing Operations,IEEE,Journals,"We propose a new approach to programming by the demonstration of finishing operations. Such operations can be carried out by industrial robots in multiple ways because an industrial robot is typically functionally redundant with respect to a finishing task. In the proposed system, a human expert demonstrates a finishing operation, and the demonstrated motion is recorded in the Cartesian space. The robots kinematic model is augmented with a virtual mechanism, which is defined according to the applied finishing tool. This way, the kinematic model is expanded with additional degrees of freedom that can be exploited to compute the optimal joint space motion of the robot without altering the essential aspects of the Cartesian space task execution as demonstrated by the human expert. Finishing operations, such as polishing and grinding, occur in contact with the treated workpiece. Since information about the contact point position is needed to control the robot during the operation, we have developed a novel approach for accurate estimation of contact points using the measured forces and torques. Finally, we applied iterative learning control to refine the demonstrated operations and compensate for inaccurate calibration and different dynamics of the robot and human demonstrator. The proposed method was verified on real robots and real polishing and grinding tasks. <i>Note to Practitioners</i>This work was motivated by the need for automation of finishing operations, such as polishing and grinding, on contemporary industrial robots. Existing approaches are both too complex and too time-consuming to be applied in flexible and small-scale production, which often requires the frequent deployment of new applications. Our approach is based on programming by demonstration and enables the programming of finishing operations also for users who are not specialists in robot programming. Programming by demonstration is especially useful for teaching finishing operations because it enables the transfer of expert knowledge about finishing skills to robots without providing lengthy task descriptions or manual coding. Besides the human demonstration of the desired operation, the proposed approach also requires the availability of the kinematic model for the machine tool applied to carry out the finishing operation. We provide several practical examples of grinding and polishing tools and how to integrate them into our approach. Another feature of the proposed system is that user demonstrations of finishing operations can be transferred between different combinations of robots and machine tools.",https://ieeexplore.ieee.org/document/9246671/,IEEE Transactions on Automation Science and Engineering,Oct. 2021,ieeexplore
10.23919/JSEE.2020.000080,A fast computational method for the landing footprints of space-to-ground vehicles,BIAI,Journals,"Fast computation of the landing footprint of a space-to- ground vehicle is a basic requirement for the deployment of parking orbits, as well as for enabling decision makers to develop real-time programs of transfer trajectories. In order to address the usually slow computational time for the determination of the landing footprint of a space-to-ground vehicle under finite thrust, this work proposes a method that uses polynomial equations to describe the boundaries of the landing footprint and uses back propagation (BP) neural networks to quickly determine the landing footprint of the space-to-ground vehicle. First, given orbital parameters and a manoeuvre moment, the solution model of the landing footprint of a space-to-ground vehicle under finite thrust is established. Second, given arbitrary orbital parameters and an arbitrary manoeuvre moment, a fast computational model for the landing footprint of a space-to-ground vehicle based on BP neural networks is provided. Finally, the simulation results demonstrate that under the premise of ensuring accuracy, the proposed method can quickly determine the landing footprint of a space-to-ground vehicle with arbitrary orbital parameters and arbitrary manoeuvre moments. The proposed fast computational method for determining a landing footprint lays a foundation for the parking-orbit configuration and supports the design of real-time transfer trajectories.",https://ieeexplore.ieee.org/document/9247427/,Journal of Systems Engineering and Electronics,Oct. 2020,ieeexplore
10.1109/42.563667,A focus-of-attention preprocessing scheme for EM-ML PET reconstruction,IEEE,Journals,"The expectation-maximization maximum-likelihood (EM-ML) algorithm belongs to a family of algorithms that compute positron emission tomography (PET) reconstructions by iteratively solving a large linear system of equations. The authors describe a preprocessing scheme for automatically focusing the attention, and thus the computational resources, on a subset of the equations and unknowns. Experimental work with a CM-5 parallel computer implementation using a simulated phantom as well as real data obtained from an ECAT 921 PET scanner indicates that quite significant savings can be obtained with respect to both time and space requirements of the EM-ML algorithm without compromising the quality of the reconstructed images.",https://ieeexplore.ieee.org/document/563667/,IEEE Transactions on Medical Imaging,April 1997,ieeexplore
10.1109/41.744412,A fully neural-network-based planning scheme for torque minimization of redundant manipulators,IEEE,Journals,"The aim of this paper is to develop a new method for minimizing joint torques of redundant manipulators in the Chebyshev sense and to present a fully neural-network-based computational scheme for its implementation. Minimax techniques are used to determine the null space acceleration vector which can guarantee to minimize the maximum joint torque. For real-time implementation, we transform the proposed method into a computation of a recurrent neural network. At each time step, the neural network is adopted for both the solution of the least-norm joint acceleration and the determination of the optimum null space acceleration vector. Compared with previous torque minimization schemes, the proposed method enables more direct monitoring and control of the magnitudes of the individual joint torques than does the minimization of the sum of squares of the components. Simulation results demonstrate that the proposed method is effective for the torque minimization control of redundant manipulators.",https://ieeexplore.ieee.org/document/744412/,IEEE Transactions on Industrial Electronics,Feb. 1999,ieeexplore
10.1109/36.628775,A neural network approach to estimating rainfall from spaceborne microwave data,IEEE,Journals,"Various techniques use microwave (MW) brightness temperature (BT) data, obtained from remote sensing orbiting platforms, to calculate rain rates. The most commonly used techniques are based on regressions or other statistical methods. An emerging tool in rainfall estimation using satellite data is artificial neural networks (NNs), NNs are mathematical models that are capable of learning complex relationships. They consist of highly interconnected, interactive data processing units. NNs are implemented in this study to estimate rainfall, and backpropagation is used as a learning scheme. The inputs for the training phase are BTs and the outputs are rainfall rates, all generated by three-dimensional (3D) simulations based on a 3D stochastic, space-time rainfall model, and a 3D radiative transfer model. Once training is complete the NNs are presented with multi-frequency and polarized (horizontal and vertical) BT data, obtained from the Special Sensor Microwave/Imager (SSM/I) instrument onboard the F10 and F11 polar-orbiting meteorological satellites. Hence, rainrates corresponding to real BT measurements are generated. The rainfall rates are also estimated using a log-linear regression model. Comparison of the two approaches, using simulated data, shows that the NN can represent more accurately the underlying relationship between BT and rainrate than the regression model, Comparison of the rates, estimated by both methods, with radar-estimated rainrates shows that NNs outperform the regression model. This study demonstrates the great potential of NNs in estimating rainfall from remotely sensed data.",https://ieeexplore.ieee.org/document/628775/,IEEE Transactions on Geoscience and Remote Sensing,Sept. 1997,ieeexplore
10.1109/81.317962,A neural network approach to the construction of Delaunay tessellation of points in R/sup d/,IEEE,Journals,"Since a neural network may be designed directly from either the Delaunay tessellation (DT) or its abstract dual, the Voronoi diagram, the procedure advanced here for training a dynamic feedforward neural network to generate the DT of specified points representing exemplars in multidimensional feature space, contributes toward the goal of an all-neural approach to the synthesis of neural networks. As the expected number of simplexes in the DT over n points is linear in n, the procedure is convenient for real-time implementation of pattern classifiers.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/317962/,IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications,Sept. 1994,ieeexplore
10.1109/72.548175,A neural network-based model for paper currency recognition and verification,IEEE,Journals,"This paper describes the neural-based recognition and verification techniques used in a banknote machine, recently implemented for accepting paper currency of different countries. The perception mechanism is based on low-cost optoelectronic devices which produce a signal associated with the light refracted by the banknotes. The classification and verification steps are carried out by a society of multilayer perceptrons whose operation is properly scheduled by an external controlling algorithm, which guarantees real-time implementation on a standard microcontroller-based platform. The verification relies mainly on the property of autoassociators to generate closed separation surfaces in the pattern space. The experimental results are very interesting, particularly when considering that the recognition and verification steps are based on low-cost sensors.",https://ieeexplore.ieee.org/document/548175/,IEEE Transactions on Neural Networks,Nov. 1996,ieeexplore
10.1109/TMTT.2005.861660,A neural-network method for the analysis of multilayered shielded microwave circuits,IEEE,Journals,"In this paper, a neural-network-based method for the analysis of practical multilayered shielded microwave circuits is presented. Using this idea, a radial basis function neural network (RBFNN) is trained to approximate the space-domain multilayered media boxed Green's functions used in the integral-equation (IE) method. Once the RBFNN has been trained, the outputs of the neural network (NN) replace the exact Green's functions, during the numerical solution of the IE. The computation of the RBFNN output values is very fast in comparison with the numerical methods used to calculate the exact Green's functions. This paper describes two novel strategies for efficiently training the RBFNN. In the first strategy, the input space of the RBFNN is divided into several spatial and frequency regions. The spatial subdivision is extended for the first time to both observation and source regions. In addition, the subdivision of the observation points regions is applied in a novel manner to the whole cross section of the metallic box. The second strategy combines the above region subdivision with an adaptive selection of the neurons variances in each region. The accuracy and the computational gain achieved with the NN method proposed makes possible the implementation of computer-aided-design tools that can be used for the analysis and design of integrated shielded microwave circuits (e.g., monolithic microwave integrated circuit devices) on a real-time basis",https://ieeexplore.ieee.org/document/1573828/,IEEE Transactions on Microwave Theory and Techniques,Jan. 2006,ieeexplore
10.1109/72.963780,A neuromorphic VLSI device for implementing 2D selective attention systems,IEEE,Journals,"Selective attention is a mechanism used to sequentially select and process salient subregions of the input space, while suppressing inputs arriving from nonsalient regions. By processing small amounts of sensory information in a serial fashion, rather than attempting to process all the sensory data in parallel, this mechanism overcomes the problem of flooding limited processing capacity systems with sensory inputs. It is found in many biological systems and can be a useful engineering tool for developing artificial systems that need to process in real-time sensory data. In this paper we present a neuromorphic hardware model of a selective attention mechanism implemented on a very large scale integration (VLSI) chip, using analog circuits. The chip makes use of a spike-based representation for receiving input signals, transmitting output signals and for shifting the selection of the attended input stimulus over time. It can be interfaced to neuromorphic sensors and actuators, for implementing multichip selective attention systems. We describe the characteristics of the circuits used in the architecture and present experimental data measured from the system.",https://ieeexplore.ieee.org/document/963780/,IEEE Transactions on Neural Networks,Nov. 2001,ieeexplore
10.1109/TPAMI.2005.88,A novel kernel method for clustering,IEEE,Journals,"Kernel methods are algorithms that, by replacing the inner product with an appropriate positive definite function, implicitly perform a nonlinear mapping of the input data into a high-dimensional feature space. In this paper, we present a kernel method for clustering inspired by the classical k-means algorithm in which each cluster is iteratively refined using a one-class support vector machine. Our method, which can be easily implemented, compares favorably with respect to popular clustering algorithms, like k-means, neural gas, and self-organizing maps, on a synthetic data set and three UCI real data benchmarks (IRIS data, Wisconsin breast cancer database, Spam database).",https://ieeexplore.ieee.org/document/1407882/,IEEE Transactions on Pattern Analysis and Machine Intelligence,May 2005,ieeexplore
10.1109/ACCESS.2019.2900095,ABNE: An Attention-Based Network Embedding for User Alignment Across Social Networks,IEEE,Journals,"User alignment across social networks can facilitate more information/knowledge transferring across networks and thereby benefit several applications, including social link prediction, cross-domain recommendation, and information diffusion. Several works try to learn a common subspace for networks by preserving the structural proximities, such that different contribution weights of neighbors are ignored as users were always connected by unweighted edges. In this paper, we propose an attention-based network embedding model that exploits the social structures for user alignment. In particular, two main components are contained in our model framework: a masked graph attention mechanism which tries to learn the alignment task driven attention weights by the supervision of pre-aligned user pairs, and an embedding algorithm tries to learn a common vector space by explicitly modeling the weighted contribution probabilities between follower-ships and followee-ships. With the learned weights and embeddings transferring between these two components, we construct a unified model for user embedding and alignment. Stochastic gradient descent and negative sampling are adopted for efficient learning and scalability. The extensive experiments on real-world social network data sets demonstrate the effectiveness and robustness of the proposed model compared with several state-of-the-art methods.",https://ieeexplore.ieee.org/document/8643796/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2020.3028847,Abnormal Trajectory Detection Based on Geospatial Consistent Modeling,IEEE,Journals,"Anomalous trajectory detection plays a significant role in fraud detection and adverse events monitoring for ride-hailing services. The spatial and temporal dynamics of road networks and the sparsity of trajectories make anomalous trajectory detection a challenging task. Most existing methods are based on density and isolation approaches, which ignore geographical information. Motivated by these challenges and shortcomings, we propose a novel method, which considers geospatial constraints of the trajectories and avoids sparsity issues. In our method, the geographical information and topological constraints of trajectories are embedded into structured vector space. Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN) are used to model common trajectory features. Our method could identify anomalous trajectories and determine which parts are responsible for anomalies by using these features. Experiments on two real-world datasets have been conducted, and results demonstrate the effectiveness and feasibility of the proposed method.",https://ieeexplore.ieee.org/document/9214405/,IEEE Access,2020,ieeexplore
10.1109/TNS.2021.3084515,Accelerated Deep Reinforcement Learning for Fast Feedback of Beam Dynamics at KARA,IEEE,Journals,"Coherent synchrotron radiation (CSR) is generated when the electron bunch length is in the order of the magnitude of the wavelength of the emitted radiation. The self-interaction of short electron bunches with their own electromagnetic fields changes the longitudinal beam dynamics significantly. Above a certain current threshold, the micro-bunching instability develops, characterized by the appearance of distinguishable substructures in the longitudinal phase space of the bunch. To stabilize the CSR emission, a real-time feedback control loop based on reinforcement learning (RL) is proposed. Informed by the available THz diagnostics, the feedback is designed to act on the radio frequency (RF) system of the storage ring to mitigate the micro-bunching dynamics. To satisfy low-latency requirements given by the longitudinal beam dynamics, the RL controller has been implemented on hardware (FPGA). In this article, a real-time feedback loop architecture and its performance is presented and compared with a software implementation using Keras-RL on CPU/GPU. The results obtained with the CSR simulation Inovesa demonstrate that the functionality of both platforms is equivalent. The training performance of the hardware implementation is similar to software solution, while it outperforms the Keras-RL implementation by an order of magnitude. The presented RL hardware controller is considered as an essential platform for the development of intelligent CSR control systems.",https://ieeexplore.ieee.org/document/9442681/,IEEE Transactions on Nuclear Science,Aug. 2021,ieeexplore
10.1109/ACCESS.2019.2950232,Accelerating API-Based Program Synthesis via API Usage Pattern Mining,IEEE,Journals,"Program Synthesis is an exciting topic in software engineering which aims to generate programs satisfying user intent automatically. Although different approaches have been proposed in program synthesis, only small or domain-specific programs can be generated in practice, the main obstacle of which lies in the intractability of program space. With the rapid growth of reusable libraries, component-based synthesis provides a promising way, such as synthesizing Java programs that are only composed of APIs. However, the efficiency of searching for proper solutions for complex tasks is still a challenge. In certain scenarios, some API methods are frequently called together. The usage of these API methods always follows some usage patterns. Incorporating the information about API usage patterns will help to accelerate the speed of program synthesis. However, state-of-the-art synthesis tools do not capture the inherent relationships between API methods. Aiming at this problem, we propose a novel approach to accelerate the speed of API-based program synthesis via API usage pattern mining. It is a general approach that can be applied to any approach of API-based synthesis. We first collect open source projects of high quality from the Internet and use an off-the-shelf API-usage-pattern-mining tool to mine API usage patterns from these code snippets. We use two strategies to incorporate the information about API usage patterns with program synthesis, and either strategy can improve the efficiency of program synthesis. We evaluate our approach on 20 real programming tasks, which shows that our approach can accelerate the speed of program synthesis by 86% compared to the baseline.",https://ieeexplore.ieee.org/document/8886418/,IEEE Access,2019,ieeexplore
10.1109/LSP.2020.2989670,Accurate and Reliable Facial Expression Recognition Using Advanced Softmax Loss With Fixed Weights,IEEE,Journals,"An important challenge for facial expression recognition (FER) is that real-world training data are usually imbalanced. Although many deep learning approaches have been proposed to enhance the discriminative power of deep expression features and enable a good predictive effect, few works have focused on the multiclass imbalance problem. When supervised by softmax loss (SL), which is widely used in FER, the classifier is often biased against minority categories (i.e., smaller interclass angular distances). In this letter, we present advanced softmax loss (ASL) to mitigate the bias induced by data imbalance and hence increase accuracy and reliability. The proposed ASL essentially magnifies the interclass diversity in the angular space to enhance discriminative power in every category. The proposed loss can easily be implemented in any deep network. Extensive experiments on the FER2013 and real-world affective faces (RAF) databases demonstrate that ASL is significantly more accurate and reliable than many state-of-the-art approaches and that it can easily be plugged into other methods and improves their performance.",https://ieeexplore.ieee.org/document/9076276/,IEEE Signal Processing Letters,2020,ieeexplore
10.1109/TIP.2013.2252622,Action Recognition From Video Using Feature Covariance Matrices,IEEE,Journals,"We propose a general framework for fast and accurate recognition of actions in video using empirical covariance matrices of features. A dense set of spatio-temporal feature vectors are computed from video to provide a localized description of the action, and subsequently aggregated in an empirical covariance matrix to compactly represent the action. Two supervised learning methods for action recognition are developed using feature covariance matrices. Common to both methods is the transformation of the classification problem in the closed convex cone of covariance matrices into an equivalent problem in the vector space of symmetric matrices via the matrix logarithm. The first method applies nearest-neighbor classification using a suitable Riemannian metric for covariance matrices. The second method approximates the logarithm of a query covariance matrix by a sparse linear combination of the logarithms of training covariance matrices. The action label is then determined from the sparse coefficients. Both methods achieve state-of-the-art classification performance on several datasets, and are robust to action variability, viewpoint changes, and low object resolution. The proposed framework is conceptually simple and has low storage and computational requirements making it attractive for real-time implementation.",https://ieeexplore.ieee.org/document/6479703/,IEEE Transactions on Image Processing,June 2013,ieeexplore
10.1109/ACCESS.2019.2893496,Adaptive Independent Subspace Analysis of Brain Magnetic Resonance Imaging Data,IEEE,Journals,"Methods for image registration, segmentation, and visualization of magnetic resonance imaging (MRI) data are used widely to help medical doctors in supporting diagnostics. The large amount and complexity of MRI data require looking for new methods that allow for efficient processing of this data. Here, we propose using the adaptive independent subspace analysis (AISA) method to discover meaningful electroencephalogram activity in the MRI scan data. The results of AISA (image subspaces) are analyzed using image texture analysis methods to calculate first order, gray-level co-occurrence matrix, gray-level size-zone matrix, gray-level run-length matrix, and neighboring gray-tone difference matrix features. The obtained feature space is mapped to the 2D space using the t-distributed stochastic neighbor embedding method. The classification results achieved using the k-nearest neighbor classifier with 10-fold cross-validation have achieved 94.7% of accuracy (and f-score of 0.9356) from the real autism spectrum disorder dataset.",https://ieeexplore.ieee.org/document/8620993/,IEEE Access,2019,ieeexplore
10.1109/TIP.2021.3050303,Adversarial Attack Against Deep Saliency Models Powered by Non-Redundant Priors,IEEE,Journals,"Saliency detection is an effective front-end process to many security-related tasks, <i>e.g.</i> automatic drive and tracking. Adversarial attack serves as an efficient surrogate to evaluate the robustness of deep saliency models before they are deployed in real world. However, most of current adversarial attacks exploit the gradients spanning the entire image space to craft adversarial examples, ignoring the fact that natural images are high-dimensional and spatially over-redundant, thus causing expensive attack cost and poor perceptibility. To circumvent these issues, this paper builds an efficient bridge between the accessible <i>partially-white-box source</i> models and the unknown <i>black-box target</i> models. The proposed method includes two steps: 1) We design a new <i>partially-white-box</i> attack, which defines the cost function in the compact hidden space to punish a fraction of feature activations corresponding to the salient regions, instead of punishing every pixel spanning the entire dense output space. This <i>partially-white-box</i> attack reduces the redundancy of the adversarial perturbation. 2) We exploit the non-redundant perturbations from some <i>source</i> models as the prior cues, and use an iterative zeroth-order optimizer to compute the <i>directional derivatives</i> along the non-redundant prior directions, in order to estimate the actual gradient of the <i>black-box target</i> model. The non-redundant priors boost the update of some critical pixels locating at non-zero coordinates of the prior cues, while keeping other redundant pixels locating at the zero coordinates unaffected. Our method achieves the best tradeoff between attack ability and perturbation redundancy. Finally, we conduct a comprehensive experiment to test the robustness of 18 state-of-the-art deep saliency models against 16 malicious attacks, under both of <i>white-box</i> and <i>black-box</i> settings, which contributes a new robustness benchmark to the saliency community for the first time.",https://ieeexplore.ieee.org/document/9325048/,IEEE Transactions on Image Processing,2021,ieeexplore
10.1109/JIOT.2020.2979413,Adversarial Learning-Enabled Automatic WiFi Indoor Radio Map Construction and Adaptation With Mobile Robot,IEEE,Journals,"Location-based service (LBS) has become an indispensable part of our daily lives. Realizing accurate LBS in indoor environments is still a challenging task. WiFi fingerprinting-based indoor positioning system (IPS) has achieved encouraging results recently, but the time and labor overhead of constructing a dense WiFi radio map remains the key bottleneck that hinders it for real-world large-scale implementation. In this article, we propose WiGAN an automatic fine-grained indoor ratio map construction and the adaptation scheme empowered by the Gaussian process regression conditioned least-squares generative adversarial networks (GPR-GANs) with a mobile robot. First, we develop a mobile robotic platform that constructs the spatial map and radio map simultaneously in the easily accessed free space. GPR-GAN first establishes a Gaussian process regression (GPR) model using the real received signal strength (RSS) measurements collected by our robotic platform via LiDAR SLAM in the free space. Then, the outputs of the GPR are adopted as the input of GAN's generator. The learning objective of GAN is to synthesize realistic RSS data in a constrained space where it has not been covered and model the irregular RSS distributions in complex indoor environments. Real-world experiments were conducted in a real-world indoor environment, which confirms the feasibility, high accuracy, and superiority of WiGAN over existing solutions in terms of both RSS estimation accuracy and localization accuracy.",https://ieeexplore.ieee.org/document/9031749/,IEEE Internet of Things Journal,Aug. 2020,ieeexplore
10.1109/TIE.2018.2847704,Algorithm for Estimating Online Bearing Fault Upon the Ability to Extract Meaningful Information From Big Data of Intelligent Structures,IEEE,Journals,"Bearing is an important machine detail that appears in almost all mechanical systems. Estimating its operating condition online in order to hold the initiative in exploiting the systems, therefore, is one of the most urgent requirements. In this paper, we propose an online bearing damage identifying method named ASSBDIM based on adaptive neuro-fuzzy inference system (ANFIS), singular spectrum analysis (SSA), and sparse filtering (SF). It is an online process with offline and online phases. In the offline phase, by applying SSA and SF to the measured data stream typed big data with noise, both preprocessing data and extracting valuable information are implemented to build two offline databases signed Off_DaB and Off_testDaB. The ANFIS identifies the dynamic response of the mechanical system via Off_DaB. Based on Off_testDaB, the parameters of the ASSBDIM are then optimized. In the online phase, at each time, another database called On_DaB is built in the way similar to the one used for building the input space of the two offline databases. On_DaB participates as inputs of the ANFIS to estimate its outputs, which are then compared with the corresponding encoded outputs to specify bearing real status at this time. Survey results based on different data sources showed the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/8408700/,IEEE Transactions on Industrial Electronics,May 2019,ieeexplore
10.1109/ACCESS.2020.3023739,AlphaGo Policy Network: A DCNN Accelerator on FPGA,IEEE,Journals,"The game of GO has long been regarded as the most challenging game for artificial intelligence because of its enormous search space and the difficulty of evaluating its board positions. In early 2016, the defeat of Lee Sedol by AlphaGo became the milestone of artificial intelligence. AlphaGo's success lies in that it efficiently combines policy and value networks with Monte Carlo tree search (MCTS). And these deep convolutional neural networks (DCNNs) are trained by the combination of supervised learning and reinforcement learning. However, large convolution operations are computationally-intensive and typically require a powerful computing platform, for example, a graphics processing unit (GPU). Therefore, it is challenging to apply DCCNs in resource-limited embedded systems. Field programmable gate array (FPGA) is proposed to be an appropriate solution to implement real-time DCCNs models. However, the limited bandwidth and on-chip memory storage are the bottlenecks for DCCNs acceleration. In this article, an AlphaGo Policy Network is designed, and efficient hardware architectures are proposed to accelerate the DCCN model. The accelerator can be fit into different FPGAs, providing the balancing between processing speed and hardware resources. As an example, the AlphaGo Policy Network is implemented on Xilinx design suite VCU118, and the results show that our implementation achieved a performance of 3036.32 GOPS and achieved up to 56x speedup compared to CPU and 22.4x speedup compared to GPU.",https://ieeexplore.ieee.org/document/9195476/,IEEE Access,2020,ieeexplore
10.1109/TKDE.2018.2833850,An Efficient Semi-Supervised Multi-label Classifier Capable of Handling Missing Labels,IEEE,Journals,"Multi-label classification has received considerable interest in recent years. Multi-label classifiers usually need to address many issues including: handling large-scale datasets with many instances and a large set of labels, compensating missing label assignments in the training set, considering correlations between labels, as well as exploiting unlabeled data to improve prediction performance. To tackle datasets with a large set of labels, embedding-based methods represent the label assignments in a low-dimensional space. Many state-of-the-art embedding-based methods use a linear dimensionality reduction to map the label assignments to a low-dimensional space. However, by doing so, these methods actually neglect the tail labels - labels that are infrequently assigned to instances. In this paper, we propose an embedding-based method that non-linearly embeds the label vectors using a stochastic approach, thereby predicting the tail labels more accurately. Moreover, the proposed method has excellent mechanisms for handling missing labels, dealing with large-scale datasets, as well as exploiting unlabeled data. Experiments on real-world datasets show that our method outperforms state-of-the-art multi-label classifiers by a large margin, in terms of prediction performance, as well as training time. Our implementation of the proposed method is available online at:https://github.com/Akbarnejad/ESMC_ Implementation.",https://ieeexplore.ieee.org/document/8356120/,IEEE Transactions on Knowledge and Data Engineering,1 Feb. 2019,ieeexplore
10.1109/ACCESS.2019.2937021,An Enhanced Version of Black Hole Algorithm via Levy Flight for Optimization and Data Clustering Problems,IEEE,Journals,"The processes of retrieving useful information from a dataset are an important data mining technique that is commonly applied, known as Data Clustering. Recently, nature-inspired algorithms have been proposed and utilized for solving the optimization problems in general, and data clustering problem in particular. Black Hole (BH) optimization algorithm has been underlined as a solution for data clustering problems, in which it is a population-based metaheuristic that emulates the phenomenon of the black holes in the universe. In this instance, every solution in motion within the search space represents an individual star. The original BH has shown a superior performance when applied on a benchmark dataset, but it lacks exploration capabilities in some datasets. Addressing the exploration issue, this paper introduces the levy flight into BH algorithm to result in a novel data clustering method Levy Flight Black Hole (LBH), which was then presented accordingly. In LBH, the movement of each star depends mainly on the step size generated by the Levy distribution. Therefore, the star explores an area far from the current black hole when the value step size is big, and vice versa. The performance of LBH in terms of finding the best solutions, prevent getting stuck in local optimum, and the convergence rate has been evaluated based on several unimodal and multimodal numerical optimization problems. Additionally, LBH is then tested using six real datasets available from UCI machine learning laboratory. The experimental outcomes obtained indicated the designed algorithm's suitability for data clustering, displaying effectiveness and robustness.",https://ieeexplore.ieee.org/document/8809737/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2020.3020550,An Online Control Approach for Forging Machine Using Reinforcement Learning and Taboo Search,IEEE,Journals,"It is noticed that offline-training and online-implementation method is dominant in the data-driven control. However, the inconsistence existing in offline data and online data may degrade the control performance. To address the aforementioned issue, an online control strategy is developed so that the control parameters can be updated online based on the real-time data measured to ensure satisfactory control performance in this study. Specifically, an online control algorithm is addressed to control the pressing-down speed of the forging machine based on the framework of the reinforcement learning that has a capability of building a complete mapping from state space to action space only according to the neighbour samples. Rather than using the way of trials and errors which is too slow to be online implementation, a taboo search is addressed to speed up the learning-working process by directly searching the control on the current states, followed by the stability conditions, derived from Lyapunov stability theory. A coarse model that is limited to get the cost information of the reinforcement learning is used to make the best of mechanism information, which prevents the occurrence of the invalid states that do not conform to system characteristics. The effectiveness of the algorithm is demonstrated by an ultra-low forging machine, which outperforms the conventional approaches such as PID and neural network control approaches. The proposed algorithm has advantages in parameter adjustments so that it is easier to implement in a practical system.",https://ieeexplore.ieee.org/document/9181549/,IEEE Access,2020,ieeexplore
10.1109/72.485671,An analog VLSI recurrent neural network learning a continuous-time trajectory,IEEE,Journals,"Real-time algorithms for gradient descent supervised learning in recurrent dynamical neural networks fail to support scalable VLSI implementation, due to their complexity which grows sharply with the network dimension. We present an alternative implementation in analog VLSI, which employs a stochastic perturbation algorithm to observe the gradient of the error index directly on the network in random directions of the parameter space, thereby avoiding the tedious task of deriving the gradient from an explicit model of the network dynamics. The network contains six fully recurrent neurons with continuous-time dynamics, providing 42 free parameters which comprise connection strengths and thresholds. The chip implementing the network includes local provisions supporting both the learning and storage of the parameters, integrated in a scalable architecture which can be readily expanded for applications of learning recurrent dynamical networks requiring larger dimensionality. We describe and characterize the functional elements comprising the implemented recurrent network and integrated learning system, and include experimental results obtained from training the network to represent a quadrature-phase oscillator.",https://ieeexplore.ieee.org/document/485671/,IEEE Transactions on Neural Networks,March 1996,ieeexplore
10.1109/TSMC.1985.6313366,An architecture for control and communications in distributed artificial intelligence systems,IEEE,Journals,"An architecture and implementation for a distributed artificial intelligence (DAI) system are presented, with emphasis given to the control and communication aspects. Problem solving by this system occurs as an iterative refinement of several mechanisms, including problem decomposition, kernel-subproblem solving, and result synthesis. In order for all related nodes to make optimum use of the information obtained from these problem-solving mechanisms, the system dynamically reconfigures itself, thereby improving its performance during operation. This approach offers the possibilities of increased real-time response, improved reliability and flexibility, and lower processing costs. A major component in the node architecture is a database of metaknowledge about the expertise of a node's own expert systems and those of the other processing nodes. This information is gradually accumulated during problem solving. Each node also has a dynamic-planning ability, which guides the problem-solving process in the most promising direction and a focus-control mechanism, which restricts the size of the explored solution space at the task level while reducing the communication bandwidths required. It also has a question-and-answer mechanism, which handles internode communications. Examples in the domain of digital-logic design are given to demonstrate the operation of the system.",https://ieeexplore.ieee.org/document/6313366/,"IEEE Transactions on Systems, Man, and Cybernetics",May-June 1985,ieeexplore
10.1109/TPAMI.2002.1017616,An efficient k-means clustering algorithm: analysis and implementation,IEEE,Journals,"In k-means clustering, we are given a set of n data points in d-dimensional space R/sup d/ and an integer k and the problem is to determine a set of k points in Rd, called centers, so as to minimize the mean squared distance from each data point to its nearest center. A popular heuristic for k-means clustering is Lloyd's (1982) algorithm. We present a simple and efficient implementation of Lloyd's k-means clustering algorithm, which we call the filtering algorithm. This algorithm is easy to implement, requiring a kd-tree as the only major data structure. We establish the practical efficiency of the filtering algorithm in two ways. First, we present a data-sensitive analysis of the algorithm's running time, which shows that the algorithm runs faster as the separation between clusters increases. Second, we present a number of empirical studies both on synthetically generated data and on real data sets from applications in color quantization, data compression, and image segmentation.",https://ieeexplore.ieee.org/document/1017616/,IEEE Transactions on Pattern Analysis and Machine Intelligence,July 2002,ieeexplore
10.1109/TSP.2005.851098,An online kernel change detection algorithm,IEEE,Journals,"A number of abrupt change detection methods have been proposed in the past, among which are efficient model-based techniques such as the Generalized Likelihood Ratio (GLR) test. We consider the case where no accurate nor tractable model can be found, using a model-free approach, called Kernel change detection (KCD). KCD compares two sets of descriptors extracted online from the signal at each time instant: The immediate past set and the immediate future set. Based on the soft margin single-class Support Vector Machine (SVM), we build a dissimilarity measure in feature space between those sets, without estimating densities as an intermediary step. This dissimilarity measure is shown to be asymptotically equivalent to the Fisher ratio in the Gaussian case. Implementation issues are addressed; in particular, the dissimilarity measure can be computed online in input space. Simulation results on both synthetic signals and real music signals show the efficiency of KCD.",https://ieeexplore.ieee.org/document/1468491/,IEEE Transactions on Signal Processing,Aug. 2005,ieeexplore
10.1109/ACCESS.2020.3005513,Analysis and Design of Computational News Angles,IEEE,Journals,"A key skill for a journalist is the ability to assess the newsworthiness of an event or situation. To this purpose journalists often rely on news angles, conceptual criteria that are used both i) to assess whether something is newsworthy and also ii) to shape the structure of the resulting news item. As journalism becomes increasingly computer-supported, and more and more sources of potentially newsworthy data become available in real time, it makes sense to try and equip journalistic software tools with operational versions of news angles, so that, when searching this vast data space, these tools can both identify effectively the events most relevant to the target audience, and also link them to appropriate news angles. In this paper we analyse the notion of news angle and, in particular, we i) introduce a formal framework and data schema for representing news angles and related concepts and ii) carry out a preliminary analysis and characterization of a number of commonly used news angles, both in terms of our formal model and also in terms of the computational reasoning capabilities that are needed to apply them effectively to real-world scenarios. This study provides a stepping stone towards our ultimate goal of realizing a solution capable of exploiting a library of news angles to identify potentially newsworthy events in a large journalistic data space.",https://ieeexplore.ieee.org/document/9127417/,IEEE Access,2020,ieeexplore
10.1109/LRA.2022.3142439,Anytime 3D Object Reconstruction Using Multi-Modal Variational Autoencoder,IEEE,Journals,"For effective human-robot teaming, it is important for the robots to be able to share their visual perception with the human operators. In a harsh remote collaboration setting, data compression techniques such as autoencoder can be utilized to obtain and transmit the data in terms of latent variables in a compact form. In addition, to ensure real-time runtime performance even under unstable environments, an anytime estimation approach is desired that can reconstruct the full contents from incomplete information. In this context, we propose a method for imputation of latent variables whose elements are partially lost. To achieve the anytime property with only a few dimensions of variables, exploiting prior information of the category-level is essential. A prior distribution used in variational autoencoders is simply assumed to be isotropic Gaussian regardless of the labels of each training datapoint. This type of flattened prior makes it difficult to perform imputation from the category-level distributions. We overcome this limitation by exploiting a category-specific multi-modal prior distribution in the latent space. The missing elements of the partially transferred data can be sampled, by finding a specific modal according to the remaining elements. Since the method is designed to use partial elements for anytime estimation, it can also be applied for data over-compression. Based on the experiments on the ModelNet and Pascal3D datasets, the proposed approach shows consistently superior performance over autoencoder and variational autoencoder up to 70% data loss. The software is open source and is available from our repository<sup>1</sup>.",https://ieeexplore.ieee.org/document/9681277/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/ACCESS.2017.2765626,Application of EOS-ELM With Binary Jaya-Based Feature Selection to Real-Time Transient Stability Assessment Using PMU Data,IEEE,Journals,"Recent studies show that pattern-recognition-based transient stability assessment (PRTSA) is a promising approach for predicting the transient stability status of power systems. However, many of the current well-known PRTSA methods suffer from excessive training time and complex tuning of parameters, resulting in inefficiency for real-time implementation and lacking the online model updating ability. In this paper, a novel PRTSA approach based on an ensemble of OS-extreme learning machine (EOSELM) with binary Jaya (BinJaya)-based feature selection is proposed with the use of phasor measurement units (PMUs) data. After briefly describing the principles of OS-ELM, an EOS-ELM-based PRTSA model is built to predict the post-fault transient stability status of power systems in real time by integrating OS-ELM and an online boosting algorithm, respectively, as a weak classifier and an ensemble learning algorithm. Furthermore, a BinJaya-based feature selection approach is put forward for selecting an optimal feature subset from the entire feature space constituted by a group of system-level classification features extracted from PMU data. The application results on the IEEE 39-bus system and a real provincial system show that the proposal has superior computation speed and prediction accuracy than other state-of-the-art sequential learning algorithms. In addition, without sacrificing the classification performance, the dimension of the input space has been reduced to about one-third of its initial value.",https://ieeexplore.ieee.org/document/8081764/,IEEE Access,2017,ieeexplore
10.1109/JIOT.2021.3068775,Applying Cross-Modality Data Processing for Infarction Learning in Medical Internet of Things,IEEE,Journals,"Cross-modality data processing is critical for the Internet-of-Things (IoT) deployment in healthcare. It can convert the innumerable raw day-to-day medical big data from massive IoT-based medical devices to diagnostic valuable data so that they can be feed to clinical routine. In this article, we propose a novel spatiotemporal two-streams generative adversarial network (SpGAN) as a cross-modality data processing approach to deploy the medical IoT in infarction learning. Our SpGAN remotely converts diagnostic valuable contrast-enhanced images (the gold standard for infarction learning, but it requires the injection of contrast agents) directly from raw nonenhanced cine MR images. This converting allows physicians to remotely perform infarction observation and analysis to break through the limitations of time and space by building a cloud computing platform of IoT-based MRI devices. Importantly, this converting offers a low-risk IoT-based manner to eliminate the potential fatal risk caused by contrast agent injection in the current infarction learning workflow. Specifically, SpGAN consists of: 1) a spatiotemporal two-stream framework as an encodingdecoding model to achieve data converting and 2) a spatiotemporal pyramid network enhances those features that are responsible to the infarction learning during encoding to improve decoding performance. Real IoT-based remote diagnosis experiments performed on 230 patients demonstrate that SpGAN provides high-quality converted images for infarction learning and promotes the in-depth application and deployment of IoT in the medical field.",https://ieeexplore.ieee.org/document/9386256/,IEEE Internet of Things Journal,"1 Dec.1, 2021",ieeexplore
10.1109/81.109247,Approximations for nonlinear functions,IEEE,Journals,Results concerning the uniform approximation of nonlinear functionals on compact sets are discussed. The results are of interest in connection with neural network-like classifiers for continuous-time signals and approximations for the input-output maps of dynamic systems. It is shown that function-space feedforward neural networks with one input layer of bounded linear functionals and one hidden nonlinear layer each are universal approximators of real continuous functionals on compact subsets of a normed linear space.&lt;<ETX>&gt;</ETX>,https://ieeexplore.ieee.org/document/109247/,IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications,Jan. 1992,ieeexplore
10.1109/TCSI.2012.2185297,Area-Efficient Configurable High-Throughput Signal Detector Supporting Multiple MIMO Modes,IEEE,Journals,"This paper presents a low-complexity, high-throughput, and configurable multiple-input multiple-output (MIMO) signal detector design solution targeting the emerging Long-Term-Evolution-Advanced (LTE-A) downlink. The detector supports signal detection of multiple MIMO modes, which are spatial-multiplexing (SM), spatial-diversity (SD), and space-division-multiple-access (SDMA). Area-efficiency is achieved by algorithm and architecture co-design where low-complexity, near-maximum-likelihood (ML) detection algorithms are proposed for these three MIMO modes respectively while keeping in mind that the operations can be reused among different modes. A parallel multistage VLSI architecture is accordingly developed that achieves high detection throughput and run-time reconfigurability. To further improve the implementation efficiency, the detector also adopts an orthogonal-real-value-decomposition (ORVD) aided candidate-sharing technology for low-cost partial Euclidean distance calculation and a distributed interference cancelation scheme for a critical path delay reduction. The proposed multi-mode MIMO detector has been designed using a 65-nm CMOS technology with a core area of 0.25 mm<sup>2</sup> (the equivalent gate-count is 88.2 K), representing a 22% less hardware-resource use than the state of art in the open literature. Operating at 1.2-V supply with 165-MHz clock, the detector achieves a 1.98 Gb/s throughput when configured to the 4  4 64-QAM spatial-multiplexing mode. The corresponding normalized energy consumption is 51.8 pJ per bit detection.",https://ieeexplore.ieee.org/document/6148313/,IEEE Transactions on Circuits and Systems I: Regular Papers,Sept. 2012,ieeexplore
10.1109/TCCN.2021.3065463,Artificial Intelligence Empowered QoS-Oriented Network Association for Next-Generation Mobile Networks,IEEE,Journals,"The increasing complexity and dynamics of 5G mobile networks have brought revolutionary changes in its modeling and control, where efficient routing and resource allocation strategies become beneficial. Software-Defined Network (SDN) makes it possible to achieve the automatic management of network resources. Relying on the powerful decision-making capability of SDNs, network association can be flexibly implemented for adapting to the dynamic of the real-time network status. In this paper, we first construct a jitter graph-based network model as well as a Poisson process-based traffic model in the context of 5G mobile networks. Second, we solve the problem of QoS routing with resource allocation based on queueing theory using a low computational complexity greedy algorithm, which takes finding a feasible path set as the main task and resource allocation as the auxiliary task. Finally, we design a QoS-oriented adaptive routing scheme based on Deep Reinforcement Learning (DRL) SPACE, which is a DRL architecture with parameterized action space, in order to find an optimal path from the source to the destination. To validate the feasibility of the greedy QoS routing strategy with resource allocation, we make a numerical packet-level simulation to model a M/M/C/N queuing system. Moreover, extensive simulation results demonstrate that our proposed routing strategy is able to improve the traffics QoS metrics, such as the packet loss ratio and queueing delay.",https://ieeexplore.ieee.org/document/9375482/,IEEE Transactions on Cognitive Communications and Networking,Sept. 2021,ieeexplore
10.1109/ACCESS.2019.2920712,Asymmetric Deep Semantic Quantization for Image Retrieval,IEEE,Journals,"Due to its fast retrieval and storage efficiency capabilities, hashing has been widely used in nearest neighbor retrieval tasks. By using deep learning-based techniques, hashing can outperform non-learning-based hashing technique in many applications. However, we argue that the current deep learning-based hashing methods ignore some critical problems (e.g., the learned hash codes are not discriminative due to the hashing methods being unable to discover rich semantic information and the training strategy having difficulty optimizing the discrete binary codes). In this paper, we propose a novel image hashing method, termed as asymmetric deep semantic quantization (ADSQ). The ADSQ is implemented using three stream frameworks, which consist of one LabelNet and two ImgNets. The LabelNet leverages the power of three fully-connected layers, which are used to capture rich semantic information between image pairs. For the two ImgNets, they each adopt the same convolutional neural network structure but with different weights (i.e., asymmetric convolutional neural networks). The two ImgNets are used to generate discriminative compact hash codes. Specifically, the function of the LabelNet is to capture rich semantic information that is used to guide the two ImgNets in minimizing the gap between the real-continuous features and the discrete binary codes. Furthermore, the ADSQ can utilize the most critical semantic information to guide the feature learning process and consider the consistency of the common semantic space and Hamming space. The experimental results on three benchmarks (i.e., CIFAR-10, NUS-WIDE, and ImageNet) demonstrate that the proposed ADSQ can outperform current state-of-the-art methods.",https://ieeexplore.ieee.org/document/8730353/,IEEE Access,2019,ieeexplore
10.1109/TNNLS.2018.2854796,Asymptotically Optimal Contextual Bandit Algorithm Using Hierarchical Structures,IEEE,Journals,"We propose an online algorithm for sequential learning in the contextual multiarmed bandit setting. Our approach is to partition the context space and, then, optimally combine all of the possible mappings between the partition regions and the set of bandit arms in a data-driven manner. We show that in our approach, the best mapping is able to approximate the best arm selection policy to any desired degree under mild Lipschitz conditions. Therefore, we design our algorithm based on the optimal adaptive combination and asymptotically achieve the performance of the best mapping as well as the best arm selection policy. This optimality is also guaranteed to hold even in adversarial environments since we do not rely on any statistical assumptions regarding the contexts or the loss of the bandit arms. Moreover, we design an efficient implementation for our algorithm using various hierarchical partitioning structures, such as lexicographical or arbitrary position splitting and binary trees (BTs) (and several other partitioning examples). For instance, in the case of BT partitioning, the computational complexity is only log-linear in the number of regions in the finest partition. In conclusion, we provide significant performance improvements by introducing upper bounds (with respect to the best arm selection policy) that are mathematically proven to vanish in the average loss per round sense at a faster rate compared to the state of the art. Our experimental work extensively covers various scenarios ranging from bandit settings to multiclass classification with real and synthetic data. In these experiments, we show that our algorithm is highly superior to the state-of-the-art techniques while maintaining the introduced mathematical guarantees and a computationally decent scalability.",https://ieeexplore.ieee.org/document/8424511/,IEEE Transactions on Neural Networks and Learning Systems,March 2019,ieeexplore
10.1109/TGCN.2021.3121877,AuGrid: Edge-Enabled Distributed Load Management for Smart Grid Service Providers,IEEE,Journals,"In this paper, we propose and design AuGrid, an LSTM-based model for geographically aware smart grid service providers, which predicts the hourly load requests from users. We also develop a pricing model which depends on the predictions obtained from AuGrid for deciding per unit cost of energy in contrast to the existing schemes that focused solely on the load requests. The crux of this work is that the suppliers may plan better with forecasts than being in uncertainty. Since smart grids are well connected, logically neighboring smart grids may exchange information and energy on the requirement. We train AuGrid with a lookback set to 2 using real-world datasets and demonstrate its robustness by predicting the load requests for different suppliers. We propose deploying the AuGrid system on geographically aware suppliers for facilitating intelligence on the edge while reducing the user sample space and increasing data security. On extensive implementation and deployment, we observe that AuGrid offers minuscule loss (below 0.1) and the pricing model offers a reduction in per-unit cost by almost 75% in comparison to existing solutions. Additionally, AuGrid requires 30% CPU and 40% RAM of single processor boards on deployment, which illustrates its suitability for resource-constrained devices.",https://ieeexplore.ieee.org/document/9592829/,IEEE Transactions on Green Communications and Networking,March 2022,ieeexplore
10.1109/TKDE.2013.174,Authentication of k Nearest Neighbor Query on Road Networks,IEEE,Journals,"Outsourcing spatial databases to the cloud provides an economical and flexible way for data owners to deliver spatial data to users of location-based services. However, in the database outsourcing paradigm, the third-party service provider is not always trustworthy, therefore, ensuring spatial query integrity is critical. In this paper, we propose an efficient road network k-nearest-neighbor query verification technique which utilizes the network Voronoi diagram and neighbors to prove the integrity of query results. Unlike previous work that verifies k-nearest-neighbor results in the Euclidean space, our approach needs to verify both the distances and the shortest paths from the query point to its kNN results on the road network. We evaluate our approach on real-world road networks together with both real and synthetic points of interest datasets. Our experiments run on Google Android mobile devices which communicate with the service provider through wireless connections. The experiment results show that our approach leads to compact verification objects (VO) and the verification algorithm on mobile devices is efficient, especially for queries with low selectivity.",https://ieeexplore.ieee.org/document/6658750/,IEEE Transactions on Knowledge and Data Engineering,June 2014,ieeexplore
10.1109/TASE.2018.2876430,Automatic Composition and Optimization of Multicomponent Predictive Systems With an Extended Auto-WEKA,IEEE,Journals,"Composition and parameterization of multicomponent predictive systems (MCPSs) consisting of chains of data transformation steps are a challenging task. Auto-WEKA is a tool to automate the combined algorithm selection and hyperparameter (CASH) optimization problem. In this paper, we extend the CASH problem and Auto-WEKA to support the MCPS, including preprocessing steps for both classification and regression tasks. We define the optimization problem in which the search space consists of suitably parameterized Petri nets forming the sought MCPS solutions. In the experimental analysis, we focus on examining the impact of considerably extending the search space (from approximately 22000 to 812 billion possible combinations of methods and categorical hyperparameters). In a range of extensive experiments, three different optimization strategies are used to automatically compose MCPSs for 21 publicly available data sets. The diversity of the composed MCPSs found is an indication that fully and automatically exploiting different combinations of data cleaning and preprocessing techniques is possible and highly beneficial for different predictive models. We also present the results on seven data sets from real chemical production processes. Our findings can have a major impact on the development of high-quality predictive models as well as their maintenance and scalability aspects needed in modern applications and deployment scenarios. Note to Practitioners-The extension of Auto-WEKA to compose and optimize multicomponent predictive systems (MCPSs) developed as part of this paper is freely available on GitHub under GPL license, and we encourage practitioners to use it on a broad variety of classification and regression problems. The software can either be used as a blackbox-where search space is made of all possible WEKA filters, predictors, and metapredictors (e.g., ensembles)-or as an optimization tool on a subset of preselected machine learning methods. The application has a graphical user interface, but it can also run from command line and can be embedded in any project as a Java library. There are three main outputs once an Auto-WEKA run has finished: 1) the trained MCPS ready to make predictions on unseen data; 2) the WEKA configuration (i.e., parameterized components); and 3) the Petri net in a Petri Net Markup Language format that can be analyzed using any tool supporting this standard language. There are, however, some practical considerations affecting the quality of the results that must be taken into consideration, such as the CPU time budget or the search starting point. These are extensively discussed in this paper.",https://ieeexplore.ieee.org/document/8550732/,IEEE Transactions on Automation Science and Engineering,April 2019,ieeexplore
10.1109/TCIAIG.2009.2038365,"Automatic Content Generation in the <emphasis emphasistype=""italic"">Galactic Arms Race</emphasis> Video Game",IEEE,Journals,"Simulation and game content includes the levels, models, textures, items, and other objects encountered and possessed by players during the game. In most modern video games and in simulation software, the set of content shipped with the product is static and unchanging, or at best, randomized within a narrow set of parameters. However, ideally, if game content could be constantly and automatically renewed, players would remain engaged longer. This paper introduces two novel technologies that take steps toward achieving this ambition: 1) a new algorithm called content-generating NeuroEvolution of Augmenting Topologies (cgNEAT) is introduced that automatically generates graphical and game content while the game is played, based on the past preferences of the players, and 2) Galactic Arms Race (GAR), a multiplayer video game, is constructed to demonstrate automatic content generation in a real online gaming platform. In GAR<b>,</b> which is available to the public and playable online, players pilot space ships and fight enemies to acquire unique particle system weapons that are automatically evolved by the cgNEAT algorithm. A study of the behavior and results from over 1000 registered online players shows that cgNEAT indeed enables players to discover a wide variety of appealing content that is not only novel, but also based on and extended from previous content that they preferred in the past. Thus, <i>GAR</i> is the first demonstration of evolutionary content generation in an online multiplayer game. The implication is that with cgNEAT it is now possible to create applications that generate their own content to satisfy users, potentially reducing the cost of content creation and increasing entertainment value from single-player to massively multiplayer online games (MMOGs) with a constant stream of evolving content.",https://ieeexplore.ieee.org/document/5352259/,IEEE Transactions on Computational Intelligence and AI in Games,Dec. 2009,ieeexplore
10.1109/TFUZZ.2004.832532,Automatic design of fuzzy controllers for car-like autonomous robots,IEEE,Journals,"This paper describes the design and implementation of a fuzzy control system for a car-like autonomous vehicle. The problem addressed is the diagonal parking in a constrained space, a typical problem in motion control of nonholonomic robots. The architecture proposed for the fuzzy controller is a hierarchical scheme which combines seven modules working in series and in parallel. The rules of each module employ the adequate fuzzy operators for its task (making a decision or generating a smoothly varying control output), and they have been obtained from heuristic knowledge and numerical data (with geometric information) depending on the module requirements (some of them are constrained to provide paths of near-minimal lengths). The computer-aided design tools of the environment Xfuzzy 3.0 (developed by some of the authors) have been employed to automate the different design stages: 1) translation of heuristic knowledge into fuzzy rules; 2) extraction of fuzzy rules from numerical data and their tuning to give paths of near-minimal lengths; 3) offline verification of the control system behavior; and 4) its synthesis to be implemented in a true robot and be verified on line. Real experiments with the autonomous vehicle ROMEO 4R (designed and built at the Escuela Superior de Ingenieros, University of Seville, Seville, Spain) demonstrate the efficiency of the described controller and of the methodology followed in its design.",https://ieeexplore.ieee.org/document/1321074/,IEEE Transactions on Fuzzy Systems,Aug. 2004,ieeexplore
10.1109/TVT.2021.3136670,Autonomous Pilot of Unmanned Surface Vehicles: Bridging Path Planning and Tracking,IEEE,Journals,"Autonomous pilot is crucial in integrally promoting the autonomy of an unmanned surface vehicle (USV). However, the integration mechanism of decision and control is still unclear within the entire autonomy. In this paper, by organically bridging path planning and tracking, an autonomous pilot framework with waypoints generation, path smoothing and policy guidance of a USV in congested waters is established, for the first time. Incorporating elite and diversity operations into the genetic algorithm (GA), an elite-duplication GA (EGA) strategy is devised to optimally generate sparse waypoints in a constrained space. The B-spline technique is further deployed to make flexibly smooth interpolation facilitating path smoothing supported by optimal sparse-waypoints. Seamlessly bridged by the parametric smooth path, deep reinforcement learning (DRL) technique is resorted to continuously extract in-depth pilotage policies, i.e., mappings from path tracking errors, collision risks and control constraints to continuous control forces/torques. Eventually, the entire spline-bridged EGA-DRL (SED) framework merits autonomous global-pilotage and local-reaction in an organically modular manner. Comprehensive validations and comparisons in various real-world geographies demonstrate the effectiveness and superiority of the proposed SED autonomous pilot framework.",https://ieeexplore.ieee.org/document/9656611/,IEEE Transactions on Vehicular Technology,March 2022,ieeexplore
10.1109/TPAMI.2013.201,Bayesian Nonparametric Models for Multiway Data Analysis,IEEE,Journals,"Tensor decomposition is a powerful computational tool for multiway data analysis. Many popular tensor decomposition approachessuch as the Tucker decomposition and CANDECOMP/PARAFAC (CP)amount to multi-linear factorization. They are insufficient to model (i) complex interactions between data entities, (ii) various data types (e.g., missing data and binary data), and (iii) noisy observations and outliers. To address these issues, we propose tensor-variate latent nonparametric Bayesian models for multiway data analysis. We name these models InfTucker. These new models essentially conduct Tucker decomposition in an infinite feature space. Unlike classical tensor decomposition models, our new approaches handle both continuous and binary data in a probabilistic framework. Unlike previous Bayesian models on matrices and tensors, our models are based on latent Gaussian or <inline-formula> <tex-math notation=""LaTeX"">$t$ </tex-math></inline-formula> processes with nonlinear covariance functions. Moreover, on network data, our models reduce to nonparametric stochastic blockmodels and can be used to discover latent groups and predict missing interactions. To learn the models efficiently from data, we develop a variational inference technique and explore properties of the Kronecker product for computational efficiency. Compared with a classical variational implementation, this technique reduces both time and space complexities by several orders of magnitude. On real multiway and network data, our new models achieved significantly higher prediction accuracy than state-of-art tensor decomposition methods and blockmodels.",https://ieeexplore.ieee.org/document/6629993/,IEEE Transactions on Pattern Analysis and Machine Intelligence,Feb. 2015,ieeexplore
10.1109/ACCESS.2019.2930236,Bayesian Signal Recovery Under Measurement Matrix Uncertainty: Performance Analysis,IEEE,Journals,"Compressive sensing (CS) has gained a lot of attention in recent years due to its benefits in saving measurement time and storage cost in many applications including biomedical imaging, wireless communications, image reconstruction, remote sensing, and so on. The CS framework enables signal recovery from a small number of linear measurements with an acceptable fidelity taking advantage of signal sparsity in some potentially unknown domain. The core idea of different variants of CS methods is incorporating prior knowledge about the input signal (e.g., prior distribution or sparsity of signals) into the recovery algorithm to restrict the search space and enhance the signal recovery performance. However, the accuracy of signal reconstruction can be significantly compromised if the designed and implemented measurement matrices do not fully match. Often times, the measurement matrix mismatch is treated as an additional noise term in the recovery algorithm ignoring the fact that this mismatch is a learnable quantity which includes random but constant or slow-varying terms during the lifetime of the measurement system. In this paper, we consider this problem for a simple case of Gaussian prior with a sparsity-driven diagonal covariance matrix and find strict bounds on the deviation of the reconstructed signal from the optimal case of fully known measurement matrix based on the properties of the mismatch matrix. The obtained bounds are general, and hence can be used to assess the performance of learning algorithms designed for learning measurement matrix uncertainty and eliminating its effect from the signal recovery. We provide numerical results to illustrate this concept in real-world applications.",https://ieeexplore.ieee.org/document/8768029/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2020.2993010,Bearing Intelligent Fault Diagnosis in the Industrial Internet of Things Context: A Lightweight Convolutional Neural Network,IEEE,Journals,"The advancement of Industry 4.0 and Industrial Internet of Things (IIoT) has laid more emphasis on reducing the parameter amount and storage space of the model in addition to the automatic and accurate fault diagnosis. In this case, this paper proposes a lightweight convolutional neural network (LCNN) method for intelligent fault diagnosis of bearing, which can largely satisfy the need of less parameter amount and storage space as well as high accuracy. First, depthiwise separable convolution is adopted, and a LCNN structure is constructed through an inverse residual structure and a linear bottleneck layer operation. Secondly, a novel decomposed Hierarchical Search Space is introduced to automatically explore the optimal LCNN for bearing fault diagnosis in the context of the IIoT. In the meantime, the real-time monitoring and fault diagnosis of the model are also deployed. In order to verify the validity of the designed model, Case Western Reserve University Bearing fault dataset and MFPT bearing fault dataset are adopted. The results demonstrate the great advantages of the model. The LCNN model can automatically learn and select the appropriate features, highly improving the fault diagnosis accuracy. Meanwhile, the computational and storage costs of the model are largely reduced, which contributes to its being applied to the mechanical system in the IIoT context.",https://ieeexplore.ieee.org/document/9088980/,IEEE Access,2020,ieeexplore
10.1109/TNSM.2021.3059075,Bloom Filter With a False Positive Free Zone,IEEE,Journals,"Bloom filters and their variants are widely used as space-efficient probabilistic data structures for representing sets and are very popular in networking applications. They support fast element insertion and deletion, along with membership queries with the drawback of false positives. Bloom filters can be designed to match the false positive rates that are acceptable for the application domain. However, in many applications, a common engineering solution is to set the false positive rate very small and ignore the existence of the very unlikely false positive answers. This article is devoted to close the gap between the two design concepts of <i>unlikely</i> and <i>not having</i> false positives. We propose a data structure called EGH filter that supports the Bloom filter operations, and besides, it can guarantee false positive free operations for a finite universe and a restricted number of elements stored in the filter. We refer to the limited universe and filter size as the false positive free zone of the filter. We describe necessary conditions for the false-positive free zone of a filter. We then generalize the filter to support the listing of the elements through the use of counters rather than bits. We detail networking applications of the filter and discuss potential generalizations. We evaluate the performance of the filter in comparison with the traditional Bloom filters. We also evaluate the price in terms of memory that needs to be paid to guarantee real false positive-free operations for having a deterministic Bloom filter-like behavior. Our data structure is based on recently developed combinatorial group testing techniques.",https://ieeexplore.ieee.org/document/9353705/,IEEE Transactions on Network and Service Management,June 2021,ieeexplore
10.1109/TNSRE.2020.2981659,Brain-Controlled Robotic Arm System Based on Multi-Directional CNN-BiLSTM Network Using EEG Signals,IEEE,Journals,"Brain-machine interfaces (BMIs) can be used to decode brain activity into commands to control external devices. This paper presents the decoding of intuitive upper extremity imagery for multi-directional arm reaching tasks in three-dimensional (3D) environments. We designed and implemented an experimental environment in which electroencephalogram (EEG) signals can be acquired for movement execution and imagery. Fifteen subjects participated in our experiments. We proposed a multi-directional convolution neural network-bidirectional long short-term memory network (MDCBN)-based deep learning framework. The decoding performances for six directions in 3D space were measured by the correlation coefficient (CC) and the normalized root mean square error (NRMSE) between predicted and baseline velocity profiles. The grand-averaged CCs of multi-direction were 0.47 and 0.45 for the execution and imagery sessions, respectively, across all subjects. The NRMSE values were below 0.2 for both sessions. Furthermore, in this study, the proposed MDCBN was evaluated by two online experiments for real-time robotic arm control, and the grand-averaged success rates were approximately 0.60 (0.14) and 0.43 (0.09), respectively. Hence, we demonstrate the feasibility of intuitive robotic arm control based on EEG signals for real-world environments.",https://ieeexplore.ieee.org/document/9040397/,IEEE Transactions on Neural Systems and Rehabilitation Engineering,May 2020,ieeexplore
10.1109/TC.2021.3061709,Building and Checking Suffix Array Simultaneously by Induced Sorting Method,IEEE,Journals,"Many efficient open-source suffix sorters using the induced sorting (IS) method to build the fundamental data structure suffix array (SA) for compressing and indexing data have been proposed. To avoid potential faults caused by possible implementation bugs, checking the output SA from any IS sorter without engineering warranty for correctness is a de-facto process. The existing SA checkers commonly perform checking after an SA is built completely, with significant time and space complexities compared with that of builders. This article proposes an efficient solution for building and checking SA simultaneously by enhancing the original IS method with a checking scheme using hash computations to on-the-fly verify the results produced by the last induction phase of IS method. Given an input of constant alphabet, this checking scheme requires linear time and constant RAM space when running on external memory, and its time and space overheads are negligible compared with that for building SA. In our experiments on real-world data, the proposed methods take advantages over the counterparts of existing SA checkers by running faster with less space. This work can help provide a value-added bonus feature for open-source IS sorters to guarantee the correctness of a built SA, and such a feature should be desirable for applications using these sorters.",https://ieeexplore.ieee.org/document/9362262/,IEEE Transactions on Computers,1 April 2022,ieeexplore
10.1109/ACCESS.2019.2923682,Burst Topic Detection in Real Time SpatialTemporal Data Stream,IEEE,Journals,"In the field of social network, fast detection of the burst topic plays a decisive role in emergency response and disposal. However, social data are noisy and sparse, which evolves with time going on and space changing make it difficult to catch the instant semantics with traditional methods. Instead of passively waiting for an emergency topic, we try to detect the latent burst topic in its budding stage. In this paper, we propose a fast burst topic detect method, namely, FBTD, which aligns data prediction with characteristic calculation to detect burst term from the real-time spatial-temporal data stream and integrates local topic detection with global topic detection to find the spatial-temporal burst topic. Our method controls the delay within a 0.1 s level while preserving the topic quality. The experiments show that preferable effects are procured, and our method outperforms the state-of-the-art approaches in terms of effectiveness.",https://ieeexplore.ieee.org/document/8741054/,IEEE Access,2019,ieeexplore
10.1109/TVT.2021.3092354,CAN-D: A Modular Four-Step Pipeline for Comprehensively Decoding Controller Area Network Data,IEEE,Journals,"Controller area networks (CANs) are a broadcast protocol for real-time communication of critical vehicle subsystems. Original equipment manufacturers of passenger vehicles hold secret their mappings of CAN data to vehicle signals, and these definitions vary according to make, model, and year. Without these mappings, the wealth of real-time vehicle information hidden in the CAN packets is uninterpretable, severely impeding vehicle-related research, including CAN cybersecurity and privacy studies, aftermarket tuning, efficiency and performance monitoring, and fault diagnosis to name a few. Guided by the four-part CAN signal definition, we present CAN-D (CAN-Decoder), a modular, four-step pipeline for identifying each signal's boundaries (start bit and length), endianness (byte ordering), signedness (bit-to-integer encoding), and by leveraging diagnostic standards, augmenting a subset of the extracted signals with meaningful, physical interpretation. En route to CAN-D, we provide a comprehensive review of the CAN signal reverse engineering research. All previous methods ignore endianness and signedness, rendering them incapable of decoding many standard CAN signal definitions. Incorporating endianness grows the search space from 128 to 4.72E21 signal tokenizations and introduces a web of changing dependencies. In response, we formulate, formally analyze, and provide an efficient solution to an optimization problem, allowing identification of the optimal set of signal boundaries and byte orderings. In addition, we provide two novel, state-of-the-art signal boundary classifiersboth of which are superior to previous approaches in precision and recall in three different test scenariosand the first signedness classification algorithm, which exhibits a <inline-formula><tex-math notation=""LaTeX"">$&gt;$</tex-math></inline-formula> 97% F-score. Overall, CAN-D is the only solution with the potential to extract any CAN signal that is also the state of the art. In evaluation on 10 vehicles of different makes, CAN-D's average <inline-formula><tex-math notation=""LaTeX"">$\ell ^1$</tex-math></inline-formula> error is five times better (81% less) than all previous methods and exhibits lower average error, even when considering only signals that meet prior methods assumptions. Finally, CAN-D is implemented in lightweight hardware, allowing for an on-board diagnostic (OBD-II) plugin for real-time in-vehicle CAN decoding.",https://ieeexplore.ieee.org/document/9466242/,IEEE Transactions on Vehicular Technology,Oct. 2021,ieeexplore
10.1109/JSAC.2019.2933764,CBMoS: Combinatorial Bandit Learning for Mode Selection and Resource Allocation in D2D Systems,IEEE,Journals,"The complexity of the mode selection and resource allocation (MS&amp;RA) problem has hampered the commercialization progress of Device-to-Device (D2D) communication in 5G networks. Furthermore, the combinatorial nature of MS&amp;RA has forced the majority of existing proposals to focus on constrained scenarios or offline solutions to contain the size of the problem. Given the real-time constraints in actual deployments, a reduction in computational complexity is necessary. Adaptability is another key requirement for mobile networks that are exposed to constant changes such as channel quality fluctuations and mobility. In this article, we propose an online learning technique (i.e., CBMoS) which leverages combinatorial multi-armed bandits (CMAB) to tackle the combinatorial nature of MS&amp;RA. Furthermore, our two-stage CMAB design results in a tight model, which eliminates the theoretically feasible but practicality invalid options from the solution space. We prototype the first SDR-based D2D testbed to verify the performance of CBMoS under real-world conditions. The simulations confirm that the fast learning speed of CBMoS leads to outperforming the benchmark schemes by up to 132%. In experiments, CBMoS exhibits even higher performance (up to 142%) than in the simulations. This stems from the adaptability/fast learning speed of CBMoS in presence of high channel dynamics which cannot be captured via statistical channel models used in the simulators.",https://ieeexplore.ieee.org/document/8790776/,IEEE Journal on Selected Areas in Communications,Oct. 2019,ieeexplore
10.1109/ACCESS.2021.3106797,CNC Machine Tool Fault Diagnosis Integrated Rescheduling Approach Supported by Digital Twin-Driven Interaction and Cooperation Framework,IEEE,Journals,"The problems of CNC machine tool (CNCMT) fault diagnosis and production rescheduling have attracted continuous attention because of their great significance to the manufacturing industry. Digital twin is a supporting technology for achieving smart manufacturing and provides a new paradigm for solving these problems. This paper explores a digital twin-driven interaction and cooperation framework and proposes the architecture and implementation mechanism to enable the sharing of data, knowledge, and resource, to realize the fusion of physical space and cyber space, and to improve the accuracy of fault diagnosis. Under this framework, aiming at the influence of CNCMT failure on the initial production planning, a self-adaptation rescheduling method based on Monte Carlo Tree Search (MCTS) algorithm is proposed to provide support for developing more efficient production planning. Finally, the effectiveness of the proposed framework is validated by experimental study. The framework and integrated rescheduling approach can provide guidance for enterprises in implementing CNCMT maintenance and production scheduling to meet high accuracy and reliability requirements.",https://ieeexplore.ieee.org/document/9520390/,IEEE Access,2021,ieeexplore
10.1109/70.88080,CONDOR: an architecture for controlling the Utah-MIT dexterous hand,IEEE,Journals,"The authors describe a fully implemented computational architecture (CONDOR) that controls the Utah-MIT dexterous hand and other complex robots. The architecture derives its power from the highly efficient real-time environment provided for its control processors, coupled with a development host that allows flexible program development. By mapping the memory of a dedicated group of processors into the address space of a host computer, efficient sharing of system resources between them is possible. The software is characterized by a few simple design concepts but provides the facilities out of which more powerful utilities such as a multiprocessor pseudo-terminal emulator, a transparent and fast file server, and a flexible symbolic debugger could be constructed.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/88080/,IEEE Transactions on Robotics and Automation,Oct. 1989,ieeexplore
10.1109/TRO.2021.3084374,Cat-Like Jumping and Landing of Legged Robots in Low Gravity Using Deep Reinforcement Learning,IEEE,Journals,"In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.",https://ieeexplore.ieee.org/document/9453856/,IEEE Transactions on Robotics,Feb. 2022,ieeexplore
10.1109/TPAMI.2004.79,Catadioptric camera calibration using geometric invariants,IEEE,Journals,"Central catadioptric cameras are imaging devices that use mirrors to enhance the field of view while preserving a single effective viewpoint. In this paper, we propose a novel method for the calibration of central catadioptric cameras using geometric invariants. Lines and spheres in space are all projected into conics in the catadioptric image plane. We prove that the projection of a line can provide three invariants whereas the projection of a sphere can only provide two. From these invariants, constraint equations for the intrinsic parameters of catadioptric camera are derived. Therefore, there are two kinds of variants of this novel method. The first one uses projections of lines and the second one uses projections of spheres. In general, the projections of two lines or three spheres are sufficient to achieve catadioptric camera calibration. One important conclusion in this paper is that the method based on projections of spheres is more robust and has higher accuracy than that based on projections of lines. The performances of our method are demonstrated by both the results of simulations and experiments with real images.",https://ieeexplore.ieee.org/document/1323795/,IEEE Transactions on Pattern Analysis and Machine Intelligence,Oct. 2004,ieeexplore
10.1109/TIFS.2010.2051543,Centered Hyperspherical and Hyperellipsoidal One-Class Support Vector Machines for Anomaly Detection in Sensor Networks,IEEE,Journals,"Anomaly detection in wireless sensor networks is an important challenge for tasks such as intrusion detection and monitoring applications. This paper proposes two approaches to detecting anomalies from measurements from sensor networks. The first approach is a linear programming-based hyperellipsoidal formulation, which is called a centered hyperellipsoidal support vector machine (CESVM). While this CESVM approach has advantages in terms of its flexibility in the selection of parameters and the computational complexity, it has limited scope for distributed implementation in sensor networks. In our second approach, we propose a distributed anomaly detection algorithm for sensor networks using a one-class quarter-sphere support vector machine (QSSVM). Here a hypersphere is found that captures normal data vectors in a higher dimensional space for each sensor node. Then summary information about the hyperspheres is communicated among the nodes to arrive at a global hypersphere, which is used by the sensors to identify any anomalies in their measurements. We show that the CESVM and QSSVM formulations can both achieve high detection accuracies on a variety of real and synthetic data sets. Our evaluation of the distributed algorithm using QSSVM reveals that it detects anomalies with comparable accuracy and less communication overhead than a centralized approach.",https://ieeexplore.ieee.org/document/5483231/,IEEE Transactions on Information Forensics and Security,Sept. 2010,ieeexplore
10.1109/TNNLS.2012.2189893,Class of Widely Linear Complex Kalman Filters,IEEE,Journals,"Recently, a class of widely linear (augmented) complex-valued Kalman filters (KFs), that make use of augmented complex statistics, have been proposed for sequential state space estimation of the generality of complex signals. This was achieved in the context of neural network training, and has allowed for a unified treatment of both second-order circular and noncircular signals, that is, both those with rotation invariant and rotation-dependent distributions. In this paper, we revisit the augmented complex KF, augmented complex extended KF, and augmented complex unscented KF in a more general context, and analyze their performances for different degrees of noncircularity of input and the state and measurement noises. For rigor, a theoretical bound for the performance advantage of widely linear KFs over their strictly linear counterparts is provided. The analysis also addresses the duality with bivariate real-valued KFs, together with several issues of implementation. Simulations using both synthetic and real world proper and improper signals support the analysis.",https://ieeexplore.ieee.org/document/6175965/,IEEE Transactions on Neural Networks and Learning Systems,May 2012,ieeexplore
10.1109/JIOT.2021.3078407,Class-Incremental Learning for Wireless Device Identification in IoT,IEEE,Journals,"Deep learning (DL) has been utilized pervasively in the Internet of Things (IoT). One typical application of DL in IoT is device identification from wireless signals, namely, noncryptographic device identification (NDI). However, learning components in NDI systems have to evolve to adapt to operational variations, such a paradigm is termed as incremental learning (IL). Various IL algorithms have been proposed and many of them require dedicated space to store the increasing amount of historical data, and therefore, they are not suitable for IoT or mobile applications. Besides, conventional IL schemes can not provide satisfying performance when historical data are not available. In this article, we address the IL problem in NDI from a new perspective, first, we provide a new metric to measure the degree of topological maturity of DNN models from the degree of conflict of class-specific fingerprints. We discover that an important cause for performance degradation in IL-enabled NDI is owing to the conflict of devices fingerprints. Second, we also show that the conventional IL schemes can lead to low topological maturity of DNN models in NDI systems. Thirdly, we propose a new channel separation-enabled IL (CSIL) scheme without using historical data, in which our strategy can automatically separate devices fingerprints in different learning stages and avoid potential conflict. Finally, We evaluated the effectiveness of the proposed framework using real data from automatic-dependent surveillance-broadcast (ADS-B), an application of IoT in aviation. The proposed framework has the potential to be applied to accurate identification of IoT devices in a variety of IoT applications and services. Data and code available at IEEE Dataport (DOI: 10.21227/1bxc-ke87) and <uri>https://github.com/pcwhy/CSIL</uri>.",https://ieeexplore.ieee.org/document/9425491/,IEEE Internet of Things Journal,"1 Dec.1, 2021",ieeexplore
10.1109/ACCESS.2021.3130231,Classification of Bird and Drone Targets Based on Motion Characteristics and Random Forest Model Using Surveillance Radar Data,IEEE,Journals,"Accurate detection and tracking of birds and drones are of great significance in various low altitude airspace surveillance scenarios. Radar is currently the most proper long range surveillance technology for this problem but also challenged by various difficulties on effective distinguishing between birds and drones. This paper explores the inherent flight mechanic and behavior mode of birds and drones. A target classification method is proposed by extracting target motion characteristics from radar tracks. The random forest model is selected for target classification in the new feature space. The proposed method is verified by real bird surveillance radar systems deployed in airport region. Classification results on birds, quadcopter drones and dynamic precipitations indicate that the proposed method could provide good classification accuracy. The Gini importance descriptors in random forest model provide extra reference on motion characteristic evaluation and mining. High sample flexibility and efficiency make the classification system capable of handling complicated low altitude target surveillance and classification problems. Limitations of the existing method and potential optimization strategy are also discussed as future works.",https://ieeexplore.ieee.org/document/9626001/,IEEE Access,2021,ieeexplore
10.1109/TCPMT.2021.3117539,Cleaning Profile Classification Using Convolutional Neural Network in Stencil Printing,IEEE,Journals,"This research proposes a novel framework to classify the stencil cleaning profile in the stencil printing process (SPP) on a real-time basis. The stencil cleaning operation is necessary to reduce printing defects. The proper control of the cleaning profiles selection process determines the quality and efficiency of the SPP. The wet profile provides high-quality cleaning but requires more time and materials. Dry cleaning is more efficient but cannot provide adequate cleaning quality with high levels of residue. This research aims to develop an intelligent model to guide the stencil cleaning profiles decision-making and improve the SPP performance. Stencil cleaning is considered a sequential detection problem. Based on the historical printed boards quality measures, a novel feature space is proposed by encoding the time-series printing data into images to better understand the abnormal and trend patterns. The resultant images are classified into the proper cleaning profile through a transfer learning-based convolutional neural network (CNN) model, denoted as cleaning profile classification (CPC) model. Transfer learning is adopted to overcome the limited data problem and enhance the models generalization capabilities. The experimental results show that the proposed CNN architecture outperforms other complex state-of-the-art CNN structures in accurately classifying the cleaning profiles, which enhances the overall SPP quality and productivity. The CPCs actual implementation to control the cleaning profile indicates that the standard deviation of the printing results and the process capability has improved by 34% and 10%, respectively, compared to the best-known cleaning parameters.",https://ieeexplore.ieee.org/document/9558771/,"IEEE Transactions on Components, Packaging and Manufacturing Technology",Nov. 2021,ieeexplore
10.1109/ACCESS.2021.3105136,Cocktail Glass Network: Fast Depth Estimation Using Channel to Space Unrolling,IEEE,Journals,"Depth-estimation from a single input image can be used in applications such as robotics and autonomous driving. Recently, depth-estimation networks with UNet encoder/decoder structures have been widely used. In these decoders, operations are repeated to gradually increase the image resolution, while decreasing the channel size. If the upsampling operation at a high magnification can be processed at once, the amount of computation in the decoder can be dramatically reduced. To achieve this, we propose a new network structure, i.e., a cocktail glass network. In this network, convolution layers in the decoder are reduced, and a novel fast upsampling method is used that is known as channel-to-space unrolling, which converts thick channel data into high-resolution data. The proposed method can be easily implemented using simple reshaping operations; therefore, it is suitable for reducing the depth-estimation network. Considering the experimental results based on the NYU V2 and KITTI datasets, we demonstrate that the proposed method reduces the amount of computation in the decoder by half, while maintaining the same level of accuracy; it can be used in both lightweight and large-model-capacity networks.",https://ieeexplore.ieee.org/document/9514839/,IEEE Access,2021,ieeexplore
10.1109/JIOT.2021.3059281,Cognitive GPR for Subsurface Object Detection Based on Deep Reinforcement Learning,IEEE,Journals,"Ground penetrating radars (GPRs) carried by mobile platforms, such as vehicles and drones, have been applied in various applications, for instance, subsurface utility detection, structural health inspection, and autonomous driving. However, existing GPR systems are not able to operate autonomously and adaptively due to several challenges, including the lack of intelligence, uncertain and dynamic nature of sensing environments, and huge state and action spaces. To overcome these challenges, in this article, we propose an autonomous cognitive GPR (AC-GPR) enabled by a deep reinforcement learning (DRL) approach. Specifically, the operation of the proposed AC-GPR is first formulated as a sequential decision process. A novel reward function is developed for the DRL model by defining and combining two different types of entropy-based rewards resulting from object detection and recognition, respectively. A deep Q-learning network (DQN) is developed to address the extreme curse of dimensionality in the state space and learn a policy directing the actions of the AC-GPR. The AC-GPR is evaluated using software called GprMax by combining DRL with GPR modeling and simulation. Results show that our proposed DRL-based AC-GPR outperforms other GPR systems using different approaches in terms of detection accuracy and operating time.",https://ieeexplore.ieee.org/document/9354231/,IEEE Internet of Things Journal,"15 July15, 2021",ieeexplore
10.1109/ACCESS.2018.2845855,Color Transfer Pulse-Coupled Neural Networks for Underwater Robotic Visual Systems,IEEE,Journals,"With rapid developments in cloud computing, artificial intelligence, and robotic systems, ever more complex tasks, such as space and ocean exploration, are being implemented by intelligent robots. Here, we propose an underwater image enhancement scheme for robotic visual systems. The proposed algorithm and its implementation enhances and outputs an image captured by an underwater robot in real time. In this scheme, pulse-coupled neural network (PCNN)-based image enhancement and color transfer algorithms are combined to enhance the underwater image. To avoid color imbalance in the underwater image and enhance details while suppressing noise, color correction is first carried out on the underwater image before converting it into the hue-saturation-intensity domain and enhancing it by PCNN. The enhanced result improves the color and contrast of the source image and enhances the details and edges of darker regions. Experiments are performed on real world data to demonstrate the effectiveness of the proposed scheme.",https://ieeexplore.ieee.org/document/8377996/,IEEE Access,2018,ieeexplore
10.1109/ACCESS.2019.2893662,Comprehensive Review of the Development of the Harmony Search Algorithm and its Applications,IEEE,Journals,"This paper presents a comprehensive overview of the development of the harmony search (HS) algorithm and its applications. HS is a well-known human-based meta-heuristic algorithm that mimics the process of creating a new harmony in music. This algorithm can be applied to different fields of research, owing to its ability to balance between exploitation (i.e., searching around the known best) and exploration (i.e., roaming the entire search space). Thus, numerous studies have been conducted to utilize HS in real-world optimization problems, and many variants and hybrid algorithms of HS have been developed to cope with different problems. In this paper, HS and its variants are reviewed from various aspects. First, we describe the HS algorithm and present how its parameters affect algorithm performance. Second, we describe HS classifications based on the well-known HS variants and hybrid algorithms, along with their applications. Finally, a discussion conducted on the strengths and weaknesses of the HS algorithm and the possibilities for its improvement. Focusing on related work from diverse fields (such as optimization, engineering, computer science, biology, and medicine), this paper can foster interests on the application of HS for multidisciplinary audiences.",https://ieeexplore.ieee.org/document/8616762/,IEEE Access,2019,ieeexplore
10.1109/TFUZZ.2019.2895572,Concise Fuzzy System Modeling Integrating Soft Subspace Clustering and Sparse Learning,IEEE,Journals,"The superior interpretability and uncertainty modeling ability of Takagi-Sugeno-Kang fuzzy system (TSK FS) make it possible to describe complex nonlinear systems intuitively and efficiently. However, classical TSK FS usually adopts the whole feature space of the data for model construction, which can result in lengthy rules for high-dimensional data and lead to degeneration in interpretability. Furthermore, for highly nonlinear modeling task, it is usually necessary to use a large number of rules which further weaken the clarity and interpretability of TSK FS. To address these issues, an enhanced soft subspace clustering (ESSC) and sparse learning (SL) based concise zero-order TSK FS construction method, called ESSC-SL-CTSK-FS, is proposed in this paper by integrating the techniques of ESSC and SL. In this method, ESSC is used to generate the antecedents and various sparse subspaces for different fuzzy rules, whereas SL is used to optimize the consequent parameters of the fuzzy rules based on which the number of fuzzy rules can be effectively reduced. Finally, the proposed ESSC-SL-CTSK-FS method is used to construct concise zero-order TSK FS that can explain the scenes in high-dimensional data modeling more clearly and easily. Experiments are conducted on various real-world datasets to confirm the advantages.",https://ieeexplore.ieee.org/document/8626516/,IEEE Transactions on Fuzzy Systems,Nov. 2019,ieeexplore
10.1109/TII.2020.3047416,ContainerGuard: A Real-Time Attack Detection System in Container-Based Big Data Platform,IEEE,Journals,"As a lightweight, flexible, and high-performance operating system virtualization, containers are used to speed up the big data platform. However, due to the imperfection of the resource isolation mechanism and the property of shared kernel, the meltdown and spectre attacks can lead to information leakage of kernel space and coresident containers. In this article, a noise-resilient and real-time detection system, named ContainerGuard, is proposed to detect meltdown and spectre attacks in the container-based big data platform. ContainerGuard uses a nonintrusive manner to collect lifecycle multivariate time-series performance event data of processes in containers and then uses ensemble of variational autoencoders as generative neural networks to learn the robust representations of normal patterns. Therefore, ContainerGuard meets the urgent need for information protection in the container-based big data platform. Our evaluations using real-world datasets show that ContainerGuard achieves excellent detection performance and only introduces about 4.5% of running performance overhead to the platform.",https://ieeexplore.ieee.org/document/9309057/,IEEE Transactions on Industrial Informatics,May 2022,ieeexplore
10.1109/TSMCA.2004.838459,Control and learning of ambience by an intelligent building,IEEE,Journals,"Modern approaches to the architecture of living and working environments emphasize the dynamic reconfiguration of space and function to meet the needs, comfort, and preferences of its inhabitants. Although it is possible for a human operator to specify a configuration explicitly, the size, sophistication, and dynamic requirements of modern buildings demands that they have autonomous intelligence that could satisfy the needs of its inhabitants without human intervention. We describe a multiagent framework for such intelligent building control that is deployed in a commercial building equipped with sensors and effectors. Multiple agents control subparts of the environment using fuzzy rules that link sensors and effectors. The agents communicate with one another by asynchronous, interest-based messaging. They implement a novel unsupervised online real-time learning algorithm that constructs a fuzzy rule-base, derived from very sparse data in a nonstationary environment. We have developed methods for evaluating the performance of systems of this kind. Our results demonstrate that the framework and the learning algorithm significantly improve the performance of the building.",https://ieeexplore.ieee.org/document/1369350/,"IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",Jan. 2005,ieeexplore
10.1109/TPAMI.2004.61,Convergence and application of online active sampling using orthogonal pillar vectors,IEEE,Journals,The analysis of convergence and its application is shown for the Active Sampling-at-the-Boundary method applied to multidimensional space using orthogonal pillar vectors. Active learning method facilitates identifying an optimal decision boundary for pattern classification in machine learning. The result of this method is compared with the standard active learning method that uses random sampling on the decision boundary hyperplane. The comparison is done through simulation and application to the real-world data from the UCI benchmark data set. The boundary is modeled as a nonseparable linear decision hyperplane in multidimensional space with a stochastic oracle.,https://ieeexplore.ieee.org/document/1316853/,IEEE Transactions on Pattern Analysis and Machine Intelligence,Sept. 2004,ieeexplore
10.1109/TITS.2018.2882439,Convolutional Neural Networks for On-Street Parking Space Detection in Urban Networks,IEEE,Journals,"The purpose of this paper is the development of data science models for the detection of empty on-street parking spaces in urban road networks based on data provided by in-vehicle cameras that are already, or soon will be, a standard vehicle equipment. A rolling spatial interval is used to identify the existence of an on-street parking space and the properties of empty spaces are used to determine the availability of the parking space. Convolutional neural networks are developed, trained, and evaluated with the use of images from a moving vehicle camera. The images are preprocessed and converted to suitable matrices, so that only the useful information for the empty on-street parking space detection problem is preserved. The optimized convolutional networks, in terms of structural and learning parameters, provided predictions for the detection of empty on-street parking spaces with approximately 90% average accuracy. The proposed model performs better than the relatively complex SVMs, which supports its appropriateness as an approach. Finally, the implementation of a framework, which integrates the developed models to produce meaningful parking information for drivers in real time, is discussed.",https://ieeexplore.ieee.org/document/8577026/,IEEE Transactions on Intelligent Transportation Systems,Dec. 2019,ieeexplore
10.1109/TITB.2004.832550,DBMap: a space-conscious data visualization and knowledge discovery framework for biomedical data warehouse,IEEE,Journals,"Advances in digital imaging modalities as well as other diagnosis and therapeutic techniques have generated a massive amount of diverse data for clinical research. The purpose of this study is to investigate and implement a new intuitive and space-conscious visualization framework, called DBMap, to facilitate efficient multidimensional data visualization and knowledge discovery against the large-scale data warehouses of integrated image and nonimage data. The DBMap framework is built upon the TreeMap concept. TreeMap is a space constrained graphical representation of large hierarchical data sets, mapped to a matrix of rectangles, whose size and color represent interested database fields. It allows the display of a large amount of numerical and categorical information in limited real estate of the computer screen with an intuitive user interface. DBMap has been implemented and integrated into a large brain research data warehouse to support neurologic and neuroradiologic research at the University of California, San Francisco Medical Center. For imaging specialists and clinical researchers, this novel DBMap framework facilitates another way to better explore and classify the hidden knowledge embedded in medical image data warehouses.",https://ieeexplore.ieee.org/document/1331412/,IEEE Transactions on Information Technology in Biomedicine,Sept. 2004,ieeexplore
10.1109/TNSM.2021.3049298,DDQP: A Double Deep Q-Learning Approach to Online Fault-Tolerant SFC Placement,IEEE,Journals,"Since Network Function Virtualization (NFV) decouples network functions (NFs) from the underlying dedicated hardware and realizes them in the form of software called Virtual Network Functions (VNFs), they are enabled to run in any resource-sufficient virtual machines. A service function chain (SFC) is composed of a sequential set of VNFs. As VNFs are vulnerable to various faults such as software failures, we consider how to deploy both active and standby SFC instances. Given the complexity and unpredictability of the network state, we propose a double deep Q-networks based online SFC placement scheme DDQP. Specifically, DDQP uses deep neural networks to deal with large continuous network state space. In the case of stateful VNFs, we offer constant generated state updates from active instances to standby instances to guarantee seamless redirection after failures. With the goal of balancing the waste of resources and ensuring service reliability, we introduce five progressive schemes of resource reservations to meet different customer needs. Our experimental results demonstrate that DDQP responds rapidly to arriving requests and reaches near-optimal performance. Specifically, DDQP outweighs the state-of-the-art method by 16.30% and 38.51% higher acceptance ratio under different schemes with 82x speedup on average. In order to enhance the integrity of the SFC state transition, we further proposed DDQP+, which extends DDQP by adding the delayed placement mechanism. Compared with DDQP, the design of the DDQP+ algorithm is more reasonable and comprehensive. The experiment results also show that DDQP+ achieved further improvement in multiple performance indicators.",https://ieeexplore.ieee.org/document/9313044/,IEEE Transactions on Network and Service Management,March 2021,ieeexplore
10.1109/TEVC.2011.2160399,DNA Sequence Compression Using Adaptive Particle Swarm Optimization-Based Memetic Algorithm,IEEE,Journals,"With the rapid development of high-throughput DNA sequencing technologies, the amount of DNA sequence data is accumulating exponentially. The huge influx of data creates new challenges for storage and transmission. This paper proposes a novel adaptive particle swarm optimization-based memetic algorithm (POMA) for DNA sequence compression. POMA is a synergy of comprehensive learning particle swarm optimization (CLPSO) and an adaptive intelligent single particle optimizer (AdpISPO)-based local search. It takes advantage of both CLPSO and AdpISPO to optimize the design of approximate repeat vector (ARV) codebook for DNA sequence compression. ARV is first introduced in this paper to represent the repeated fragments across multiple sequences in direct, mirror, pairing, and inverted patterns. In POMA, candidate ARV codebooks are encoded as particles and the optimal solution, which covers the most approximate repeated fragments with the fewest base variations, is identified through the exploration and exploitation of POMA. In each iteration of POMA, the leader particles in the swarm are selected based on weighted fitness values and each leader particle is fine-tuned with an AdpISPO-based local search, so that the convergence of the search in local region is accelerated. A detailed comparison study between POMA and the counterpart algorithms is performed on 29 (23 basic and 6 composite) benchmark functions and 11 real DNA sequences. POMA is observed to obtain better or competitive performance with a limited number of function evaluations. POMA also attains lower bits-per-base than other state-of-the-art DNA-specific algorithms on DNA sequence data. The experimental results suggest that the cooperation of CLPSO and AdpISPO in the framework of memetic algorithm is capable of searching the ARV codebook space efficiently.",https://ieeexplore.ieee.org/document/6031913/,IEEE Transactions on Evolutionary Computation,Oct. 2011,ieeexplore
10.1109/ACCESS.2020.3020318,DPSO and Inverse Jacobian-Based Real-Time Inverse Kinematics With Trajectory Tracking Using Integral SMC for Teleoperation,IEEE,Journals,"A six-degree-of-freedom robotic manipulator inverse kinematics (IK) for position control is proposed for the bilateral teleoperation process that is implemented through a joystick for nuclear power plant dismantling operations. The control strategy of the manipulator includes the use of the joystick to generate the Cartesian space trajectory followed by the IK to yield the joint space trajectory for implementing position control. In this paper, a novel technique for the IK is proposed. It involves the use of the particle swarm optimization (PSO) algorithm with the inverse Jacobian (IJ). The special case of the dual PSO is based on dividing the PSO algorithm into two such that the trajectory position and orientation are separately optimized by the algorithms, resulting in a faster convergence. In contrast, the inverse Jacobian aids in generating a smooth joint trajectory. The integral sliding mode control (ISMC) is proposed for position control because it does not require information on system dynamics. The ISMC improves the system trajectory tracking performance by using a switching gain to compensate for system dynamics and perturbations (disturbance and unmatched uncertainties), ultimately reducing the time delay. The effectiveness of the PSO combined with the IJ and the robustness of the ISMC in the teleoperation process are confirmed by the experimental results.",https://ieeexplore.ieee.org/document/9180300/,IEEE Access,2020,ieeexplore
10.1109/TITS.2020.3039617,Data Freshness and Energy-Efficient UAV Navigation Optimization: A Deep Reinforcement Learning Approach,IEEE,Journals,"In this paper, we design a navigation policy for multiple unmanned aerial vehicles (UAVs) where mobile base stations (BSs) are deployed to improve the data freshness and connectivity to the Internet of Things (IoT) devices. First, we formulate an energy-efficient trajectory optimization problem in which the objective is to maximize the energy efficiency by optimizing the UAV-BS trajectory policy. We also incorporate different contextual information such as energy and age of information (AoI) constraints to ensure the data freshness at the ground BS. Second, we propose an agile deep reinforcement learning with experience replay model to solve the formulated problem concerning the contextual constraints for the UAV-BS navigation. Moreover, the proposed approach is well-suited for solving the problem, since the state space of the problem is extremely large and finding the best trajectory policy with useful contextual features is too complex for the UAV-BSs. By applying the proposed trained model, an effective real-time trajectory policy for the UAV-BSs captures the observable network states over time. Finally, the simulation results illustrate the proposed approach is 3.6% and 3.13% more energy efficient than those of the greedy and baseline deep Q Network (DQN) approaches.",https://ieeexplore.ieee.org/document/9285214/,IEEE Transactions on Intelligent Transportation Systems,Sept. 2021,ieeexplore
10.1109/TCIAIG.2012.2230632,Database-Driven Real-Time Heuristic Search in Video-Game Pathfinding,IEEE,Journals,"Real-time heuristic search algorithms satisfy a constant bound on the amount of planning per action, independent of the problem size. These algorithms are useful when the amount of time or memory resources are limited, or a rapid response time is required. An example of such a problem is pathfinding in video games where numerous units may be simultaneously required to react promptly to a player's commands. Classic real-time heuristic search algorithms cannot be deployed due to their obvious state revisitation (scrubbing). Recent algorithms have improved performance by using a database of precomputed subgoals. However, a common issue is that the precomputation time can be large, and there is no guarantee that the precomputed data adequately cover the search space. In this paper, we present a new approach that guarantees coverage by abstracting the search space, using the same algorithm that performs the real-time search. It reduces the precomputation time via the use of dynamic programming. The new approach eliminates the learning component and the resultant scrubbing. Experimental results on maps of tens of millions of grid cells from Counter-Strike: Source and benchmark maps from Dragon Age: Origins show significantly faster execution times and improved optimality results compared to previous real-time algorithms.",https://ieeexplore.ieee.org/document/6365249/,IEEE Transactions on Computational Intelligence and AI in Games,Sept. 2013,ieeexplore
10.1109/TIP.2019.2957930,Deep Image-to-Video Adaptation and Fusion Networks for Action Recognition,IEEE,Journals,"Existing deep learning methods for action recognition in videos require a large number of labeled videos for training, which is labor-intensive and time-consuming. For the same action, the knowledge learned from different media types, e.g., videos and images, may be related and complementary. However, due to the domain shifts and heterogeneous feature representations between videos and images, the performance of classifiers trained on images may be dramatically degraded when directly deployed to videos. In this paper, we propose a novel method, named Deep Image-to-Video Adaptation and Fusion Networks (DIVAFN), to enhance action recognition in videos by transferring knowledge from images using video keyframes as a bridge. The DIVAFN is a unified deep learning model, which integrates domain-invariant representations learning and cross-modal feature fusion into a unified optimization framework. Specifically, we design an efficient cross-modal similarities metric to reduce the modality shift among images, keyframes and videos. Then, we adopt an autoencoder architecture, whose hidden layer is constrained to be the semantic representations of the action class names. In this way, when the autoencoder is adopted to project the learned features from different domains to the same space, more compact, informative and discriminative representations can be obtained. Finally, the concatenation of the learned semantic feature representations from these three autoencoders are used to train the classifier for action recognition in videos. Comprehensive experiments on four real-world datasets show that our method outperforms some state-of-the-art domain adaptation and action recognition methods.",https://ieeexplore.ieee.org/document/8931264/,IEEE Transactions on Image Processing,2020,ieeexplore
10.1109/COMST.2019.2904897,Deep Learning in Mobile and Wireless Networking: A Survey,IEEE,Journals,"The rapid uptake of mobile devices and the rising popularity of mobile applications and services pose unprecedented demands on mobile and wireless networking infrastructure. Upcoming 5G systems are evolving to support exploding mobile traffic volumes, real-time extraction of fine-grained analytics, and agile management of network resources, so as to maximize user experience. Fulfilling these tasks is challenging, as mobile environments are increasingly complex, heterogeneous, and evolving. One potential solution is to resort to advanced machine learning techniques, in order to help manage the rise in data volumes and algorithm-driven applications. The recent success of deep learning underpins new and powerful tools that tackle problems in this space. In this paper, we bridge the gap between deep learning and mobile and wireless networking research, by presenting a comprehensive survey of the crossovers between the two areas. We first briefly introduce essential background and state-of-the-art in deep learning techniques with potential applications to networking. We then discuss several techniques and platforms that facilitate the efficient deployment of deep learning onto mobile systems. Subsequently, we provide an encyclopedic review of mobile and wireless networking research based on deep learning, which we categorize by different domains. Drawing from our experience, we discuss how to tailor deep learning to mobile environments. We complete this survey by pinpointing current challenges and open future directions for research.",https://ieeexplore.ieee.org/document/8666641/,IEEE Communications Surveys & Tutorials,thirdquarter 2019,ieeexplore
10.1109/TSMC.2020.2967936,Deep Q-Learning With Q-Matrix Transfer Learning for Novel Fire Evacuation Environment,IEEE,Journals,"Deep reinforcement learning (RL) is achieving significant success in various applications like control, robotics, games, resource management, and scheduling. However, the important problem of emergency evacuation, which clearly could benefit from RL, has been largely unaddressed. Indeed, emergency evacuation is a complex task that is difficult to solve with RL. An emergency situation is highly dynamic, with a lot of changing variables and complex constraints that make it challenging to solve. Also, there is no standard benchmark environment available that can be used to train RL agents for evacuation. A realistic environment can be complex to design. In this article, we propose the first fire evacuation environment to train RL agents for evacuation planning. The environment is modeled as a graph capturing the building structure. It consists of realistic features like fire spread, uncertainty, and bottlenecks. The implementation of our environment is in the OpenAI gym format, to facilitate future research. We also propose a new RL approach that entails pretraining the network weights of a DQN-based agent [DQN/Double-DQN (DDQN)/Dueling-DQN] to incorporate information on the shortest path to the exit. We achieved this by using tabular <inline-formula> <tex-math notation=""LaTeX"">$Q$ </tex-math></inline-formula>-learning to learn the shortest path on the building models graph. This information is transferred to the network by deliberately overfitting it on the <inline-formula> <tex-math notation=""LaTeX"">$Q$ </tex-math></inline-formula>-matrix. Then, the pretrained DQN model is trained on the fire evacuation environment to generate the optimal evacuation path under time varying conditions due to fire spread, bottlenecks, and uncertainty. We perform comparisons of the proposed approach with state-of-the-art RL algorithms like DQN, DDQN, Dueling-DQN, PPO, VPG, state-action-reward-state-action (SARSA), actorcritic method, and ACKTR. The results show that our method is able to outperform state-of-the-art models by a huge margin including the original DQN-based models. Finally, our model is tested on a large and complex real building consisting of 91 rooms, with the possibility to move to any other room, hence giving 8281 actions. In order to reduce the action space, we propose a strategy that involves one step simulation. That is, an action importance vector is added to the final output of the pretrained DQN and acts like an attention mechanism. Using this strategy, the action space is reduced by 90.1%. In this manner, the model is able to deal with large action spaces. Hence, our model achieves near optimal performance on the real world emergency environment.",https://ieeexplore.ieee.org/document/8989970/,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",Dec. 2021,ieeexplore
10.1109/TPAMI.2020.2964173,Deep Residual Correction Network for Partial Domain Adaptation,IEEE,Journals,"Deep domain adaptation methods have achieved appealing performance by learning transferable representations from a well-labeled source domain to a different but related unlabeled target domain. Most existing works assume source and target data share the identical label space, which is often difficult to be satisfied in many real-world applications. With the emergence of big data, there is a more practical scenario called partial domain adaptation, where we are always accessible to a more large-scale source domain while working on a relative small-scale target domain. In this case, the conventional domain adaptation assumption should be relaxed, and the target label space tends to be a subset of the source label space. Intuitively, reinforcing the positive effects of the most relevant source subclasses and reducing the negative impacts of irrelevant source subclasses are of vital importance to address partial domain adaptation challenge. This paper proposes an efficiently-implemented Deep Residual Correction Network (DRCN) by plugging one residual block into the source network along with the task-specific feature layer, which effectively enhances the adaptation from source to target and explicitly weakens the influence from the irrelevant source classes. Specifically, the plugged residual block, which consists of several fully-connected layers, could deepen basic network and boost its feature representation capability correspondingly. Moreover, we design a weighted class-wise domain alignment loss to couple two domains by matching the feature distributions of shared classes between source and target. Comprehensive experiments on partial, traditional and fine-grained cross-domain visual recognition demonstrate that DRCN is superior to the competitive deep domain adaptation approaches.",https://ieeexplore.ieee.org/document/8951442/,IEEE Transactions on Pattern Analysis and Machine Intelligence,1 July 2021,ieeexplore
10.1109/ACCESS.2021.3064928,Deep Space Network Scheduling via Mixed-Integer Linear Programming,IEEE,Journals,"NASAs Deep Space Network (DSN) is a globally-spanning communications network responsible for supporting the interplanetary spacecraft missions of NASA and other international users. The DSN is a highly utilized asset, and the large demand for its services makes the assignment of DSN resources a daunting computational problem. In this paper we study the DSN scheduling problem, which is the problem of assigning the DSNs limited resources to its users within a given time horizon. The DSN scheduling problem is oversubscribed, meaning that only a subset of the activities can be scheduled, and network operators must decide which activities to exclude from the schedule. We first formulate this challenging scheduling task as a Mixed-Integer Linear Programming (MILP) optimization problem. Next, we develop a sequential algorithm which solves the resulting MILP formulation to produce valid schedules for large-scale instances of the DSN scheduling problem. We use real world DSN data from week 44 of 2016 in order to evaluate our algorithms performance. We find that given a fixed run time, our algorithm outperforms a simple implementation of our MILP model, generating a feasible schedule in which 17% more activities are scheduled by the algorithm than by the simple implementation. We design a non-MILP based heuristic to further validate our results. We find that our algorithm also outperforms this heuristic, scheduling 8% more activities and 20% more tracking time than the best results achieved by the non-MILP implementation.",https://ieeexplore.ieee.org/document/9373338/,IEEE Access,2021,ieeexplore
10.1109/TII.2020.3038745,Deep-LearningBased App Sensitive Behavior Surveillance for Android Powered CyberPhysical Systems,IEEE,Journals,"Android as an operating system is now increasingly being adopted in industrial information systems, especially with cyber-physical systems (CPS). This also puts Android devices onto the front line of handling security-related data and conducting sensitive behaviors, which could be misused by the increasing number of polymorphic and metamorphic malicious applications targeting the platform. The existence of such malware threats, therefore, call for more accurate identification and surveillance of sensitive Android app behaviors, which is essential to the security of CPS and Internet of Things (IoT) devices powered by Android. Nevertheless, achieving dynamic app behavior monitoring and identification on real CPS powered by Android is challenging because of restrictions from the security and privacy model of the platform. In this article, the authors investigate how the latest advances in deep learning could address this security problem with better accuracy. Specifically, a deep learning engine is proposed that detects sensitive app behaviors by classifying patterns of system-wide statistics, such as available storage space and transmitted packet volume, using a customized deep neural network based on existing models called Encoder and ResNet. Meanwhile, to handle resource limitations on typical CPS and IoT devices, sparse learning is adopted to reduce the amount of valid parameters in the trained neural network. Evaluations show that the proposed model outperforms a well-established group of baselines on time series classification in identifying sensitive app behaviors with background noise and the targeted behaviors potentially overlapping.",https://ieeexplore.ieee.org/document/9262070/,IEEE Transactions on Industrial Informatics,Aug. 2021,ieeexplore
10.1109/TMC.2019.2893250,DeepWear: Adaptive Local Offloading for On-Wearable Deep Learning,IEEE,Journals,"Due to their on-body and ubiquitous nature, wearables can generate a wide range of unique sensor data creating countless opportunities for deep learning tasks. We propose DeepWear, a deep learning (DL) framework for wearable devices to improve the performance and reduce the energy footprint. DeepWear strategically offloads DL tasks from a wearable device to its paired handheld device through local network connectivity such as Bluetooth. Compared to the remote-cloud-based offloading, DeepWear requires no Internet connectivity, consumes less energy, and is robust to privacy breach. DeepWear provides various novel techniques such as context-aware offloading, strategic model partition, and pipelining support to efficiently utilize the processing capacity from nearby paired handhelds. Deployed as a user-space library, DeepWear offers developer-friendly APIs that are as simple as those in traditional DL libraries such as TensorFlow. We have implemented DeepWear on the Android OS and evaluated it on COTS smartphones and smartwatches with real DL models. DeepWear brings up to 5.08 and 23.0 execution speedup, as well as 53.5 and 85.5 percent energy saving compared to wearable-only and handheld-only strategies, respectively.",https://ieeexplore.ieee.org/document/8618364/,IEEE Transactions on Mobile Computing,1 Feb. 2020,ieeexplore
10.1109/TITS.2021.3125745,Deployment Optimization for Shared e-Mobility Systems With Multi-Agent Deep Neural Search,IEEE,Journals,"Shared e-mobility services have been widely tested and piloted in cities across the globe, and already woven into the fabric of modern urban planning. This paper studies a practical yet important problem in those systems: how to deploy and manage their infrastructure across space and time, so that the services are <i>ubiquitous</i> to the users while <i>sustainable</i> in profitability. However, in real-world systems evaluating the performance of different deployment strategies and then finding the optimal plan is prohibitively expensive, as it is often infeasible to conduct many iterations of trial-and-error. We tackle this by designing a high-fidelity simulation environment, which abstracts the key operation details of the shared e-mobility systems at fine-granularity, and is calibrated using data collected from the real-world. This allows us to try out arbitrary deployment plans to learn the optimal given specific context, before actually implementing any in the real-world systems. In particular, we propose a novel multi-agent neural search approach, in which we design a hierarchical controller to produce tentative deployment plans. The generated deployment plans are then tested using a multi-simulation paradigm, i.e., evaluated in parallel, where the results are used to train the controller with deep reinforcement learning. With this closed loop, the controller can be steered to have higher probability of generating better deployment plans in future iterations. The proposed approach has been evaluated extensively in our simulation environment, and experimental results show that it outperforms baselines e.g., human knowledge, and state-of-the-art heuristic-based optimization approaches in both service coverage and net revenue.",https://ieeexplore.ieee.org/document/9616397/,IEEE Transactions on Intelligent Transportation Systems,March 2022,ieeexplore
10.1109/ACCESS.2019.2893354,Depth Estimation From a Light Field Image Pair With a Generative Model,IEEE,Journals,"In this paper, we propose a novel method to estimate the disparity maps from a light field image pair captured by a pair of light field cameras. Our method integrates two types of critical depth cues, which are separately inferred from the epipolar plane images and binocular stereo vision into a global solution. At the same time, in order to produce highly accurate disparity maps, we adopt a generative model, which can estimate a light field image only with the central subaperture view and corresponding hypothesized disparity map. The objective function of our method is formulated to minimize two energy terms/differences. One is the difference between the two types of previously extracted disparity maps and the target disparity maps, directly optimized in the gray-scale disparity space. The other indicates the difference between the estimated light field images and the input light field images, optimized in the RGB color space. Comprehensive experiments conducted on real and virtual scene light field image pairs demonstrate the effectiveness of our method.",https://ieeexplore.ieee.org/document/8620996/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2018.2840086,Detecting Anomalies in Time Series Data via a Meta-Feature Based Approach,IEEE,Journals,"Anomaly detection of time series is an important topic that has been widely studied in many application areas. A number of computational methods were developed for this task in the past few years. However, the existing approaches still have many drawbacks when they were applied to specific questions. In this paper, we proposed a meta-feature-based anomaly detection approach (MFAD) to identify the abnormal states of a univariate or multivariate time series based on local dynamics. Differing from the traditional strategies of sliding window in anomaly detection, our method first defined six meta-features to statistically describe the local dynamics of a 1-D sequence with arbitrary length. Second, multivariate time series was converted to a new 1-D sequence, so that each of its segmented subsequence was represented as one sample with six meta-features. Finally, the anomaly detection of univariate/multivariate time series was implemented by identifying the outliers from the samples in a 6-D transformed space. In order to validate the effectiveness of MFAD, we applied our method on various univariate and multivariate time series datasets, including six well-known standard datasets (e.g. ECG and Air Quality) and eight real-world datasets in shield tunneling construction. The simulation results show that the proposed method MFAD not only identifies the local abnormal states in the original time series but also drastically reduces the computational complexity. In summary, the proposed method effectively identified the abnormal states of dynamical parameters in various application fields.",https://ieeexplore.ieee.org/document/8364549/,IEEE Access,2018,ieeexplore
10.1109/ACCESS.2020.2999351,Detecting Memory Life-Cycle Bugs With Extended Define-Use Chain Analysis,IEEE,Journals,"OS kernels leverage various memory allocation functions to carry out memory allocation, and memory data in kernel space of OS should be cautiously handled, e.g., allocating with kmalloc() and freeing with kfree(). However, real cases do exist where memory data is incorrectly allocated/freed, not checked before dereferenced, or left unfreed when out of use. We define these cases as Memory Life-cycle (MLC) bugs, and according to what we know, this new type of software bugs has not been deeply researched yet. In this paper, we go deep into the life-cycle of kernel memory space, including allocation, dereference and free, and propose the first systematical study of MLC bugs and build an automated and scalable detection framework, MLC bug sanitizer (MLCSan). MLCSan is capable of revealing memory allocation and free functions OS kernels. Besides, the occurrences of allocating, dereferencing and freeing sites can be automatically detected by MLCSan, leading to cases where MLC bugs may appear. Moreover, experiment result of analyzing the latest mainline OS kernels with MLCSan is a strong proof that MLCSan is effective in detecting MLC bugs and can scale to different platforms, in which 41 new bugs are identified in Linux and FreeBSD. And undoubtedly, we will open source MLCSan prototype to contribute to the security research in this area.",https://ieeexplore.ieee.org/document/9106323/,IEEE Access,2020,ieeexplore
10.23919/TST.2017.7986945,Detecting isolate safe areas in wireless sensor monitoring systems,TUP,Journals,"Wireless sensors are deployed widely to monitor space, emergent events, and disasters. Collected realtime sensory data are precious for completing rescue missions quickly and efficiently. Detecting isolate safe areas is significant for various applications of event and disaster monitoring since valuable real-time information can be provided for the rescue crew to save persons who are trapped in isolate safe areas. We propose a centralized method to detect isolate safe areas via discovering holes in event areas. In order to shorten the detection delay, a distributed isolate safe area detection method is studied. The distributed method detects isolate safe areas during the process of event detection. Moreover, detecting isolate safe areas in a building is addressed particularly since the regular detecting method is not applicable. Our simulation results show that the distributed method can detect all isolate safe areas in an acceptable short delay.",https://ieeexplore.ieee.org/document/7986945/,Tsinghua Science and Technology,Aug. 2017,ieeexplore
10.1109/ACCESS.2021.3071795,Developing Software Signature Search Engines Using Paragraph Vector Model: A Triage Approach for Digital Forensics,IEEE,Journals,"Today, with the growth of information and communication technology, digital crimes have also spread. Advanced storage technologies and their low cost have led to a significant increase in their use. Therefore, the high volume of digital data to be analyzed is a challenge facing digital forensic investigators. Digital forensic triage solutions aim to alleviate the forensic backlog. A promising triage technique is to quickly find the software packages run on the target system to narrow down the search space. In this paper, we propose a software signature search engine (S3E) to identify software on the system under investigation. The document collection of this search engine consists of software signatures, and the query is the features extracted from the system's hard disk. We propose a forensic differential analysis model to build software signatures. Besides, we use the paragraph vector model to construct the corresponding vectors of each software signature and find similarities between the query vector and the signature vectors. Different design parameters are involved in making software signature search engines, and distinct values of these parameters lead to different models. We have measured the performance of these S3E models against several controlled systems and some pseudo-real systems. The experimental results on both datasets show that some S3E models achieve perfect recall, and many of them have a recall of more than 90%. Besides, we find that the recall rate of the S3E models in both datasets is higher than the averaged word2vec model and the TF-IDF model.",https://ieeexplore.ieee.org/document/9399093/,IEEE Access,2021,ieeexplore
10.1109/18.985979,Diagonal algebraic space-time block codes,IEEE,Journals,"We construct a new family of linear space-time (ST) block codes by the combination of rotated constellations and the Hadamard transform, and we prove them to achieve the full transmit diversity over a quasi-static or fast fading channels. The proposed codes transmit at a normalized rate of 1 symbol/s. When the number of transmit antennas n=1, 2, or n is a multiple of four, we spread a rotated version of the information symbol vector by the Hadamard transform and send it over n transmit antennas and n time periods; for other values of n, we construct the codes by sending the components of a rotated version of the information symbol vector over the diagonal of an n /spl times/ n ST code matrix. The codes maintain their rate, diversity, and coding gains for all real and complex constellations carved from the complex integers ring Z [i], and they outperform the codes from orthogonal design when using complex constellations for n &gt; 2. The maximum-likelihood (ML) decoding of the proposed codes can be implemented by the sphere decoder at a moderate complexity. It is shown that using the proposed codes in a multiantenna system yields good performances with high spectral efficiency and moderate decoding complexity.",https://ieeexplore.ieee.org/document/985979/,IEEE Transactions on Information Theory,March 2002,ieeexplore
10.1109/TIP.2019.2892703,Discrete Hashing With Multiple Supervision,IEEE,Journals,"Supervised hashing methods have achieved more promising results than unsupervised ones by leveraging label information to generate compact and accurate hash codes. Most of the prior supervised hashing methods construct an <inline-formula> <tex-math notation=""LaTeX"">$n\times n$ </tex-math></inline-formula> instance-pairwise similarity matrix, where <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> is the number of training samples. Nevertheless, this kind of similarity matrix results in high memory space cost and makes the optimization time-consuming, making it unacceptable in many real applications. In addition, most of the methods relax the discrete constraints to solve the optimization problem, which may cause large quantization errors and finally lead to poor performance. To address these limitations, in this paper, we present a novel hashing method, named discrete hashing with multiple supervision (MSDH). MSDH supervises the hash code learning with both class-wise and instance-class similarity matrices, whose space cost is much less than the instance-pairwise similarity matrix. With multiple supervision information, better hash codes can be learned. Besides, an iterative optimization algorithm is proposed to directly learn the discrete hash codes instead of relaxing the binary constraints. Experimental results on several widely used benchmark datasets demonstrate that MSDH outperforms some state-of-the-art methods.",https://ieeexplore.ieee.org/document/8610117/,IEEE Transactions on Image Processing,June 2019,ieeexplore
10.1109/TIP.2018.2859589,"Discriminant Analysis via Joint Euler Transform and <inline-formula> <tex-math notation=""LaTeX"">$\ell_{2,1}$ </tex-math></inline-formula>-Norm",IEEE,Journals,"Linear discriminant analysis (LDA) has been widely used for face recognition. However, when identifying faces in the wild, the existence of outliers that deviate significantly from the rest of the data can arbitrarily skew the desired solution. This usually deteriorates LDA's performance dramatically, thus preventing it from mass deployment in real-world applications. To handle this problem, we propose an effective distance metric learning method-based LDA, namely, Euler LDA-L21 (e-LDA-L21). e-LDA-L21 is carried out in two stages, in which each image is mapped into a complex space by Euler transform in the first stage and the <sub>2,1</sub>-norm is adopted as the distance metric in the second stage. This not only reveals nonlinear features but also exploits the geometric structure of data. To solve e-LDA-L21 efficiently, we propose an iterative algorithm, which is a closed-form solution at each iteration with convergence guaranteed. Finally, we extend e-LDA-L21 to Euler 2DLDA-L21 (e-2DLDA-L21) which further exploits the spatial information embedded in image pixels. Experimental results on several face databases demonstrate its superiority over the state-of-the-art algorithms.",https://ieeexplore.ieee.org/document/8419742/,IEEE Transactions on Image Processing,Nov. 2018,ieeexplore
10.1109/TFUZZ.2020.3039371,Disjunctive Fuzzy Neural Networks: A New Splitting-Based Approach to Designing a TS Fuzzy Model,IEEE,Journals,"This article proposes a new network approach toward the implementation of TakagiSugeno (TS) fuzzy models referred to as disjunctive fuzzy neural networks (DJFNNs). The proposed DJFNN involves a novel network architecture and a greedy learning algorithm. Being different from the existing grid-based and clustering-based network architectures, the proposed architecture adds an OR neural layer positioned between the fuzzification layer and the rule layer. In this way, the implied constraint between the number of rules and the number of fuzzy labels is excluded so that a curse of dimensionality can be overcome and more interpretable models are formed. Furthermore, inspired by the core algorithm for building a decision tree, a topdown, nonbacktracking, and greedy algorithm is proposed to learn the unknown parameters of the networks. The input space splits into smaller and smaller subspace along the predefined fuzzy grids in a supervised manner meanwhile the associated conditions of the TS fuzzy model are identified. The greedy algorithm is applicable to high-dimensional problems since there is no exponential growth in time or space as the dimensionality increases. The new network architecture and greedy learning algorithm make the proposed DJFNN a regression model of high interpretability and good prediction capability, particularly suitable for solving the high-dimensional problems. The DJFNN was experimented with using a synthetic dataset and 28 real-world datasets and compared with classical and state-of-the-art methods through nonparametric statistical tests. The results confirmed the effectiveness of the DJFNN in terms of accuracy, interpretability, and computational cost.",https://ieeexplore.ieee.org/document/9264752/,IEEE Transactions on Fuzzy Systems,Feb. 2022,ieeexplore
10.1109/TBC.2020.2996087,Distributed Caching in Converged Networks: A Deep Reinforcement Learning Approach,IEEE,Journals,"Content caching is an effective technique to alleviate the burden on backhaul links and reduce the traffic in cellular networks. In converged networks, broadcasting networks can push non-real-time popular services to different types of terminals during off-peak hours, while cellular networks are deployed to meet personalized needs. However, the storage capacity of terminals are usually limited. In this context, we study on the converged networks to push and cache the popular services in the router nodes close to the terminators. In this scheme, the most popular services are transmitted by the broadcasting base station and cached in the router nodes in a distributed cache network. Then, users can access the cached services in a more energy efficient manner. Due to the limited storage capacity of the router node, we assume that the user can access the cached services within two hops. Then, we formulate the service scheduling problem as a Markov Decision Process, aiming to maximize equivalent throughput (ET). Due to the large state space involved in the distributed cache network, it is quite challenging to obtain a tractable solution by the classical optimization algorithms. To handle this problem, a deep reinforcement learning based framework is proposed to tackle this problem. The simulation results show that the proposed algorithms are very effective; and they outperform the conventional one in term of the ET, especially when the users in the network subject to Poisson point process distribution.",https://ieeexplore.ieee.org/document/9107459/,IEEE Transactions on Broadcasting,March 2021,ieeexplore
10.1109/TAC.2019.2955999,Distributed Mechanism Design With Learning Guarantees for Private and Public Goods Problems,IEEE,Journals,"Mechanism design for fully strategic agents typically assumes that agents broadcast their messages to a central authority that performs allocation and taxes/subsidizes agents. In addition, most of the designed mechanisms do not provide any guarantees that the Nash equilibrium (NE) will be reached asymptotically. Whenever such results are provided, they refer to convergence of specific predesigned learning dynamics and it is assumed that agents will follow such prescription. Both assumptions are an impediment in practical application of mechanism design in real-life allocation problems. In this article, we consider two common resource allocation problems: sharing K infinitely divisible resources among strategic agents for their private consumption (private goods), and determining the level for an infinitely divisible public good with P features, that is shared between strategic agents. For both cases, we present a distributed mechanism for a set of agents who communicate through a given network. In a distributed mechanism, agents' messages are not broadcasted to all other agents as in the standard mechanism design framework, but are exchanged only in the local neighborhood of each agent. The presented mechanisms produce a unique NE, they fully implement the social welfare maximizing allocation, and are budget-balanced at NE. Subsequently, it is shown that the mechanisms induce a game with contractive best-response, leading to guaranteed convergence for all learning dynamics within the adaptive best-response dynamics class, including dynamics such as Cournot best-response, k-period best-response, and Fictitious Play. While the abovementioned mechanisms have message dimensionality that grows quadratically with the number of agents, we show that if one is willing to forgo the strong convergence guarantees, it is possible to design a distributed mechanism with total message space dimensionality growing only linearly with the number of agents. Finally, we demonstrate the abovementioned results through a numerical study of convergence under repeated play, for various communication graphs and learning dynamics.",https://ieeexplore.ieee.org/document/8913590/,IEEE Transactions on Automatic Control,Oct. 2020,ieeexplore
10.1109/ACCESS.2020.2982581,Distributed Optimal Deployment on a Circle for Cooperative Encirclement of Autonomous Mobile Multi-Agents,IEEE,Journals,"A distributed encirclement points deployment scheme for a group of autonomous mobile agents is addressed in this paper. Herein, each agent can measure its own azimuth related to the common target and can at least communicate with its two adjacent neighbors. Given its space-cooperative character, the encirclement points deployment problem is formulated as the coverage control problem on a circle. The measurement range of azimuth sensor is taken into consideration when doing problem formulation, which is closer to the facts in real-world applications. Then, the fully distributed control protocols are put forward based on geometric principle and the convergence is proved strictly with algebraic method. The proposed control protocols can steer the mobile agents to distribute evenly on the circle such that the coverage cost function is minimized, and meanwhile the mobile agents' spatial order on the circle is preserved throughout the systems' evolution. A noteworthy feature of the proposed control protocols is that only the azimuths of a mobile agent and its two adjacent neighbors are needed to calculate the mobile agent's control input, so that the control protocols can be easily implemented in general. Moreover, an adjustable feedback gain is introduced, and it can be employed to improve the convergence rate effectively. Finally, numerical simulations are carried out to verify the effectiveness of the proposed distributed control protocols.",https://ieeexplore.ieee.org/document/9044350/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2019.2918480,Distribution Network Reconfiguration Using Selective Firefly Algorithm and a Load Flow Analysis Criterion for Reducing the Search Space,IEEE,Journals,"This paper proposes an alternative to solve the distribution network reconfiguration (DNR) problem, aiming real power losses' minimization. For being a problem that has complexity for its solution, approximate techniques are adequate for solving it. Here, the proposition is a technique based on the firefly metaheuristic, named selective firefly algorithm, where the positioning of these insects is compressed in a selective range of values. The algorithm is applied to the DNR, and all its implementation and adequacy to the problem studied are presented. To define the search space, the methodology presented initially considers a set of candidate switches for opening based on the studied systems' mesh analysis. To reduce these possibilities, a refinement through a load flow analysis criterion (LFAC) is proposed. This LFAC considers the real power losses on each branch for a configuration with all switches closed, then, selecting possible switches to elimination from the set previously established. To demonstrate the behavior and the viability of the LFAC, it was initially applied on a 5 buses' and 7 branches' system. Also, to avoid getting stuck on results that may be considered not good, a disturbance resetting the population is set to occur every time a counter reaches a pre-defined number of times that the best solution does not change. Results found for simulations with 33, 70, and 84 buses are presented and comparisons with selective particle swarm optimization (SPSO) and selective bat algorithm (SBAT) are made.",https://ieeexplore.ieee.org/document/8720166/,IEEE Access,2019,ieeexplore
10.1109/TNNLS.2014.2301951,Divisive Gaussian Processes for Nonstationary Regression,IEEE,Journals,"Standard Gaussian process regression (GPR) assumes constant noise power throughout the input space and stationarity when combined with the squared exponential covariance function. This can be unrealistic and too restrictive for many real-world problems. Nonstationarity can be achieved by specific covariance functions, though prior knowledge about this nonstationarity can be difficult to obtain. On the other hand, the homoscedastic assumption is needed to allow GPR inference to be tractable. In this paper, we present a divisive GPR model which performs nonstationary regression under heteroscedastic noise using the pointwise division of two nonparametric latent functions. As the inference on the model is not analytically tractable, we propose a variational posterior approximation using expectation propagation (EP) which allows for accurate inference at reduced cost. We have also made a Markov chain Monte Carlo implementation with elliptical slice sampling to assess the quality of the EP approximation. Experiments support the usefulness of the proposed approach.",https://ieeexplore.ieee.org/document/6737292/,IEEE Transactions on Neural Networks and Learning Systems,Nov. 2014,ieeexplore
10.1109/ACCESS.2020.2983829,Dynamic Graph Regularization and Label Relaxation-Based Sparse Matrix Regression for Two-Dimensional Feature Selection,IEEE,Journals,"Sparse matrix regression (SMR) is a two-dimensional supervised feature selection method that can directly select the features on matrix data. It uses several couples of left and right regression vectors for each classifier and integrates them in formulating the regression function. However, SMR does not consider the local geometry of image samples, and it assumes that the training samples should exactly fit a linear model or a strict binary label matrix by left and right regression matrices. In order to enlarge margins between different classes and preserve the intrinsic geometry structure of samples in the transformed space, we will propose dynamic graph regularization and label relaxation-based SMR (abbreviated as DGRLR-SMR) method for two-dimensional supervised feature selection. First, the label relaxation SMR is established by relaxing the strict binary label matrix into a slack variable matrix via a nonnegative label relaxation matrix by the $\varepsilon $ -dragging technique. Second, we construct a dynamic graph matrix learning model, rather than using the heat kernel function to obtain a fixed graph matrix, to capture the discriminative information and the local manifold structure of the image samples. Therefore, the proposed model not only enlarges margins between different classes, but also obtains a sparse transformation matrix and avoids the problem of over-fitting. An optimization algorithm is devised to solve this model, and it has closed-form solutions in each iteration so that it can be implemented easily in real application. Extensive experiments on several data sets demonstrate the superiority of our method.",https://ieeexplore.ieee.org/document/9049130/,IEEE Access,2020,ieeexplore
10.1109/TCBB.2013.119,EEG/ERP Adaptive Noise Canceller Design with Controlled Search Space (CSS) Approach in Cuckoo and Other Optimization Algorithms,IEEE,Journals,"This paper explores the migration of adaptive filtering with swarm intelligence/evolutionary techniques employed in the field of electroencephalogram/event-related potential noise cancellation or extraction. A new approach is proposed in the form of controlled search space to stabilize the randomness of swarm intelligence techniques especially for the EEG signal. Swarm-based algorithms such as Particles Swarm Optimization, Artificial Bee Colony, and Cuckoo Optimization Algorithm with their variants are implemented to design optimized adaptive noise canceler. The proposed controlled search space technique is tested on each of the swarm intelligence techniques and is found to be more accurate and powerful. Adaptive noise canceler with traditional algorithms such as least-mean-square, normalized least-mean-square, and recursive least-mean-square algorithms are also implemented to compare the results. ERP signals such as simulated visual evoked potential, real visual evoked potential, and real sensorimotor evoked potential are used, due to their physiological importance in various EEG studies. Average computational time and shape measures of evolutionary techniques are observed 8.21E-01 sec and 1.73E-01, respectively. Though, traditional algorithms take negligible time consumption, but are unable to offer good shape preservation of ERP, noticed as average computational time and shape measure difference, 1.41E-02 sec and 2.60E+00, respectively.",https://ieeexplore.ieee.org/document/6606790/,IEEE/ACM Transactions on Computational Biology and Bioinformatics,Nov.-Dec. 2013,ieeexplore
10.1109/LSP.2020.2996935,Effect of the Latent Structure on Clustering With GANs,IEEE,Journals,"Generative adversarial networks (GANs) have shown remarkable success in the generation of data from natural data manifolds such as images. In several scenarios, it is desirable that generated data is well-clustered, especially when there is severe class imbalance. In this paper, we focus on the problem of clustering in the generated space of GANs and uncover its relationship with the characteristics of the latent space. We derive from first principles, the necessary and sufficient conditions needed to achieve faithful clustering in the GAN framework: (i) presence of a multimodal latent space with adjustable priors, (ii) existence of a latent space inversion mechanism and, (iii) imposition of the desired cluster priors on the latent space. We also identify the GAN models in the literature that partially satisfy these conditions and demonstrate the importance of all the components required, through ablative studies on multiple real-world image datasets. Additionally, we describe a procedure to construct a multimodal latent space which facilitates learning of cluster priors with sparse supervision. Codes for our implementation is available at https://github.com/NEMGAN/NEMGAN-P.",https://ieeexplore.ieee.org/document/9099059/,IEEE Signal Processing Letters,2020,ieeexplore
10.1109/TPAMI.2007.70815,Effective Proximity Retrieval by Ordering Permutations,IEEE,Journals,"We introduce a new probabilistic proximity search algorithm for range and A""-nearest neighbor (A""-NN) searching in both coordinate and metric spaces. Although there exist solutions for these problems, they boil down to a linear scan when the space is intrinsically high dimensional, as is the case in many pattern recognition tasks. This, for example, renders the A""-NN approach to classification rather slow in large databases. Our novel idea is to predict closeness between elements according to how they order their distances toward a distinguished set of anchor objects. Each element in the space sorts the anchor objects from closest to farthest to it and the similarity between orders turns out to be an excellent predictor of the closeness between the corresponding elements. We present extensive experiments comparing our method against state-of-the-art exact and approximate techniques, both in synthetic and real, metric and nonmetric databases, measuring both CPU time and distance computations. The experiments demonstrate that our technique almost always improves upon the performance of alternative techniques, in some cases by a wide margin.",https://ieeexplore.ieee.org/document/4378393/,IEEE Transactions on Pattern Analysis and Machine Intelligence,Sept. 2008,ieeexplore
10.1109/TSE.2017.2786222,Effectively Incorporating Expert Knowledge in Automated Software Remodularisation,IEEE,Journals,"Remodularising the components of a software system is challenging: sound design principles (e.g., coupling and cohesion) need to be balanced against developer intuition of which entities conceptually belong together. Despite this, automated approaches to remodularisation tend to ignore domain knowledge, leading to results that can be nonsensical to developers. Nevertheless, suppling such knowledge is a potentially burdensome task to perform manually. A lot information may need to be specified, particularly for large systems. Addressing these concerns, we propose the SUpervised reMOdularisation (SUMO) approach. SUMO is a technique that aims to leverage a small subset of domain knowledge about a system to produce a remodularisation that will be acceptable to a developer. With SUMO, developers refine a modularisation by iteratively supplying corrections. These corrections constrain the type of remodularisation eventually required, enabling SUMO to dramatically reduce the solution space. This in turn reduces the amount of feedback the developer needs to supply. We perform a comprehensive systematic evaluation using 100 real world subject systems. Our results show that SUMO guarantees convergence on a target remodularisation with a tractable amount of user interaction.",https://ieeexplore.ieee.org/document/8259332/,IEEE Transactions on Software Engineering,1 July 2018,ieeexplore
10.1109/ACCESS.2021.3079932,Efficient Query Refinement for View Recommendation in Visual Data Exploration,IEEE,Journals,"The need for efficient and effective data exploration has resulted in several solutions that automatically recommend interesting visualizations. The main idea underlying those solutions is to automatically generate all possible views of data, and recommend the top-k interesting views. However, those solutions assume that the analyst is able to formulate a well-defined query that selects a subset of data, which contains insights. Meanwhile, in reality, it is typically a challenging task to pose an exploratory query, which can immediately reveal some insights. To address that challenge, in this work we propose utilizing query refinement as one technique that allows to automatically adjust the analyst's input query to discover such valuable insights. However, a naive query refinement, in addition to generating a prohibitively large search space, also raises other problems such as deviating from the user's preference and recommending statistically insignificant views. In this paper, we address those problems and propose a novel suit of schemes, which efficiently navigate the refined queries search space to recommend the top-k insights that meet all of the analyst's pre-specified criteria.",https://ieeexplore.ieee.org/document/9430506/,IEEE Access,2021,ieeexplore
10.1109/TITB.2004.828883,Efficient migration of complex off-line computer vision software to real-time system implementation on generic computer hardware,IEEE,Journals,"This paper addresses the problem of migrating large and complex computer vision code bases that have been developed off-line, into efficient real-time implementations avoiding the need for rewriting the software, and the associated costs. Creative linking strategies based on Linux loadable kernel modules are presented to create a simultaneous realization of real-time and off-line frame rate computer vision systems from a single code base. In this approach, systemic predictability is achieved by inserting time-critical components of a user-level executable directly into the kernel as a virtual device driver. This effectively emulates a single process space model that is nonpreemptable, nonpageable, and that has direct access to a powerful set of system-level services. This overall approach is shown to provide the basis for building a predictable frame-rate vision system using commercial off-the-shelf hardware and a standard uniprocessor Linux operating system. Experiments on a frame-rate vision system designed for computer-assisted laser retinal surgery show that this method reduces the variance of observed per-frame central processing unit cycle counts by two orders of magnitude. The conclusion is that when predictable application algorithms are used, it is possible to efficiently migrate to a predictable frame-rate computer vision system.",https://ieeexplore.ieee.org/document/1303557/,IEEE Transactions on Information Technology in Biomedicine,June 2004,ieeexplore
10.1109/TRO.2018.2878363,"Efficient, Multifidelity Perceptual Representations via Hierarchical Gaussian Mixture Models",IEEE,Journals,"This paper presents a probabilistic environment representation that allows efficient high-fidelity modeling and inference toward enabling informed planning (active perception) on a computationally constrained mobile autonomous system. The proposed approach exploits the fact that real-world environments inherently possess structure that introduces dependencies between spatially distinct locations. Gaussian mixture models are employed to capture these structural dependencies and learn a semiparametric, arbitrary resolution spatial representation. A hierarchy of spatial models is proposed to enable a multifidelity representation with the variation in fidelity quantified via information-theoretic measures. Crucially for active perception, the proposed modeling approach enables a distribution over occupancy with an associated measure of uncertainty via incorporation of free space information. Evaluation of the proposed technique via a real-time graphics processing unit based implementation is presented on real-world data sets in diverse environments. The proposed approach is shown to perform favorably as compared to state-of-the-art occupancy mapping techniques in terms of memory footprint, prediction accuracy, and generalizability to structurally diverse environments.",https://ieeexplore.ieee.org/document/8534383/,IEEE Transactions on Robotics,Feb. 2019,ieeexplore
10.1109/ACCESS.2019.2905000,Electrocardiogram Reconstruction Based on Compressed Sensing,IEEE,Journals,"Compressed Sensing (CS) attempts to acquire and reconstruct a sparse signal from a sampling much below the Nyquist rate. In this paper, we proposed novel CS algorithms for reconstructing under-sampled and compressed electrocardiogram (ECG) signal. In the proposed CS-ECG scheme, the ECG signal was first sub-sampled randomly and mapped onto a two-dimensional (2D) space by using Cut and Align (CAB), for the purpose of promoting sparsity. A nonlinear optimization model was then used to reconstruct the 2D signal. In the compression scheme, the ECG signal was mapped into the frequency domain, and the compression was achieved by a series of multiplying and accumulating between the original ECG and a Gaussian random matrix. For the reconstruction, two matching pursuits (MP) methods and two blocks sparse Bayesian learning (BSBL) methods were implemented and evaluated by the percentage root-mean-square difference (PRD). Based on the test with real ECG data, it was found that the proposed CS scheme was capable of faithfully reconstructing ECG signals with only 30% acquisition.",https://ieeexplore.ieee.org/document/8667447/,IEEE Access,2019,ieeexplore
10.1109/JIOT.2021.3063686,Enabling Massive IoT Toward 6G: A Comprehensive Survey,IEEE,Journals,"Nowadays, many disruptive Internet-of-Things (IoT) applications emerge, such as augmented/virtual reality online games, autonomous driving, and smart everything, which are massive in number, data intensive, computation intensive, and delay sensitive. Due to the mismatch between the fifth generation (5G) and the requirements of such massive IoT-enabled applications, there is a need for technological advancements and evolutions for wireless communications and networking toward the sixth-generation (6G) networks. 6G is expected to deliver extended 5G capabilities at a very high level, such as Tbps data rate, sub-ms latency, cm-level localization, and so on, which will play a significant role in supporting massive IoT devices to operate seamlessly with highly diverse service requirements. Motivated by the aforementioned facts, in this article, we present a comprehensive survey on 6G-enabled massive IoT. First, we present the drivers and requirements by summarizing the emerging IoT-enabled applications and the corresponding requirements, along with the limitations of 5G. Second, visions of 6G are provided in terms of core technical requirements, use cases, and trends. Third, a new network architecture provided by 6G to enable massive IoT is introduced, i.e., space-air-ground-underwater/sea networks enhanced by edge computing. Fourth, some breakthrough technologies, such as machine learning and blockchain, in 6G are introduced, where the motivations, applications, and open issues of these technologies for massive IoT are summarized. Finally, a use case of fully autonomous driving is presented to show 6G supports massive IoT.",https://ieeexplore.ieee.org/document/9369324/,IEEE Internet of Things Journal,"1 Aug.1, 2021",ieeexplore
10.1109/LRA.2020.3012951,End-to-End Tactile Feedback Loop: From Soft Sensor Skin Over Deep GRU-Autoencoders to Tactile Stimulation,IEEE,Journals,"Tactile feedback is a key sensory channel that contributes to our ability to perform precise manipulations. In this regard, sensor skin provides robots with the sense of touch making them increasingly capable of dexterous object manipulation. However, in applications like teleoperation, the complex sensory input of an infinite number of different textures must be projected to the human user's skin in a meaningful manner. In addressing this issue, a deep gated recurrent unit-based autoencoder (GRU-AE) that captured the perceptual dimensions of tactile textures in latent space was deployed to implicitly understand unseen textures. The expression of unknown textures in this latent space allowed for the definition of a control law to effectively drive tactile displays and to convey tactile feedback in a psycho-physically meaningful manner. The approach was experimentally verified by evaluating the prediction performance of the GRU-AE on seen and unseen data that were gathered during active tactile exploration of objects commonly encountered in daily living. A user study on a custom-made tactile display was conducted in which real tactile perceptions in response to active tactile object exploration were compared to the emulated tactile feedback using the proposed tactile feedback loop. The results suggest that the deep GRU-AE for tactile display control offers an effective and intuitive method for efficient end-to-end tactile feedback during active tactile texture exploration.",https://ieeexplore.ieee.org/document/9152113/,IEEE Robotics and Automation Letters,Oct. 2020,ieeexplore
10.1109/ACCESS.2021.3050780,Energy Efficient and Safe Control Strategy for Electric Vehicles Including Driver Preference,IEEE,Journals,"In this manuscript, a control strategy for electric vehicles is developed to optimise the energy consumption while respecting constraints associated with both inter-vehicle safety and comfort, which is a challenge in typical optimal control solution methodologies. Firstly, the long-term optimal control is developed using Pontryagin's Maximum Principle (PMP). Thereafter, the obtained PMP solution is used to bound the state space for a computationally tractable Dynamic Programming (DP) optimisation to ensure the satisfaction of the safety constraints. While the acceleration subproblem solution is similar to internal combustion engine vehicles (ICEVs), the combination of regenerative and hydraulic braking significantly alters the nature of the optimal braking profile. Since a DP solution is not tractable for real-time implementation of combined braking, a fast heuristic is developed, which achieves 98% of the optimal energy recovery calculated by the DP in the simulated cases. Simulation results demonstrate that the proposed strategy respects the acceleration and safety constraints while saving approximately 5% energy use without significantly increasing travel time. Further simulations were conducted to evaluate the effect of driver preferences on energy use. It was shown that a 9.5% reduction in energy use if the driver is willing to accept a 10% speed reduction.",https://ieeexplore.ieee.org/document/9319677/,IEEE Access,2021,ieeexplore
10.1109/TSG.2019.2892595,Energy Theft Detection Using Gradient Boosting Theft Detector With Feature Engineering-Based Preprocessing,IEEE,Journals,"For the smart grid energy theft identification, this letter introduces a gradient boosting theft detector (GBTD) based on the three latest gradient boosting classifiers (GBCs): 1) extreme gradient boosting; 2) categorical boosting; and 3) light gradient boosting method. While most of existing machine learning (ML) algorithms just focus on fine tuning the hyperparameters of the classifiers, our ML algorithm, GBTD, focuses on the feature engineering-based preprocessing to improve detection performance as well as time-complexity. GBTD improves both detection rate and false positive rate (FPR) of those GBCs by generating stochastic features like standard deviation, mean, minimum, and maximum value of daily electricity usage. GBTD also reduces the classifier complexity with weighted featureimportance-based extraction techniques. Emphasis has been laid upon the practical application of the proposed ML for theft detection by minimizing FPR and reducing data storage space and improving time-complexity of the GBTD classifiers. Additionally, this letter proposes an updated version of the existing six theft cases to mimic real-world theft patterns and applies them to the dataset for numerical evaluation of the proposed algorithm.",https://ieeexplore.ieee.org/document/8610248/,IEEE Transactions on Smart Grid,March 2019,ieeexplore
10.1109/ACCESS.2020.3014185,Entity Thematic Similarity Measurement for Personal Explainable Searching Services in the Edge Environment,IEEE,Journals,"Currently, search engines are widely used to address the information overload problem. Different from the existing client-and-sever-based frameworks, edge computing (EC) technology can provide a new architecture for personalized searching services. The issue of how to measure the similarities among entities by using the context information generated by user behavior in the edge environment is vital in the task of entity-related personal searching. To analyze and measure the similarities among entities, existing methods are mainly based on either the textual content or relationships unilaterally, and the results usually have a fixed degree of similarity. However, the similarities among entities depend on the set of properties that belong to the entities. This approach should be used in determining the similarity or dissimilarity associated with the surrounding context. To address this limitation, we propose a novel semantic augmentation method with a double attention mechanism. The method refers to a dynamic representation learning process that maps an entity to a real number vector in semantic space. In this article, different from the existing similarity measurement methods, we propose a thematic similarity measure approach to analyze the connotation and denotation similarities among entities. The experimental results show that the double attention mechanism leads to a significant improvement in the entity thematic similarity measurement tasks. The model can make a separation among the entities from different domains effectively. In addition, it can take similar entities that are closer in the same domain. It also shows excellent performance on the task of entity thematic similarity, which makes the recommendation results more explainable.",https://ieeexplore.ieee.org/document/9157841/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.3047343,Establishing Trust in Online Advertising With Signed Transactions,IEEE,Journals,"Programmatic advertising operates one of the most sophisticated and efficient service platforms on the Internet. However, the complexity of this ecosystem is a direct cause of one of the most important problems in online advertising, the lack of transparency. This lack of transparency enables subsequent problems such as advertising fraud, which causes billions of dollars in losses. In this paper we propose Ads.chain, a technological solution to the lack-of-transparency problem in programmatic advertising. Ads.chain extends the current effort of the Internet Advertising Bureau (IAB) in providing traceability in online advertising through the Ads.txt and Ads.cert solutions, addressing the limitations of these techniques. Ads.chain is (to the best of the authors' knowledge) the first solution that provides end-to-end cryptographic traceability at the ad transaction level. It is a communication protocol that can be seamlessly embedded into ad-tags and the OpenRTB protocol, the de-facto standards for communications in online advertising, allowing an incremental adoption by the industry. We have implemented Ads.chain and made the code publicly available. We assess the performance of Ads.chain through a thorough analysis in a lab environment that emulates a real ad delivery process at real-life throughputs. The obtained results show that Ads.chain can be implemented with limited impact on the hardware resources and marginal delay increments at the publishers lower than 0.20 milliseconds per ad space on webpages and 2.6 milliseconds at the programmatic advertising platforms. These results confirm that Ads.chain's impact on the user experience and the overall operation of the programmatic ad delivery process can be considered negligible.",https://ieeexplore.ieee.org/document/9306812/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.3017014,Evolution-Based Real-Time Job Scheduling for Co-Optimizing Processor and Memory Power Savings,IEEE,Journals,"With the recent advances in battery-based mobile computing technologies, power-saving techniques in real-time embedded devices are becoming increasingly important. This paper presents a novel job scheduling policy for real-time systems, which aims at minimizing the power consumption of processor and memory without missing the deadline constraints of real-time jobs. To do so, we formulate the power saving techniques of processor voltage/frequency scaling and memory job placement as a unified measure, and show that it is a complex search problem that has the exponential time complexity. Thus, an efficient heuristic based on evolutionary computation is performed to cut down the huge searching space and find a reasonable schedule within the feasible time budget. To evaluate the proposed scheduling policy, we conduct experiments under various workload conditions. Our experimental results show that the proposed policy significantly reduces the energy consumption of real-time systems. Specifically, the average reduction in the energy consumption is 41.7% without deadline misses.",https://ieeexplore.ieee.org/document/9169623/,IEEE Access,2020,ieeexplore
10.1109/TRO.2005.844684,Evolutionary route planner for unmanned air vehicles,IEEE,Journals,"Based on evolutionary computation, a novel real-time route planner for unmanned air vehicles is presented. In the evolutionary route planner, the individual candidates are evaluated with respect to the workspace so that the computation of the configuration space is not required. The planner incorporates domain-specific knowledge, can handle unforeseeable changes of the environment, and take into account different kinds of mission constraints such as minimum route leg length and flying altitude, maximum turning angle, and fixed approach vector to goal position. Furthermore, the novel planner can be used to plan routes both for a single vehicle and for multiple ones. With Digital Terrain Elevation Data, the resultant routes can increase the surviving probability of the vehicles using the terrain masking effect.",https://ieeexplore.ieee.org/document/1492477/,IEEE Transactions on Robotics,Aug. 2005,ieeexplore
10.1109/JIOT.2019.2902410,Experiences With IoT and AI in a Smart Campus for Optimizing Classroom Usage,IEEE,Journals,"Increasing demand for university education is putting pressure on campuses to make better use of their real-estate resources. Evidence indicates that enrollments are rising, yet attendance is falling due to diverse demands on student time and easy access to online content. This paper outlines our efforts to address classroom under-utilization in a real university campus arising from the gap between enrollment and attendance. We do so by instrumenting classrooms with Internet of Things (IoT) sensors to measure real-time usage, using AI to predict attendance, and performing optimal allocation of rooms to courses so as to minimize space wastage. Our first contribution undertakes an evaluation of several IoT sensing approaches for measuring class occupancy, and comparing them in terms of cost, accuracy, privacy, and ease of deployment/operation. Our second contribution instruments nine lecture halls of varying capacity across campus, collects and cleans live occupancy data spanning about 250 courses over two sessions, and draws insights into attendance patterns, including identification of canceled lectures and class tests, while also releasing our data openly to the public. Our third contribution is to use AI techniques for predicting classroom attendance, applying them to real data, and accurately predicting future attendance with an root-mean-square error as low as 0.16. Our final contribution is to develop an optimal allocation of classes to rooms based on predicting attendance rather than enrollment, resulting in over 10% savings in room costs with very low risk of room overflows.",https://ieeexplore.ieee.org/document/8656525/,IEEE Internet of Things Journal,Oct. 2019,ieeexplore
10.1109/ACCESS.2019.2928014,Experimental Demonstration of 3D Visible Light Positioning Using Received Signal Strength With Low-Complexity Trilateration Assisted by Deep Learning Technique,IEEE,Journals,"In this paper, a 3D indoor visible light positioning (VLP) system with fast computation time using received signal strength (RSS) is proposed and experimentally demonstrated. Assisted by the deep learning techniques, the complexity of the trilateration problem is greatly reduced, and the trilateration problem can be formulated as a linear mapping leading to faster position estimation than the conventional estimation. Moreover, a new method of off-line preparation is adopted to minimize the workload of the VLP system deployment for more practical usage. The proposition is implemented on an atto-cellular VLP unit, through which the real-time performance and positioning accuracy are demonstrated and validated in a 3D positioning experiment performed in a space of 1.2  1.2  2 m<sup>3</sup>. The experimental results show that a positioning accuracy of 11.93 cm in confidence of 90% is achieved with 50 times faster the computation time compared to the conventional scheme.",https://ieeexplore.ieee.org/document/8759033/,IEEE Access,2019,ieeexplore
10.1109/TIFS.2013.2265677,Exposing Digital Image Forgeries by Illumination Color Classification,IEEE,Journals,"For decades, photographs have been used to document space-time events and they have often served as evidence in courts. Although photographers are able to create composites of analog pictures, this process is very time consuming and requires expert knowledge. Today, however, powerful digital image editing software makes image modifications straightforward. This undermines our trust in photographs and, in particular, questions pictures as evidence for real-world events. In this paper, we analyze one of the most common forms of photographic manipulation, known as image composition or splicing. We propose a forgery detection method that exploits subtle inconsistencies in the color of the illumination of images. Our approach is machine-learning-based and requires minimal user interaction. The technique is applicable to images containing two or more people and requires no expert interaction for the tampering decision. To achieve this, we incorporate information from physics- and statistical-based illuminant estimators on image regions of similar material. From these illuminant estimates, we extract texture- and edge-based features which are then provided to a machine-learning approach for automatic decision-making. The classification performance using an SVM meta-fusion classifier is promising. It yields detection rates of 86% on a new benchmark dataset consisting of 200 images, and 83% on 50 images that were collected from the Internet.",https://ieeexplore.ieee.org/document/6522874/,IEEE Transactions on Information Forensics and Security,July 2013,ieeexplore
10.1109/ACCESS.2021.3136138,Extending the Space of Software Test Monitoring: Practical Experience,IEEE,Journals,"Software reliability depends on the performed tests. Bug detection and diagnosis are based on test outcome (oracle) analysis. Most of practical test reports do not provide sufficient information for localizing and correcting bugs. We have found the need to extend the space of test result observation in data and time perspectives. This resulted in tracing supplementary test result features in event logs. They are explored with combined text mining and log parsing techniques. Another important point is correlating test life cycle with project development history journaled in issue tracking and software version control repositories. Dealing with the outlined problems, neglected in the literature, we have introduced original analysis schemes. They focus on assessing test coverage, reasons of low diagnosability, and test result profiles. Multidimensional investigation of test features and their management is supported with the developed test infrastructure. This assures a holistic insight into the test efficiency to identify test scheme deficiencies (e.g., functional inadequacy, aging, insufficient coverage) and possible improvements (test set updates). Our studies have been verified in relevance to a real commercial project and confronted with the experience of testers engaged in other projects.",https://ieeexplore.ieee.org/document/9652520/,IEEE Access,2021,ieeexplore
10.1109/JSAIT.2020.2991005,Extracting Robust and Accurate Features via a Robust Information Bottleneck,IEEE,Journals,"We propose a novel strategy for extracting features in supervised learning that can be used to construct a classifier which is more robust to small perturbations in the input space. Our method builds upon the idea of the information bottleneck, by introducing an additional penalty term that encourages the Fisher information of the extracted features to be small when parametrized by the inputs. We present two formulations where the relevance of the features to output labels is measured using either mutual information or MMSE. By tuning the regularization parameter, we can explicitly trade off the opposing desiderata of robustness and accuracy when constructing a classifier. We derive optimal solutions to both robust information bottleneck formulations when the inputs and outputs are jointly Gaussian, proving that the optimally robust features are also jointly Gaussian in this setting. We also propose methods for optimizing variational bounds on the robust information bottleneck objectives in general settings using stochastic gradient descent, which may be implemented efficiently in neural networks. Our experimental results for synthetic and real data sets show that the proposed feature extraction methods indeed produce classifiers with increased robustness to perturbations.",https://ieeexplore.ieee.org/document/9088132/,IEEE Journal on Selected Areas in Information Theory,May 2020,ieeexplore
10.1109/TPAMI.2007.1139,Extreme Compression and Modeling of Bidirectional Texture Function,IEEE,Journals,"The recent advanced representation for realistic real-world materials in virtual reality applications is the Bidirectional Texture Function (BTF) which describes rough texture appearance for varying illumination and viewing conditions. Such a function can be represented by thousands of measurements (images) per material sample. The resulting BTF size excludes its direct rendering in graphical applications and some compression of these huge BTF data spaces is obviously inevitable. In this paper we present a novel, fast probabilistic model-based algorithm for realistic BTF modeling allowing an extreme compression with the possibility of a fast hardware implementation. Its ultimate aim is to create a visual impression of the same material without a pixel-wise correspondence to the original measurements. The analytical step of the algorithm starts with a BTF space segmentation and a range map estimation by photometric stereo of the BTF surface, followed by the spectral and spatial factorization of selected sub-space color texture images. Single mono-spectral band-limited factors are independently modeled by their dedicated spatial probabilistic model. During rendering, the sub-space images of arbitrary size are synthesized and both color (possibly multi-spectral) and range information is combined in a bump-mapping filter according to the view and illumination directions. The presented model offers a huge BTF compression ratio unattainable by any alternative sampling-based BTF synthesis method. Simultaneously this model can be used to reconstruct missing parts of the BTF measurement space.",https://ieeexplore.ieee.org/document/4293214/,IEEE Transactions on Pattern Analysis and Machine Intelligence,Oct. 2007,ieeexplore
10.1109/TASLP.2018.2795754,FMLLR Speaker Normalization With i-Vector: In Pseudo-FMLLR and Distillation Framework,IEEE,Journals,"When an automatic speech recognition (ASR) system is deployed for real-world applications, it often receives only one utterance at a time for decoding. This single utterance could be of short duration depending on the ASR task. In these cases, robust estimation of speaker normalizing methods like feature-space maximum likelihood linear regression (FMLLR) and i-vectors may not be feasible. In this paper, we propose two unsupervised speaker normalization techniques-one at feature level and other at model level of acoustic modeling-to overcome the drawbacks of FMLLR and i-vectors in real-time scenarios. At feature level, we propose the use of deep neural networks (DNN) to generate pseudo-FMLLR features from time-synchronous pair of filterbank and FMLLR features. These pseudo-FMLLR features can then be used for DNN acoustic model training and decoding. At model level, we propose a generalized distillation framework, where a teacher DNN trained on FMLLR features guides the training and optimization of a student DNN trained on filterbank features. In both the proposed methods, the ambiguity in choosing the speaker-specific FMLLR transform can be reduced by augmenting i-vectors to the input filterbank features. Experiments conducted on 33-h and 110-h subsets of Switchboard corpus show that the proposed methods provide significant gains over DNNs trained on FMLLR, i-vector appended FMLLR, filterbank and i -vector appended filterbank features, in real-time scenario.",https://ieeexplore.ieee.org/document/8263609/,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",April 2018,ieeexplore
10.1109/TIP.2014.2347201,Face Super-Resolution via Multilayer Locality-Constrained Iterative Neighbor Embedding and Intermediate Dictionary Learning,IEEE,Journals,"Based on the assumption that low-resolution (LR) and high-resolution (HR) manifolds are locally isometric, the neighbor embedding super-resolution algorithms try to preserve the geometry (reconstruction weights) of the LR space for the reconstructed HR space, but neglect the geometry of the original HR space. Due to the degradation process of the LR image (e.g., noisy, blurred, and down-sampled), the neighborhood relationship of the LR space cannot reflect the truth. To this end, this paper proposes a coarse-to-fine face super-resolution approach via a multilayer locality-constrained iterative neighbor embedding technique, which intends to represent the input LR patch while preserving the geometry of original HR space. In particular, we iteratively update the LR patch representation and the estimated HR patch, and meanwhile an intermediate dictionary learning scheme is employed to bridge the LR manifold and original HR manifold. The proposed method can faithfully capture the intrinsic image degradation shift and enhance the consistency between the reconstructed HR manifold and the original HR manifold. Experiments with application to face super-resolution on the CAS-PEAL-R1 database and real-world images demonstrate the power of the proposed algorithm.",https://ieeexplore.ieee.org/document/6876203/,IEEE Transactions on Image Processing,Oct. 2014,ieeexplore
10.1109/TNN.2005.857952,Fast Modular network implementation for support vector machines,IEEE,Journals,"Support vector machines (SVMs) have been extensively used. However, it is known that SVMs face difficulty in solving large complex problems due to the intensive computation involved in their training algorithms, which are at least quadratic with respect to the number of training examples. This paper proposes a new, simple, and efficient network architecture which consists of several SVMs each trained on a small subregion of the whole data sampling space and the same number of simple neural quantizer modules which inhibit the outputs of all the remote SVMs and only allow a single local SVM to fire (produce actual output) at any time. In principle, this region-computing based modular network method can significantly reduce the learning time of SVM algorithms without sacrificing much generalization performance. The experiments on a few real large complex benchmark problems demonstrate that our method can be significantly faster than single SVMs without losing much generalization performance.",https://ieeexplore.ieee.org/document/1528540/,IEEE Transactions on Neural Networks,Nov. 2005,ieeexplore
10.1109/TPAMI.2018.2861732,Fast Multi-Instance Multi-Label Learning,IEEE,Journals,"In many real-world tasks, particularly those involving data objects with complicated semantics such as images and texts, one object can be represented by multiple instances and simultaneously be associated with multiple labels. Such tasks can be formulated as multi-instance multi-label learning (MIML) problems, and have been extensively studied during the past few years. Existing MIML approaches have been found useful in many applications; however, most of them can only handle moderate-sized data. To efficiently handle large data sets, in this paper we propose the MIMLfast approach, which first constructs a low-dimensional subspace shared by all labels, and then trains label specific linear models to optimize approximated ranking loss via stochastic gradient descent. Although the MIML problem is complicated, MIMLfast is able to achieve excellent performance by exploiting label relations with shared space and discovering sub-concepts for complicated labels. Experiments show that the performance of MIMLfast is highly competitive to state-of-the-art techniques, whereas its time cost is much less. Moreover, our approach is able to identify the most representative instance for each label, and thus providing a chance to understand the relation between input patterns and output label semantics.",https://ieeexplore.ieee.org/document/8423669/,IEEE Transactions on Pattern Analysis and Machine Intelligence,1 Nov. 2019,ieeexplore
10.1109/TIP.2017.2740564,Fast Open-World Person Re-Identification,IEEE,Journals,"Existing person re-identification (re-id) methods typically assume that: 1) any probe person is guaranteed to appear in the gallery target population during deployment (i.e., closed-world) and 2) the probe set contains only a limited number of people (i.e., small search scale). Both assumptions are artificial and breached in real-world applications, since the probe population in target people search can be extremely vast in practice due to the ambiguity of probe search space boundary. Therefore, it is unrealistic that any probe person is assumed as one target people, and a large-scale search in person images is inherently demanded. In this paper, we introduce a new person re-id search setting, called large scale open-world (LSOW) re-id, characterized by huge size probe images and open person population in search thus more close to practical deployments. Under LSOW, the under-studied problem of person re-id efficiency is essential in addition to that of commonly studied re-id accuracy. We, therefore, develop a novel fast person re-id method, called Cross-view Identity Correlation and vErification (X-ICE) hashing, for joint learning of cross-view identity representation binarisation and discrimination in a unified manner. Extensive comparative experiments on three large-scale benchmarks have been conducted to validate the superiority and advantages of the proposed X-ICE method over a wide range of the state-of-the-art hashing models, person re-id methods, and their combinations.",https://ieeexplore.ieee.org/document/8011476/,IEEE Transactions on Image Processing,May 2018,ieeexplore
10.1109/ACCESS.2021.3100139,Fast Shot Boundary Detection Based on Separable Moments and Support Vector Machine,IEEE,Journals,"The large number of visual applications in multimedia sharing websites and social networks contribute to the increasing amounts of multimedia data in cyberspace. Video data is a rich source of information and considered the most demanding in terms of storage space. With the huge development of digital video production, video management becomes a challenging task. Video content analysis (VCA) aims to provide big data solutions by automating the video management. To this end, shot boundary detection (SBD) is considered an essential step in VCA. It aims to partition the video sequence into shots by detecting shot transitions. High computational cost in transition detection is considered a bottleneck for real-time applications. Thus, in this paper, a balance between detection accuracy and speed for SBD is addressed by presenting a new method for fast video processing. The proposed SBD framework is based on the concept of candidate segment selection with frame active area and separable moments. First, for each frame, the active area is selected such that only the informative content is considered. This leads to a reduction in the computational cost and disturbance factors. Second, for each active area, the moments are computed using orthogonal polynomials. Then, an adaptive threshold and inequality criteria are used to eliminate most of the non-transition frames and preserve candidate segments. For further elimination, two rounds of bisection comparisons are applied. As a result, the computational cost is reduced in the subsequent stages. Finally, machine learning statistics based on the support vector machine is implemented to detect the cut transitions. The enhancement of the proposed fast video processing method over existing methods in terms of computational complexity and accuracy is verified. The average improvements in terms of frame percentage and transition accuracy percentage are 1.63% and 2.05%, respectively. Moreover, for the proposed SBD algorithm, a comparative study is performed with state-of-the-art algorithms. The comparison results confirm the superiority of the proposed algorithm in computation time with improvement of over 38%.",https://ieeexplore.ieee.org/document/9496657/,IEEE Access,2021,ieeexplore
10.1109/TVCG.2004.26,Fast evolution of image manifolds and application to filtering and segmentation in 3D medical images,IEEE,Journals,"In many instances, numerical integration of space-scale PDEs is the most time consuming operation of image processing. This is because the scale step is limited by conditional stability of explicit schemes. We introduce the unconditionally stable semiimplicit linearized difference scheme that is fashioned after additive operator split (AOS) [Weickert, J. et al. (1998)], [Goldenberg, R et al., (2001)] for Beltrami and the subjective surface computation. The Beltrami flow [Kimmel, R. (1997) (1999)], [Sochen, N. et al. (1998)], is one of the most effective denoising algorithms in image processing. For gray-level images, we show that the flow equation can be arranged in an advection-diffusion form, revealing the edge-enhancing properties of this flow. This also suggests the application of AOS method for faster convergence. The subjective surface [Sarti, A. et al. (2002)] deals with constructing a perceptually meaningful interpretation from partial image data by mimicking the human visual system. However, initialization of the surface is critical for the final result and its main drawbacks are very slow convergence and the huge number of iterations required. We first show that the governing equation for the subjective surface flow can be rearranged in an AOS implementation, providing a near real-time solution to the shape completion problem in 2D and 3D. Then, we devise a new initialization paradigm where we first ""condition"" the viewpoint surface using the fast-marching algorithm. We compare the original method with our new algorithm on several examples of real 3D medical images, thus revealing the improvement achieved.",https://ieeexplore.ieee.org/document/1310278/,IEEE Transactions on Visualization and Computer Graphics,Sept.-Oct. 2004,ieeexplore
10.1109/TNN.2002.804315,Fast minimization of structural risk by nearest neighbor rule,IEEE,Journals,"In this paper, we present a novel nearest neighbor rule-based implementation of the structural risk minimization principle to address a generic classification problem. We propose a fast reference set thinning algorithm on the training data set similar to a support vector machine (SVM) approach. We then show that the nearest neighbor rule based on the reduced set implements the structural risk minimization principle, in a manner which does not involve selection of a convenient feature space. Simulation results on real data indicate that this method significantly reduces the computational cost of the conventional SVMs, and achieves a nearly comparable test error performance.",https://ieeexplore.ieee.org/document/1176133/,IEEE Transactions on Neural Networks,Jan. 2003,ieeexplore
10.1109/72.97911,Fast training algorithms for multilayer neural nets,IEEE,Journals,"An algorithm that is faster than back-propagation and for which it is not necessary to specify the number of hidden units in advance is described. The relationship with other fast pattern-recognition algorithms, such as algorithms based on k-d trees, is discussed. The algorithm has been implemented and tested on artificial problems, such as the parity problem, and on real problems arising in speech recognition. Experimental results, including training times and recognition accuracy, are given. Generally, the algorithm achieves accuracy as good as or better than nets trained using back-propagation. Accuracy is comparable to that for the nearest-neighbor algorithm, which is slower and requires more storage space.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/97911/,IEEE Transactions on Neural Networks,May 1991,ieeexplore
10.1109/TMI.2015.2409265,Feature-Preserving Noise Removal,IEEE,Journals,"Conventional image restoration algorithms use transform-domain filters, which separate the noise from the sparse signal among the transform components or apply spatial smoothing filters in real space whose design relies on prior assumptions about the noise statistics. These filters also reduce the information content of the image by suppressing spatial frequencies or by recognizing only a limited set of shapes. Here we show that denoising can be efficiently done using a nonlinear filter, which operates along patch neighborhoods and multiple copies of the original image. The use of patches enables the algorithm to account for spatial correlations in the random field whereas the multiple copies are used to recognize the noise statistics. The nonlinear filter, which is implemented by a hierarchical multistage system of multilayer perceptrons, outperforms state-of-the-art denoising algorithms such as those based on collaborative filtering and total variation. Compared to conventional denoising algorithms, our filter can restore images without blurring them, making it attractive for use in medical imaging where the preservation of anatomical details is critical.",https://ieeexplore.ieee.org/document/7055927/,IEEE Transactions on Medical Imaging,Sept. 2015,ieeexplore
10.1109/TNSE.2020.3016035,Federated Region-Learning for Environment Sensing in Edge Computing System,IEEE,Journals,"In the last decades, environmental pollution has grown up to be a major problem that influences people's health. Providing accurate environmental sensing services is of great significance. To realize environmental sensing, distributed monitoring sites are used to collect comprehensive long-term environmental data. However, sparse sensory data caused by insufficient monitoring sites and their incomplete records become the main challenge of fine-grained environment sensing. At the same time, due to the limitations of network bandwidth and storage space, traditional centralized training is difficult to meet the task training requirements based on big data. In this paper, we develop a novel distributed inference framework, named Federated Region-Learning (FRL) for urban environment sensing. It inherits the basic idea of federated learning avoiding transmission and centralized storage of data, and also considers the regional characteristics of each monitoring site. Through an elaborate designed edge computing system, a local regional model is customized for the micro cloud to improve the inference accuracy. Moreover, we develop two types of global model aggregation strategies to better target different bandwidth requirements. Extensive experiments based on two real-world datasets are performed to prove universality and effectiveness.",https://ieeexplore.ieee.org/document/9165960/,IEEE Transactions on Network Science and Engineering,1 Oct.-Dec. 2020,ieeexplore
10.1109/JSTQE.2020.2975579,Femtojoule per MAC Neuromorphic Photonics: An Energy and Technology Roadmap,IEEE,Journals,"Photonic artificial neural networks have garnered enormous attention due to their potential to perform multiply-accumulate (MAC) operations at much higher clock rates and consuming significantly lower power and chip real-estate compared to digital electronic alternatives. Herein, we present a comprehensive power consumption analysis of photonic neurons, taking into account global design parameters and concluding to analytical expressions for the neuron's energy- and footprint efficiencies. We identify the optimal design-space and analyze the performance plateaus and their dependence on a range of physical parameters, highlighting the existence of an optimal data-rate for maximizing the energy efficiency. Following a survey of the best-in-class integrated photonic devices, including on-chip lasers, photodetectors, modulators and weighting elements, the mathematically calculated energy and footprint efficiencies are mapped into real photonic neuron deployment scenarios. We reveal that silicon photonics can compete with the best-performing currently available digital electronic neural network engines, reaching TMAC/s/mm<sup>2</sup> footprint- and sub-pJ/MAC energy efficiencies. Simultaneously, neuromorphic plasmonics, plasmo-photonics and sub-wavelength photonics hold the credentials for 1 to 3 orders of magnitude improvements even when the laser requirements and a reasonable waveguide pitch are accounted for, promising performance at a few fJ/MAC and up to a few TMAC/s/mm<sup>2</sup>.",https://ieeexplore.ieee.org/document/9006831/,IEEE Journal of Selected Topics in Quantum Electronics,Sept.-Oct. 2020,ieeexplore
10.1109/ACCESS.2017.2698208,Finding Abnormal Vessel Trajectories Using Feature Learning,IEEE,Journals,"Global Positioning System technology has been widely used in vehicle tracking and road planning applications. An enormous amount of data concerning the trajectories of vehicles has been collected and stored for tracking purposes. A trajectory contains not only the footprints of a moving object but also additional information, such as speed and stopping points. Therefore, the large-scale trajectory data sets provide rich information and are currently attracting considerable attention; there have been many successful studies of event detection based on trajectory data. However, most of these studies have focused only on vehicles traveling in a road network and have note considered maritime trajectories. A maritime trajectory also contains auxiliary data (e.g., speed and rotation) in addition to the movements of a ship. However, ships are not bound to road networks, and consequently, it is difficult to apply traditional mining algorithms based on road networks. In addition, even if the amount of maritime trajectory data is very large, these data are also spatially sparse, which will significantly reduce the effectiveness of most existing mining algorithms. In this paper, we propose a new method of abnormal trajectory detection to address this problem. This method can detect abnormal vessel trajectories from Automatic Identification System (AIS), records for vessels via our feature learning algorithm. To reduce the search space, we invoke reference points as well as the Piecewise Linear Segmentation (PLS), algorithm to compress the trajectories without losing important information. A time-aware and spatially correlated collaborative algorithm is proposed to increase the density of the trajectories to improve the accuracy of the detection algorithm, which is based on Dynamic Time Warping (DTW). Finally, we report experiments conducted on a real-world data set, which demonstrate that the proposed detection method can detect anomalous trajectories effectively.",https://ieeexplore.ieee.org/document/7913665/,IEEE Access,2017,ieeexplore
10.1109/TIM.2021.3132332,Finger Vein Recognition Algorithm Based on Lightweight Deep Convolutional Neural Network,IEEE,Journals,"Even though the deep neural networks have strong feature representation capability and high recognition accuracy in finger vein recognition, the deep models are computationally intensive and poor in timeliness. To address these issues, this article proposes a lightweight algorithm for finger vein image recognition and matching. The proposed algorithm uses a lightweight convolutional model in the backbone network and employs a triplet loss function to train the model, which not only improves the matching accuracy, but also satisfies the real-time matching requirements. In addition, the Mini-region of interest (RoI) and finger vein pattern feature extraction also effectively solve the problems of large amounts of calculation and background noise. Moreover, the present model recognizes new categories based on the feature vector space constructed by the finger vein recognition system, so that new categories can be recognized without retraining the model. The results show that the finger vein recognition and matching algorithm proposed in this article achieves 99.3% and 99.6% in recognition accuracy and 14.2 and 16.5 ms in matching time for the dataset Shandong University Machine Learning and Applications Laboratory-Homologous Multimodal Biometric Traits (SDUMLA-HMT) and Peking University Finger Vein Dataset (PKU-FVD), respectively. These metrics show that our approach is time-saving and more effective than previous algorithms. Compared with the state-of-the-art finger vein recognition algorithm, the proposed algorithm improves 1.45% in recognition accuracy while saving 45.7% in recognition time.",https://ieeexplore.ieee.org/document/9633979/,IEEE Transactions on Instrumentation and Measurement,2022,ieeexplore
10.1109/83.704306,Foveal automatic target recognition using a multiresolution neural network,IEEE,Journals,"This paper presents a method for detecting and classifying a target from its foveal (graded resolution) imagery using a multiresolution neural network. Target identification decisions are based on minimizing an energy function. This energy function is evaluated by comparing a candidate blob with a library of target models at several levels of resolution simultaneously available in the current foveal image. For this purpose, a concurrent (top-down-and-bottom-up) matching procedure is implemented via a novel multilayer Hopfield (1985) neural network. The associated energy function supports not only interactions between cells at the same resolution level, but also between sets of nodes at distinct resolution levels. This permits features at different resolution levels to corroborate or refute one another contributing to an efficient evaluation of potential matches. Gaze control, refoveation to more salient regions of the available image space, is implemented as a search for high resolution features which will disambiguate the candidate blob. Tests using real two-dimensional (2-D) objects and their simulated foveal imagery are provided.",https://ieeexplore.ieee.org/document/704306/,IEEE Transactions on Image Processing,Aug. 1998,ieeexplore
10.1109/TOH.2017.2753233,Functional Contour-following via Haptic Perception and Reinforcement Learning,IEEE,Journals,"Many tasks involve the fine manipulation of objects despite limited visual feedback. In such scenarios, tactile and proprioceptive feedback can be leveraged for task completion. We present an approach for real-time haptic perception and decision-making for a haptics-driven, functional contour-following task: the closure of a ziplock bag. This task is challenging for robots because the bag is deformable, transparent, and visually occluded by artificial fingertip sensors that are also compliant. A deep neural net classifier was trained to estimate the state of a zipper within a robot's pinch grasp. A Contextual Multi-Armed Bandit (C-MAB) reinforcement learning algorithm was implemented to maximize cumulative rewards by balancing exploration versus exploitation of the state-action space. The C-MAB learner outperformed a benchmark Q-learner by more efficiently exploring the state-action space while learning a hard-to-code task. The learned C-MAB policy was tested with novel ziplock bag scenarios and contours (wire, rope). Importantly, this work contributes to the development of reinforcement learning approaches that account for limited resources such as hardware life and researcher time. As robots are used to perform complex, physically interactive tasks in unstructured or unmodeled environments, it becomes important to develop methods that enable efficient and effective learning with physical testbeds.",https://ieeexplore.ieee.org/document/8039205/,IEEE Transactions on Haptics,1 Jan.-March 2018,ieeexplore
10.1109/TFUZZ.2006.889930,Fuzzy Classification Using Pattern Discovery,IEEE,Journals,"Rule-based classifiers allow rationalization of classifications made. This in turn improves understanding which is essential for effective decision support. As a rule based classifier, the pattern discovery (PD) algorithm functions well in discrete, nominal and continuous data domains. A drawback when using PD as a classifier for decision support is that it has an unbounded decision space that confounds the understanding of the degree of support for a decision. Incorporating PD into a fuzzy inference system (FIS) allows the the degree of support for a decision to be expressed with intuitively understandable terms. In addition, using discrete algorithms in continuous domains can result in reduced accuracy due to quantization. Fuzzification reduces this ldquocost of quantizationrdquo and improves classification performance. In this work, the PD algorithm was used as a source of rules for a series of FISs implemented using different rule weighting and defuzzification schemes, each providing a linguistic basis for rule description and a bounded space for expression of decision support. The output of each FIS consists of a suggested outcome, a strong confidence metric describing suggestions within this space and a linguistic expression of the rules. This constitutes a stronger basis for decision making than that provided by PD alone. A variety of synthetic, continuous class distributions with varying degrees of separation was used to evaluate the performance of fuzzy, PD, back-propagation and Bayesian classifiers. Overall, the accuracy of the fuzzy system was found to be similar, but slightly below, that of the inherently continuous valued classifiers and was somewhat improved with respect to the PD classifiers. For the difficult spiral class distributions studied, the fuzzy classifiers were able to make more classifications than the PD classifiers. The correct classification rates for the fuzzy classifiers were similar across the various rule weighting and defuzzification schemes, demonstrating the strength of the statistical method for rule generation. Analysis of several real-world data sets shows that a PD-based FIS has comparable performance to a neuro-fuzzy system. The use of a PD based FIS however, provides insight into the structure of the data analyzed not available through the other approaches.",https://ieeexplore.ieee.org/document/4343108/,IEEE Transactions on Fuzzy Systems,Oct. 2007,ieeexplore
10.1109/TASE.2020.3024725,Fuzzy Logic-Driven Variable Time-Scale Prediction-Based Reinforcement Learning for Robotic Multiple Peg-in-Hole Assembly,IEEE,Journals,"Reinforcement learning (RL) has been increasingly used for single peg-in-hole assembly, where assembly skill is learned through interaction with the assembly environment in a manner similar to skills employed by human beings. However, the existing RL algorithms are difficult to apply to the multiple peg-in-hole assembly because the much more complicated assembly environment requires sufficient exploration, resulting in a long training time and less data efficiency. To this end, this article focuses on how to predict the assembly environment and how to use the predicted environment in assembly action control to improve the data efficiency of the RL algorithm. Specifically, first, the assembly environment is exactly predicted by a variable time-scale prediction (VTSP) defined as general value functions (GVFs), reducing the unnecessary exploration. Second, we propose a fuzzy logic-driven variable time-scale prediction-based reinforcement learning (FLDVTSP-RL) for assembly action control to improve the efficiency of the RL algorithm, in which the predicted environment is mapped to the impedance parameter in the proposed impedance action space by a fuzzy logic system (FLS) as the action baseline. To demonstrate the effectiveness of VTSP and the data efficiency of the FLDVTSP-RL methods, a dual peg-in-hole assembly experiment is set up; the results show that FLDVTSP-deep Q-learning (DQN) decreases the assembly time about 44% compared with DQN and FLDVTSP-deep deterministic policy gradient (DDPG) decreases the assembly time about 24% compared with DDPG. <i>Note to Practitioners</i>The complicated assembly environment of the multiple peg-in-hole assembly results in a contact state that cannot be recognized exactly from the force sensor. Therefore, contact-model-based methods that require tuning of the control parameters based on the contact state recognition cannot be applied directly in this complicated environment. Recently, reinforcement learning (RL) methods without contact state recognition have recently attracted scientific interest. However, the existing RL methods still rely on numerous explorations and a long training time, which cannot be directly applied to real-world tasks. This article takes inspiration from the manner in which human beings can learn assembly skills with a few trials, which relies on the variable time-scale predictions (VTSPs) of the environment and the optimized assembly action control strategy. Our proposed fuzzy logic-driven variable time-scale prediction-based reinforcement learning (FLDVTSP-RL) can be implemented in two steps. First, the assembly environment is predicted by the VTSP defined as general value functions (GVFs). Second, assembly action control is realized in an impedance action space with a baseline defined by the impedance parameter mapped from the predicted environment by the fuzzy logic system (FLS). Finally, a dual peg-in-hole assembly experiment is conducted; compared with deep Q-learning (DQN), FLDVTSP-DQN can decrease the assembly time about 44%; compared with deep deterministic policy gradient (DDPG), FLDVTSP-DDPG can decrease the assembly time about 24%.",https://ieeexplore.ieee.org/document/9210190/,IEEE Transactions on Automation Science and Engineering,Jan. 2022,ieeexplore
10.1109/ACCESS.2019.2923419,Fuzzy Theory Based Single Belief State Generation for Partially Observable Real-Time Strategy Games,IEEE,Journals,"As the basic problem of the real-time strategy (RTS) games, AI planning has attracted wide attention of researchers, but it still remains as a huge challenge due to its large searching space and real-time nature. The situation may get worse when the planning in RTS games is implemented under a partially observable environment considering the existence of the fog-of-war. Given the recorded past positions of an agent, it would be helpful if the targets' next position can be predicted based on the recorded data since this will increase the certainty of the target. Therefore, this paper proposes a fuzzy theory-based single belief state generation method named FTH to do what based on multi-layer information sets extracted from the history position information. Besides, we incorporate the FTH generation method into adversarial hierarchical task network repairing (AHTNR) planning algorithm, which can be used for the prediction of the unit's position and task planning. Finally, we carry out an empirical study based on the RTS game and validate its effectiveness by comparing its performance with that of other state-of-the-art algorithms.",https://ieeexplore.ieee.org/document/8737962/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2022.3148126,GDLL: A Scalable and Share Nothing Architecture Based Distributed Graph Neural Networks Framework,IEEE,Journals,"Deep learning has recently been shown to be effective in uncovering hidden patterns in non-Euclidean space, where data is represented as graphs with complex object relationships and interdependencies. Because of the implicit data dependence in the big graphs with millions of nodes and billions of edges, it is hard for industrial communities to exploit these methods to address real-world challenges at scale. The skewness property of big graphs, distributed file system performance penalty on small k-hop neighborhood subgraphs, and varying size of subgraph makes Graph Neural Networks (GNNs) training further challenging in a distributed environment using parameter servers. To address such issues, we propose a scalable, layered, fault-tolerance, and in-memory distributed computing-based graph neural network framework called Graph Distributed Learning Library (GDLL). The base layer utilizes an optimized distributed file system and a scalable graph data store to reduce the performance penalty. The second layer provides distributed graph processing using in-memory graph programming models while optimizing and hiding the underlying complexity of information complete subgraph computation. In the third layer, GNN modules are deployed on top of the first two layers for efficient distributed training using parameter servers. Finally, we evaluate and compare GDLL with the state-of-the-art solutions and outperform it significantly in terms of efficiency while maintaining similar GNN convergence.",https://ieeexplore.ieee.org/document/9698236/,IEEE Access,2022,ieeexplore
10.1109/LRA.2018.2807810,Gait Reconstruction From Motion Artefact Corrupted Fabric-Embedded Sensors,IEEE,Journals,"``Fabric-embedded sensors are of growing interest in clinical diagnostics and rehabilitation studies that desire the measurement and analysis of human movement outside the laboratory environment. A major issue limiting their usage is the undesired effect of fabric motion artefacts corrupting movement signals. While supervised calibration methods can be used to eliminate these artefacts, these methods make assumptions on the fabric motion, and are unable to address changes in user motion (e.g., locomotion speed) or clothing deformation. In this letter, an unsupervised latent space regression method is presented for learning body movements from fabric motion corrupted sensors, while simultaneously allowing for automatic recalibration. Experiments in this letter show that unsupervised gait learning performs equally as well as supervised learning when removing motion artefacts. This allows for the implementation of adaptive motion artefact methods in real-world sensor-embedded clothing.",https://ieeexplore.ieee.org/document/8295102/,IEEE Robotics and Automation Letters,July 2018,ieeexplore
10.1109/TCOMM.2005.847166,Generalized PSK in space-time coding,IEEE,Journals,"A wireless communication system using multiple antennas promises reliable transmission under Rayleigh flat fading assumptions. Design criteria and practical schemes have been presented for both coherent and noncoherent communication channels. In this paper, we generalize one-dimensional (1-D) phase-shift keying (PSK) signals and introduce space-time constellations from generalized PSK (GPSK) signals based on the complex and real orthogonal designs. The resulting space-time constellations reallocate the energy for each transmitting antenna and feature good diversity products; consequently, their performances are better than some of the existing comparable codes. Moreover, since the maximum-likelihood (ML) decoding of our proposed codes can be decomposed to 1-D PSK signal demodulation, the ML decoding of our codes can be implemented in a very efficient way.",https://ieeexplore.ieee.org/document/1431123/,IEEE Transactions on Communications,May 2005,ieeexplore
10.1109/LRA.2019.2955941,Generative Localization With Uncertainty Estimation Through Video-CT Data for Bronchoscopic Biopsy,IEEE,Journals,"Robot-assisted endobronchial intervention requires accurate localization based on both intra- and pre-operative data. Most existing methods achieve this by registering 2D videos with 3D CT models according to a defined similarity metric with local features. Instead, we formulate the bronchoscopic localization as a learning-based global localisation using deep neural networks. The proposed network consists of two generative architectures and one auxiliary learning component. The cycle generative architecture bridges the domain variance between the real bronchoscopic videos and virtual views derived from pre-operative CT data so that the proposed approach can be trained through a large number of generated virtual images but deployed through real images. The auxiliary learning architecture leverages complementary relative pose regression to constrain the search space, ensuring consistent global pose predictions. Most importantly, the uncertainty of each global pose is obtained through variational inference by sampling within the learned underlying probability distribution. Detailed validation results demonstrate the localization accuracy with reasonable uncertainty achieved and its potential clinical value. A demonstration video demo can be found on the website <uri>https://youtu.be/ci9LMY49aF8</uri>.",https://ieeexplore.ieee.org/document/8913461/,IEEE Robotics and Automation Letters,Jan. 2020,ieeexplore
10.1109/TMECH.2021.3079935,Geometrical-Based Displacement Measurement With Pseudostereo Monocular Camera on Bidirectional Cascaded Linear Actuator,IEEE,Journals,"This article details the development of a geometrical-based displacement extraction framework capable of automatically extracting critical infrastructure measurements in one sequence. The framework is a novel rail viaduct bearing inspection pipeline implemented on Bearing Inspector for Narrow-space Observation Version 2 (BINOv2). BINOv2 is a tethered custom unmanned aerial vehicle system utilized to supplant labor-intensive pipelines and enhance inspection accuracy of infrastructure conditions in confined remote locations. The algorithm accepts stereoscopic images taken from a single monocular camera on a bidirectional cascaded linear actuator system in a rack-and-pinion configuration. A point cloud model generated from the image sets then runs through a hierarchical neural network for 3-D segmentation to extract targeted regions of interest. Our training pipeline generates and forms the full model's training dataset using only a small sample of real point clouds. The point cloud generated is inadequate to form the full bearing geometry profile. Therefore, the proposed framework projects best-fit circles based on the point cloud curvature to form the full bearing geometry profile so that the required displacement measurement is available for extraction. Several experiments were conducted on a mock-up and actual operational site to validate the proposed framework's accuracy, its robustness and comparison with other state-of-the-art alternatives.",https://ieeexplore.ieee.org/document/9430727/,IEEE/ASME Transactions on Mechatronics,Aug. 2021,ieeexplore
10.1109/LRA.2022.3146544,Global-Reasoned Multi-Task Learning Model for Surgical Scene Understanding,IEEE,Journals,"Global and local relational reasoning enable scene understanding models to perform human-like scene analysis and understanding. Scene understanding enables better semantic segmentation and object-to-object interaction detection. In the medical domain, a robust surgical scene understanding model allows the automation of surgical skill evaluation, real-time monitoring of surgeons performance and post-surgical analysis. This letter introduces a globally-reasoned multi-task surgical scene understanding model capable of performing instrument segmentation and tool-tissue interaction detection. Here, we incorporate global relational reasoning in the latent interaction space and introduce multi-scale local (neighborhood) reasoning in the coordinate space to improve segmentation. Utilizing the multi-task model setup, the performance of the visual-semantic graph attention network in interaction detection is further enhanced through global reasoning. The global interaction space features from the segmentation module are introduced into the graph network, allowing it to detect interactions based on both node-to-node and global interaction reasoning. Our model reduces the computation cost compared to running two independent single-task models by sharing common modules, which is indispensable for practical applications. Using a sequential optimization technique, the proposed multi-task model outperforms other state-of-the-art single-task models on the MICCAI endoscopic vision challenge 2018 dataset. Additionally, we also observe the performance of the multi-task model when trained using the knowledge distillation technique. The official code implementation is made available in GitHub.",https://ieeexplore.ieee.org/document/9695281/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/ACCESS.2019.2953176,Group Recommendation via Self-Attention and Collaborative Metric Learning Model,IEEE,Journals,"Group recommendation has attracted wide attention owing to its significance in real applications. One of the big challenges for group recommendation systems is how to integrate individual preferences of each group member and attain overall preferences for the group. Most of the traditional group recommendation solutions regard group members as equal participants and assign a same weight to each member. As a result, performance of this type of recommendation methods is not as good as expected. To improve the performance of group recommendation, a novel group recommendation model via Self-Attention and Collaborative Metric Learning (SACML) is presented in this paper. With the employment of Self-Attention mechanism, the SACML model can learn the similarity interactions between group members and services and decide a different weight for different group member. Based on these weights, group preferences for services can be generated by the aggregation of group members' preferences and the group's own preference. Similar metric space between group and services is obtained via collaborative metric learning with the group preferences and positive and negative services' features. Group recommendation is finally implemented based on the obtained metric space. Simulation has been conducted on CAMRa2011 and Meetup datasets, and experimental results show that the proposed SACML model has better performance in comparison with those baseline methods.",https://ieeexplore.ieee.org/document/8896961/,IEEE Access,2019,ieeexplore
10.1109/TNSM.2018.2890754,"HQTimer: A Hybrid <inline-formula> <tex-math notation=""LaTeX"">${Q}$ </tex-math></inline-formula>-Learning-Based Timeout Mechanism in Software-Defined Networks",IEEE,Journals,"Software-defined networking (SDN) has enabled flexible control over the network by leveraging data plane programming languages such as OpenFlow. However, this fine-grained control is potentially at odds with data plane performance due to the high storage load and limited flow table space of SDN switches. Wildcard rules and timeout mechanisms are the main approaches to relieve the load. However, wildcard rules introduce the rule dependency problem, which poses obstacles to preserve the semantics of network policies and design the timeout mechanism. Therefore, exploiting the limited flow table effectively as well as designing a safe timeout mechanism become the main challenge. In this paper, we propose HQTimer: a hybrid Q-learning-based timeout mechanism in SDN. HQTimer employs a hybrid timeout mechanism and a Q-learning-based adaptation logic. HQTimer is safe, as its timeout mechanism ensures the forwarding logic is not violated by the rule dependency problem. HQTimer is adaptive, as it assigns different timeout values to different rules according to the traffic dynamics and the data plane performance based on Q-learning. The extensive experiments based on real and synthetic workloads show that HQTimer achieves both a higher table-hit rate and a lower overflow number compared with existing timeout mechanisms. Specifically, in contrast to a well-tuned, static idle timeout mechanism, HQTimer improves the table-hit rate from 97.6% to 99.4% while decreasing the overflow number by 83.8%.",https://ieeexplore.ieee.org/document/8600383/,IEEE Transactions on Network and Service Management,March 2019,ieeexplore
10.1109/TIP.2020.3045634,"HRSiam: High-Resolution Siamese Network, Towards Space-Borne Satellite Video Tracking",IEEE,Journals,"Tracking moving objects from space-borne satellite videos is a new and challenging task. The main difficulty stems from the extremely small size of the target of interest. First, because the target usually occupies only a few pixels, it is hard to obtain discriminative appearance features. Second, the small object can easily suffer from occlusion and illumination variation, making the features of objects less distinguishable from features in surrounding regions. Current state-of-the-art tracking approaches mainly consider high-level deep features of a single frame with low spatial resolution, and hardly benefit from inter-frame motion information inherent in videos. Thus, they fail to accurately locate such small objects and handle challenging scenarios in satellite videos. In this article, we successfully design a lightweight parallel network with a high spatial resolution to locate the small objects in satellite videos. This architecture guarantees real-time and precise localization when applied to the Siamese Trackers. Moreover, a pixel-level refining model based on online moving object detection and adaptive fusion is proposed to enhance the tracking robustness in satellite videos. It models the video sequence in time to detect the moving targets in pixels and has ability to take full advantage of tracking and detecting. We conduct quantitative experiments on real satellite video datasets, and the results show the proposed HIGH-RESOLUTION SIAMESE NETWORK (HRSiam) achieves state-of-the-art tracking performance while running at over 30 FPS.",https://ieeexplore.ieee.org/document/9350236/,IEEE Transactions on Image Processing,2021,ieeexplore
10.1109/ACCESS.2017.2768405,Hand-Mouse Interface Using Virtual Monitor Concept for Natural Interaction,IEEE,Journals,"The growing interest in human-computer interaction has prompted research in this area. In addition, research has been conducted on a natural user interface/natural user experience (NUI/NUX), which utilizes a user's gestures and voice. In the case of NUI/NUX, a recognition algorithm is needed for the gestures or voice. However, such recognition algorithms have weaknesses because their implementation is complex, and they require a large amount of time for training. Therefore, steps that include pre-processing, normalization, and feature extraction are needed. In this paper, we designed and implemented a hand-mouse interface that introduces a new concept called a virtual monitor, to extract a user's physical features through Kinect in real time. This virtual monitor allows a virtual space to be controlled by the hand mouse. It is possible to map the coordinates on the virtual monitor to the coordinates on the real monitor accurately. A hand-mouse interface based on the virtual monitor concept maintains the outstanding intuitiveness that is the strength of the previous study and enhances the accuracy of mouse functions. In order to evaluate the intuitiveness and accuracy of the interface, we conducted an experiment with 50 volunteers ranging from teenagers to those in their 50s. The results of this intuitiveness experiment showed that 84% of the subjects learned how to use the mouse within 1 min. In addition, the accuracy experiment showed the high accuracy level of the mouse functions [drag (80.9%), click (80%), double-click (76.7%)]. This is a good example of an interface for controlling a system by hand in the future.",https://ieeexplore.ieee.org/document/8091117/,IEEE Access,2017,ieeexplore
10.1109/ACCESS.2020.3037254,Hardware-Based Real-Time Deep Neural Network Lossless Weights Compression,IEEE,Journals,"Deep Neural Networks (DNN) are widely applied to many mobile applications demanding real-time implementation and large memory space. Therefore, it presents a new challenge for low-power and efficient implementation of a diversity of applications, such as speech recognition and image classification, for embedded edge devices. This work presents a hardware-based DNN compression approach to address the limited memory resources in edge devices. We propose a new entropy-based compression algorithm for encoding DNN weights, as well as a real-time decoding method and efficient dedicated hardware implementation. The proposed approach enables a significant reduction of the required DNN weights memory (approximately 70% and 63% for AlexNet and VGG19, respectively), while allowing the decoding of one weight per clock cycle. Results show a high compression ratio compared to well-known lossless compression algorithms. The proposed hardware decoder enables an efficient implementation of large DNN networks in low-power edge devices with limited memory resources.",https://ieeexplore.ieee.org/document/9253521/,IEEE Access,2020,ieeexplore
10.1109/TVCG.2005.46,Hardware-assisted visibility sorting for unstructured volume rendering,IEEE,Journals,"Harvesting the power of modern graphics hardware to solve the complex problem of real-time rendering of large unstructured meshes is a major research goal in the volume visualization community. While, for regular grids, texture-based techniques are well-suited for current GPUs, the steps necessary for rendering unstructured meshes are not so easily mapped to current hardware. We propose a novel volume rendering technique that simplifies the CPU-based processing and shifts much of the sorting burden to the GPU, where it can be performed more efficiently. Our hardware-assisted visibility sorting algorithm is a hybrid technique that operates in both object-space and image-space. In object-space, the algorithm performs a partial sort of the 3D primitives in preparation for rasterization. The goal of the partial sort is to create a list of primitives that generate fragments in nearly sorted order. In image-space, the fragment stream is incrementally sorted using a fixed-depth sorting network. In our algorithm, the object-space work is performed by the CPU and the fragment-level sorting is done completely on the GPU. A prototype implementation of the algorithm demonstrates that the fragment-level sorting achieves rendering rates of between one and six million tetrahedral cells per second on an ATI Radeon 9800.",https://ieeexplore.ieee.org/document/1407861/,IEEE Transactions on Visualization and Computer Graphics,May-June 2005,ieeexplore
10.1109/TPAMI.2019.2914897,Hashing with Mutual Information,IEEE,Journals,"Binary vector embeddings enable fast nearest neighbor retrieval in large databases of high-dimensional objects, and play an important role in many practical applications, such as image and video retrieval. We study the problem of learning binary vector embeddings under a supervised setting, also known as hashing. We propose a novel supervised hashing method based on optimizing an information-theoretic quantity, mutual information. We show that optimizing mutual information can reduce ambiguity in the induced neighborhood structure in the learned Hamming space, which is essential in obtaining high retrieval performance. To this end, we optimize mutual information in deep neural networks with minibatch stochastic gradient descent, with a formulation that maximally and efficiently utilizes available supervision. Experiments on four image retrieval benchmarks, including ImageNet, confirm the effectiveness of our method in learning high-quality binary embeddings for nearest neighbor retrieval.",https://ieeexplore.ieee.org/document/8705282/,IEEE Transactions on Pattern Analysis and Machine Intelligence,1 Oct. 2019,ieeexplore
10.1109/TVCG.2019.2946769,Heter-Sim: Heterogeneous Multi-Agent Systems Simulation by Interactive Data-Driven Optimization,IEEE,Journals,"Interactive multi-agent simulation algorithms are used to compute the trajectories and behaviors of different entities in virtual reality scenarios. However, current methods involve considerable parameter tweaking to generate plausible behaviors. We introduce a novel approach (Heter-Sim) that combines physics-based simulation methods with data-driven techniques using an optimization-based formulation. Our approach is general and can simulate heterogeneous agents corresponding to human crowds, traffic, vehicles, or combinations of different agents with varying dynamics. We estimate motion states from real-world datasets that include information about position, velocity, and control direction. Our optimization algorithm considers several constraints, including velocity continuity, collision avoidance, attraction, direction control. Other constraints are implemented by introducing a novel energy function to control the motions of heterogeneous agents. To accelerate the computations, we reduce the search space for both collision avoidance and optimal solution computation. Heter-Sim can simulate tens or hundreds of agents at interactive rates and we compare its accuracy with real-world datasets and prior algorithms. We also perform user studies that evaluate the plausible behaviors generated by our algorithm and a user study that evaluates the plausibility of our algorithm via VR.",https://ieeexplore.ieee.org/document/8865441/,IEEE Transactions on Visualization and Computer Graphics,1 March 2021,ieeexplore
10.1109/3477.499796,Hidden state and reinforcement learning with instance-based state identification,IEEE,Journals,"Real robots with real sensors are not omniscient. When a robot's next course of action depends on information that is hidden from the sensors because of problems such as occlusion, restricted range, bounded field of view and limited attention, we say the robot suffers from the hidden state problem. State identification techniques use history information to uncover hidden state. Some previous approaches to encoding history include: finite state machines, recurrent neural networks and genetic programming with indexed memory. A chief disadvantage of all these techniques is their long training time. This paper presents instance-based state identification, a new approach to reinforcement learning with state identification that learns with much fewer training steps. Noting that learning with history and learning in continuous spaces both share the property that they begin without knowing the granularity of the state space, the approach applies instance-based (or ""memory-based"") learning to history sequences-instead of recording instances in a continuous geometrical space, we record instances in action-percept-reward sequence space. The first implementation of this approach, called Nearest Sequence Memory, learns with an order of magnitude fewer steps than several previous approaches.",https://ieeexplore.ieee.org/document/499796/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",June 1996,ieeexplore
10.1109/LSP.2020.3003517,Hierarchical Neural Architecture Search for Single Image Super-Resolution,IEEE,Journals,"Deep neural networks have exhibited promising performance in image super-resolution (SR). Most SR models follow a hierarchical architecture that contains both the cell-level design of computational blocks and the network-level design of the positions of upsampling blocks. However, designing SR models heavily relies on human expertise and is very labor-intensive. More critically, these SR models often contain a huge number of parameters and may not meet the requirements of computation resources in real-world applications. To address the above issues, we propose a Hierarchical Neural Architecture Search (HNAS) method to automatically design promising architectures with different requirements of computation cost. To this end, we design a hierarchical SR search space and propose a hierarchical controller for architecture search. Such a hierarchical controller is able to simultaneously find promising cell-level blocks and network-level positions of upsampling layers. Moreover, to design compact architectures with promising performance, we build a joint reward by considering both the performance and computation cost to guide the search process. Extensive experiments on five benchmark datasets demonstrate the superiority of our method over existing methods.",https://ieeexplore.ieee.org/document/9120351/,IEEE Signal Processing Letters,2020,ieeexplore
10.1109/ACCESS.2018.2804764,HotSpot: Anomaly Localization for Additive KPIs With Multi-Dimensional Attributes,IEEE,Journals,"Additive key performance indicators (KPIs) (such as page view (PV), revenue, and error count) with multi-dimensional attributes (such as ISP, Province, and DataCenter) are common and important in monitoring metrics in Internet companies. When an anomaly happens to an overall KPI, it is critical but challenging to localize the root cause, which is one (or more) combination of attribute values in multiple dimensions. For example, is the total PV decrease caused by the PV decrease from Beijingor China Mobile in Beijing, or Beijing and Shanghai? However, this task is very challenging for two major reasons. First, the PVs of different combinations are interdependent; thus, the PV anomalies at the root cause can cause the changes of many other PVs at different aggregation levels. Second, there could be tens of thousands of combinations to investigate in multi-dimensional attribute space. It is a difficulty to find the root cause from a huge search space. To address the first challenge, our approach HotSpot uses a novel potential score based on the ripple effect for anomaly propagation that we reveal. To address the second challenge, HotSpot adopts the Monte Carlo Tree Search algorithm and a hierarchical pruning strategy. Using the real-world data from a top global search engine, we show that HotSpot achieves a great improvement on effectiveness and robustness, i.e., 95% of all types of root cause cases using HotSpot (compared with only 15% using existing approaches) achieves an F-score over 90%. Operational experiences show that HotSpot can reduce the localization time from more than 1 h in manual efforts to less than 20 s.",https://ieeexplore.ieee.org/document/8288614/,IEEE Access,2018,ieeexplore
10.1109/ACCESS.2021.3096662,Human-Augmented Prescriptive Analytics With Interactive Multi-Objective Reinforcement Learning,IEEE,Journals,"The rise of Artificial Intelligence (AI) enables enterprises to manage large amounts of data in order to derive predictions about future performance and to gain meaningful insights. In this context, descriptive and predictive analytics has gained a significant research attention; however, prescriptive analytics has just started to emerge as the next step towards increasing data analytics maturity and leading to optimized decision making ahead of time. Although machine learning for decision making has been identified as one of the most important applications of AI, up to now, prescriptive analytics is mainly addressed with domain-specific optimization models. On the other hand, existing literature lacks generalized prescriptive analytics models capable of being dynamically adapted according to the human preferences. Reinforcement Learning, as the third machine learning paradigm alongside supervised learning and unsupervised learning, has the potential to deal with the dynamic, uncertain and time-variant environments, the huge states space of sequential decision making processes, as well as the incomplete knowledge. In this paper, we propose a human-augmented prescriptive analytics approach using Interactive Multi-Objective Reinforcement Learning (IMORL) in order to cope with the complexity of real-life environments and the need for optimized human-machine collaboration. The decision making process is modelled in a generalized way in order to assure scalability and applicability in a wide range of problems and applications. We deployed the proposed approach in a stock market case study in order to evaluate the proactive trading decisions that will lead to the maximum return and the minimum risk that the user's experience and the available data can yield in combination.",https://ieeexplore.ieee.org/document/9481230/,IEEE Access,2021,ieeexplore
10.1109/TGRS.2021.3102136,Hy-Demosaicing: Hyperspectral Blind Reconstruction From Spectral Subsampling,IEEE,Journals,"This article proposes a smart hyperspectral sensing strategy, implemented in the spectral domain, conceived for spaceborne sensor systems, where physical space, storage resources, and communication bandwidth are extremely scarce and expensive. Smart sensing means faster and hardware-friendly imaging. Instead of acquiring all band samples in the spectral domain, we randomly select a few band samples per spatial pixel location. A periodic structure of spectral band selector array (SBSA) is designed so that we can learn a subspace basis from subsamples, which is essential to the underlying hyperspectral image (HSI) recovery algorithm. This spectral subsampling sensing strategy yields a demosaicing problem. We propose a blind hyperspectral reconstruction technique termed hyperspectral demosaicing (Hy-demosaicing) exploiting spectral low-rankness and spatial correlation of HSIs. It is blind in the sense that the signal subspace is learned from measured spectral subsamples. The subspace basis is data-adaptive and provides a more compact representation than other non-adaptive representations. This adaptiveness leads to improved image recovery as illustrated in experiments with real data.",https://ieeexplore.ieee.org/document/9513279/,IEEE Transactions on Geoscience and Remote Sensing,2022,ieeexplore
10.1109/ACCESS.2020.3047077,Hybrid Deep Learning-Based Model for Wind Speed Forecasting Based on DWPT and Bidirectional LSTM Network,IEEE,Journals,"Accurate wind speed forecasting is essential for the reliability and security of the power system, and optimal operation and management of wind integrated smart grids. However, it is still a challenging task due to the highly uncertain and volatile nature of wind speed. Accordingly, in this work, a novel deep learning-based model integrating the discrete wavelet packet transform (DWPT) and bidirectional long short-term memory (BLSTM) is developed to precisely capture deep temporal features and learn the time-varying relationship of wind speed time series. In the proposed method, by applying the DWPT, both approximations and details parts are decomposed by passing through the filters to choose the frequency band related to the features of the original signal more adaptively. The BLSTM networks are incorporated to deal with the uncertainties more effectively as they have bidirectional memory capability (feedforward and feedback loops) to investigate both previous and future hidden layers data. To simultaneously improve the forecasting performance and decrease the learning complexity, the reconstructed state space of historical wind data is employed to reflect the evolution laws of wind speed. Two case studies using real-world wind speed datasets gathered from Flatirons campus (M2) of National Renewable Energy Laboratory (NREL) located in Colorado, USA and weather station of Edmonton, Canada are implemented to demonstrate the effectiveness and superiority of the proposed hybrid method compared to the shallow architectures and state-of-the-art deep learning models in the recent literature.",https://ieeexplore.ieee.org/document/9306756/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2019.2894524,Hybrid Stochastic Exploration Using Grey Wolf Optimizer and Coordinated Multi-Robot Exploration Algorithms,IEEE,Journals,"Multi-robot exploration is a search of uncertainty in restricted space seeking to build a finite map by a group of robots. It has the main task to distribute the search assignments among robots in real time. In this paper, we proposed a stochastic optimization for multi-robot exploration that mimics the coordinated predatory behavior of grey wolves via simulation. Here, the robot movement is computed by the combined deterministic and metaheuristic techniques. It uses the Coordinated Multi-Robot Exploration and GreyWolf Optimizer algorithms as a new method called the hybrid stochastic exploration. Initially, the deterministic cost and utility determine the precedence of adjacent cells around a robot. Then, the stochastic optimization improves the overall solution. It implies that the robots evaluate the environment by the deterministic approach and move on using the metaheuristic algorithm. The proposed hybrid method was implemented on simple and complex maps and compared with the Coordinated Multi-Robot Exploration algorithm. The simulation results show that the stochastic optimization enhances the deterministic approach to completely explore and map out the areas.",https://ieeexplore.ieee.org/document/8631022/,IEEE Access,2019,ieeexplore
10.1109/70.63270,Hybrid hierarchical scheduling and control systems in manufacturing,IEEE,Journals,"Some experiments on the integration of algorithmic techniques with knowledge-based ones are discussed. Two case studies are presented: an FMS cell and a press shop. It was found that the algorithmic procedures developed for production scheduling resulted in limiting the ability to cope with the complexity of the real manufacturing world. The scheduling problem, seen as a constraint satisfaction problem, can be approached with rule-based techniques. Nevertheless, algorithmic techniques are found to be valuable for their efficiency and ability to deal with aggregated data. This ability is fundamental for an efficient implementation of hierarchical control systems in general and in the manufacturing context in particular. This suggests that the integration of rule-based techniques with algorithmic ones can increase the efficiency of searching in the space of possible solutions. The ability to deal with aggregated data can have little value when detailed real-time operation scheduling is needed. In this case, simple dispatching rules are often used, and sophisticated operations research methods are not used. In such a dynamic situation, a purely-rule based approach may be more suitable.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/63270/,IEEE Transactions on Robotics and Automation,Dec. 1990,ieeexplore
10.1109/ACCESS.2020.3029174,Hyperspectral Image Classification With CapsNet and Markov Random Fields,IEEE,Journals,"Hyperspectral image (HSI) classification is one of the most challenging problems in understanding HSI. Convolutional neural network(CNN), with the strong ability to extract features using the hidden layers in the network, has been introduced to solve this problem. However, several fully connected layers are always appended at the end of CNN, which dramatically reduced the efficiency of space utilization and make the classification algorithm hard to converge. Recently, a new network architecture called capsule network (CapsNet) was presented to improve the CNN. It uses groups of neurons as capsules to replace the neurons in traditional neural networks. Since the capsule can provide superior spectral features and spatial information extracted, its performance is better than the most advanced CNN in some fields. Motivated by this idea, a new remote sensing hyperspectral image classification algorithm called Conv-Caps is proposed to make full use of the advantages of both. We integrate spectral and spatial information into the proposed framework and combine Conv-Caps with Markov Random Field (MRF), which uses the graph cut expansion method to solve the classification task. The Caps-MRF method is further proposed. First, select an initial feature extractor, which a CNN without fully connected layers. Then, the initial recognition feature map is put into the newly designed CapsNet to obtain the probability map. Finally, the MRF model is used to calculate the subdivision labels. The presented method is trained with three real HSI datasets and is compared with the latest methods. We find the framework can produce competitive classification performance.",https://ieeexplore.ieee.org/document/9214813/,IEEE Access,2020,ieeexplore
10.1109/21.179845,Identification of model structure via qualitative simulation,IEEE,Journals,"Different approaches and programs for qualitative simulation are briefly reviewed to illustrate the characteristic features of QUALSIM, an object-oriented package for supporting the identification of the structure of dynamic systems. The package is based on a qualitative interface and a stochastic calculus using variables' ranges defined in the real number space. The intuitive way of defining variables and connections and the possibility of avoiding the definitions of equations linking the variables make it possible to use the package for fast model prototyping and as an educational tool. The identification of a model of lake eutrophication is shown as an example.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/179845/,"IEEE Transactions on Systems, Man, and Cybernetics",Sept.-Oct. 1992,ieeexplore
10.1109/ACCESS.2020.2988284,Image Denoising With Generative Adversarial Networks and its Application to Cell Image Enhancement,IEEE,Journals,"This paper proposes an image denoising training framework based on Wasserstein Generative Adversarial Networks (WGAN) and applies it to cell image denoising. Cell image denoising is a challenging task which has high requirement on the recovery of feature details. Current popular convolutional neural network (CNN) based denoising methods encounter a blurriness issue that denoised images are blurry on texture details, which is fatal for the cell image denoising. In this paper, to solve the blurriness issue, we first theoretically analyze the cause of the blurriness issue. Subsequently, an image denoising training framework with WGAN based adversarial learning is proposed. This training framework solves the blurriness issue by guiding the denoising network to find the distribution space of real clean images rather than the distribution space of blurry images and introducing feature information. Experimental results show that this training framework can effectively solve the blurriness issue and achieve better denoising performance than the state-of-the-art denoising methods. Meanwhile, the application of this training framework on cell image denoising also achieves satisfactory performance. Recovered cell images of this training framework are clear on feature details.",https://ieeexplore.ieee.org/document/9069260/,IEEE Access,2020,ieeexplore
10.1109/LGRS.2021.3091592,Image Registration Via Marginal Distribution Adaptation,IEEE,Journals,"The distribution of feature vectors plays a critical role in image registration. In this letter, we propose a novel approach for remote sensing image registration based on marginal distribution adaptation. First, we map the feature vectors of reference and sensed images into a latent space. Transfer component analysis (TCA) is employed to compute the transformation matrix by minimizing the maximum mean discrepancy (MMD). Then, we match feature vectors in the latent space where their marginal distributions are similar, which can increase correct correspondences and enhance registration accuracy. Finally, we test the proposed algorithm on ten real image pairs. The effectiveness and efficiency of our approach are verified by experimental results.",https://ieeexplore.ieee.org/document/9598187/,IEEE Geoscience and Remote Sensing Letters,2022,ieeexplore
10.1109/TIP.2009.2025560,Image Segmentation Based on GrabCut Framework Integrating Multiscale Nonlinear Structure Tensor,IEEE,Journals,"In this paper, we propose an interactive color natural image segmentation method. The method integrates color feature with multiscale nonlinear structure tensor texture (MSNST) feature and then uses GrabCut method to obtain the segmentations. The MSNST feature is used to describe the texture feature of an image and integrated into GrabCut framework to overcome the problem of the scale difference of textured images. In addition, we extend the Gaussian Mixture Model (GMM) to MSNST feature and GMM based on MSNST is constructed to describe the energy function so that the texture feature can be suitably integrated into GrabCut framework and fused with the color feature to achieve the more superior image segmentation performance than the original GrabCut method. For easier implementation and more efficient computation, the symmetric KL divergence is chosen to produce the estimates of the tensor statistics instead of the Riemannian structure of the space of tensor. The Conjugate norm was employed using Locality Preserving Projections (LPP) technique as the distance measure in the color space for more discriminating power. An adaptive fusing strategy is presented to effectively adjust the mixing factor so that the color and MSNST texture features are efficiently integrated to achieve more robust segmentation performance. Last, an iteration convergence criterion is proposed to reduce the time of the iteration of GrabCut algorithm dramatically with satisfied segmentation accuracy. Experiments using synthesis texture images and real natural scene images demonstrate the superior performance of our proposed method.",https://ieeexplore.ieee.org/document/5075666/,IEEE Transactions on Image Processing,Oct. 2009,ieeexplore
10.1109/41.499806,Image fusion and subpixel parameter estimation for automated optical inspection of electronic components,IEEE,Journals,"The authors present a new approach to automated optical inspection (AOI) of circular features that combines image fusion with subpixel edge detection and parameter estimation. In their method, several digital images are taken of each part as it moves past a camera, creating an image sequence. These images are fused to produce a high-resolution image of the features to be inspected. Subpixel edge detection is performed on the high-resolution image, producing a set of data points that is used for ellipse parameter estimation. The fitted ellipses are then back-projected into 3-space in order to obtain the sizes of the circular features being inspected, assuming that the depth is known. The method is accurate, efficient, and easily implemented. The authors present experimental results for real intensity images of circular features of varying sizes. Their results demonstrate that their algorithm shows greatest improvement over traditional methods in cases where the feature size is small relative to the resolution of the imaging device.",https://ieeexplore.ieee.org/document/499806/,IEEE Transactions on Industrial Electronics,June 1996,ieeexplore
10.1109/TMECH.2013.2245337,Image-Based Visual Servoing of a 7-DOF Robot Manipulator Using an Adaptive Distributed Fuzzy PD Controller,IEEE,Journals,"This paper is concerned with the design and implementation of a distributed proportional-derivative (PD) controller of a 7-degrees of freedom (DOF) robot manipulator using the Takagi-Sugeno (T-S) fuzzy framework. Existing machine learning approaches to visual servoing involve system identification of image and kinematic Jacobians. In contrast, the proposed approach actuates a control signal primarily as a function of the error and derivative of the error in the desired visual feature space. This approach leads to a significant reduction in the computational burden as compared to model-based approaches, as well as existing learning approaches to model inverse kinematics. The simplicity of the controller structure will make it attractive in industrial implementations where PD/PID type schemes are in common use. While the initial values of PD gain are learned with the help of model-based controller, an online adaptation scheme has been proposed that is capable of compensating for local uncertainties associated with the system and its environment. Rigorous experiments have been performed to show that visual servoing tasks such as reaching a static target and tracking of a moving target can be achieved using the proposed distributed PD controller. It is shown that the proposed adaptive scheme can dynamically tune the controller parameters during visual servoing, so as to improve its initial performance based on parameters obtained while mimicking the model-based controller. The proposed control scheme is applied and assessed in real-time experiments using an uncalibrated eye-in-hand robotic system with a 7-DOF PowerCube robot manipulator.",https://ieeexplore.ieee.org/document/6471828/,IEEE/ASME Transactions on Mechatronics,April 2014,ieeexplore
10.1109/TAMD.2011.2106781,Implicit Sensorimotor Mapping of the Peripersonal Space by Gazing and Reaching,IEEE,Journals,"Primates often perform coordinated eye and arm movements, contextually fixating and reaching towards nearby objects. This combination of looking and reaching to the same target is used by infants to establish an implicit visuomotor representation of the peripersonal space, useful for both oculomotor and arm motor control. In this work, taking inspiration from such behavior and from primate visuomotor mechanisms, a shared sensorimotor map of the environment, built on a radial basis function framework, is configured and trained by the coordinated control of eye and arm movements. Computational results confirm that the approach seems especially suitable for the problem at hand, and for its implementation on a real humanoid robot. By exploratory gazing and reaching actions, either free or goal-based, the artificial agent learns to perform direct and inverse transformations between stereo vision, oculomotor, and joint-space representations. The integrated sensorimotor map that allows to contextually represent the peripersonal space through different vision and motor parameters is never made explicit, but rather emerges thanks to the interaction of the agent with the environment.",https://ieeexplore.ieee.org/document/5703113/,IEEE Transactions on Autonomous Mental Development,March 2011,ieeexplore
10.1109/TAMD.2011.2128318,Improved Binocular Vergence Control via a Neural Network That Maximizes an Internally Defined Reward,IEEE,Journals,"We describe the autonomous development of binocular vergence control in an active robotic vision system through attention-gated reinforcement learning (AGREL). The control policy is implemented by a neural network, which maps the outputs from a population of disparity energy neurons to a set of vergence commands. The network learns to maximize a reward signal that is based on an internal representation of the visual input: the total activation in the population of disparity energy neurons. This system extends previous work using Q learning by increasing the complexity of the policy in two ways. First, the input state space is continuous, rather than discrete, and is based upon a larger diversity of neurons. Second, we increase the number of possible actions. We evaluate the network learning and performance on natural images and with real objects in a cluttered environment. The policies learned by the network outperform policies by Q learning in two ways: the mean squared errors are smaller and the closed loop frequency response has larger bandwidth.",https://ieeexplore.ieee.org/document/5734799/,IEEE Transactions on Autonomous Mental Development,Sept. 2011,ieeexplore
10.1109/ACCESS.2019.2920913,Improved Multi-Agent Reinforcement Learning for Path Planning-Based Crowd Simulation,IEEE,Journals,"The combination of multi-agent technology and reinforcement learning methods has been recognized as an effective way which is used in path planning-based crowd simulation. However, the existing solution is still not satisfactory due to the problem in the mutual influence of agents. Therefore, an improved multi-agent reinforcement learning method (IMARL algorithm) is introduced. In this method, the intersection of the pedestrian trajectory extracted from the real video is first used as the state space for reinforcement learning. The crowd is grouped and the leader is selected. A bulletin board is added to the reinforcement learning algorithm of multi-agent to store the empirical knowledge of the learning process, and the navigation agent passes information between the leader and the bulletin board. The original social force model was improved, and the cohesive force of visual factors was added to the force formula. The IMARL algorithm is combined with the improved social force model for crowd evacuation simulation. Using a two-layer control mechanism, the leader in the upper layer uses the decision process based on the IMARL algorithm to select the path, and the individuals in the bottom group use the improved social force model to evacuate. The method of this paper not only solves the dimensionality disaster problem of reinforcement learning but also improves the convergence speed. The evacuation efficiency is effectively improved in crowd evacuation simulation experiments. In addition, it can also provide specific guidance scheme for crowd evacuation improvement and assistant decision support for the prevention and management of large-scale group trampling incidents.",https://ieeexplore.ieee.org/document/8731958/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2019.2902564,Improving Indoor Fingerprinting Positioning With Affinity Propagation Clustering and Weighted Centroid Fingerprint,IEEE,Journals,"Nowadays, research and development of various indoor positioning systems (IPS) have been increasing owing to flourishing social and commercial interest in location-based services (LBSs). Among LBS technologies, we used the Bluetooth low energy beacon in our system, which consumes less energy and is embedded in many current smartphones and tablets. In particular, the fingerprinting method has become a prime choice in the design of IPS owing to its good location estimation and the fact that a line-of-sight from access points is not required. We propose an improved two-step fingerprinting localization using multiple fingerprint features to enhance the localization accuracy. The proposed system uses a propagation model to convert RSS of beacons to distance and estimate the weighted centroid (WC) of nearby beacons. The estimated WCs along with signal strength and rank of the nearby beacons are stored in the server database for localization instead of RSS from all the deployed beacons. First, the proposed system makes use of diverse fingerprinting features to increase localization accuracy that also reduces both the physical size of the database and the amount of data communication with the server in the execution phase; second, affinity propagation clustering minimizes the searching space of RPs and reduces the computational cost; third, exponential averaging is introduced to smooth the noisy RSS. The experimental results obtained by real field deployment show that the proposed method significantly improves the performance of the positioning system in both the positioning accuracy and radio-map database size.",https://ieeexplore.ieee.org/document/8667640/,IEEE Access,2019,ieeexplore
10.1109/TMM.2021.3050086,Improving Robustness of DASH Against Unpredictable Network Variations,IEEE,Journals,"Most video players use adaptive bitrate (ABR) algorithms to provide good quality-of-experience (QoE) in dynamic network conditions. To deal with the adaptation challenges, many ABR algorithms select bitrate by optimizing a defined QoE function. Within the framework, various algorithms mainly differ in how the optimization problem is solved, including prediction-based approaches and learn-based approaches. However, these algorithms suffer from limited performance in the current popular mobile streaming which has limited resources and rapidly changing link rates. Existing machine-learning approaches face deployment difficulties on mobile devices, and prediction-based approaches that rely on throughput prediction experience large buffer occupancy variations in cellular networks, resulting in rebuffering frequently. To provide a robust and lightweight ABR algorithm for mobile streaming, this work improves the robustness of prediction-based scheme against unpredictable network variations and develops RBC (Robust Bitrate Controller) algorithm. Rather than optimizing QoE over the entire buffer capacity, RBC creates buffer margins to absorb the impact of throughput jitters and solves QoE maximization on the narrowed buffer range. The amount of buffer margin is dynamically adjusted based on the real-time throughput fluctuation to ensure sufficient de-jitter space. For online lightweight deployment, RBC provides a closed-form solution of the desired bitrate with small computation complexity by using adaptive control approach. Trace-driven experiments and real-world tests show that RBC effectively reduces the playback freezing and gains an improvement in overall QoE.",https://ieeexplore.ieee.org/document/9317786/,IEEE Transactions on Multimedia,2022,ieeexplore
10.1109/ACCESS.2020.2975032,Inattentional Blindness for Redirected Walking Using Dynamic Foveated Rendering,IEEE,Journals,"Redirected walking is a Virtual Reality(VR) locomotion technique which enables users to navigate virtual environments (VEs) that are spatially larger than the available physical tracked space. In this work we present a novel technique for redirected walking in VR based on the psychological phenomenon of inattentional blindness. Based on the user's visual fixation points we divide the user's view into zones. Spatially-varying rotations are applied according to the zone's importance and are rendered using foveated rendering. Our technique is real-time and applicable to small and large physical spaces. Furthermore, the proposed technique does not require the use of stimulated saccades but rather takes advantage of naturally occurring saccades and blinks for a complete refresh of the framebuffer. We performed extensive testing and present the analysis of the results of three user studies conducted for the evaluation.",https://ieeexplore.ieee.org/document/9003250/,IEEE Access,2020,ieeexplore
10.1109/TNSE.2020.3048902,Inductive Representation Learning via CNN for Partially-Unseen Attributed Networks,IEEE,Journals,"Network embedding aims to map a complex network into a low-dimensional vector space while maximally preserving the properties of the original network. An attributed network is a typical real-world network that models the relationships and attributes of real-world entities. Its analysis is of great significance in many applications. However, most such networks are incomplete with partially-known attributes, links and labels. Traditional network embedding methods are designed for a complete network and cannot be applied to a network with incomplete information. Thus, this work proposes an inductive embedding model to learn the robust representations for a partially-unseen attributed network. It is designed based on a multi-core convolutional neural network and a semi-supervised learning mechanism, which can preserve the properties of such a network and generate the effective representations for unseen nodes in a model training process. We evaluate its performance on the task of inductive node classification and community detection via three real-world attributed networks. Experimental results show that it significantly outperforms the state-of-the-art.",https://ieeexplore.ieee.org/document/9314087/,IEEE Transactions on Network Science and Engineering,1 Jan.-March 2021,ieeexplore
10.1109/ACCESS.2020.3039602,Influence of Initializing Krill Herd Algorithm With Low-Discrepancy Sequences,IEEE,Journals,"The krill herd (KH) algorithm is a global metaheuristic algorithm that was initially proposed for solving continuous optimization problems. The KH algorithm, since inception, has generated considerable real-world application interests in the research community. The standard algorithm solution implementation steps follow the initialization mechanism, which relies mainly on educated guesses or random initialization solution generation. Therefore, to improve the performance of the KH algorithm, the current study is set to investigate the influence of initializing the KH algorithm with three low-discrepancy sequences, such as the Faure sequence, Sobol sequence, and Van der Corput sequence. These low-discrepancy sequences are known to be more uniformly distributed across the problem search space than the commonly used random number initialization method. The study also evaluates the influence of population size on the performance of the proposed variants of the improved KH algorithms. The experimental results show significant improvements for the enhanced KH algorithms in terms of performance and the quality of solutions obtained; particularly on standard benchmarked high-dimension test problem instances, where the enhanced KH variants outperformed the existing basic KH algorithm for all the test functions evaluated. Similarly, the results for low dimension test cases showed less sensitivity to the initialization schemes, as the performance of our proposed improved scheme was comparable to that of the basic KH algorithm. However, in most cases, as the problem dimension was scaled up, the enhanced KH outperformed the basic KH. Evaluation results based on the population size of the algorithm, revealed that when the number of Krill is set at 25, the Sobol based KH initialization scheme performed better than did the other methods. Although, the Van der Corput and Faure based KH initialization schemes showed similar sensitivity when the dimension was set at 20. As we varied the population size of Krill, it was observed that the performance of the Sobol based KH initialization scheme deteriorated, whereas the other two methods showed superior performance. Overall, the findings from this study revealed that there are significant improvements in the performance of KH algorithm when initialized with low-discrepancy sequences.",https://ieeexplore.ieee.org/document/9265184/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3109862,Influential Attributed Communities Search in Large Networks (InfACom),IEEE,Journals,"Community search is a fundamental problem in graph analysis. In many applications, network nodes have specific properties that are essential for making sense of communities. In these networks, attributes are associated with nodes to capture their properties. The community influence is a key property of the community that can be employed to sort the communities in a network based on the relevance/importance of certain attributes. Unfortunately, most of the previously introduced community search algorithms over attributed networks neglected the community influence. In this paper, we study the influential attributed community search problem. Different factors for measuring the influence are discussed. Also, different Influential Attributed Community (InfACom) algorithms based on the concept of k-clique are proposed. Two techniques are presented one for sequential implementation with three variations and one for parallel implementation. In addition, we propose efficient algorithms for maintaining the proposed algorithms on dynamic graphs. The proposed algorithms are evaluated on different real datasets. The experimental results show that the summarization technique reduces the size of the graph by approximately half. In addition, it shows that the proposed algorithms <inline-formula> <tex-math notation=""LaTeX"">$EnhancedExact$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=""LaTeX"">$Approximate$ </tex-math></inline-formula> outperform the state-of-the-art approaches Incremental Time efficient <inline-formula> <tex-math notation=""LaTeX"">$(Inc-T)$ </tex-math></inline-formula>, Incremental Space efficient <inline-formula> <tex-math notation=""LaTeX"">$(Inc-S)$ </tex-math></inline-formula>, <inline-formula> <tex-math notation=""LaTeX"">$Exact$ </tex-math></inline-formula>, and 2-Approximation <inline-formula> <tex-math notation=""LaTeX"">$(AppInc)$ </tex-math></inline-formula> in both efficiency and effectiveness. For the <inline-formula> <tex-math notation=""LaTeX"">$EnhancedExact$ </tex-math></inline-formula> algorithm, the results show that the efficiency is at least 7 times faster than the <inline-formula> <tex-math notation=""LaTeX"">$Inc-S$ </tex-math></inline-formula> algorithm, at least 4.5 times faster than the <inline-formula> <tex-math notation=""LaTeX"">$Inc-T$ </tex-math></inline-formula> algorithm, and 2 times faster than the <inline-formula> <tex-math notation=""LaTeX"">$AppInc$ </tex-math></inline-formula> algorithm. For the <inline-formula> <tex-math notation=""LaTeX"">$Approximate$ </tex-math></inline-formula> algorithm, the results show that its efficiency is at least 10 times faster than the <inline-formula> <tex-math notation=""LaTeX"">$Inc-S$ </tex-math></inline-formula> algorithm, at least 6.4 times faster than the <inline-formula> <tex-math notation=""LaTeX"">$Inc-T$ </tex-math></inline-formula> algorithm, and 3 times faster than the <inline-formula> <tex-math notation=""LaTeX"">$AppInc$ </tex-math></inline-formula> algorithm. Finally, the results show that the proposed algorithms retrieve cohesive communities with a smaller diameter than all the state-of-the-art approaches.",https://ieeexplore.ieee.org/document/9528361/,IEEE Access,2021,ieeexplore
10.1109/TIP.2008.925377,"Infrared Image Enhancement using <formula formulatype=""inline""><tex>$H_{\infty}$</tex> </formula> Bounds for Surveillance Applications",IEEE,Journals,"In this paper, two algorithms have been presented to enhance the infrared (IR) images. Using the autoregressive moving average model structure and H<sub>infin</sub> optimal bounds, the image pixels are mapped from the IR pixel space into normal optical image space, thus enhancing the IR image for improved visual quality. Although H<sub>infin</sub>-based system identification algorithms are very common now, they are not quite suitable for real-time applications owing to their complexity. However, many variants of such algorithms are possible that can overcome this constraint. Two such algorithms have been developed and implemented in this paper. Theoretical and algorithmic results show remarkable enhancement in the acquired images. This will help in enhancing the visual quality of IR images for surveillance applications.",https://ieeexplore.ieee.org/document/4549751/,IEEE Transactions on Image Processing,Aug. 2008,ieeexplore
10.1109/TPWRS.2021.3071918,Insecurity Early Warning for Large Scale Hybrid AC/DC Grids Based on Decision Tree and Semi-Supervised Deep Learning,IEEE,Journals,"Fast insecurity early warning is the key technique to resist the dynamic insecurity risk, which becomes intractable due to the strong nonlinearity of hybrid AC/DC grids and the high uncertainty of wind generation. Considering dynamic security constraints and wind power uncertainty, this paper presents an insecurity early warning method based on decision tree (DT) and semi-supervised deep learning. First, semi-supervised deep learning is deployed to estimate the dynamic security limit of the critical interface of hybrid AC/DC grids. The system security is assessed by comparing the actual power transfer of the critical interface with the security limit. Then, operating conditions (OCs) are ranked into different insecure levels according to the type of preventive control actions that is needed to ensure the system security. Finally, oblique DT is utilized to identify insecurity classification boundaries in the wind power injection space. Insecure OC sets are constructed based on these classification boundaries. Simulation results of the real-life Jiangsu-Shanghai interconnected grid in east China demonstrate that the proposed method can fast construct the insecure OC sets corresponding to different insecure levels.",https://ieeexplore.ieee.org/document/9399284/,IEEE Transactions on Power Systems,Nov. 2021,ieeexplore
10.1109/LGRS.2020.3014676,Intelligent Sampling for Vegetation Nitrogen Mapping Based on Hybrid Machine Learning Algorithms,IEEE,Journals,"Upcoming satellite imaging spectroscopy missions will deliver spatiotemporal explicit data streams to be exploited for mapping vegetation properties, such as nitrogen (N) content. Within retrieval workflows for real-time mapping over agricultural regions, such crop-specific information products need to be derived precisely and rapidly. To allow fast processing, intelligent sampling schemes for training databases should be incorporated to establish efficient machine learning (ML) models. In this study, we implemented active learning (AL) heuristics using kernel ridge regression (KRR) to minimize and optimize a training database for variational heteroscedastic Gaussian processes regression (VHGPR) to estimate aboveground N content. Several uncertainty and diversity criteria were applied on a lookup table (LUT) composed of aboveground N content and corresponding hyperspectral reflectance simulated by the PROSAIL-PRO model. The best-performing AL criteria were Euclidian distance-based diversity (EBD) resulting in a reduction of the LUT training data set by 81% (50 initial samples plus 141 samples selected from a pool of 1000 samples). This reduced LUT was used for training VHGPR, which is not only a competitive algorithm but also provides uncertainty estimates. Validation against <i>in situ</i> N reference data provided excellent results with a root-mean-square error (RMSE) of 1.84 g/m<sup>2</sup> and a coefficient of determination (<inline-formula> <tex-math notation=""LaTeX"">$R^{2}$ </tex-math></inline-formula>) of 0.92. Mapping aboveground N content over an agricultural region yielded reliable estimates and meaningful associated uncertainties. These promising results encourage the transfer of such hybrid workflows into space and time within the frame of future operational N monitoring from satellite imaging spectroscopy data.",https://ieeexplore.ieee.org/document/9169812/,IEEE Geoscience and Remote Sensing Letters,Dec. 2021,ieeexplore
10.1109/7.784074,Intelligent control of a planning system for astronaut training,IEEE,Journals,"This work intends to design, analyze and solve, from the systems control perspective, a complex, dynamic, and multiconstrained planning system for generating training plans for crew members of the NASA-led International Space Station. Various intelligent planning systems have been developed within the framework of artificial intelligence. These planning systems generally lack a rigorous mathematical formalism to allow a reliable and flexible methodology for their design, modeling, and performance analysis in a dynamical, time-critical, and multiconstrained environment. Formulating the planning problem in the domain of discrete-event systems under a unified framework such that it can be modeled, designed, and analyzed as a control system will provide a self-contained theory for such planning systems. This will also provide a means to certify various planning systems for operations in the dynamical and complex environments in space. The work presented here completes the design, development, and analysis of an intricate, large-scale, and representative mathematical formulation for intelligent control of a real planning system for Space Station crew training. This planning system has been tested and used at NASA-Johnson Space Center.",https://ieeexplore.ieee.org/document/784074/,IEEE Transactions on Aerospace and Electronic Systems,July 1999,ieeexplore
10.1109/TVCG.2019.2963015,Interactive Simulation of Scattering Effects in Participating Media Using a Neural Network Model,IEEE,Journals,"Rendering participating media is important to the creation of photorealistic images. Participating media has a translucent aspect that comes from light being scattered inside the material. For materials with a small mean-free-path (mfp), multiple scattering effects dominate. Simulating these effects is computationally intensive, as it requires tracking a large number of scattering events inside the material. Existing approaches precompute multiple scattering events inside the material and store the results in a table. During rendering time, this table is used to compute the scattering effects. While these methods are faster than explicit scattering computation, they incur higher storage costs. In this paper, we present a new representation for double and multiple scattering effects that uses a neural network model. The scattering response from all homogeneous participating media is encoded into a neural network in a preprocessing step. At run time, the neural network is then used to predict the double and multiple scattering effects. We demonstrate the effects combined with Virtual Ray Lights (VRL), although our approach can be integrated with other rendering algorithms. Our algorithm is implemented on GPU. Double and multiple scattering effects for the entire participating media space are encoded using only 23.6 KB of memory. Our method achieves 50 ms per frame in typical scenes and provides results almost identical to the reference.",https://ieeexplore.ieee.org/document/8945399/,IEEE Transactions on Visualization and Computer Graphics,1 July 2021,ieeexplore
10.1109/LRA.2018.2870466,Introduction to the Special Issue on AI for Long-Term Autonomy,IEEE,Journals,"The papers in this special section focus on the use of artificial intelligence (AI) for long term autonomy. Autonomous systems have a long history in the fields of AI and robotics. However, only through recent advances in technology has it been possible to create autonomous systems capable of operating in long-term, real-world scenarios. Examples include autonomous robots that operate outdoors on land, in air, water, and space; and indoors in offices, care homes, and factories. Designing, developing, and maintaining intelligent autonomous systems that operate in real-world environments over long periods of time, i.e. weeks, months, or years, poses many challenges. This special issue focuses on such challenges and on ways to overcome them using methods from AI. Long-term autonomy can be viewed as both a challenge and an opportunity. The challenge of long-term autonomy requires system designers to ensure that an autonomous system can continue operating successfully according to its real-world application demands in unstructured and semi-structured environments. This means addressing issues related to hardware and software robustness (e.g., gluing in screws and profiling for memory leaks), as well as ensuring that all modules and functions of the system can deal with the variation in the environment and tasks that is expected to occur over its operating time. ",https://ieeexplore.ieee.org/document/8478420/,IEEE Robotics and Automation Letters,Oct. 2018,ieeexplore
10.1109/LRA.2020.3013937,Invariant Transform Experience Replay: Data Augmentation for Deep Reinforcement Learning,IEEE,Journals,"Deep Reinforcement Learning (RL) is a promising approach for adaptive robot control, but its current application to robotics is currently hindered by high sample requirements. To alleviate this issue, we propose to exploit the symmetries present in robotic tasks. Intuitively, symmetries from observed trajectories define transformations that leave the space of feasible RL trajectories invariant and can be used to generate new feasible trajectories, which could be used for training. Based on this data augmentation idea, we formulate a general framework, called Invariant Transform Experience Replay that we present with two techniques: (i) Kaleidoscope Experience Replay exploits reflectional symmetries and (ii) Goal-augmented Experience Replay which takes advantage of lax goal definitions. In the Fetch tasks from OpenAI Gym, our experimental results show significant increases in learning rates and success rates. Particularly, we attain a 13, 3, and 5 times speedup in the pushing, sliding, and pick-and-place tasks respectively in the multi-goal setting. Performance gains are also observed in similar tasks with obstacles and we successfully deployed a trained policy on a real Baxter robot. Our work demonstrates that invariant transformations on RL trajectories are a promising methodology to speed up learning in deep RL. Code, video, and supplementary materials are available at [1].",https://ieeexplore.ieee.org/document/9158366/,IEEE Robotics and Automation Letters,Oct. 2020,ieeexplore
10.1093/mnras/stab1879,Investigating cosmological GAN emulators using latent space interpolation,OUP,Journals,"Generative adversarial networks (GANs) have been recently applied as a novel emulation technique for large-scale structure simulations. Recent results show that GANs can be used as a fast and efficient emulator for producing novel weak lensing convergence maps as well as cosmic web data in 2D and 3D. However, like any algorithm, the GAN approach comes with a set of limitations, such as an unstable training procedure, inherent randomness of the produced outputs, and difficulties when training the algorithm on multiple data sets. In this work, we employ a number of techniques commonly used in the machine learning literature to address the mentioned limitations. Specifically, we train a GAN to produce weak lensing convergence maps and dark matter overdensity field data for multiple redshifts, cosmological parameters, and modified gravity models. In addition, we train a GAN using the newest Illustris data to emulate dark matter, gas, and internal energy distribution data simultaneously. Finally, we apply the technique of latent space interpolation as a tool for understanding the feature space of the GAN algorithm. We show that the latent space interpolation procedure allows the generation of outputs with intermediate cosmological parameters that were not included in the training data. Our results indicate a 120 percent difference between the power spectra of the GAN-produced and the test data samples depending on the data set used and whether Gaussian smoothing was applied. Similarly, the Minkowski functional analysis indicates a good agreement between the emulated and the real images for most of the studied data sets.",https://ieeexplore.ieee.org/document/9521040/,Monthly Notices of the Royal Astronomical Society,July 2021,ieeexplore
10.1109/ACCESS.2019.2895626,Joint Access Mode Selection and Spectrum Allocation for Fog Computing Based Vehicular Networks,IEEE,Journals,"The explosive growth of data with the requirements of high reliability and low latency has posed huge challenges to the vehicular networks. One potential solution is to deploy the fog computing servers geographically closer to the vehicles to serve the vehicle-based applications in real time. However, due to the constraint of caching storage space as well as lack of tractable access mode selection and spectrum allocation algorithm, it is very challenging to balance the network transmission performance and fronthaul savings. In this paper, we investigate the joint optimization problem of access mode selection and spectrum allocation in fog computing-based vehicular networks. To solve the problem in an efficient way, the original high complexity optimization problem is divided into two subproblems. Wherein, the vehicle access mode selection problem is solved by Q-learning-based algorithm by considering spectrum allocation profiles and the fronthaul link cost. The randomly vehicular network topologies are modeled as CoX processes and the closed-form payoff expressions are derived by the stochastic geometry tools. On the other hand, the optimal spectrum allocation indicator value can be finally obtained via convex optimization. The analytical results for the proposed algorithm as well as the traditional baseline approaches are evaluated with different weight factors, which verify our theoretical analysis and confirm the proposed approach can achieve significant performance gains.",https://ieeexplore.ieee.org/document/8629049/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2020.2979163,Joint Learning of Embedding-Based Parent Components and Information Diffusion for Social Networks,IEEE,Journals,"Diffusion on social networks refers to the phenomena that opinions are spread via connected nodes. Given a set of observed cascades, the underlying diffusion process can be inferred for social network analysis. Earlier studies for modeling the diffusion process often assume that the activation of a node depends independently on the activations of its neighbors (or called parent nodes). Nevertheless, the activation of a node also depends on the connectivity of its neighbors. For instance, the opinions from the neighbors of the same closely connected social group are often similar, and thus those neighbors exhibit similar influence. Some recent studies incorporate the structural dependency of neighbors as connected components, which allow more accurate diffusion models to be inferred. However, the effectiveness of such component-based models often depends on how the components are identified. Existing methods are not designed to directly preserve the local connectivity of neighbors. In this paper, we propose to incorporate network embedding to enhance the performance of component-based diffusion models in social networks. In particular, we embed nodes in a social network in a latent vector space with local connectivity of the nodes preserved. Parent component identification then becomes a clustering task in the embedding space. A united probabilistic framework is proposed so that the parent components and the component-based diffusion models can be inferred simultaneously using a two-level EM algorithm based on observed information cascades. For performance evaluation, we apply the proposed model to both synthetic and real world data sets with promising results obtained. The empirical results also show how the use of the embedding-based framework can enhance both the component identification and the diffusion model.",https://ieeexplore.ieee.org/document/9027935/,IEEE Access,2020,ieeexplore
10.1109/TPAMI.2006.97,Joint multiregion segmentation and parametric estimation of image motion by basis function representation and level set evolution,IEEE,Journals,"The purpose of this study is to investigate a variational method for joint segmentation and parametric estimation of image motion by basis function representation of motion and level set evolution. The functional contains three terms. One term is of classic regularization to bias the solution toward a segmentation with smooth boundaries. A second term biases the solution toward a segmentation with boundaries which coincide with motion discontinuities, following a description of motion discontinuities by a function of the image spatio-temporal variations. The third term refers to region information and measures conformity of the parametric representation of the motion of each region of segmentation to the image spatio-temporal variations. The components of motion in each region of segmentation are represented as functions in a space generated by a set of basis functions. The coefficients of the motion components considered combinations of the basis functions are the parameters of representation. The necessary conditions for a minimum of the functional, which are derived taking into consideration the dependence of the motion parameters on segmentation, lead to an algorithm which condenses to concurrent curve evolution, implemented via level sets, and estimation of the parameters by least squares within each region of segmentation. The algorithm and its implementation are verified on synthetic and real images using a basis of cosine transforms.",https://ieeexplore.ieee.org/document/1608040/,IEEE Transactions on Pattern Analysis and Machine Intelligence,May 2006,ieeexplore
10.1109/ACCESS.2021.3139137,Knowledge-Driven Multi-Objective Evolutionary Scheduling Algorithm for Cloud Workflows,IEEE,Journals,"Cloud workflow scheduling often encounters two conflicting optimization objectives of makespan and monetary cost, and is a representative multi-objective optimization problem (MOP). Its challenges mainly come from three aspects: 1) a large number of tasks in a workflow cause large-scale decision variables; 2) the two optimization objectives are of quite different scales; 3) and cloud resources are heterogeneous and elastic. So far, many studies focus on adopting multi-objective evolutionary algorithms (MOEAs) to solve the cloud workflow scheduling problem without mining the domain knowledge. To make a good trade-off between the makespan and monetary cost, this paper puts forward a knowledge-driven multi-objective evolutionary workflow scheduling algorithm, abbreviated as KMEWSA, including two novel features. On the one hand, the structural knowledge of workflow is mined to simplify the large-scale decision variables into a series of small-scale components, such accelerating the convergence speed of MOEAs. On the other hand, the knowledge on the Pareto front range is mined to estimate the ideal and nadir points for the objective space normalization during the search process, which helps maintain population diversity for MOEAs. At last, based on twenty real-world workflows and parameters of Amazon EC2, extensive experiments are performed to compare the KMEWSA with three baseline algorithms. The results demonstrate the effectiveness of the KMEWSA in balancing makespan and monetary cost for deploying workflows into cloud computing.",https://ieeexplore.ieee.org/document/9664570/,IEEE Access,2022,ieeexplore
10.1109/ACCESS.2018.2864240,LAIM: A Linear Time Iterative Approach for Efficient Influence Maximization in Large-Scale Networks,IEEE,Journals,"The problem of influence maximization (IM) has been extensively studied in recent years and has many practical applications such as social advertising and viral marketing. Given the network and diffusion model, IM aims to find an influential set of seed nodes so that targeting them as diffusion sources will trigger the maximum cascade of influenced individuals. The largest challenge of the IM problem is its NP-hardness, and most of the existing approaches are with polynomial time complexity, making themselves unscalable to very large networks. To address this issue, in this paper, we propose LAIM: a linear time iterative approach for efficient IM on large-scale networks. Our framework has two steps: 1) influence approximation and 2) seed set selection. In the first step, we propose an iterative algorithm to compute the local influence of a node based on a recursive formula and use the local influence to approximate its global influence. In the second step, the k influential seed nodes are mined based on the approximated influence in the first step. Based on our model, we theoretically prove that the proposed approach has linear time and space complexity. We further accelerate our algorithm with simple modifications and propose its fast version. Experimental results on eight real-world large-scale networks exhibit the superiority of our approach over the state-of-the-art methods in terms of both effectiveness and efficiency.",https://ieeexplore.ieee.org/document/8428631/,IEEE Access,2018,ieeexplore
10.1109/ACCESS.2020.2999727,LPD-AE: Latent Space Representation of Large-Scale 3D Point Cloud,IEEE,Journals,"The effective latent space representation of point cloud provides a foremost and fundamental manner that can be used for challenging tasks, including point cloud based place recognition and reconstruction, especially in large-scale dynamic environments. In this paper, we present a novel deep neural network, LPD-AE(Large-scale Place Description AutoEncoder Network), to obtain meaningful local and contextual features for the generation of latent space from 3D point cloud directly. The encoder network constructs the discriminative global descriptors to realize high accuracy and robust place recognition, which contributed by extracting the local neighbor geometric features and aggregating neighborhood relationships both in feature space and physical space. The decoder network performs hierarchical reconstruction on coarse key points and ultimately produce dense point clouds, which shows that it is capable of reconstructing a full point cloud frame from a single compact but high dimensional descriptor. Our proposed network demonstrates performance that is comparable to the state-of-the-art approaches. With the benefit of the LPD-AE, many computationally complex tasks that rely directly on point clouds can be effortlessly conducted on latent space with lower memory costs, such as relocalization, loop closure detection, and map compression reconstruction. Comprehensive validations on Oxford RobotCar dataset, KITTI dataset, and our freshly collected dataset, which contains multiple trials of repeated routes in different weather and at different times, manifest its potency for real robotic and self-driving implementation. The source code is available at https://github.com/Suoivy/LPD-AE.",https://ieeexplore.ieee.org/document/9107146/,IEEE Access,2020,ieeexplore
10.1109/TKDE.2019.2947040,Label Enhancement for Label Distribution Learning,IEEE,Journals,"Label distribution is more general than both single-label annotation and multi-label annotation. It covers a certain number of labels, representing the degree to which each label describes the instance. The learning process on the instances labeled by label distributions is called label distribution learning (LDL). Unfortunately, many training sets only contain simple logical labels rather than label distributions due to the difficulty of obtaining the label distributions directly. To solve this problem, one way is to recover the label distributions from the logical labels in the training set via leveraging the topological information of the feature space and the correlation among the labels. Such process of recovering label distributions from logical labels is defined as label enhancement (LE), which reinforces the supervision information in the training sets. This paper proposes a novel LE algorithm called Graph Laplacian Label Enhancement (GLLE). Experimental results on one artificial dataset and fourteen real-world LDL datasets show clear advantages of GLLE over several existing LE algorithms. Furthermore, experimental results on eleven multi-label learning datasets validate the advantage of GLLE over the state-of-the-art multi-label learning approaches.",https://ieeexplore.ieee.org/document/8868206/,IEEE Transactions on Knowledge and Data Engineering,1 April 2021,ieeexplore
10.1109/JPROC.2010.2050290,LabelMe: Online Image Annotation and Applications,IEEE,Journals,"Central to the development of computer vision systems is the collection and use of annotated images spanning our visual world. Annotations may include information about the identity, spatial extent, and viewpoint of the objects present in a depicted scene. Such a database is useful for the training and evaluation of computer vision systems. Motivated by the availability of images on the Internet, we introduced a web-based annotation tool that allows online users to label objects and their spatial extent in images. To date, we have collected over 400 000 annotations that span a variety of different scene and object classes. In this paper, we show the contents of the database, its growth over time, and statistics of its usage. In addition, we explore and survey applications of the database in the areas of computer vision and computer graphics. Particularly, we show how to extract the real-world 3-D coordinates of images in a variety of scenes using only the user-provided object annotations. The output 3-D information is comparable to the quality produced by a laser range scanner. We also characterize the space of the images in the database by analyzing 1) statistics of the co-occurrence of large objects in the images and 2) the spatial layout of the labeled images.",https://ieeexplore.ieee.org/document/5483185/,Proceedings of the IEEE,Aug. 2010,ieeexplore
10.1109/TNN.2005.849821,Large margin nearest neighbor classifiers,IEEE,Journals,"The nearest neighbor technique is a simple and appealing approach to addressing classification problems. It relies on the assumption of locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with a finite number of examples due to the curse of dimensionality. Severe bias can be introduced under these conditions when using the nearest neighbor rule. The employment of a locally adaptive metric becomes crucial in order to keep class conditional probabilities close to uniform, thereby minimizing the bias of estimates. We propose a technique that computes a locally flexible metric by means of support vector machines (SVMs). The decision function constructed by SVMs is used to determine the most discriminant direction in a neighborhood around the query. Such a direction provides a local feature weighting scheme. We formally show that our method increases the margin in the weighted space where classification takes place. Moreover, our method has the important advantage of online computational efficiency over competing locally adaptive techniques for nearest neighbor classification. We demonstrate the efficacy of our method using both real and simulated data.",https://ieeexplore.ieee.org/document/1461432/,IEEE Transactions on Neural Networks,July 2005,ieeexplore
10.1109/TPAMI.2008.157,Latent-Space Variational Bayes,IEEE,Journals,"Variational Bayesian expectation-maximization (VBEM), an approximate inference method for probabilistic models based on factorizing over latent variables and model parameters, has been a standard technique for practical Bayesian inference. In this paper, we introduce a more general approximate inference framework for conjugate-exponential family models, which we call latent-space variational Bayes (LSVB). In this approach, we integrate out model parameters in an exact way, leaving only the latent variables. It can be shown that the LSVB approach gives better estimates of the model evidence as well as the distribution over latent variables than the VBEM approach, but in practice, the distribution over latent variables has to be approximated. As a practical implementation, we present a first-order LSVB (FoLSVB) algorithm to approximate this distribution over latent variables. From this approximate distribution, one can estimate the model evidence and the posterior over model parameters. The FoLSVB algorithm is directly comparable to the VBEM algorithm and has the same computational complexity. We discuss how LSVB generalizes the recently proposed collapsed variational methods [20] to general conjugate-exponential families. Examples based on mixtures of Gaussians and mixtures of Bernoullis with synthetic and real-world data sets are used to illustrate some advantages of our method over VBEM.",https://ieeexplore.ieee.org/document/4670325/,IEEE Transactions on Pattern Analysis and Machine Intelligence,Dec. 2008,ieeexplore
10.1109/OJCS.2021.3085846,"<sc>Lattice</sc>: A Vision for Machine Learning, Data Engineering, and Policy Considerations for Digital Agriculture at Scale",IEEE,Journals,"Digital agriculture, with the incorporation of Internet-of-Things (IoT)-based technologies, presents the ability to control a system at multiple levels (individual, local, regional, and global) and generates tools that allow for improved decision making and higher productivity. Recent advances in IoT hardware, e.g., networks of heterogeneous embedded devices, and software, e.g., lightweight computer vision algorithms and cloud optimization solutions, make it possible to efficiently process data from diverse sources in a connected (smart) farm. By interconnecting these IoT devices, often across large geographical distances, it is possible to collect data at different time scales, including in near real-time (i.e., with delays of only a few tens of seconds). This data can then be used for actionable insights, e.g., precise applications of soil supplements and reduced environmental footprint. Through LATTICE, we present an integrated vision for IoT solutions, data processing, and actionable analytics for digital agriculture. We couple this with discussion of economics and policy considerations that will underlie adoption of such IoT and ML technologies. Our paper starts off with the types of datasets in typical field operations, followed by the lifecycle for the data and storage, cloud and edge analytics, and fast information-retrieval solutions. We discuss what algorithms are proving to be most impactful in this space, e.g., approximate data analytics and on-device/in-network processing. We conclude by discussing analytics for alternative agriculture for generation of biofuels and policy challenges in the implementation of digital agriculture in the wild.",https://ieeexplore.ieee.org/document/9444818/,IEEE Open Journal of the Computer Society,2021,ieeexplore
10.1109/LGRS.2020.3023086,Learn to Recognize Unknown SAR Targets From Reflection Similarity,IEEE,Journals,"In real-world recognition tasks, it is difficult to collect training samples for all types of synthetic aperture radar (SAR) targets. The existing zero-shot learning (ZSL) methods use visual features and label semantics simultaneously to ensure a high recognition rate of unknown targets. However, meaningless information could influence the stability of the reference space. The negative impact will further drop the recognition rate of the ZSL methods. To overcome this problem, a new ZSL method is proposed by only adopting reflection similarity to recognize unknown targets. The proposed method adopts a triple-part network, in which the constructor-net and the supervisor-net jointly construct the reference space. It makes the space have a good representation and distinction. The third part of the triple-part network, as an interpreter-net, provides the uniformity of the target interpretation. The experimental results on moving and stationary target acquisition and recognition (MSTAR) data set show that the reference space with good quality can be obtained by discarding meaningless information. Especially, when the samples of known targets have slightly changed, the reference space still keeps good stability. Compared with the traditional ZSL methods, the recognition reliability of the proposed method on zero-shot targets is improved.",https://ieeexplore.ieee.org/document/9204735/,IEEE Geoscience and Remote Sensing Letters,2022,ieeexplore
10.1109/JBHI.2019.2946676,Learning Brain Effective Connectivity Network Structure Using Ant Colony Optimization Combining With Voxel Activation Information,IEEE,Journals,"Learning brain effective connectivity (EC) networks from functional magnetic resonance imaging (fMRI) data has become a new hot topic in the neuroinformatics field. However, how to accurately and efficiently learn brain EC networks is still a challenging problem. In this paper, we propose a new algorithm to learn the brain EC network structure using ant colony optimization (ACO) algorithm combining with voxel activation information, named as VACOEC. First, VACOEC uses the voxel activation information to measure the independence between each pair of brain regions and effectively restricts the space of candidate solutions, which makes many unnecessary searches of ants be avoided. Then, by combining the global score increase of a solution with the voxel activation information, a new heuristic function is designed to guide the process of ACO to search for the optimal solution. The experimental results on simulated datasets show that the proposed method can accurately and efficiently identify the directions of the brain EC networks. Moreover, the experimental results on real-world data show that patients with Alzheimers disease (AD) exhibit decreased effective connectivity not only in the intra-network within the default mode network (DMN) and salience network (SN), but also in the inter-network between DMN and SN, compared with normal control (NC) subjects. The experimental results demonstrate that VACOEC is promising for practical applications in the neuroimaging studies of geriatric subjects and neurological patients.",https://ieeexplore.ieee.org/document/8863995/,IEEE Journal of Biomedical and Health Informatics,July 2020,ieeexplore
10.1109/TIP.2021.3093387,Learning Efficient Hash Codes for Fast Graph-Based Data Similarity Retrieval,IEEE,Journals,"Traditional operations, e.g. graph edit distance (GED), are no longer suitable for processing the massive quantities of graph-structured data now available, due to their irregular structures and high computational complexities. With the advent of graph neural networks (GNNs), the problems of graph representation and graph similarity search have drawn particular attention in the field of computer vision. However, GNNs have been less studied for efficient and fast retrieval after graph representation. To represent graph-based data, and maintain fast retrieval while doing so, we introduce an efficient hash model with graph neural networks (HGNN) for a newly designed task (i.e. fast graph-based data retrieval). Due to its flexibility, HGNN can be implemented in both an unsupervised and supervised manner. Specifically, by adopting a graph neural network and hash learning algorithms, HGNN can effectively learn a similarity-preserving graph representation and compute pair-wise similarity or provide classification via low-dimensional compact hash codes. To the best of our knowledge, our model is the first to address graph hashing representation in the Hamming space. Our experimental results reach comparable prediction accuracy to full-precision methods and can even outperform traditional models in some cases. In real-world applications, using hash codes can greatly benefit systems with smaller memory capacities and accelerate the retrieval speed of graph-structured data. Hence, we believe the proposed HGNN has great potential in further research.",https://ieeexplore.ieee.org/document/9474952/,IEEE Transactions on Image Processing,2021,ieeexplore
10.1109/LRA.2020.3010739,Learning Force Control for Contact-Rich Manipulation Tasks With Rigid Position-Controlled Robots,IEEE,Journals,"Reinforcement Learning (RL) methods have been proven successful in solving manipulation tasks autonomously. However, RL is still not widely adopted on real robotic systems because working with real hardware entails additional challenges, especially when using rigid position-controlled manipulators. These challenges include the need for a robust controller to avoid undesired behavior, that risk damaging the robot and its environment, and constant supervision from a human operator. The main contributions of this work are, first, we proposed a learning-based force control framework combining RL techniques with traditional force control. Within said control scheme, we implemented two different conventional approaches to achieve force control with position-controlled robots; one is a modified parallel position/force control, and the other is an admittance control. Secondly, we empirically study both control schemes when used as the action space of the RL agent. Thirdly, we developed a fail-safe mechanism for safely training an RL agent on manipulation tasks using a real rigid robot manipulator. The proposed methods are validated both on simulation and a real robot with an UR3 e-series robotic arm.",https://ieeexplore.ieee.org/document/9145608/,IEEE Robotics and Automation Letters,Oct. 2020,ieeexplore
10.1109/TKDE.2019.2961882,Learning Graph Representation With Generative Adversarial Nets,IEEE,Journals,"Graph representation learning aims to embed each vertex in a graph into a low-dimensional vector space. Existing graph representation learning methods can be classified into two categories: generative models that learn the underlying connectivity distribution in a graph, and discriminative models that predict the probability of edge between a pair of vertices. In this paper, we propose GraphGAN, an innovative graph representation learning framework unifying the above two classes of methods, in which the generative and the discriminative model play a game-theoretical minimax game. Specifically, for a given vertex, the generative model tries to fit its underlying true connectivity distribution over all other vertices and produces fake samples to fool the discriminative model, while the discriminative model tries to detect whether the sampled vertex is from ground truth or generated by the generative model. With the competition between these two models, both of them can alternately and iteratively boost their performance. Moreover, we propose a novel graph softmax as the implementation of the generative model to overcome the limitations of traditional softmax function, which can be proven satisfying desirable properties of normalization, graph structure awareness, and computational efficiency. Through extensive experiments on real-world datasets, we demonstrate that GraphGAN achieves substantial gains in a variety of applications, including graph reconstruction, link prediction, node classification, recommendation, and visualization, over state-of-the-art baselines.",https://ieeexplore.ieee.org/document/8941296/,IEEE Transactions on Knowledge and Data Engineering,1 Aug. 2021,ieeexplore
10.1109/TNNLS.2013.2256797,Learning in the Model Space for Cognitive Fault Diagnosis,IEEE,Journals,"The emergence of large sensor networks has facilitated the collection of large amounts of real-time data to monitor and control complex engineering systems. However, in many cases the collected data may be incomplete or inconsistent, while the underlying environment may be time-varying or unformulated. In this paper, we develop an innovative cognitive fault diagnosis framework that tackles the above challenges. This framework investigates fault diagnosis in the model space instead of the signal space. Learning in the model space is implemented by fitting a series of models using a series of signal segments selected with a sliding window. By investigating the learning techniques in the fitted model space, faulty models can be discriminated from healthy models using a one-class learning algorithm. The framework enables us to construct a fault library when unknown faults occur, which can be regarded as cognitive fault isolation. This paper also theoretically investigates how to measure the pairwise distance between two models in the model space and incorporates the model distance into the learning algorithm in the model space. The results on three benchmark applications and one simulated model for the Barcelona water distribution network confirm the effectiveness of the proposed framework.",https://ieeexplore.ieee.org/document/6515601/,IEEE Transactions on Neural Networks and Learning Systems,Jan. 2014,ieeexplore
10.1364/JOCN.11.000226,Learning life cycle to speed up autonomic optical transmission and networking adoption,IEEE,Journals,"Autonomic optical transmission and networking requires machine learning (ML) models to be trained with large datasets. However, the availability of enough real data to produce accurate ML models is rarely ensured since new optical equipment and techniques are continuously being deployed in the network. One option is to generate data from simulations and lab experiments, but such data could not cover the whole features space and would translate into inaccuracies in the ML models. In this paper, we propose an ML-based algorithm life cycle to facilitate ML deployment in real operator networks. The dataset for ML training can be initially populated based on the results from simulations and lab experiments. Once ML models are generated, ML retraining can be performed after inaccuracies are detected to improve their precision. Illustrative numerical results show the benefits of the proposed learning cycle for general use cases. In addition, two specific use cases are proposed and demonstrated that implement different learning strategies: (i) a two-phase strategy performing out-of-field training using data from simulations and lab experiments with generic equipment, followed by an in-field adaptation to support heterogeneous equipment (the accuracy of this strategy is shown for a use case of failure detection and identification), and (ii) in-field retraining, where ML models are retrained after detecting model inaccuracies. Different approaches are analyzed and evaluated for a use case of autonomic transmission, where results show the significant benefits of collective learning.",https://ieeexplore.ieee.org/document/8717575/,Journal of Optical Communications and Networking,May 2019,ieeexplore
10.1109/TSC.2016.2633264,Learning the Evolution Regularities for BigService-Oriented Online Reliability Prediction,IEEE,Journals,"Service computing is an emerging technology in System of Systems Engineering (SoS Engineering or SoSE), which regards a System as a Service, and aims at constructing a robust and value-added complex system by outsourcing external component systems through service composition. The burgeoning Big Service computing just covers the significant challenges in constructing and maintaining a stable service-oriented SoS. A service-oriented SoS runs under a volatile and uncertain environment. As a step toward big service, service fault tolerance (FT) can guarantee the run-time quality of a service-oriented SoS. To successfully deploy FT in an SoS, online reliability time series prediction, which aims at predicting the reliability in near future for a service-oriented SoS arises as a grand challenge in SoS research. In particular, we need to tackle a number of big data related issues given the large and fast increasing size of the historical data that will be used for prediction purpose. The decision-making of prediction solution space be more complex. To provide highly accurate prediction results, we tackle the prediction challenges by identifying the evolution regularities of component systems' running states via different machine learning models. We present in this paper the motifs-based Dynamic Bayesian Networks (or m_DBNs) to perform one-step-ahead online reliability time series prediction. We also propose a multi-steps trajectory DBNs (or multi_DBNs) to further improve the accuracy of future reliability prediction. Finally, a Convolutional Neural Networks (CNN)-based prediction approach is developed to deal with the big data challenges. Extensive experiments conducted on real-world Web services demonstrate that our models outperform other well-known approaches consistently.",https://ieeexplore.ieee.org/document/7762161/,IEEE Transactions on Services Computing,1 May-June 2019,ieeexplore
10.1109/3477.499794,Learning to select useful landmarks,IEEE,Journals,"To navigate effectively, an autonomous agent must be able to quickly and accurately determine its current location. Given an initial estimate of its position (perhaps based on dead-reckoning) and an image taken of a known environment, our agent first attempts to locate a set of landmarks (real-world objects at known locations), then uses their angular separation to obtain an improved estimate of its current position. Unfortunately, some landmarks may not be visible, or worse, may be confused with other landmarks, resulting in both time wasted in searching for the undetected landmarks, and in further errors in the agent's estimate of its position. To address these problems, we propose a method that uses previous experiences to learn a selection function that, given the set of landmarks that might be visible, returns the subset that can be used to reliably provide an accurate registration of the agent's position. We use statistical techniques to prove that the learned selection function is, with high probability, effectively at a local optimum in the space of such functions. This paper also presents empirical evidence, using real-world data, that demonstrate the effectiveness of our approach.",https://ieeexplore.ieee.org/document/499794/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",June 1996,ieeexplore
10.1109/ACCESS.2019.2956556,Light-Weight Spliced Convolution Network-Based Automatic Water Meter Reading in Smart City,IEEE,Journals,"Automatic reading for water meter is one of the practical demands in smart city applications. Due to the high cost, it is not feasible to replace the old mechanical water meter with a new embedded electronic device. Recently, image recognition based meter reading methods have become research hotspots. However, illumination, occlusion, energy and computational consuming in IoT environment bring challenges to these methods. In this paper, we design and implement a smart water meter reading system to handle this issue. Specifically, we first propose a novel light-weight spliced convolution network to recognize the meter number, which simplifies standard 3 x 3 convolutions by splicing a certain number of 1 x 1 and 3 x 3 size kernel. We then prove the superiority of our network by theoretical analysis. Second, we have implemented the prototype which can handle huge real-time data base on the distributed cloud platform. Base on this system, our system can provide industrial service. Finally, we conduct real-world dataset to verify the performance of the system. The experimental results demonstrate that our proposed light-weight spliced convolution network can reduce nearly 10x computational consuming, 7x model space, and save 3x running time comparing with standard convolution network.",https://ieeexplore.ieee.org/document/8917620/,IEEE Access,2019,ieeexplore
10.1109/TC.2021.3057082,<sc>Lime</sc>: Low-Cost and Incremental Learning for Dynamic Heterogeneous Information Networks,IEEE,Journals,"Understanding the interconnected relationships of large-scale information networks like social, scholar and Internet of Things networks is vital for tasks like recommendation and fraud detection. The vast majority of the real-world networks are inherently heterogeneous and dynamic, containing many different types of nodes and edges and can change drastically over time. The dynamicity and heterogeneity make it extremely challenging to reason about the network structure. Unfortunately, existing approaches are inadequate in modeling real-life dynamical networks as they either have strong assumption of a given stochastic process or fail to capture the heterogeneity of network structure, and they all require extensive computational resources. We introduce <small>Lime</small>, a better approach for modeling dynamic and heterogeneous information networks. <small>Lime</small> is designed to extract high-quality network representation with significantly lower memory resources and computational time over the state-of-the-arts. Unlike prior work that uses a vector to encode each network node, we exploit the semantic relationships among network nodes to encode multiple nodes with similar semantics in shared vectors. By using many fewer node vectors, our approach significantly reduces the required memory space for encoding large-scale networks. To effectively trade information sharing for reduced memory footprint, we employ the recursive neural network (RsNN) with carefully designed optimization strategies to explore the node semantics in a novel cuboid space. We then go further by showing, for the first time, how an effective incremental learning approach can be developed  with the help of RsNN, our cuboid structure, and a set of novel optimization techniques  to allow a learning framework to quickly and efficiently adapt to a constantly evolving network. We evaluate <small>Lime</small> by applying it to three representative network-based tasks, node classification, node clustering and anomaly detection, performing on three large-scale datasets. We compare <small>Lime</small> against eleven prior state-of-the-art approaches for learning network representation. Our extensive experiments demonstrate that <small>Lime</small> not only reduces the memory footprint by over 80 percent and the processing time over 2x when learning network representation but also delivers comparable performance for downstream processing tasks. We show that our incremental learning method can boost the learning time by up to 20x without compromising the quality of the learned network representation.",https://ieeexplore.ieee.org/document/9353275/,IEEE Transactions on Computers,1 March 2022,ieeexplore
10.1109/TIT.2009.2027479,"Local Minimax Learning of Functions With Best Finite Sample Estimation Error Bounds: Applications to Ridge and Lasso Regression, Boosting, Tree Learning, Kernel Machines, and Inverse Problems",IEEE,Journals,"Optimal local estimation is formulated in the minimax sense for inverse problems and nonlinear regression. This theory provides best mean squared finite sample error bounds for some popular statistical learning algorithms and also for several optimal improvements of other existing learning algorithms such as smoothing splines and kernel regularization. The bounds and improved algorithms are not based on asymptotics or Bayesian assumptions and are truly local for each query, not depending on cross validating estimates at other queries to optimize modeling parameters. Results are given for optimal local learning of approximately linear functions with side information (context) using real algebraic geometry. In particular, finite sample error bounds are given for ridge regression and for a local version of lasso regression. The new regression methods require only quadratic programming with linear or quadratic inequality constraints for implementation. Greedy additive expansions are then combined with local minimax learning via a change in metric. An optimal strategy is presented for fusing the local minimax estimators of a class of experts-providing optimal finite sample prediction error bounds from (random) forests. Local minimax learning is extended to kernel machines. Best local prediction error bounds for finite samples are given for Tikhonov regularization. The geometry of reproducing kernel Hilbert space is used to derive improved estimators with finite sample mean squared error (MSE) bounds for class membership probability in two class pattern classification problems. A purely local, cross validation free algorithm is proposed which uses Fisher information with these bounds to determine best local kernel shape in vector machine learning. Finally, a locally quadratic solution to the finite Fourier moments problem is presented. After reading the first three sections the reader may proceed directly to any of the subsequent applications sections.",https://ieeexplore.ieee.org/document/5319754/,IEEE Transactions on Information Theory,Dec. 2009,ieeexplore
10.1109/ACCESS.2017.2655150,Location-Aware Personalized News Recommendation With Deep Semantic Analysis,IEEE,Journals,"With the popularity of mobile devices and the quick growth of the mobile Web, users can now browse news wherever they want; so, their news preferences are usually related to their geographical contexts. Consequently, many research efforts have been put on location-aware news recommendation, which recommends to users news happening nearest to them. Nevertheless, in a real-world context, users' news preferences are not only related to their locations, but also strongly related to their personal interests. Therefore, in this paper, we propose a hybrid method called location-aware personalized news recommendation with explicit semantic analysis (LP-ESA), which recommends news using both the users' personal interests and their geographical contexts. However, the Wikipedia-based topic space in LP-ESA suffers from the problems of high dimensionality, sparsity, and redundancy, which greatly degrade the performance of LP-ESA. To address these problems, we further propose a novel method called LP-DSA to exploit recommendation-oriented deep neural networks to extract dense, abstract, low dimensional, and effective feature representations for users, news, and locations. Experimental results show that LP-ESA and LP-DSA both significantly outperform the state-of-the-art baselines. In addition, LP-DSA offers more effective (19.8%-179.6% better) online news recommendation with much lower time cost (25 times quicker) than LP-ESA.",https://ieeexplore.ieee.org/document/7823033/,IEEE Access,2017,ieeexplore
10.1109/JSEN.2013.2263381,Log-Logistic Modeling of Sensory Flow Delays in Networked Telerobots,IEEE,Journals,"This paper deals with the modeling of the delays in the transmission of sensory data coming from a networked telerobot, which would allow us to predict future times of arrival and provide guarantees on the time requirements of these systems. Considering these delay sequences as an uni-dimensional temporal signal, they easily exhibit rich stochastic behaviorabrupt changes of regime and burstsdue to the heterogeneity of the hardware and software components in the data path. There exist approaches for modeling this kind of signals without explicit knowledge of the system components: state-space reconstruction, hidden Markov models, neural networks, etc., but they are mostly focused on the stochasticity of the network only, without taking into account other elements in the sensory flow that also have an important influence in the delays. Previously, we have proposed simpler statistical methods that do not require any component knowledge either and are suitable for more lightweight implementations (e.g., in mobile phone interfaces). In this sense, we report elsewhere a log-normal three-parametrical model that fits reasonably well these delays as long as change detection is completely solved. Now we propose a more flexible solution: the log-logistic distribution, which has been found to fit delays better than the log-normal. In addition, we present two algorithms to model an entire delay signal, including abrupt nonlinearities, based on the log-logistic assumption. Our results show quite good fittings of real datasets gathered from a number of combinations of sensors, networks, and application software, provided that some mild assumptions hold.",https://ieeexplore.ieee.org/document/6516944/,IEEE Sensors Journal,Aug. 2013,ieeexplore
10.1109/TNN.2010.2093537,Logistic Regression by Means of Evolutionary Radial Basis Function Neural Networks,IEEE,Journals,"This paper proposes a hybrid multilogistic methodology, named logistic regression using initial and radial basis function (RBF) covariates. The process for obtaining the coefficients is carried out in three steps. First, an evolutionary programming (EP) algorithm is applied, in order to produce an RBF neural network (RBFNN) with a reduced number of RBF transformations and the simplest structure possible. Then, the initial attribute space (or, as commonly known as in logistic regression literature, the covariate space) is transformed by adding the nonlinear transformations of the input variables given by the RBFs of the best individual in the final generation. Finally, a maximum likelihood optimization method determines the coefficients associated with a multilogistic regression model built in this augmented covariate space. In this final step, two different multilogistic regression algorithms are applied: one considers all initial and RBF covariates (multilogistic initial-RBF regression) and the other one incrementally constructs the model and applies cross validation, resulting in an automatic covariate selection [simplelogistic initial-RBF regression (SLIRBF)]. Both methods include a regularization parameter, which has been also optimized. The methodology proposed is tested using 18 benchmark classification problems from well-known machine learning problems and two real agronomical problems. The results are compared with the corresponding multilogistic regression methods applied to the initial covariate space, to the RBFNNs obtained by the EP algorithm, and to other probabilistic classifiers, including different RBFNN design methods [e.g., relaxed variable kernel density estimation, support vector machines, a sparse classifier (sparse multinomial logistic regression)] and a procedure similar to SLIRBF but using product unit basis functions. The SLIRBF models are found to be competitive when compared with the corresponding multilogistic regression methods and the RBFEP method. A measure of statistical significance is used, which indicates that SLIRBF reaches the state of the art.",https://ieeexplore.ieee.org/document/5659484/,IEEE Transactions on Neural Networks,Feb. 2011,ieeexplore
10.1109/JBHI.2019.2961264,Low-Memory CNNs Enabling Real-Time Ultrasound Segmentation Towards Mobile Deployment,IEEE,Journals,"Convolutional Neural Networks (CNNs), which are currently state-of-the-art for most image analysis tasks, are ill suited to leveraging the key benefits of ultrasound imaging - specifically, ultrasound's portability and real-time capabilities. CNNs have large memory footprints, which obstructs their implementation on mobile devices, and require numerous floating point operations, which results in slow CPU inference times. In this article, we propose three approaches to training efficient CNNs that can operate in real-time on a CPU (catering to a clinical setting), with a low memory footprint, for minimal compromise in accuracy. We first demonstrate the power of `thin' CNNs (with very few feature channels) for fast medical image segmentation. We then leverage separable convolutions to further speed up inference, reduce parameter count and facilitate mobile deployment. Lastly, we propose a novel knowledge distillation technique to boost the accuracy of light-weight models, while maintaining inference speed-up. For a negligible sacrifice in test set Dice performance on the challenging ultrasound analysis task of nerve segmentation, our final proposed model processes images at 30 fps on a CPU, which is 9 faster than the standard U-Net, while requiring 420 less space in memory. Code for this work is available at: https://github.com/sagarvaze96/lightweight_unet.",https://ieeexplore.ieee.org/document/8999615/,IEEE Journal of Biomedical and Health Informatics,April 2020,ieeexplore
10.1109/LSENS.2019.2950335,Low-Power Sensor Localization in Three-Dimensional Using a Recurrent Neural Network,IEEE,Journals,"This letter discusses a novel low-power circuit to self-localize a mobile sensor node in the three-dimensional space using a passive optical receiver. Self-localization of sensors, where a sensor node computes its spatial location by itself, reduces transmission demand, and improves real-time conformity of mobile wireless sensor systems. Our approach forms an over-determined system of angles-of-arrival (AoAs) to mobile sensor received from an optical anchor grid. The AoA system is solved with a linear program (LP) solver, which is implemented using a nonlinear feedback recurrent neural network. To solve the primal and dual LP optimization problems in the AoA system, we show a single multifunctional data path that does not require matrix inversions; thereby, enables within-sensor low-power computations to self-localize. Additionally, unlike other optical indoor positioning architectures, our approach does not require measurements of received signal strength and, thereby, is insensitive to power and alignment imbalances in the anchor grid. We show proof-of-concept field programmable gate array (FPGA) and ASIC simulations of our approach and validate its operation under noisy AoA data and for different numbers of anchors. An FPGA implementation in 180 nm CMOS achieves a peak ~0.12 mega localization operations per second per Watt (MOPS/W), while ASIC design in 45 nm CMOS shows a peak ~7.7 MOPS/W.",https://ieeexplore.ieee.org/document/8886474/,IEEE Sensors Letters,Dec. 2019,ieeexplore
10.1109/LGRS.2021.3049251,Low-Rank Nonlocal Representation for Remote Sensing Scene Classification,IEEE,Journals,"The nonlocal mechanism (NM) has shown its effectiveness in many real-world applications. However, it is usually criticized for its costly computation complexity in time and space, which are both <inline-formula> <tex-math notation=""LaTeX"">$\mathscr {O}(H^{2}W^{2})$ </tex-math></inline-formula>, where <inline-formula> <tex-math notation=""LaTeX"">$H\times W$ </tex-math></inline-formula> is the spatial dimension of input feature maps in height and width. In this letter, we first show that the NM can be expressed as a low-rank representation by theory. Then we propose a down-to-earth low-complexity global context acquisition mechanism, termed as the low-rank nonlocal representation (LNR), whose complexity in time and space are both approximatively <inline-formula> <tex-math notation=""LaTeX"">$\mathscr {O}(HW)$ </tex-math></inline-formula>. LNR is a general module that can be deployed on an arbitrary convolutional neural network (CNN) hierarchy for any visual recognition tasks. To demonstrate its superiority, experiments are carried out on four standard remote sensing scene classification benchmarks. Experimental results show that our proposed LNR can significantly reduce the computation cost by boosting the performance gain. Implementing LNR on the classical ResNet, in particular, can reduce at most 3.93 M model parameters.",https://ieeexplore.ieee.org/document/9335047/,IEEE Geoscience and Remote Sensing Letters,2022,ieeexplore
10.1109/JETCAS.2021.3097699,Machine-Learning-Based Microwave Sensing: A Case Study for the Food Industry,IEEE,Journals,"Despite the meticulous attention of food industries to prevent hazards in packaged goods, some contaminants may still elude the controls. Indeed, standard methods, like X-rays, metal detectors and near-infrared imaging, cannot detect low-density materials. Microwave sensing is an alternative method that, combined with machine learning classifiers, can tackle these deficiencies. In this paper we present a design methodology applied to a case study in the food sector. Specifically, we offer a complete flow from microwave dataset acquisition to deployment of the classifiers on real-time hardware and we show the effectiveness of this method in terms of detection accuracy. In the case study, we apply the machine-learning based microwave sensing approach to the case of food jars flowing at high speed on a conveyor belt. First, we collected a dataset from hazelnut-cocoa spread jars which were uncontaminated or contaminated with various intrusions, including low-density plastics. Then, we performed a design space exploration to choose the best MLPs as binary classifiers, which resulted to be exceptionally accurate. Finally, we selected the two most light-weight models for implementation on both an ARM-based CPU and an FPGA SoC, to cover a wide range of possible latency requirements, from loose to strict, to detect contaminants in real-time. The proposed design flow facilitates the design of the FPGA accelerator that might be required to meet the timing requirements by using a high-level approach, which might be suited for the microwave domain experts without specific digital hardware skills.",https://ieeexplore.ieee.org/document/9489295/,IEEE Journal on Emerging and Selected Topics in Circuits and Systems,Sept. 2021,ieeexplore
10.1109/LRA.2020.2972819,Marker-Less Micro Aerial Vehicle Detection and Localization Using Convolutional Neural Networks,IEEE,Journals,"A relative localization system for micro aerial vehicles (MAVs), which is able to work without any markers or other specialized equipment, is presented in this letter. The system utilizes images from an onboard camera to detect nearby MAVs using a convolutional neural network. When compared to traditional computer vision-based relative localization systems, this approach removes the need for specialized markers to be placed on the MAVs, saving weight and space, while also enabling localization of noncooperating robots. The system is designed and implemented to run online, onboard an MAV platform in order to enable relative stabilization of several MAVs in a formation or swarm-like behavior, when operating in a closed feedback loop with the control system of the MAVs. We demonstrate the viability and robustness of the proposed method in real-world experiments. The method was also designed for the purpose of autonomous aerial interception and is a fitting complement to other MAV detection and relative localization methods for this purpose, as is shown in the experiments.",https://ieeexplore.ieee.org/document/8988144/,IEEE Robotics and Automation Letters,April 2020,ieeexplore
10.1109/TWC.2019.2909874,Message Passing Based Distributed Learning for Joint Resource Allocation in Millimeter Wave Heterogeneous Networks,IEEE,Journals,"Millimeter wave (mmWave) provides an enormous spectrum for future broadband cellular communications. The corresponding challenging characteristics of radio propagation, and the use of highly directional transmission and the dense deployment, lead to more complex and critical resource allocation problems than in traditional cellular systems, where the channels are better tamed. In this paper, we study the joint power control and user association problem in heterogeneous networks (HetNets) by considering the dynamics of links as a Markov Decision Process (MDP). A reinforcement learning framework is proposed to study the problem. The large state/action space is handled by decomposing the large scale problem into multiple local problems, based on the topology of mmWave HetNet, which is motivated by the celebrated belief propagation (BP) algorithm in probabilistic graphical models. The decomposed problem is solved with a distributed message passing method and accelerated by the prior knowledge of the mmWave dynamics. Two categories of learning frameworks are proposed for time sensitive and power sensitive conditions. The real-world measurements in the 60GHz band are collected and used in the simulation of the proposed framework.",https://ieeexplore.ieee.org/document/8692747/,IEEE Transactions on Wireless Communications,May 2019,ieeexplore
10.1109/JSTSP.2015.2389196,Mining the Situation: Spatiotemporal Traffic Prediction With Big Data,IEEE,Journals,"With the vast availability of traffic sensors from which traffic information can be derived, a lot of research effort has been devoted to developing traffic prediction techniques, which in turn improve route navigation, traffic regulation, urban area planning, etc. One key challenge in traffic prediction is how much to rely on prediction models that are constructed using historical data in real-time traffic situations, which may differ from that of the historical data and change over time. In this paper, we propose a novel online framework that could learn from the current traffic situation (or context) in real-time and predict the future traffic by matching the current situation to the most effective prediction model trained using historical data. As real-time traffic arrives, the traffic context space is adaptively partitioned in order to efficiently estimate the effectiveness of each base predictor in different situations. We obtain and prove both short-term and long-term performance guarantees (bounds) for our online algorithm. The proposed algorithm also works effectively in scenarios where the true labels (i.e., realized traffic) are missing or become available with delay. Using the proposed framework, the context dimension that is the most relevant to traffic prediction can also be revealed, which can further reduce the implementation complexity as well as inform traffic policy making. Our experiments with real-world data in real-life conditions show that the proposed approach significantly outperforms existing solutions.",https://ieeexplore.ieee.org/document/7001625/,IEEE Journal of Selected Topics in Signal Processing,June 2015,ieeexplore
10.1109/ACCESS.2019.2915535,Motion Based Inference of Social Circles via Self-Attention and Contextualized Embedding,IEEE,Journals,"Extracting knowledge from human mobility data is an important task for many downstream applications such as point-of-interest recommendation, motion trace identification, and personalized trip planning. A specific problem that has recently spurred research interest is the so-called Social Circle Inference from Mobility data (SCIM), aiming at inferring relationships among users based on mobility data and without any explicit structured network information. The existing methods either require partial social ties or fail to model the implicit correlations between user links, thereby suffering from critical inference bias. We present a novel SCIM framework, called SCIM via self-Attention and Contextualized-embedding (SCIMAC) - a methodology capturing multiple aspects of users' check-in behavior and complex motion patterns of different users. Instead of directly applying the recurrent model on training user trajectories, the proposed method introduces a new module for context-aware check-in representation learning by adaptively incorporating the internal states of the recurrent layers, which is more effective than the context-independent check-in embedding used in existing social circle inference approaches. To model the underlying correlations between labels, SCIMAC leverages a more sophisticated label embedding technique to adjust the penalties for correlated users, enabling a better understanding of the user's hierarchy in the label space, and alleviating the inference bias. We empirically demonstrate that our SCIMAC model significantly outperforms several state-of-the-art baselines on real-world datasets.",https://ieeexplore.ieee.org/document/8709700/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2020.2986089,Motion Control of Magnetic Microrobot Using Uniform Magnetic Field,IEEE,Journals,"A microrobot with untethered control in 3D space is a good choice to be applied in the fields of biomedicine with in small and confined workspace. In this paper, an electromagnetic actuation system (EMA) which combined with Helmholtz coil and Maxwell coil for the microrobot 5 DOF locomotion in 3D space is built. The magnetic field analysis of the proposed 3D EMA system was analyzed by finite-element-method (FEM) with multi-physics COMSOL software. The proposed EMA system can produce magnetic field with different characteristics such as a controllable uniform gradient magnetic field, a rotating magnetic field and a oscillating magnetic field in a three-dimensional space by independently changing the current in each coil. In this paper the 3D motion dynamic equation model of microrobot was established. A novel control method for the gravity compensation for the wireless locomotive microrobot was proposed. The proposed method has the property that the direction of magnetic flux and the locomotion path of the microrobot are independent. Meanwhile, it can achieve the horizontal motions or nearly horizontal motions and overcome the gravity well at the same time. It has been verified by experiments in 3D liquid environment. With the proposed method, the microrobot shows good performance in horizontal motions as well as various motions in the 3D space.",https://ieeexplore.ieee.org/document/9057623/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.3024926,MuLViS: Multi-Level Encryption Based Security System for Surveillance Videos,IEEE,Journals,"Video Surveillance (VS) systems are commonly deployed for real-time abnormal event detection and autonomous video analytics. Video captured by surveillance cameras in real-time often contains identifiable personal information, which must be privacy protected, sometimes along with the locations of the surveillance and other sensitive information. Within the Surveillance System, these videos are processed and stored on a variety of devices. The processing and storage heterogeneity of those devices, together with their network requirements, make real-time surveillance systems complex and challenging. This paper proposes a surveillance system, named as Multi-Level Video Security (MuLViS) for privacy-protected cameras. Firstly, a Smart Surveillance Security Ontology (SSSO) is integrated within the MuLViS, with the aim of autonomously selecting the privacy level matching the operating device's hardware specifications and network capabilities. Overall, along with its device-specific security, the system leads to relatively fast indexing and retrieval of surveillance video. Secondly, information within the videos are protected at the times of capturing, streaming, and storage by means of differing encryption levels. An extensive evaluation of the system, through visual inspection and statistical analysis of experimental video results, such as by the Encryption Space Ratio (ESR), has demonstrated the aptness of the security level assignments. The system is suitable for surveillance footage protection, which can be made General Data Protection Regulation (GDPR) compliant, ensuring that lawful data access respects individuals' privacy rights.",https://ieeexplore.ieee.org/document/9199837/,IEEE Access,2020,ieeexplore
10.1093/comjnl/bxu058,Multi-Agent Architecture for Control of Heating and Cooling in a Residential Space,OUP,Journals,"Energy demand in a smart grid is directly related to energy consumption, as defined by user needs and comfort experience. This article presents a multi-agent architecture for smart control of space heating and cooling processes, in an attempt to enable flexible ways of monitoring and adjusting energy supply and demand. In this proposed system, control agents are implemented in order to perform temperature set-point delegation for heating and cooling systems in a building, offering a means to observe and learn from both the environment and the occupant. Operation of the proposed algorithms is compared with traditional algorithms utilized for room heating, using a simulated model of a residential building and real data about user behaviour. The results show (i) the performance of machine learning for the occupancy forecasting problem and for the problem of calculating the time to heat or cool a room; and (ii) the performance of the control algorithms, with respect to energy consumption and occupant comfort. The proposed control agents make it possible to significantly improve an occupant comfort with a relatively small increase in energy consumption, compared with simple control strategies that always maintain predefined temperatures. The findings enable the smart grid to anticipate the energy needs of the building.",https://ieeexplore.ieee.org/document/8131352/,The Computer Journal,June 2015,ieeexplore
10.1109/TVT.2020.3002983,Multi-Agent Deep Reinforcement Learning-Based Flexible Satellite Payload for Mobile Terminals,IEEE,Journals,"Information dissemination in mobile networks turns out to be a problem when the network is sparse. Mobile networks begin to establish a separate cluster attributable to the limited communication range of terminals. The multi-beam satellite communication systems can play a significant role in providing direct-to-user satellite mobile services and connecting the separated clusters. This paper focuses on how to efficiently schedule limited satellite-based radio resources to enhance transmission efficiency and meet the requested traffic with low complexity. Taking the inter-beam interference and resource utilization variance into consideration, we build a game-theoretic based model for bandwidth allocation in the forward link. As the size of satellite beams increases, the size of the action space for deep reinforcement learning based on a single agent becomes large, resulting in high time complexity. Thus, we extend the single-agent deep reinforcement learning to the multi-agent context and then propose a cooperative multi-agent deep reinforcement learning method to achieve the optimal bandwidth allocation strategy. Each beam works as a player who is willing to satisfy the request traffic with flexible payloads. We built a multi-beam satellite platform using real historical data. The experimental results show that this approach is capable of enhancing transmission efficiency and can be flexible to achieve the desired goal with low complexity.",https://ieeexplore.ieee.org/document/9119854/,IEEE Transactions on Vehicular Technology,Sept. 2020,ieeexplore
10.1109/ACCESS.2021.3065969,Multi-Attention Generative Adversarial Network for Multivariate Time Series Prediction,IEEE,Journals,"Multivariate Time series data play important roles in our daily life. How to use these data in the process of prediction is a highly attractive study for many researchers. To achieve this goal, in this paper, we present a novel multivariate time series prediction method based on multi-attention generative adversarial network. This method includes three phases to explore multivariate time series prediction. Firstly, the encoder stage consists of two modules, from which the input-attention and self-attention can encode the exogenous sequence into latent space. Secondly, the decoder stage consists of the temporal-convolution-attention module, which can extract long-term temporal patterns. To solve the problem of low accuracy in long-term prediction, inspired by the weight clipping method, we design an improved discrimination network finally. The experiment results indicate that multi-attention mechanism is useful and the discrimination network can improve the performance in multivariate time series prediction. We also tested extensive empirical studies with five real world datasets (NASDAQ100, SML2010, Energy, EEG and Air Quality) demonstrate the effectiveness and robustness of our proposed approach.",https://ieeexplore.ieee.org/document/9378517/,IEEE Access,2021,ieeexplore
10.1109/TKDE.2018.2857766,Multi-Label Learning from Crowds,IEEE,Journals,"We consider multi-label crowdsourcing learning in two scenarios. In the first scenario, we aim at inferring instances' groundtruth given the crowds' annotations. We propose two approaches NAM/RAM (Neighborhood/Relevance Aware Multi-label crowdsourcing) modeling the crowds' expertise and label correlations from different perspectives. Extended from single-label crowdsourcing methods, NAM models the crowds' expertise on individual labels, but based on the idea that for rational workers, their annotations for instances similar in the feature space should also be similar, NAM utilizes information from the feature space and incorporates the local influence of neighborhoods' annotations. Noting that the crowds tend to act in an effort-saving manner while labeling multiple labels, i.e., rather than carefully annotating every proper label, they would prefer scanning and tagging a few most relevant labels, RAM models the crowds' expertise as their ability to distinguish the relevance between label pairs. In the second scenario, we care about cost-efficient crowdsourcing where the labeling and learning process are conducted in tandem. We extend NAM/RAM to the active paradigm and propose instance, label, and worker selection criteria such that the labeling cost is significantly saved compared to passive learning without labeling control. The proposals' effectiveness are validated on simulated and real data.",https://ieeexplore.ieee.org/document/8413163/,IEEE Transactions on Knowledge and Data Engineering,1 July 2019,ieeexplore
10.1109/TCSVT.2019.2903563,Multi-Manifold Positive and Unlabeled Learning for Visual Analysis,IEEE,Journals,"Positive and Unlabeled (PU) learning has attracted intensive research interests in recent years, which is capable of training a binary classifier solely based on positive and unlabeled examples when the negative data are absent or too are diverse. However, the existing PU learning methods largely overlook the relationship between the examples when handling the unlabeled data, leading to insufficient exploitation of data structure which actually contains useful distribution information. Therefore, by following the multi-manifold assumption which is observed in many real-world vision problems, this paper proposes a novel algorithm termed Multi-Manifold PU learning (MMPU), which assumes that the data belonging to different classes lie on different underlying manifolds. As such, the structural information revealed by the dataset is deployed, which is helpful in deciding the labels of unlabeled examples. Our MMPU contains two main steps, namely, multi-manifold exploration and positive confidence training, where the former is accomplished by computing the local similarity, structural similarity, and semantic similarity of pairwise data, and the latter establishes a binary classifier in reproducing kernel Hilbert space based on the real-valued confidence level of each example to be positive. Experimentally, we not only test the proposed MMPU on five highly nonlinear synthetic datasets but also apply MMPU to various typical computer vision tasks, including handwritten digit recognition, violent behavior detection, and hyperspectral image classification. The results demonstrate that MMPU can obtain a superior performance compared to the state-of-the-art PU learning methodologies.",https://ieeexplore.ieee.org/document/8662698/,IEEE Transactions on Circuits and Systems for Video Technology,May 2020,ieeexplore
10.1109/TKDE.2020.2983366,Multi-View Clustering With the Cooperation of Visible and Hidden Views,IEEE,Journals,"Multi-view data are becoming common in real-world applications and many multi-view clustering algorithms have thus been proposed. The existing algorithms usually focus on the cooperation of different visible views in the original space but neglect the influence of the hidden information among these visible views, or they only consider the hidden information among the views. The algorithms are therefore not efficient since the available information is not fully exploited, particularly the otherness information in different views and the consistency information among them. In practice, the otherness and consistency information in multi-view data are both very useful for effective clustering analyses. In this study, a Multi-View clustering algorithm with the Cooperation of Visible and Hidden views, i.e., MV-Co-VH, is proposed. The MV-Co-VH algorithm first projects the multiple views from different visible spaces to the common hidden space by using non-negative matrix factorization to obtain the common hidden view data. Collaborative learning is then implemented in the clustering procedure based on the visible views and the shared hidden view. The experimental results of extensive experiments on UCI multi-view datasets and real-world image multi-view datasets show that the clustering performance of the proposed algorithm is competitive with or even better than that of the existing algorithms.",https://ieeexplore.ieee.org/document/9051812/,IEEE Transactions on Knowledge and Data Engineering,1 Feb. 2022,ieeexplore
10.1109/LSP.2020.3026943,Multi-View Robust Feature Learning for Data Clustering,IEEE,Journals,"Multi-view feature learning can provide basic information for consistent grouping, and is very common in practical applications, such as judicial document clustering. However, it is a challenge to combine multiple heterogeneous features to learn a comprehensive description of data samples. To solve this problem, many methods explore the correlation between various features across views by assuming that all views share the same semantic information. Inspired by this, in this paper we propose a new multi-view robust feature learning (MRFL) method. In addition to projecting features from different views to a shared semantic subspace, our approach also learns the irrelevant information of data space to capture the feature dependencies between views in potential common subspaces. Therefore, the MRFL can obtain flexible feature associations hidden in multi-view data. A new objective function is designed to derive, and solve the effective optimization process of MRFL. Experiments on real-world multi-view datasets show that the proposed MRFL method is superior to the state-of-the-art multi-view learning methods.",https://ieeexplore.ieee.org/document/9206140/,IEEE Signal Processing Letters,2020,ieeexplore
10.1109/TKDE.2008.128,Multiclass MTS for Simultaneous Feature Selection and Classification,IEEE,Journals,"Multiclass Mahalanobis-Taguchi system (MMTS), the extension of MTS, is developed for simultaneous multiclass classification and feature selection. In MMTS, the multiclass measurement scale is constructed by establishing an individual Mahalanobis space for each class. To increase the validity of the measurement scale, the Gram-Schmidt process is performed to mutually orthogonalize the features and eliminate the multicollinearity. The important features are identified using the orthogonal arrays and the signal-to-noise ratio, and are then used to construct a reduced model measurement scale. The contribution of each important feature to classification is also derived according to the effect gain to develop a weighted Mahalanobis distance which is finally used as the distance metric for the classification of MMTS. Using the reduced model measurement scale, an unknown example will be classified into the class with minimum weighted Mahalanobis distance considering only the important features. For evaluating the effectiveness of MMTS, a numerical experiment is implemented, and the results show that MMTS outperforms other well-known algorithms not only on classification accuracy but also on feature selection efficiency. Finally, a real case about gestational diabetes mellitus is studied, and the results indicate the practicality of MMTS in real-world applications.",https://ieeexplore.ieee.org/document/4564454/,IEEE Transactions on Knowledge and Data Engineering,Feb. 2009,ieeexplore
10.1109/TSMC.2017.2746762,Multidomain Features-Based GA Optimized Artificial Immune System for Bearing Fault Detection,IEEE,Journals,"This paper proposes a novel multidomain features-based genetic algorithm (GA) optimized artificial immune system (AIS) framework for fault detection in real systems. Different from native real-valued negative selection algorithm (RNSA) that operates in original data space, this algorithm utilizes feature space transformation and diversity factor-based GA for optimized detector distribution in nonself feature space. The proposed framework comprises three stages namely; feature extraction, unsupervised feature selection, and GA optimized AIS. In the first stage, signal processing methods are applied to extract multidomain features (time-domain statistical, frequency domain statistical, and special features) of the system. In the second stage, two unsupervised methods namely, k-NN clustering and pretraining using deep learning neural network are proposed for dominant fault-characterizing feature selection. Finally, in the third stage, the fault-characterizing feature vectors are used for system status categorization (i.e., normal, fault) using selected (fault-characterizing) features-based AIS method. The efficacy of the proposed framework is verified through experiments on motor bearing fault detection using vibration signal. The major accomplishment of the proposed combination of space transformation, feature selection and AIS (anomaly classification) techniques is the alleviation of computational burden on RNSA implementation. Moreover, GA optimized AIS fault diagnosis based on well-established features gives improved detection performance.",https://ieeexplore.ieee.org/document/8031077/,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",Jan. 2020,ieeexplore
10.1109/41.499811,Multilayered fuzzy behavior fusion for real-time reactive control of systems with multiple sensors,IEEE,Journals,"Fuzzy linguistic rules provide an intuitive and powerful means for defining control behavior. Most applications that use fuzzy control feature a single layer of fuzzy inference, mapping a function from one or two inputs to equally few outputs. Highly complex systems, with large numbers of inputs, may also benefit from the use of qualitative linguistic rules if the control task is properly partitioned. This paper presents a modular fuzzy control architecture and inference engine that can be used to control complex systems. The control function is broken down into multiple local agents, each of which samples a subset of a large sensor input space. Additional fuzzy agents are employed to fuse the recommendations of the local agents. Real-time implementation without special hardware is possible by using singleton output values during fuzzy rule evaluation. A development tool is used to translate a fuzzy programming language offline for fast execution at run time. Using this system, a multilayered fuzzy behavior fusion based reactive control system has been implemented on an autonomous mobile robot, MARGE, with great success. MARGE won first place in Event III of the 1993 Robot Competition sponsored by the American Association for Artificial Intelligence.",https://ieeexplore.ieee.org/document/499811/,IEEE Transactions on Industrial Electronics,June 1996,ieeexplore
10.1109/TIE.2007.903993,Multimodal Approach to Human-Face Detection and Tracking,IEEE,Journals,"The constructive need for robots to coexist with humans requires human-machine interaction. It is a challenge to operate these robots in such dynamic environments, which requires continuous decision-making and environment-attribute update in real-time. An autonomous robot guide is well suitable in places such as museums, libraries, schools, hospital, etc. This paper addresses a scenario where a robot tracks and follows a human. A neural network is utilized to learn the skin and nonskin colors. The skin-color probability map is utilized for skin classification and morphology-based preprocessing. Heuristic rule is used for face-ratio analysis and Bayesian cost analysis for label classification. A face-detection module, based on a 2D color model in the and YUV color space, is selected over the traditional skin-color model in a 3D color space. A modified continuously adaptive mean shift tracking mechanism in a 1D hue, saturation, and value color space is developed and implemented onto the mobile robot. In addition to the visual cues, the tracking process considers 16 sonar scan and tactile sensor readings from the robot to generate a robust measure of the person's distance from the robot. The robot thus decides an appropriate action, namely, to follow the human subject and perform obstacle avoidance. The proposed approach is orientation invariant under varying lighting conditions and invariant to natural transformations such as translation, rotation, and scaling. Such a multimodal solution is effective for face detection and tracking.",https://ieeexplore.ieee.org/document/4392479/,IEEE Transactions on Industrial Electronics,March 2008,ieeexplore
10.1109/TCIAIG.2014.2345842,Multiobjective Monte Carlo Tree Search for Real-Time Games,IEEE,Journals,"Multiobjective optimization has been traditionally a matter of study in domains like engineering or finance, with little impact on games research. However, action-decision based on multiobjective evaluation may be beneficial in order to obtain a high quality level of play. This paper presents a multiobjective Monte Carlo tree search algorithm for planning and control in real-time game domains, those where the time budget to decide the next move to make is close to 40 ms. A comparison is made between the proposed algorithm, a single-objective version of Monte Carlo tree search and a rolling horizon implementation of nondominated sorting evolutionary algorithm II (NSGA-II). Two different benchmarks are employed, deep sea treasure (DST) and the multiobjective physical traveling salesman problem (MO-PTSP). Using the same heuristics on each game, the analysis is focused on how well the algorithms explore the search space. Results show that the algorithm proposed outperforms NSGA-II. Additionally, it is also shown that the algorithm is able to converge to different optimal solutions or the optimal Pareto front (if achieved during search).",https://ieeexplore.ieee.org/document/6872573/,IEEE Transactions on Computational Intelligence and AI in Games,Dec. 2015,ieeexplore
10.1109/ACCESS.2020.3046604,Multiple Kernel Learning With Minority Oversampling for Classifying Imbalanced Data,IEEE,Journals,"Class imbalance problems, developed due to the sampling bias or measurement error, occur frequently in real-world pattern classification tasks. The traditional classifiers focus on the overall classification accuracy and ignore the minority class, which may degrade the classification performance. However, existing oversampling algorithms generally make specific assumptions to balance the class size and do not sufficiently consider irregularities present in imbalanced data. As a result, these methods can perform well only on certain benchmarks. In this paper, by incorporating minority oversampling and cost-sensitive learning, we propose multiple kernel learning with minority oversampling (MKLMO), for efficiently handling the class imbalance problem with small disjuncts, overlapping, and nonlinear shape. Unlike existing methods where oversampling of the minority class is performed first and then a standard classifier is deployed on the rebalanced data, the proposed MKLMO generates synthetic instances and trains classifier synchronously in the same feature space. Specially, we define a distance metric in the optimal feature space by multiple kernel learning and use kernel trick to expand the original Gram matrix. Moreover, we assign different weights to instances, based on the imbalance ratio, for reducing the bias of the classifier towards the majority class. In order to evaluate the proposed MKLMO method, several experiments are performed with nine artificial and twenty-one real-world datasets. The experimental results show that our algorithm outperforms other baseline algorithms significantly in terms of the assessment metric geometric mean (G-mean), especially in the presence of data irregularities.",https://ieeexplore.ieee.org/document/9305259/,IEEE Access,2021,ieeexplore
10.1109/TIE.2020.3040669,Multireceptive Field Graph Convolutional Networks for Machine Fault Diagnosis,IEEE,Journals,"Deep learning (DL) based methods have swept the field of mechanical fault diagnosis, because of the powerful ability of feature representation. However, many of existing DL methods fail in relationship mining between signals explicitly. Unlike those deep neural networks, graph convolutional networks (GCNs) taking graph data with topological structure as input is more efficient for data relationship mining, making GCN to be powerful for feature representation from graph data in non-Euclidean space. Nevertheless, existing GCNs have two limitations. First, most GCNs are constructed on unweighted graphs, considering importance of neighbors as the same, which is not in line with reality. Second, the receptive field of GCNs is fixed, which limits the effectiveness of GCNs for feature representation. To address these issues, a multireceptive field graph convolutional network (MRF-GCN) is proposed for effective intelligent fault diagnosis. In MRF-GCN, data samples are converted into weighted graphs to indicate differences in relationship of data samples. Moreover, MRF-GCN learns not only features from different receptive field, but also fuses learned features as an enhanced feature representation. To verify the efficacy of MRF-GCN for machine fault diagnosis, case studies are implemented, and the results show that MRF-GCN can achieve superior performance even under imbalanced dataset.",https://ieeexplore.ieee.org/document/9280401/,IEEE Transactions on Industrial Electronics,Dec. 2021,ieeexplore
10.1109/ACCESS.2019.2947460,NA-Caching: An Adaptive Content Management Approach Based on Deep Reinforcement Learning,IEEE,Journals,"Video streaming is a dominant application over today's Internet. The current mainstream video streaming solution is to utilize the services of a Content Delivery Network (CDN) provider. By replicating video content closer to the network edge, caching provides an effective mechanism for alleviating the demand for massive bandwidth for the Internet backbone. It reduces the network traffic and capital expense for streaming the video content, and in the meantime, enhance Internet's Quality of Service (QoS). In this paper, we propose a neural adaptive caching approach, named NA-Caching, for helping cache learn to make caching decisions from its own experiences rather than a specific mathematical model, in a way similar to how a human being learns a new skill (e.g. cycling, swimming). NA-Caching leverages the benefits of the Recurrent Neural Network (RNN) as well as the Deep Reinforcement Learning (DRL) to maximize the cache efficiency by jointly learning request features, caching space dynamics and making decisions. Specifically, we utilize Gated Recurrent Unit (GRU) to characterize the evolving features of the dynamic requests and caching space. Moreover, the above GRU-based representation network is integrated into a Deep Q-Network (DQN) framework for making adaptive caching decisions online. To evaluate the performance of the proposed approach, we conduct extensive experiments on anonymized real-world traces from a video provider. The results demonstrate that our algorithm significantly outperform several candidate methods.",https://ieeexplore.ieee.org/document/8869760/,IEEE Access,2019,ieeexplore
10.1109/OJITS.2021.3139393,NAPC: A Neural Algorithm for Automated Passenger Counting in Public Transport on a Privacy-Friendly Dataset,IEEE,Journals,"Real-time load information in public transport is of high importance for both passengers and service providers. Neural algorithms have shown a high performance on various object counting tasks and play a continually growing methodological role in developing automated passenger counting systems. However, the publication of public-space video footage is often contradicted by legal and ethical considerations to protect the passengers privacy. This work proposes an end-to-end Long Short-Term Memory network with a problem-adapted cost function that learned to count boarding and alighting passengers on a publicly available, comprehensive dataset of approx.13,000 manually annotated low-resolution 3D LiDAR video recordings (depth information only) from the doorways of a regional train. These depth recordings do not allow the identification of single individuals. For each door opening phase, the trained models predict the correct passenger count (ranging from 0 to 67) in approx.96% of boarding and alighting, respectively. Repeated training with different training and validation sets confirms the independence of this result from a specific test set.",https://ieeexplore.ieee.org/document/9665722/,IEEE Open Journal of Intelligent Transportation Systems,2022,ieeexplore
10.1109/ACCESS.2017.2690987,NC-Link: A New Linkage Method for Efficient Hierarchical Clustering of Large-Scale Data,IEEE,Journals,"In various disciplines, hierarchical clustering (HC) has been an effective tool for data analysis due to its ability to summarize hierarchical structures of data in an intuitive and interpretable manner. A run of HC requires multiple iterations, each of which needs to compute and update the pairwise distances between all intermediate clusters. This makes the exact algorithm for HC inevitably suffer from quadratic time and space complexities. To address large-scale data, various approximate/parallel algorithms have been proposed to reduce the computational cost of HC. However, such algorithms still rely on conventional linkage methods (such as single, centroid, average, complete, or Ward's) for defining pairwise distances, mostly focusing on the approximation/parallelization of linkage computations. Given that the choice of linkage profoundly affects not only the quality but also the efficiency of HC, we propose a new linkage method named NC-link and design an exact algorithm for NC-link-based HC. To guarantee the exactness, the proposed algorithm maintains the quadratic nature in time complexity but exhibits only linear space complexity, thereby allowing us to address million-object data on a personal computer. To underpin the extensibility of our approach, we showthat the algorithmic nature of NC-link enables single instruction multiple data (SIMD) parallelization and subquadratic-time approximation of HC. To verify our proposal, we thoroughly tested it with a number of large-scale real and synthetic data sets. In terms of efficiency, NC-link allowed us to perform HC substantially more space efficiently or faster than conventional methods: compared with average and complete linkages, using NC-link incurred only 0.7%-1.75% of the memory usage, and the NC-link-based implementation delivered speedups of approximately 3.5 times over the centroid and Ward's linkages. With regard to clustering quality, the proposed method was able to retrieve hierarchical structures from input data as faithfully as in the popular average and centroid linkage methods. We anticipate that the existing approximation/parallel algorithms will be able to benefit from adopting NC-link as their linkage method for obtaining better clustering results and reduced time and space demands.",https://ieeexplore.ieee.org/document/7891959/,IEEE Access,2017,ieeexplore
10.1109/TPAMI.2020.3013269,NWPU-Crowd: A Large-Scale Benchmark for Crowd Counting and Localization,IEEE,Journals,"In the last decade, crowd counting and localization attract much attention of researchers due to its wide-spread applications, including crowd monitoring, public safety, space design, etc. Many convolutional neural networks (CNN) are designed for tackling this task. However, currently released datasets are so small-scale that they can not meet the needs of the supervised CNN-based algorithms. To remedy this problem, we construct a large-scale congested crowd counting and localization dataset, NWPU-Crowd, consisting of 5,109 images, in a total of 2,133,375 annotated heads with points and boxes. Compared with other real-world datasets, it contains various illumination scenes and has the largest density range (0 ~ 20,0330~20,033). Besides, a benchmark website is developed for impartially evaluating the different methods, which allows researchers to submit the results of the test set. Based on the proposed dataset, we further describe the data characteristics, evaluate the performance of some mainstream state-of-the-art (SOTA) methods, and analyze the new problems that arise on the new data. What's more, the benchmark is deployed at https://www.crowdbenchmark.com/, and the dataset/code/models/results are available at https://gjy3035.github.io/NWPU-Crowd-Sample-Code/.",https://ieeexplore.ieee.org/document/9153156/,IEEE Transactions on Pattern Analysis and Machine Intelligence,1 June 2021,ieeexplore
10.1109/JPROC.2018.2856739,Navigating the Landscape for Real-Time Localization and Mapping for Robotics and Virtual and Augmented Reality,IEEE,Journals,"Visual understanding of 3-D environments in real time, at low power, is a huge computational challenge. Often referred to as simultaneous localization and mapping (SLAM), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, and virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are: 1) tools and methodology for systematic quantitative evaluation of SLAM algorithms; 2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives; 3) end-to-end simulation tools to enable optimization of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches; and 4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",https://ieeexplore.ieee.org/document/8436423/,Proceedings of the IEEE,Nov. 2018,ieeexplore
10.1109/TNNLS.2015.2451535,Near-Optimal Controller for Nonlinear Continuous-Time Systems With Unknown Dynamics Using Policy Iteration,IEEE,Journals,"This paper presents a single-network adaptive critic-based controller for continuous-time systems with unknown dynamics in a policy iteration (PI) framework. It is assumed that the unknown dynamics can be estimated using the Takagi-Sugeno-Kang fuzzy model with arbitrary precision. The successful implementation of a PI scheme depends on the effective learning of critic network parameters. Network parameters must stabilize the system in each iteration in addition to approximating the critic and the cost. It is found that the critic updates according to the Hamilton-Jacobi-Bellman formulation sometimes lead to the instability of the closed-loop systems. In the proposed work, a novel critic network parameter update scheme is adopted, which not only approximates the critic at current iteration but also provides feasible solutions that keep the policy stable in the next step of training by combining a Lyapunov-based linear matrix inequalities approach with PI. The critic modeling technique presented here is the first of its kind to address this issue. Though multiple literature exists discussing the convergence of PI, however, to the best of our knowledge, there exists no literature, which focuses on the effect of critic network parameters on the convergence. Computational complexity in the proposed algorithm is reduced to the order of (F<sub>z</sub>)<sup>n-1</sup>, where n is the fuzzy state dimensionality and F<sub>z</sub> is the number of fuzzy zones in the states space. A genetic algorithm toolbox of MATLAB is used for searching stable parameters while minimizing the training error. The proposed algorithm also provides a way to solve for the initial stable control policy in the PI scheme. The algorithm is validated through real-time experiment on a commercial robotic manipulator. Results show that the algorithm successfully finds stable critic network parameters in real time for a highly nonlinear system.",https://ieeexplore.ieee.org/document/7174547/,IEEE Transactions on Neural Networks and Learning Systems,July 2016,ieeexplore
10.1109/TSMC.2017.2700889,Nearest Neighbor Classification for High-Speed Big Data Streams Using Spark,IEEE,Journals,"Mining massive and high-speed data streams among the main contemporary challenges in machine learning. This calls for methods displaying a high computational efficacy, with ability to continuously update their structure and handle ever-arriving big number of instances. In this paper, we present a new incremental and distributed classifier based on the popular nearest neighbor algorithm, adapted to such a demanding scenario. This method, implemented in Apache Spark, includes a distributed metric-space ordering to perform faster searches. Additionally, we propose an efficient incremental instance selection method for massive data streams that continuously update and remove outdated examples from the case-base. This alleviates the high computational requirements of the original classifier, thus making it suitable for the considered problem. Experimental study conducted on a set of real-life massive data streams proves the usefulness of the proposed solution and shows that we are able to provide the first efficient nearest neighbor solution for high-speed big and streaming data.",https://ieeexplore.ieee.org/document/7993020/,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",Oct. 2017,ieeexplore
10.1109/TKDE.2020.2971490,Network Embedding With Completely-Imbalanced Labels,IEEE,Journals,"Network embedding, aiming to project a network into a low-dimensional space, is increasingly becoming a focus of network research. Semi-supervised network embedding takes advantage of labeled data, and has shown promising performance. However, existing semi-supervised methods would get unappealing results in the <i>completely-imbalanced</i> label setting where some classes have no labeled nodes at all. To alleviate this, we propose two novel semi-supervised network embedding methods. The first one is a shallow method named RSDNE. Specifically, to benefit from the completely-imbalanced labels, RSDNE guarantees both intra-class similarity and inter-class dissimilarity in an approximate way. The other method is RECT which is a new class of graph neural networks. Different from RSDNE, to benefit from the completely-imbalanced labels, RECT explores the class-semantic knowledge. This enables RECT to handle networks with node features and multi-label setting. Experimental results on several real-world datasets demonstrate the superiority of the proposed methods.",https://ieeexplore.ieee.org/document/8979355/,IEEE Transactions on Knowledge and Data Engineering,1 Nov. 2021,ieeexplore
10.1109/60.475850,Neural-net based coordinated stabilizing control for the exciter and governor loops of low head hydropower plants,IEEE,Journals,"This paper presents a design technique of a new adaptive optimal controller of the low head hydropower plant using artificial neural networks (ANN). The adaptive controller is to operate in real time to improve the generating unit transients through the exciter input, the guide vane position and the runner blade position. The new design procedure is based on self-organization and the predictive estimation capabilities of neural-nets implemented through the cluster-wise segmented associative memory scheme. The developed neural-net based controller (NNC) whose control signals are adjusted using the on-line measurements, can offer better damping effects for generator oscillations over a wide range of operating conditions than conventional controllers. Digital simulations of hydropower plant equipped with low head Kaplan turbines are performed and the comparisons of conventional excitation-governor state-space optimal control and neural-net based control are presented. Results obtained on the nonlinear mathematical model demonstrate that the effects of the NNC closely agree with those obtained using the state-space multivariable discrete-time optimal controllers.",https://ieeexplore.ieee.org/document/475850/,IEEE Transactions on Energy Conversion,Dec. 1995,ieeexplore
10.1109/TNN.2008.2005582,Nonlinear Dimensionality Reduction by Locally Linear Inlaying,IEEE,Journals,"High-dimensional data is involved in many fields of information processing. However, sometimes, the intrinsic structures of these data can be described by a few degrees of freedom. To discover these degrees of freedom or the low-dimensional nonlinear manifold underlying a high-dimensional space, many manifold learning algorithms have been proposed. Here we describe a novel algorithm, locally linear inlaying (LLI), which combines simple geometric intuitions and rigorously established optimality to compute the global embedding of a nonlinear manifold. Using a divide-and-conquer strategy, LLI gains some advantages in itself. First, its time complexity is linear in the number of data points, and hence LLI can be implemented efficiently. Second, LLI overcomes problems caused by the nonuniform sample distribution. Third, unlike existing algorithms such as isometric feature mapping (Isomap), local tangent space alignment (LTSA), and locally linear coordination (LLC), LLI is robust to noise. In addition, to evaluate the embedding results quantitatively, two criteria based on information theory and Kolmogorov complexity theory, respectively, are proposed. Furthermore, we demonstrated the efficiency and effectiveness of our proposal by synthetic and real-world data sets.",https://ieeexplore.ieee.org/document/4749257/,IEEE Transactions on Neural Networks,Feb. 2009,ieeexplore
10.1109/TPAMI.2020.2965531,Novelty Detection and Online Learning for Chunk Data Streams,IEEE,Journals,"Datastream analysis aims at extracting discriminative information for classification from continuously incoming samples. It is extremely challenging to detect novel data while incrementally updating the model efficiently and stably, especially for high-dimensional and/or large-scale data streams. This paper proposes an efficient framework for novelty detection and incremental learning for unlabeled chunk data streams. First, an accurate factorization-free kernel discriminative analysis (FKDA-X) is put forward through solving a linear system in the kernel space. FKDA-X produces a Reproducing Kernel Hilbert Space (RKHS), in which unlabeled chunk data can be detected and classified by multiple known-classes in a single decision model with a deterministic classification boundary. Moreover, based on FKDA-X, two optimal methods FKDA-CX and FKDA-C are proposed. FKDA-CX uses the micro-cluster centers of original data as the input to achieve excellent performance in novelty detection. FKDA-C and incremental FKDA-C (IFKDA-C) using the class centers of original data as their input have extremely fast speed in online learning. Theoretical analysis and experimental validation on under-sampled and large-scale real-world datasets demonstrate that the proposed algorithms make it possible to learn unlabeled chunk data streams with significantly lower computational costs and comparable accuracies than the state-of-the-art approaches.",https://ieeexplore.ieee.org/document/8955936/,IEEE Transactions on Pattern Analysis and Machine Intelligence,1 July 2021,ieeexplore
10.1109/TIP.2020.3025437,On Aggregation of Unsupervised Deep Binary Descriptor With Weak Bits,IEEE,Journals,"Despite the thrilling success achieved by existing binary descriptors, most of them are still in the mire of three limitations: 1) vulnerable to the geometric transformations; 2) incapable of preserving the manifold structure when learning binary codes; 3) NO guarantee to find the true match if multiple candidates happen to have the same Hamming distance to a given query. All these together make the binary descriptor less effective, given large-scale visual recognition tasks. In this paper, we propose a novel learning-based feature descriptor, namely Unsupervised Deep Binary Descriptor (UDBD), which learns transformation invariant binary descriptors via projecting the original data and their transformed sets into a joint binary space. Moreover, we involve a <sub>2,1</sub>-norm loss term in the binary embedding process to gain simultaneously the robustness against data noises and less probability of mistakenly flipping bits of the binary descriptor, on top of it, a graph constraint is used to preserve the original manifold structure in the binary space. Furthermore, a weak bit mechanism is adopted to find the real match from candidates sharing the same minimum Hamming distance, thus enhancing matching performance. Extensive experimental results on public datasets show the superiority of UDBD in terms of matching and retrieval accuracy over state-of-the-arts.",https://ieeexplore.ieee.org/document/9206151/,IEEE Transactions on Image Processing,2020,ieeexplore
10.1109/TFUZZ.2016.2646746,On Distributed Fuzzy Decision Trees for Big Data,IEEE,Journals,"Fuzzy decision trees (FDTs) have shown to be an effective solution in the framework of fuzzy classification. The approaches proposed so far to FDT learning, however, have generally neglected time and space requirements. In this paper, we propose a distributed FDT learning scheme shaped according to the MapReduce programming model for generating both binary and multiway FDTs from big data. The scheme relies on a novel distributed fuzzy discretizer that generates a strong fuzzy partition for each continuous attribute based on fuzzy information entropy. The fuzzy partitions are, therefore, used as an input to the FDT learning algorithm, which employs fuzzy information gain for selecting the attributes at the decision nodes. We have implemented the FDT learning scheme on the Apache Spark framework. We have used ten real-world publicly available big datasets for evaluating the behavior of the scheme along three dimensions: 1) performance in terms of classification accuracy, model complexity, and execution time; 2) scalability varying the number of computing units; and 3) ability to efficiently accommodate an increasing dataset size. We have demonstrated that the proposed scheme turns out to be suitable for managing big datasets even with a modest commodity hardware support. Finally, we have used the distributed decision tree learning algorithm implemented in the MLLib library and the Chi-FRBCS-BigData algorithm, a MapReduce distributed fuzzy rule-based classification system, for comparative analysis.",https://ieeexplore.ieee.org/document/7803561/,IEEE Transactions on Fuzzy Systems,Feb. 2018,ieeexplore
10.1093/mnras/stx687,On the realistic validation of photometric redshifts,OUP,Journals,"Two of the main problems encountered in the development and accurate validation of photometric redshift (photo-z) techniques are the lack of spectroscopic coverage in the feature space (e.g. colours and magnitudes) and the mismatch between the photometric error distributions associated with the spectroscopic and photometric samples. Although these issues are well known, there is currently no standard benchmark allowing a quantitative analysis of their impact on the final photo-z estimation. In this work, we present two galaxy catalogues, Teddy and Happy, built to enable a more demanding and realistic test of photo-z methods. Using photometry from the Sloan Digital Sky Survey and spectroscopy from a collection of sources, we constructed data sets that mimic the biases between the underlying probability distribution of the real spectroscopic and photometric sample. We demonstrate the potential of these catalogues by submitting them to the scrutiny of different photo-z methods, including machine learning (ML) and template fitting approaches. Beyond the expected bad results from most ML algorithms for cases with missing coverage in the feature space, we were able to recognize the superiority of global models in the same situation and the general failure across all types of methods when incomplete coverage is convoluted with the presence of photometric errors  a data situation which photo-z methods were not trained to deal with up to now and which must be addressed by future large-scale surveys. Our catalogues represent the first controlled environment allowing a straightforward implementation of such tests. The data are publicly available within the COINtoolbox (https://github.com/COINtoolbox/photoz_catalogues).",https://ieeexplore.ieee.org/document/8209446/,Monthly Notices of the Royal Astronomical Society,Jan. 2017,ieeexplore
10.1109/TVT.2019.2945037,On-Board Deep Q-Network for UAV-Assisted Online Power Transfer and Data Collection,IEEE,Journals,"Unmanned Aerial Vehicles (UAVs) with Microwave Power Transfer (MPT) capability provide a practical means to deploy a large number of wireless powered sensing devices into areas with no access to persistent power supplies. The UAV can charge the sensing devices remotely and harvest their data. A key challenge is online MPT and data collection in the presence of on-board control of a UAV (e.g., patrolling velocity) for preventing battery drainage and data queue overflow of the devices, while up-to-date knowledge on battery level and data queue of the devices is not available at the UAV. In this paper, an on-board deep Q-network is developed to minimize the overall data packet loss of the sensing devices, by optimally deciding the device to be charged and interrogated for data collection, and the instantaneous patrolling velocity of the UAV. Specifically, we formulate a Markov Decision Process (MDP) with the states of battery level and data queue length of devices, channel conditions, and waypoints given the trajectory of the UAV; and solve it optimally with Q-learning. Furthermore, we propose the on-board deep Q-network that enlarges the state space of the MDP, and a deep reinforcement learning based scheduling algorithm that asymptotically derives the optimal solution online, even when the UAV has only outdated knowledge on the MDP states. Numerical results demonstrate that our deep reinforcement learning algorithm reduces the packet loss by at least 69.2%, as compared to existing non-learning greedy algorithms.",https://ieeexplore.ieee.org/document/8854903/,IEEE Transactions on Vehicular Technology,Dec. 2019,ieeexplore
10.1109/TCAD.2012.2188401,On-Chip Network-Enabled Multicore Platforms Targeting Maximum Likelihood Phylogeny Reconstruction,IEEE,Journals,"In phylogenetic inference, which aims at finding a phylogenetic tree that best explains the evolutionary relationship among a given set of species, statistical estimation approaches such as maximum likelihood (ML) and Bayesian inference provide more accurate estimates than other nonstatistical approaches. However, the improved quality comes at a higher computational cost, as these approaches, even though heuristic driven, involve optimization over multidimensional real continuous space. The number of possible search trees in ML is at least exponential, thereby making runtimes on even modest-sized datasets to clock up to several million CPU hours. Evaluation of these trees, involving node-level likelihood vector computation and branch-length optimization, can be partitioned into tasks (or kernels), providing the application with the potential to benefit from hardware acceleration. The range of hardware acceleration architectures tried so far offer limited degree of fine-grain parallelism. Network-on-chip (NoC) is an emerging paradigm that can efficiently support integration of massive number of cores on a chip. In this paper, we explore the design and performance evaluation of 2-D and 3-D NoC architectures for RAxML, which is one of the most widely used ML software suites. Specifically, we implement the computation kernels of the top three functions consuming more than 85% of the total software runtime. Simulations show that through appropriate choice of NoC architecture, and novel core design, allocation and placement strategies, our NoC-based implementation can achieve individual function-level speedups of 390x to 847x, speed up the targeted kernels in excess of 6500x, and provide end-to-end runtime reductions up to 5x over state-of-the-art multithreaded software.",https://ieeexplore.ieee.org/document/6218233/,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,July 2012,ieeexplore
10.1109/JIOT.2021.3063147,On-Device Learning Systems for Edge Intelligence: A Software and Hardware Synergy Perspective,IEEE,Journals,"Modern machine learning (ML) applications are often deployed in the cloud environment to exploit the computational power of clusters. However, this in-cloud computing scheme cannot satisfy the demands of emerging edge intelligence scenarios, including providing personalized models, protecting user privacy, adapting to real-time tasks, and saving resource cost. In order to conquer the limitations of conventional in-cloud computing, there comes the rise of on-device learning, which makes the end-to-end ML procedure totally on user devices, without unnecessary involvement of the cloud. In spite of the promising advantages of on-device learning, implementing a high-performance on-device learning system still faces with many severe challenges, such as insufficient user training data, backward propagation (BP) blocking, and limited peak processing speed. Observing the substantial improvement space in the implementation and acceleration of on-device learning systems, we intend to present a comprehensive analysis of the latest research progress and point out potential optimization directions from the system perspective. This survey presents a software and hardware synergy of on-device learning techniques, covering the scope of model-level neural network design, algorithm-level training optimization, and hardware-level instruction acceleration. We hope this survey could bring fruitful discussions and inspire the researchers to further promote the field of edge intelligence.",https://ieeexplore.ieee.org/document/9366901/,IEEE Internet of Things Journal,"1 Aug.1, 2021",ieeexplore
10.1109/ACCESS.2020.3021684,Online Clustering of Evolving Data Streams Using a Density Grid-Based Method,IEEE,Journals,"In recent years, a significant boost in data availability for persistent data streams has been observed. These data streams are continually evolving, with the clusters frequently forming arbitrary shapes instead of regular shapes in the data space. This characteristic leads to an exponential increase in the processing time of traditional clustering algorithms for data streams. In this study, we propose a new online method, which is a density grid-based method for data stream clustering. The primary objectives of the density grid-based method are to reduce the number of distant function calls and to improve the cluster quality. The method is conducted entirely online and consists of two main phases. The first phase generates the Core Micro-Clusters (CMCs), and the second phase combines the CMCs into macro clusters. The grid-based method was utilized as an outlier buffer in order to handle multi-density data and noises. The method was tested on real and synthetic data streams employing different quality metrics and was compared with the popular method of clustering evolving data streams into arbitrary shapes. The proposed method was demonstrated to be an effective solution for reducing the number of calls to the distance function and improving the cluster quality.",https://ieeexplore.ieee.org/document/9186700/,IEEE Access,2020,ieeexplore
10.1109/TCSVT.2016.2606998,Online Human Interaction Detection and Recognition With Multiple Cameras,IEEE,Journals,"We address the problem of detecting and recognizing online the occurrence of human interactions as seen by a network of multiple cameras. We represent interactions by forming temporal trajectories, coupling together the body motion of each individual and their proximity relationships with others, and also sound whenever available. Such trajectories are modeled with kernel state-space (KSS) models. Their advantage is being suitable for the online interaction detection, recognition, and also for fusing information from multiple cameras, while enabling a fast implementation based on online recursive updates. For recognition, in order to compare interaction trajectories in the space of KSS models, we design so-called pairwise kernels with a special symmetry. For detection, we exploit the geometry of linear operators in Hilbert space, and extend to KSS models the concept of parity space, originally defined for linear models. For fusion, we combine KSS models with kernel construction and multiview learning techniques. We extensively evaluate the approach on four single view publicly available data sets, and we also introduce, and will make public, a new challenging human interactions data set that we have collected using a network of three cameras. The results show that the approach holds promise to become an effective building block for the analysis of real-time human behavior from multiple cameras.",https://ieeexplore.ieee.org/document/7563312/,IEEE Transactions on Circuits and Systems for Video Technology,March 2017,ieeexplore
10.1109/TSP.2021.3095709,Online Joint State Inference and Learning of Partially Unknown State-Space Models,IEEE,Journals,"A computationally efficient method for online joint state inference and dynamical model learning is presented. The dynamical model combines an a priori known, physically derived, state-space model with a radial basis function expansion representing unknown system dynamics and inherits properties from both physical and data-driven modeling. The method uses an extended Kalman filter approach to jointly estimate the state of the system and learn the unknown system dynamics, via the parameters of the basis function expansion. The key contribution is a computational complexity reduction compared to a similar approach with globally supported basis functions. By using compactly supported radial basis functions and an approximate Kalman gain, the computational complexity is considerably reduced and is essentially determined by the support of the basis functions. The approximation works well when the system dynamics exhibit limited correlation between points well separated in the state-space domain. The method is exemplified via two intelligent vehicle applications where it is shown to: (i) have competitive system dynamics estimation performance compared to the globally supported basis function method, and (ii) be real-time applicable to problems with a large-scale state-space.",https://ieeexplore.ieee.org/document/9479713/,IEEE Transactions on Signal Processing,2021,ieeexplore
10.1109/ACCESS.2019.2912215,Online Shape Modification of Molecular Weight Distribution Based on the Principle of Active Disturbance Rejection Controller,IEEE,Journals,"Molecular weight distribution (MWD), an important microcosmic quality index of high polymer, is online unmeasurable, which makes its closed-loop control extremely difficult. To solve this problem, an online shape modification strategy for polymer MWD is proposed based on the active disturbance rejection controller (ADRC). The temporal-spatial property of MWD is estimated in real time by a three-layers forward network based on orthogonal polynomials basis function, and the shape modification of distribution function is transformed into the tracking control of moment statistics in the state-space description. Taking full advantage of the online measured lower-order moments, dual ADRCs are constructed with two manipulated variables (the flow rate of monomer and initiator) to achieve the high precision tracking of lower-order moments and distribution functions, simultaneously. Furthermore, the stability condition of the closed loop system is proved which can guide the parameter tuning of ADRCs. The proposed control strategy is implemented on the polymerization reaction in the laboratory scale continuous stirred tank reactor (CSTR). The feasibility and robustness are verified in the simulation.",https://ieeexplore.ieee.org/document/8694847/,IEEE Access,2019,ieeexplore
10.1109/LGRS.2019.2916225,Optimal Cannys Parameters Regressions for Coastal Line Detection in Satellite-Based SAR Images,IEEE,Journals,"Canny's algorithm is a very well-known and widely implemented multistage edge detector. The extraction of coastal lines in space-borne-based synthetic aperture radar (SAR) images using this algorithm is particularly complicated because of the multiplicative speckle noise present in them and can only be used if Canny's parameters (CaPP) are chosen appropriately. This letter introduces a methodology for computing functional forms for the CaPP, using functions of the image characteristics through a system that combines artificial neural networks (ANN) with statistical regression. A set of CaPP functional forms is obtained by applying this method on synthetic SAR images. Pratt's figure of merit (PFoM) is used to measure the performance of them, obtaining more than 0.75, on average, in the 14400 synthetic SAR images analyzed. Finally, this set of formulas has been tested for extracting coastal edges from real polynyas SAR images, acquired from Sentinel-1.",https://ieeexplore.ieee.org/document/8736022/,IEEE Geoscience and Remote Sensing Letters,Jan. 2020,ieeexplore
10.1109/TITB.2006.884374,Optimal Coding of Vectorcardiographic Sequences Using Spatial Prediction,IEEE,Journals,"This paper discusses principles, implementation details, and advantages of sequence coding algorithm applied to the compression of vectocardiograms (VCG). The main novelty of the proposed method is the automatic management of distortion distribution controlled by the local signal contents in both technical and medical aspects. As in clinical practice, the VCG loops representing P, QRS, and T waves in the three-dimensional (3-D) space are considered here as three simultaneous sequences of objects. Because of the similarity of neighboring loops, encoding the values of prediction error significantly reduces the data set volume. The residual values are de-correlated with the discrete cosine transform (DCT) and truncated at certain energy threshold. The presented method is based on the irregular temporal distribution of medical data in the signal and takes advantage of variable sampling frequency for automatically detected VCG loops. The features of the proposed algorithm are confirmed by the results of the numerical experiment carried out for a wide range of real records. The average data reduction ratio reaches a value of 8.15 while the percent root-mean-square difference (PRD) distortion ratio for the most important sections of signal does not exceed 1.1%",https://ieeexplore.ieee.org/document/4167889/,IEEE Transactions on Information Technology in Biomedicine,May 2007,ieeexplore
10.1109/TII.2021.3091597,Optimal Sizing and Efficient Routing of Electric Vehicles for a Vehicle-on-Demand System,IEEE,Journals,"Due to the steep rise in global population, urbanization, and industrialization, most of the cities in the world today are witnessing increased carbon footprints and reduced per capita space. In such a scenario, vehicle sharing and carpooling systems, specifically with electric vehicles (EV), can significantly help due to the reduced cost of ownership, maintenance, and parking space. In this article, we study the challenging problem of optimal sizing and efficient routing for an electric vehicle-on-demand system. Users demand EVs at the pooling stations at different time instances with individual deadlines to reach the destinations. The objective is to fulfill all the demands respecting the deadlines with minimum investment, which essentially translates to minimizing the total number of EVs. We define the problem formally using mixed-integer linear programming formulation and propose a set of intelligent and efficient heuristic algorithms to solve it efficiently. The proposed algorithms performances are tested and validated in a simulated environment on a reasonable size city network with many EV demands. The results obtained show that the proposed heuristic algorithms are competent by reducing 200360 EVs per day on a network of 282 charging ports, indicating their scalability to be implemented in real-world scenarios.",https://ieeexplore.ieee.org/document/9462468/,IEEE Transactions on Industrial Informatics,March 2022,ieeexplore
10.1109/JSAC.2019.2959181,Optimal VNF Placement via Deep Reinforcement Learning in SDN/NFV-Enabled Networks,IEEE,Journals,"The emerging paradigm - Software-Defined Networking (SDN) and Network Function Virtualization (NFV) - makes it feasible and scalable to run Virtual Network Functions (VNFs) in commercial-off-the-shelf devices, which provides a variety of network services with reduced cost. Benefitting from centralized network management, lots of information about network devices, traffic and resources can be collected in SDN/NFV-enabled networks. Using powerful machine learning tools, algorithms can be designed in a customized way according to the collected information to efficiently optimize network performance. In this paper, we study the VNF placement problem in SDN/NFV-enabled networks, which is naturally formulated as a Binary Integer Programming (BIP) problem. Using deep reinforcement learning, we propose a Double Deep Q Network-based VNF Placement Algorithm (DDQN-VNFPA). Specifically, DDQN determines the optimal solution from a prohibitively large solution space and DDQN-VNFPA then places/releases VNF Instances (VNFIs) following a threshold-based policy. We evaluate DDQN-VNFPA with trace-driven simulations on a real-world network topology. Evaluation results show that DDQN-VNFPA can get improved network performance in terms of the reject number and reject ratio of Service Function Chain Requests (SFCRs), throughput, end-to-end delay, VNFI running time and load balancing compared with the algorithms in existing literatures.",https://ieeexplore.ieee.org/document/8932445/,IEEE Journal on Selected Areas in Communications,Feb. 2020,ieeexplore
10.1109/ACCESS.2019.2925088,PMT: Opposition-Based Learning Technique for Enhancing Meta-Heuristic Performance,IEEE,Journals,"Meta-heuristic algorithms have shown promising performance in solving sophisticated real-world optimization problems. Nevertheless, many meta-heuristic algorithms are still suffering from a low convergence rate because of the poor balance between exploration (i.e., roaming new potential search areas) and exploitation (i.e., exploiting the existing neighbors). In some complex problems, the convergence rate can still be poor owing to becoming trapped in local optima. Addressing these issues, this research proposes a new general opposition-based learning (OBL) technique inspired by a natural phenomenon of parallel mirrors systems called the parallel mirrors technique (PMT). Like existing OBL-based approaches, the PMT generates new potential solutions based on the currently selected candidate. Unlike existing OBL-based techniques, the PMT generates more than one candidate in multiple solution-space directions. To evaluate the PMT's performance and adaptability, the PMT has been applied to four contemporary meta-heuristic algorithms, differential evolution (DE), particle swarm optimization (PSO), simulated annealing (SA), and whale optimization algorithm (WOA), to solve 15 well-known benchmark functions. The experimentally, the PMT shows promising results by accelerating the convergence rate against the original algorithms with the same number of fitness evaluations.",https://ieeexplore.ieee.org/document/8746627/,IEEE Access,2019,ieeexplore
10.1109/TMM.2017.2751966,PROVID: Progressive and Multimodal Vehicle Reidentification for Large-Scale Urban Surveillance,IEEE,Journals,"Compared with person reidentification, which has attracted concentrated attention, vehicle reidentification is an important yet frontier problem in video surveillance and has been neglected by the multimedia and vision communities. Since most existing approaches mainly consider the general vehicle appearance for reidentification while overlooking the distinct vehicle identifier, such as the license plate number, they attain suboptimal performance. In this paper, we propose PROVID, a PROgressive Vehicle re-IDentification framework based on deep neural networks. In particular, our framework not only utilizes the multimodality data in large-scale video surveillance, such as visual features, license plates, camera locations, and contextual information, but also considers vehicle reidentification in two progressive procedures: coarse-to-fine search in the feature domain, and near-to-distant search in the physical space. Furthermore, to evaluate our progressive search framework and facilitate related research, we construct the VeRi dataset, which is the most comprehensive dataset from real-world surveillance videos. It not only provides large numbers of vehicles with varied labels and sufficient cross-camera recurrences but also contains license plate numbers and contextual information. Extensive experiments on the VeRi dataset demonstrate both the accuracy and efficiency of our progressive vehicle reidentification framework.",https://ieeexplore.ieee.org/document/8036238/,IEEE Transactions on Multimedia,March 2018,ieeexplore
10.1109/TNNLS.2020.3041018,Pairwise Two-Stream ConvNets for Cross-Domain Action Recognition With Small Data,IEEE,Journals,"In this work, we target cross-domain action recognition (CDAR) in the video domain and propose a novel end-to-end pairwise two-stream ConvNets (<i>PTC</i>) algorithm for real-life conditions, in which only a few labeled samples are available. To cope with the limited training sample problem, we employ pairwise network architecture that can leverage training samples from a source domain and, thus, requires only a few labeled samples per category from the target domain. In particular, a frame self-attention mechanism and an adaptive weight scheme are embedded into the <i>PTC</i> network to adaptively combine the RGB and flow features. This design can effectively learn domain-invariant features for both the source and target domains. In addition, we propose a sphere boundary sample-selecting scheme that selects the training samples at the boundary of a class (in the feature space) to train the <i>PTC</i> model. In this way, a well-enhanced generalization capability can be achieved. To validate the effectiveness of our <i>PTC</i> model, we construct two CDAR data sets (<i>SDAI Action I</i> and <i>SDAI Action II</i>) that include indoor and outdoor environments; all actions and samples in these data sets were carefully collected from public action data sets. To the best of our knowledge, these are the first data sets specifically designed for the CDAR task. Extensive experiments were conducted on these two data sets. The results show that <i>PTC</i> outperforms state-of-the-art video action recognition methods in terms of both accuracy and training efficiency. It is noteworthy that when only two labeled training samples per category are used in the <i>SDAI Action I</i> data set, <i>PTC</i> achieves 21.9% and 6.8% improvement in accuracy over two-stream and temporal segment networks models, respectively. As an added contribution, the <i>SDAI Action I</i> and <i>SDAI Action II</i> data sets will be released to facilitate future research on the CDAR task.",https://ieeexplore.ieee.org/document/9288873/,IEEE Transactions on Neural Networks and Learning Systems,March 2022,ieeexplore
10.1109/ACCESS.2020.2967817,Patent Analytic Citation-Based VSM: Challenges and Applications,IEEE,Journals,"Patent citations are significant components of patents, which play a vital role in the implementation of patent analysis. However, most of the existed models only focus on the text of patents and do not realize that citations can remedy missing information in the text. A method for citation modeling in patent analysis is proposed to generate patent citation trees in this paper. Correspondingly, a specific neural network is designed for extracting abstract features in patent citation trees. Then, on the basis of extracted features, a new citation-based vector space model (CVSM) combining citations with text of the patent database is constructed for the subsequent applications. An experiment is conducted based on real patents of USPTO. The experimental results show that the proposed CVSM has good performances in several applications, which demonstrate the effectiveness of the proposed CVSM.",https://ieeexplore.ieee.org/document/8963731/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2018.2875677,Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record,IEEE,Journals,"The wide implementation of electronic health record (EHR) systems facilitates the collection of large-scale health data from real clinical settings. Despite the significant increase in adoption of EHR systems, these data remain largely unexplored, but present a rich data source for knowledge discovery from patient health histories in tasks, such as understanding disease correlations and predicting health outcomes. However, the heterogeneity, sparsity, noise, and bias in these data present many complex challenges. This complexity makes it difficult to translate potentially relevant information into machine learning algorithms. In this paper, we propose a computational framework, Patient2Vec, to learn an interpretable deep representation of longitudinal EHR data, which is personalized for each patient. To evaluate this approach, we apply it to the prediction of future hospitalizations using real EHR data and compare its predictive performance with baseline methods. Patient2Vec produces a vector space with meaningful structure, and it achieves an area under curve around 0.799, outperforming baseline methods. In the end, the learned feature importance can be visualized and interpreted at both the individual and population levels to bring clinical insights.",https://ieeexplore.ieee.org/document/8490816/,IEEE Access,2018,ieeexplore
10.1109/TNNLS.2015.2416353,Perception Evolution Network Based on Cognition Deepening ModelAdapting to the Emergence of New Sensory Receptor,IEEE,Journals,"The proposed perception evolution network (PEN) is a biologically inspired neural network model for unsupervised learning and online incremental learning. It is able to automatically learn suitable prototypes from learning data in an incremental way, and it does not require the predefined prototype number or the predefined similarity threshold. Meanwhile, being more advanced than the existing unsupervised neural network model, PEN permits the emergence of a new dimension of perception in the perception field of the network. When a new dimension of perception is introduced, PEN is able to integrate the new dimensional sensory inputs with the learned prototypes, i.e., the prototypes are mapped to a high-dimensional space, which consists of both the original dimension and the new dimension of the sensory inputs. In the experiment, artificial data and real-world data are used to test the proposed PEN, and the results show that PEN can work effectively.",https://ieeexplore.ieee.org/document/7097084/,IEEE Transactions on Neural Networks and Learning Systems,March 2016,ieeexplore
10.1109/ACCESS.2021.3071485,Perceptual Borderline for Balancing Multi-Class Spontaneous Emotional Data,IEEE,Journals,"Speech is a behavioural biometric signal that can provide important information to understand the human intends as well as their emotional status. The paper is centered on the speech-based identification of the seniors's emotional status during their interaction with a virtual agent playing the role of a health professional coach. Under real conditions, we can just identify a small set of task-dependent spontaneous emotions. The number of identified samples is largely different for each emotion, which results in an imbalanced dataset problem. This research proposes the dimensional model of emotions as a perceptual representation space alternative to the generally used acoustic one. The main contribution of the paper is the definition of a perceptual borderline for the oversampling of minority emotion classes in this space. This limit, based on arousal and valence criteria, leads to two methods of balancing the data: the Perceptual Borderline oversampling and the Perceptual Borderline SMOTE (Synthetic Minority Oversampling TEchnique). Both methods are implemented and compared to state-of-the-art approaches of Random oversampling and SMOTE. The experimental evaluation was carried out on three imbalanced datasets of spontaneous emotions acquired in human-machine scenarios in three different cultures: Spain, France and Norway. The emotion recognition results obtained by neural networks classifiers show that the proposed perceptual oversampling methods led to significant improvements when compared with the state-of-the art, for all scenarios and languages.",https://ieeexplore.ieee.org/document/9398699/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2019.2940381,Perceptual Vibration Hashing by Sub-Band Coding: An Edge Computing Method for Condition Monitoring,IEEE,Journals,"High data throughput during real-time vibration monitoring can easily lead to network congestion, insufficient data storage space, heavy computing burden, and high communication costs. As a new computing paradigm, edge computing is deemed to be a good solution to these problems. In this paper, perceptual hashing is proposed as an edge computing form, aiming not only to reduce the data dimensionality but also to extract and represent the machine condition information. A sub-band coding method based on wavelet packet transform, two-dimensional discrete cosine transform, and symbolic aggregate approximation is developed for perceptual vibration hashing. When the sub-band coding method is implemented on a monitoring terminal, the acquired kilobyte-long vibration signal can be transformed into a machine condition hash occupying only a few bytes. Therefore, the efficiency of condition monitoring can benefit from the compactness of the machine condition hash, while comparable diagnostic and prognostic results can still be achieved. The effectiveness of the developed method is verified with two benchmark bearing datasets. Considerations on practical condition monitoring applications are also presented.",https://ieeexplore.ieee.org/document/8832158/,IEEE Access,2019,ieeexplore
10.1109/TGRS.2020.3045790,Physically Constrained Transfer Learning Through Shared Abundance Space for Hyperspectral Image Classification,IEEE,Journals,"Hyperspectral image (HSI) classification is one of the most active research topics and has achieved promising results boosted by the recent development of deep learning. However, most state-of-the-art approaches tend to perform poorly when the training and testing images are on different domains, e.g., the source domain and target domain, respectively, due to the spectral variability caused by different acquisition conditions. Transfer learning-based methods address this problem by pretraining in the source domain and fine-tuning on the target domain. Nonetheless, a considerable amount of data on the target domain has to be labeled and nonnegligible computational resources are required to retrain the whole network. In this article, we propose a new transfer learning scheme to bridge the gap between the source and target domains by projecting the HSI data from the source and target domains into a shared abundance space based on their own physical characteristics. In this way, the domain discrepancy would be largely reduced such that the model trained on the source domain could be applied to the target domain without extra efforts for data labeling or network retraining. The proposed method is referred to as physically constrained transfer learning through shared abundance space (PCTL-SAS). Extensive experimental results demonstrate the superiority of the proposed method as compared to the state of the art. The success of this endeavor would largely facilitate the deployment of HSI classification for real-world sensing scenarios.",https://ieeexplore.ieee.org/document/9318553/,IEEE Transactions on Geoscience and Remote Sensing,Dec. 2021,ieeexplore
10.1109/TIM.2021.3092518,Pipeline Safety Early Warning by Multifeature-Fusion CNN and LightGBM Analysis of Signals From Distributed Optical Fiber Sensors,IEEE,Journals,"Energy pipelines are the backbones of global energy systems. Monitoring their safety and automatically identifying and locating third-party damage events are crucial to energy supply. However, most traditional methods lack in-depth consideration of distributed fiber signals and have not been tested on real-world long-distance pipelines, making it difficult to deploy them in operating long-distance pipelines. In this study, we utilize a novel real-time machine-learning method based on phase-sensitive optical time domain reflectometer technology to monitor the safety of oil and gas pipelines. Specifically, we build a multifeature-fusion convolutional neural network and LightGBM fusion model based on two novel complementary spatiotemporal features. The method was applied to a large amount of data collected from real-world oilgas transportation pipelines of the China National Petroleum Corporation. The proposed method could accurately locate and identify third-party damage events in real-time under conditions of strong noise and various types of system hardware, and could effectively handle signal drift in the time and space dimensions. Our methodology has been deployed at real long-distance energy pipeline sites and our work will contribute to energy pipeline safety and energy supply security. Furthermore, the proposed solution could be generalized to other fields, such as industrial inspection, measurement, and monitoring.",https://ieeexplore.ieee.org/document/9541184/,IEEE Transactions on Instrumentation and Measurement,2021,ieeexplore
10.1109/TBME.2018.2854632,Predicting Athlete Ground Reaction Forces and Moments From Spatio-Temporal Driven CNN Models,IEEE,Journals,"The accurate prediction of three-dimensional (3-D) ground reaction forces and moments (GRF/Ms) outside the laboratory setting would represent a watershed for on-field biomechanical analysis. To extricate the biomechanist's reliance on ground embedded force plates, this study sought to improve on an earlier partial least squares (PLS) approach by using deep learning to predict 3-D GRF/Ms from legacy marker based motion capture sidestepping trials, ranking multivariate regression of GRF/Ms from five convolutional neural network (CNN) models. In a possible first for biomechanics, tactical feature engineering techniques were used to compress space-time and facilitate fine-tuning from three pretrained CNNs, from which a model derivative of ImageNet called CaffeNet achieved the strongest average correlation to ground truth GRF/Ms <sub>r</sub>(F<sub>mean</sub>) 0.9881 and <sub>r</sub>(M<sub>mean</sub>) 0.9715 (<sub>r</sub>RMSE 4.31 and 7.04%). These results demonstrate the power of CNN models to facilitate real-world multivariate regression with practical application for spatio-temporal sports analytics.",https://ieeexplore.ieee.org/document/8408711/,IEEE Transactions on Biomedical Engineering,March 2019,ieeexplore
10.1109/TCST.2011.2180386,Prediction of Short-Term Traffic Variables Using Intelligent Swarm-Based Neural Networks,IEEE,Journals,"This brief presents an innovative algorithm integrated with particle swarm optimization and artificial neural networks to develop short-term traffic flow predictors, which are intended to provide traffic flow forecasting information for traffic management in order to reduce traffic congestion and improve mobility of transportation. The proposed algorithm aims to address the issues of development of short-term traffic flow predictors which have not been addressed fully in the current literature namely that: 1) strongly non-linear characteristics are unavoidable in traffic flow data; 2) memory space for implementation of short-term traffic flow predictors is limited; 3) specification of model structures for short-term traffic flow predictors which do not involve trial and error methods based on human expertise; and 4) adaptation to newly-captured, traffic flow data is required. The proposed algorithm was applied to forecast traffic flow conditions on a section of freeway in Western Australia, whose traffic flow information is newly-captured. These results clearly demonstrate the effectiveness of using the proposed algorithm for real-time traffic flow forecasting.",https://ieeexplore.ieee.org/document/6126004/,IEEE Transactions on Control Systems Technology,Jan. 2013,ieeexplore
10.1109/TPAMI.2006.193,Probabilistic fusion of stereo with color and contrast for bilayer segmentation,IEEE,Journals,"This paper describes models and algorithms for the real-time segmentation of foreground from background layers in stereo video sequences. Automatic separation of layers from color/contrast or from stereo alone is known to be error-prone. Here, color, contrast, and stereo matching information are fused to infer layers accurately and efficiently. The first algorithm, layered dynamic programming (LDP), solves stereo in an extended six-state space that represents both foreground/background layers and occluded regions. The stereo-match likelihood is then fused with a contrast-sensitive color model that is learned on-the-fly and stereo disparities are obtained by dynamic programming. The second algorithm, layered graph cut (LGC), does not directly solve stereo. Instead, the stereo match likelihood is marginalized over disparities to evaluate foreground and background hypotheses and then fused with a contrast-sensitive color model like the one used in LDP. Segmentation is solved efficiently by ternary graph cut. Both algorithms are evaluated with respect to ground truth data and found to have similar performance, substantially better than either stereo or color/contrast alone. However, their characteristics with respect to computational efficiency are rather different. The algorithms are demonstrated in the application of background substitution and shown to give good quality composite video output.",https://ieeexplore.ieee.org/document/1661549/,IEEE Transactions on Pattern Analysis and Machine Intelligence,Sept. 2006,ieeexplore
10.1109/ACCESS.2020.2966667,RCCM: Reinforce Cycle Cascade Model for Image Recognition,IEEE,Journals,"Due to the disadvantages that complex network structure, time-consuming training process and the insufficient feature extraction ability existing in deep structures and broad structures, Reinforce Cycle Cascade Model (RCCM) is proposed to offer a novel algorithm for image recognition. In RCCM, the cycle cascade model is used to extract the discriminative features with a small amount of time consumption, where the multi-layer cascade is used to extract low-level and advanced features layer by layer, which provides conditions for features evolution as well. In addition, the cycle mechanism is introduced to reinforce features gradually, which also lighten model complexity by adopting time update instead of space stack. Visualization maps prove the feasibility of RCCM, and the discussion of cascade layers shows the extension of our proposed algorithm. To demonstrate the performance of RCCM, we have conducted extensive experiments on some benchmark image datasets, and the results show that RCCM is significantly better than several state-of-the-art algorithms. Specially, no matter in the training process of testing phase, our method can achieve recognition in real-time even on common PC without GPU, and the maximum testing speed reaches 31073 samples per second. RCCM improves the efficiency and accuracy of image recognition significantly.",https://ieeexplore.ieee.org/document/8959225/,IEEE Access,2020,ieeexplore
10.1109/TKDE.2020.2982898,RHINE: Relation Structure-Aware Heterogeneous Information Network Embedding,IEEE,Journals,"Heterogeneous information network (HIN) embedding aims to learn the low-dimensional representations of nodes while preserving structures and semantics in HINs. Although most existing methods consider heterogeneous relations and achieve promising performance, they usually employ one single model for all relations without distinction, which inevitably restricts the capability of HIN embedding. In this paper, we argue that heterogeneous relations have different structural characteristics, and propose a novel Relation structure-aware HIN Embedding model, called RHINE. By exploring four real-world networks with thorough analysis, we present two structure-related measures which consistently distinguish heterogeneous relations into two categories: Affiliation Relations (ARs) and Interaction Relations (IRs). To respect the distinctive structural characteristics of relations, in RHINE, we propose different models specifically tailored to handle ARs and IRs, which can better capture the structures in HINs. Finally, we combine and optimize these models in a unified manner. Furthermore, considering that nodes connected via heterogeneous relations may have multi-aspect semantics and each relation focuses on one aspect, we introduce relation-specific projection matrices to learn node and relation embeddings in separate spaces rather than a common space, which can better preserve the semantics in HINs, referring to a new model RHINE-M. Experiments on four real-world datasets demonstrate that our models significantly outperform the state-of-the-art methods in four tasks.",https://ieeexplore.ieee.org/document/9050490/,IEEE Transactions on Knowledge and Data Engineering,1 Jan. 2022,ieeexplore
10.1109/TIE.2020.3048285,ROpenPose: A Rapider OpenPose Model for Astronaut Operation Attitude Detection,IEEE,Journals,"This article proposes a rapider OpenPose model (ROpenPose) to solve the posture detection problem of astronauts in a space capsule in a weightless environment. The ROpenPose model has three innovations as follows: 1) It uses MobileNets instead of VGG-19 to achieve lighter calculations while ensuring the accuracy of model recognition. 2) Three small convolution kernels replace the large convolution kernel of the original OpenPose, which significantly reduces the computational complexity of the model. 3) Through the parameter sharing of a convolution process, the original two-branch structure is changed to a single-branch structure, which obviously improves the calculation speed of the model. A residual network is proposed to suppress the hidden danger of gradient disappearance. The deployment of ROpenPose greatly improves astronauts detection efficiency while ensuring their high detection performance, and thereby realizing the real-time monitoring of their operation attitude. Experimental results show that ROpenPose runs at speed higher than and detection performance comparable to a number of the existing state-of-the-art models.",https://ieeexplore.ieee.org/document/9316908/,IEEE Transactions on Industrial Electronics,Jan. 2022,ieeexplore
10.1109/ACCESS.2021.3068998,Random Satisfiability: A Higher-Order Logical Approach in Discrete Hopfield Neural Network,IEEE,Journals,"A conventional systematic satisfiability logic suffers from a nonflexible logical structure that leads to a lack of interpretation. To resolve this problem, the advantage of introducing nonsystematic satisfiability logic is important to improve the flexibility of the logical structure. This paper proposes Random 3 Satisfiability (RAN3SAT) with three types of logical combinations ( k = 1, 3, k = 2, 3, and k = 1, 2, 3) to report the behaviors of multiple logical structures. The different types of RAN3SAT enforced with Discrete Hopfield Neural Network (DHNN) are included with benchmark searching techniques, such as Exhaustive Search algorithm. Additionally, to strengthen and certify the behavior of the proposed model, we extensively conducted several performance evaluation metrics with a specific number of neurons. In particular, the experimental results revealed that RAN3SAT was able to be implemented in DHNN, and each logical combination has its characteristics. Nonetheless, RAN3SAT provides more neuron variations in the whole solution space. The proposed model can also be applied in real-world applications such as the logic mining approach since RAN3SAT consists of various logic combinations that behave as input language to transform raw data into informative output.",https://ieeexplore.ieee.org/document/9387311/,IEEE Access,2021,ieeexplore
10.1109/TCSI.2004.827654,Reaction-diffusion navigation robot control: from chemical to VLSI analogic processors,IEEE,Journals,"We introduce a new methodology and experimental implementations for real-time wave-based robot navigation in a complex, dynamically changing environment. The main idea behind the approach is to consider the robot arena as an excitable medium, in which moving objects-obstacles and the target-are represented by sites of autowave generation: the target generates attractive waves, while the obstacles repulsive ones. The moving robot detects traveling and colliding wave fronts and uses the information about dynamics of the autowaves to adapt its direction of collision-free motion toward the target. This approach allows us to achieve a highly adaptive robot behavior and thus an optimal path along which the robot reaches the target while avoiding obstacles. At the computational and experimental levels, we adopt principles of computation in reaction-diffusion (RD) nonlinear active media. Nonlinear media where autowaves are used for information processing purposes can therefore be considered as RD computing devices. In this paper, we design and experiment with three types of RD processors: experimental and computational Belousov-Zhabotinsky chemical processor, computational CNN processor, and experimental RD-CNN very large-scale integration chip-the complex analog and logic computing engine (CACE1k). We demonstrate how to experimentally implement robot navigation using space-time snapshots of active chemical medium and how to overcome low-speed limitation of this ""wetware"" implementation in CNN-based silicon processors.",https://ieeexplore.ieee.org/document/1296805/,IEEE Transactions on Circuits and Systems I: Regular Papers,May 2004,ieeexplore
10.1109/ACCESS.2019.2948017,Real Time Dynamic Magnetic Resonance Imaging Via Dictionary Learning and Combined Fourier Transform,IEEE,Journals,"Real time dynamic magnetic resonance imaging (dMRI) requires that the image acquisition and reconstruction are carried out simultaneously and the reconstruction speed catches up with imaging speed. In this paper, a novel compressed sensing (CS) reconstruction algorithm for real time dynamic MRI is proposed. The first frame with more k-space measurements is reconstructed precisely as the reference image. Different from previous methods who start their reconstructions from zero-filled k-space measurements, a Combined Fourier Transform (CFT) algorithm is implemented in our method, which can dynamically aggregate the k-space measurements from previous sampled frames to create a highly accurate predictive image for the current frame. We then combine the CFT algorithm with a 3D path-based dictionary leaning algorithm, which is named as DLCFT in our work for fast real time dMRI reconstruction. The proposed algorithm is compared with four state-of-the-art online and offline methods on two real and complex perfusion MR sequences and a real functional brain MR sequence. Experimental results show that the proposed algorithm outperforms these methods with faster convergence and higher reconstruction accuracy.",https://ieeexplore.ieee.org/document/8873554/,IEEE Access,2019,ieeexplore
10.1109/TCYB.2020.2982947,Real-Time 3-D Semantic Scene Parsing With LiDAR Sensors,IEEE,Journals,"This article proposes a novel deep-learning framework, called RSSP, for real-time 3-D scene understanding with LiDAR sensors. To this end, we introduce new sparse strided operations based on the sparse tensor representation of point clouds. Compared with conventional convolution operations, the time and space complexity of our sparse strided operations are proportional to the number of occupied voxels <inline-formula> <tex-math notation=""LaTeX"">${N}$ </tex-math></inline-formula> rather than the input spatial size <inline-formula> <tex-math notation=""LaTeX"">${r} ^{3}$ </tex-math></inline-formula> (often <i>N</i> <inline-formula> <tex-math notation=""LaTeX"">$\ll $ </tex-math></inline-formula> <i>r</i> <sup>3</sup> for LiDAR data). This enables our method to process point clouds at high resolutions (e.g., 2048<sup>3</sup>) with a high speed (130 ms for classifying a single frame from Velodyne HDL-64). The main structure includes a CNN model built upon our sparse strided operations and a conditional random field (CRF) model to impose spatial consistency on the final predictions. A highly parallel implementation of our system is presented for both CPU-GPU and CPU-only environments. The efficiency and effectiveness of our approach are demonstrated on two public datasets (Semantic3D.net and KITTI). The experimental results and benchmark tests show that our system can be effectively applied for online 3-D data analyses with comparable or better accuracy than the state-of-the-art methods.",https://ieeexplore.ieee.org/document/9072337/,IEEE Transactions on Cybernetics,March 2022,ieeexplore
10.1162/089976604323057443,Real-Time Computation at the Edge of Chaos in Recurrent Neural Networks,MIT Press,Journals,"Depending on the connectivity, recurrent networks of simple computational units can show very different types of dynamics, ranging from totally ordered to chaotic. We analyze how the type of dynamics (ordered or chaotic) exhibited by randomly connected networks of threshold gates driven by a time-varying input signal depends on the parameters describing the distribution of the connectivity matrix. In particular, we calculate the critical boundary in parameter space where the transition from ordered to chaotic dynamics takes place. Employing a recently developed framework for analyzing real-time computations, we show that only near the critical boundary can such networks perform complex computations on time series. Hence, this result strongly supports conjectures that dynamical systems that are capable of doing complex computational tasks should operate near the edge of chaos, that is, the transition from ordered to chaotic dynamics.",https://ieeexplore.ieee.org/document/6790172/,Neural Computation,1 July 2004,ieeexplore
10.1109/TIFS.2007.902037,Real-Time Face Detection and Motion Analysis With Application in Liveness Assessment,IEEE,Journals,"A robust face detection technique along with mouth localization, processing every frame in real time (video rate), is presented. Moreover, it is exploited for motion analysis onsite to verify ""liveness"" as well as to achieve lip reading of digits. A methodological novelty is the suggested quantized angle features (""quangles"") being designed for illumination invariance without the need for preprocessing (e.g., histogram equalization). This is achieved by using both the gradient direction and the double angle direction (the structure tensor angle), and by ignoring the magnitude of the gradient. Boosting techniques are applied in a quantized feature space. A major benefit is reduced processing time (i.e., that the training of effective cascaded classifiers is feasible in very short time, less than 1 h for data sets of order 10<sup>4</sup>). Scale invariance is implemented through the use of an image scale pyramid. We propose ""liveness"" verification barriers as applications for which a significant amount of computation is avoided when estimating motion. Novel strategies to avert advanced spoofing attempts (e.g., replayed videos which include person utterances) are demonstrated. We present favorable results on face detection for the YALE face test set and competitive results for the CMU-MIT frontal face test set as well as on ""liveness"" verification barriers.",https://ieeexplore.ieee.org/document/4291551/,IEEE Transactions on Information Forensics and Security,Sept. 2007,ieeexplore
10.1109/TCYB.2013.2275291,Real-Time Multiple Human Perception With Color-Depth Cameras on a Mobile Robot,IEEE,Journals,"The ability to perceive humans is an essential requirement for safe and efficient human-robot interaction. In real-world applications, the need for a robot to interact in real time with multiple humans in a dynamic, 3-D environment presents a significant challenge. The recent availability of commercial color-depth cameras allow for the creation of a system that makes use of the depth dimension, thus enabling a robot to observe its environment and perceive in the 3-D space. Here we present a system for 3-D multiple human perception in real time from a moving robot equipped with a color-depth camera and a consumer-grade computer. Our approach reduces computation time to achieve real-time performance through a unique combination of new ideas and established techniques. We remove the ground and ceiling planes from the 3-D point cloud input to separate candidate point clusters. We introduce the novel information concept, depth of interest, which we use to identify candidates for detection, and that avoids the computationally expensive scanning-window methods of other approaches. We utilize a cascade of detectors to distinguish humans from objects, in which we make intelligent reuse of intermediary features in successive detectors to improve computation. Because of the high computational cost of some methods, we represent our candidate tracking algorithm with a decision directed acyclic graph, which allows us to use the most computationally intense techniques only where necessary. We detail the successful implementation of our novel approach on a mobile robot and examine its performance in scenarios with real-world challenges, including occlusion, robot motion, nonupright humans, humans leaving and reentering the field of view (i.e., the reidentification challenge), human-object and human-human interaction. We conclude with the observation that the incorporation of the depth information, together with the use of modern techniques in new ways, we are able to create an accurate system for real-time 3-D perception of humans by a mobile robot.",https://ieeexplore.ieee.org/document/6583249/,IEEE Transactions on Cybernetics,Oct. 2013,ieeexplore
10.1109/JPHOT.2022.3147844,Real-Time Optical-Wireless Video Surveillance System for High Visual-Fidelity Underwater Monitoring,IEEE,Journals,"Real-time, dynamic, and visual monitoring of underwater scenes would become an important research topic with the development of underwater wireless optical communication (UWOC), autonomous underwater vehicle, and video fusion technologies. To this end, we developed a UWOC-based 2K real-time digital video surveillance prototype named AquaF-seer. Using a light-emitting diode and an avalanche photodetector based diffused line-of-sight UWOC system, real-time video transmission with a high resolution of 1920  1080 pixels is achieved over various channels, i.e., 1.5-m free space channel, pure water channel, pure water channel with 1.53-mL/s bubble-induced turbulence, pure water channel with 42.40-mL/s bubble-induced turbulence, simulated pure sea water channel, and coastal ocean water channel. Moreover, 46-m and 5-m video monitoring are implemented in free space and an outdoor diving pool. It indicates the reliability of the prototype, which is the first step to realize underwater visual monitoring in future human-robot interaction applications.",https://ieeexplore.ieee.org/document/9706307/,IEEE Photonics Journal,April 2022,ieeexplore
10.1109/TNSRE.2021.3138297,Real-Time Optimization of Retinal Ganglion Cell Spatial Activity in Response to Epiretinal Stimulation,IEEE,Journals,"Retinal prostheses aim to improve visual perception in patients blinded by photoreceptor degeneration. However, shape and letter perception with these devices is currently limited due to low spatial resolution. Previous research has shown the retinal ganglion cell (RGC) spatial activity and phosphene shapes can vary due to the complexity of retina structure and electrode-retina interactions. Visual percepts elicited by single electrodes differ in size and shapes for different electrodes within the same subject, resulting in interference between phosphenes and an unclear image. Prior work has shown that better patient outcomes correlate with spatially separate phosphenes. In this study we use calcium imaging, <i>in vitro</i> retina, neural networks (NN), and an optimization algorithm to demonstrate a method to iteratively search for optimal stimulation parameters that create focal RGC activation. Our findings indicate that we can converge to stimulation parameters that result in focal RGC activation by sampling less than 1/3 of the parameter space. A similar process implemented clinically can reduce time required for optimizing implant operation and enable personalized fitting of retinal prostheses.",https://ieeexplore.ieee.org/document/9662355/,IEEE Transactions on Neural Systems and Rehabilitation Engineering,2021,ieeexplore
10.1109/TCSVT.2016.2515338,Real-Time Traffic Light Recognition Based on Smartphone Platforms,IEEE,Journals,"Traffic light recognition is of great significance for driver assistance or autonomous driving. In this paper, a traffic light recognition system based on smartphone platforms is proposed. First, an ellipsoid geometry threshold model in Hue Saturation Lightness color space is built to extract interesting color regions. These regions are further screened with a postprocessing step to obtain candidate regions that satisfy both color and brightness conditions. Second, a new kernel function is proposed to effectively combine two heterogeneous features, histograms of oriented gradients and local binary pattern, which is used to describe the candidate regions of traffic light. A kernel extreme learning machine (K-ELM) is designed to validate these candidate regions and simultaneously recognize the phase and type of traffic lights. Furthermore, a spatial-temporal analysis framework based on a finite-state machine is introduced to enhance the reliability of the recognition of the phase and type of traffic light. Finally, a prototype of the proposed system is implemented on a Samsung Note 3 smartphone. To achieve a real-time computational performance of the proposed K-ELM, a CPU-GPU fusion-based approach is adopted to accelerate the execution. The experimental results on different road environments show that the proposed system can recognize traffic lights accurately and rapidly.",https://ieeexplore.ieee.org/document/7373593/,IEEE Transactions on Circuits and Systems for Video Technology,May 2017,ieeexplore
10.1109/TNN.2005.849817,"Recognizing partially occluded, expression variant faces from single training image per person with SOM and soft k-NN ensemble",IEEE,Journals,"Most classical template-based frontal face recognition techniques assume that multiple images per person are available for training, while in many real-world applications only one training image per person is available and the test images may be partially occluded or may vary in expressions. This paper addresses those problems by extending a previous local probabilistic approach presented by Martinez, using the self-organizing map (SOM) instead of a mixture of Gaussians to learn the subspace that represented each individual. Based on the localization of the training images, two strategies of learning the SOM topological space are proposed, namely to train a single SOM map for all the samples and to train a separate SOM map for each class, respectively. A soft k nearest neighbor (soft k-NN) ensemble method, which can effectively exploit the outputs of the SOM topological space, is also proposed to identify the unlabeled subjects. Experiments show that the proposed method exhibits high robust performance against the partial occlusions and variant expressions.",https://ieeexplore.ieee.org/document/1461430/,IEEE Transactions on Neural Networks,July 2005,ieeexplore
10.1109/TKDE.2016.2593720,Recommendation for Repeat Consumption from User Implicit Feedback,IEEE,Journals,"Recommender system has been studied as a useful tool to discover novel items for users while fitting their personalized interest. Thus, the previously consumed items are usually out of consideration due to the lack of novelty. However, as time elapses, people may forget those previously consumed and preferred items which could become novel again. Meanwhile, repeat consumption accounts for a major portion of people's observed activities; examples include: eating regularly at a same restaurant, or repeatedly listening to the same songs. Therefore, we believe that recommending repeat consumption will have a real utility at certain times. In this paper, we formulate the problem of recommendation for repeat consumption with user implicit feedback. A time-sensitive personalized pairwise ranking (TS-PPR) method based on user behavioral features is proposed to address this problem. The proposed method factorizes the temporal user-item interactions via learning the mappings from the behavioral features in observable space to the preference features in latent space, and combines users' static and dynamic preferences together in recommendation. An empirical study on real-world data sets shows encouraging results.",https://ieeexplore.ieee.org/document/7518642/,IEEE Transactions on Knowledge and Data Engineering,1 Nov. 2016,ieeexplore
10.1109/JPROC.2015.2396000,Reconfigurable Antennas: Design and Applications,IEEE,Journals,"The advancement in wireless communications requires the integration of multiple radios into a single platform to maximize connectivity. In this paper, the design process of reconfigurable antennas is discussed. Reconfigurable antennas are proposed to cover different wireless services that operate over a wide frequency range. They show significant promise in addressing new system requirements. They exhibit the ability to modify their geometries and behavior to adapt to changes in surrounding conditions. Reconfigurable antennas can deliver the same throughput as a multiantenna system. They use dynamically variable and adaptable single-antenna geometry without increasing the real estate required to accommodate multiple antennas. The optimization of reconfigurable antenna design and operation by removing unnecessary redundant switches to alleviate biasing issues and improve the system's performance is discussed. Controlling the antenna reconfiguration by software, using Field Programmable Gate Arrays (FPGAs) or microcontrollers is introduced herein. The use of Neural Networks and its integration with graph models on programmable platforms and its effect on the operation of reconfigurable antennas is presented. Finally, the applications of reconfigurable antennas for cognitive radio, Multiple Input Multiple Output (MIMO) channels, and space applications are highlighted.",https://ieeexplore.ieee.org/document/7086418/,Proceedings of the IEEE,March 2015,ieeexplore
10.1109/TSP.2005.850339,Recursive EM and SAGE-inspired algorithms with application to DOA estimation,IEEE,Journals,"This paper is concerned with recursive estimation using augmented data. We study two recursive procedures closely linked with the well-known expectation and maximization (EM) and space alternating generalized EM (SAGE) algorithms. Unlike iterative methods, the recursive EM and SAGE-inspired algorithms give a quick update on estimates given new data. Under mild conditions, estimates generated by these procedures are strongly consistent and asymptotically normally distributed. These mathematical properties are valid for a broad class of problems. When applied to direction of arrival (DOA) estimation, the recursive EM and SAGE-inspired algorithms lead to a very simple and fast implementation of the maximum-likelihood (ML) method. The most complicated computation in each recursion is inversion of the augmented information matrix. Through data augmentation, this matrix is diagonal and easy to invert. More importantly, there is no search in such recursive procedures. Consequently, the computational time is much less than that associated with existing numerical methods for finding ML estimates. This feature greatly increases the potential of the ML approach in real-time processing. Numerical experiments show that both algorithms provide good results with low computational cost.",https://ieeexplore.ieee.org/document/1468463/,IEEE Transactions on Signal Processing,Aug. 2005,ieeexplore
10.1109/ACCESS.2020.3037348,Reducing Congestion in an Intelligent Traffic System With Collaborative and Adaptive Signaling on the Edge,IEEE,Journals,"The advancements in Edge computing have paved the way for deep learning in real-time systems. One of the beneficiaries is an adaptive traffic control system that responds to real-time traffic observations by governing the signal phase and timings. Reinforcement Learning (RL) is extensively utilized in the literature in order to decrease traffic congestion in a road network. However, most of the previous works leverage centralized and cloud-based RL due to the computational complexity of underlying deep neural networks (DNN). Therefore, a persistent challenge towards adopting Edge learning is in devising a Multi-Agent RL in which agents are simplified, and their state spaces are localized but they perform comparable to the centralized RL. This article presents a Collaborative and Adaptive Signaling on the Edge (CASE), a novel Multi-Agent RL approach to control the traffic signals' phase and timing. Each signalized intersection in the road network is provided with an Edge Learning Platform which hosts an RL-Agent that observes local traffic states and learns an optimum signal policy. Moreover, CASE allows collaboration among RL-Agents by sharing their signal phase and timings to achieve convergence and performance. This collaboration is limited to one's direct neighbours only to minimize the computational complexity. We performed rigorous evaluations in terms of the choice of RL methods and their state space/reward and found that our collaborative state-space has resulted in a performance comparable to a centralized RL yet with a cost similar to the decentralized RL. Finally, a performance comparison of the CASE controller ported to the state-of-the-art Edge learning platforms is presented in this article. The results show that the proposed CASE controller can achieve real-time performance when ported to a general-purpose GPU-based platform. This arrangement achieves more than 8 times improvement in computational time over conventional embedded platforms.",https://ieeexplore.ieee.org/document/9257072/,IEEE Access,2020,ieeexplore
10.1109/TNN.2006.886854,Reducing and Filtering Point Clouds With Enhanced Vector Quantization,IEEE,Journals,"Modern scanners are able to deliver huge quantities of three-dimensional (3-D) data points sampled on an object's surface, in a short time. These data have to be filtered and their cardinality reduced to come up with a mesh manageable at interactive rates. We introduce here a novel procedure to accomplish these two tasks, which is based on an optimized version of soft vector quantization (VQ). The resulting technique has been termed enhanced vector quantization (EVQ) since it introduces several improvements with respect to the classical soft VQ approaches. These are based on computationally expensive iterative optimization; local computation is introduced here, by means of an adequate partitioning of the data space called hyperbox (HB), to reduce the computational time so as to be linear in the number of data points N, saving more than 80% of time in real applications. Moreover, the algorithm can be fully parallelized, thus leading to an implementation that is sublinear in N. The voxel side and the other parameters are automatically determined from data distribution on the basis of the Zador's criterion. This makes the algorithm completely automatic. Because the only parameter to be specified is the compression rate, the procedure is suitable even for nontrained users. Results obtained in reconstructing faces of both humans and puppets as well as artifacts from point clouds publicly available on the web are reported and discussed, in comparison with other methods available in the literature. EVQ has been conceived as a general procedure, suited for VQ applications with large data sets whose data space has relatively low dimensionality",https://ieeexplore.ieee.org/document/4049818/,IEEE Transactions on Neural Networks,Jan. 2007,ieeexplore
10.1109/TSE.2020.2979701,Reinforcement-Learning-Guided Source Code Summarization Using Hierarchical Attention,IEEE,Journals,"Code summarization (aka comment generation) provides a high-level natural language description of the function performed by code, which can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, the state-of-the-art approaches follow an encoder-decoder framework which encodes source code into a hidden space and later decodes it into a natural language space. Such approaches suffer from the following drawbacks: (a) they are mainly input by representing code as a sequence of tokens while ignoring code hierarchy; (b) most of the encoders only input simple features (e.g., tokens) while ignoring the features that can help capture the correlations between comments and code; (c) the decoders are typically trained to predict subsequent words by maximizing the likelihood of subsequent ground truth words, while in real world, they are excepted to generate the entire word sequence from scratch. As a result, such drawbacks lead to inferior and inconsistent comment generation accuracy. To address the above limitations, this paper presents a new code summarization approach using hierarchical attention network by incorporating multiple code features, including type-augmented abstract syntax trees and program control flows. Such features, along with plain code sequences, are injected into a deep reinforcement learning (DRL) framework (e.g., actor-critic network) for comment generation. Our approach assigns weights (pays attention) to tokens and statements when constructing the code representation to reflect the hierarchical code structure under different contexts regarding code features (e.g., control flows and abstract syntax trees). Our reinforcement learning mechanism further strengthens the prediction results through the actor network and the critic network, where the actor network provides the confidence of predicting subsequent words based on the current state, and the critic network computes the reward values of all the possible extensions of the current state to provide global guidance for explorations. Eventually, we employ an advantage reward to train both networks and conduct a set of experiments on a real-world dataset. The experimental results demonstrate that our approach outperforms the baselines by around 22 to 45 percent in BLEU-1 and outperforms the state-of-the-art approaches by around 5 to 60 percent in terms of S-BLEU and C-BLEU.",https://ieeexplore.ieee.org/document/9031440/,IEEE Transactions on Software Engineering,1 Jan. 2022,ieeexplore
10.1109/JSEN.2020.3035057,Remote Appliance Load Monitoring and Identification in a Modern Residential System With Smart Meter Data,IEEE,Journals,"In this article, an innovative procedure for residential electrical load monitoring applicable to smart meters is proposed based on a modified decisive multi-objective optimization. In this procedure, different load features are extracted from household loads for optimization-based monitoring. Each load monitoring feature reflects an objective function, which is also simultaneously minimized using modified Artificial Bee Colony (ABC) algorithm. This technique is not heavily dependent on high amount of training data which is essential for the most machine learning techniques with some modification in choosing the initial search space. The proposed technique uses with real life raw data using low sampling rates. An Internet of Things (IoT) based approach is also implemented for remote monitoring of connected loads in the system along with monitoring. The proposed technique is user friendly, requires only load signature data for verification purpose which can be easily extracted. The paper demonstrates event-based appliance load monitoring and comparison with benchmark dataset which shows marked improvement over existing state-of-the-art-techniques.",https://ieeexplore.ieee.org/document/9245531/,IEEE Sensors Journal,"15 Feb.15, 2021",ieeexplore
10.1109/TPAMI.2012.82,Removing Atmospheric Turbulence via Space-Invariant Deconvolution,IEEE,Journals,"To correct geometric distortion and reduce space and time-varying blur, a new approach is proposed in this paper capable of restoring a single high-quality image from a given image sequence distorted by atmospheric turbulence. This approach reduces the space and time-varying deblurring problem to a shift invariant one. It first registers each frame to suppress geometric deformation through B-spline-based nonrigid registration. Next, a temporal regression process is carried out to produce an image from the registered frames, which can be viewed as being convolved with a space invariant near-diffraction-limited blur. Finally, a blind deconvolution algorithm is implemented to deblur the fused image, generating a final output. Experiments using real data illustrate that this approach can effectively alleviate blur and distortions, recover details of the scene, and significantly improve visual quality.",https://ieeexplore.ieee.org/document/6178259/,IEEE Transactions on Pattern Analysis and Machine Intelligence,Jan. 2013,ieeexplore
10.1109/TNNLS.2020.3001377,Reservoir Computing Approaches for Representation and Classification of Multivariate Time Series,IEEE,Journals,"Classification of multivariate time series (MTS) has been tackled with a large variety of methodologies and applied to a wide range of scenarios. Reservoir computing (RC) provides efficient tools to generate a vectorial, fixed-size representation of the MTS that can be further processed by standard classifiers. Despite their unrivaled training speed, MTS classifiers based on a standard RC architecture fail to achieve the same accuracy of fully trainable neural networks. In this article, we introduce the reservoir model space, an unsupervised approach based on RC to learn vectorial representations of MTS. Each MTS is encoded within the parameters of a linear model trained to predict a low-dimensional embedding of the reservoir dynamics. Compared with other RC methods, our model space yields better representations and attains comparable computational performance due to an intermediate dimensionality reduction procedure. As a second contribution, we propose a modular RC framework for MTS classification, with an associated open-source Python library. The framework provides different modules to seamlessly implement advanced RC architectures. The architectures are compared with other MTS classifiers, including deep learning models and time series kernels. Results obtained on the benchmark and real-world MTS data sets show that RC classifiers are dramatically faster and, when implemented using our proposed representation, also achieve superior classification accuracy.",https://ieeexplore.ieee.org/document/9127499/,IEEE Transactions on Neural Networks and Learning Systems,May 2021,ieeexplore
10.1109/TII.2021.3086080,Resilient Operation of Distribution Grids Using Deep Reinforcement Learning,IEEE,Journals,"This article utilizes deep reinforcement learning to develop an intelligent resilience controller (IRC) that devises fast real-time operation decisions to strategically dispatch distributed generation and energy storage units for restoring power to customers after sudden outages. The proposed IRC learns the failure development pattern of uncertain high-impact events and is able to explore a large action space in the partially observable state space of distribution grids under widespread outages. A spatiotemporal hurricane impact analysis model is presented as an example of uncertain high-impact events, and its parameters are used in training the IRC model and preparing it for similar events. In the proposed model, the distribution grid operation under uncertainty is modeled as a Markov decision process (MDP), and actions taken by the operator are rewarded based on operation costs. Since the number of distributed energy resources can be significant, the scalability issue of the method is addressed by reformulating the problem as a sequential MDP. The proposed model is implemented on a test distribution grid undergoing a hurricane, and its performance is compared with common operation strategies, indicating the superiority of the proposed model in terms of reduced operation cost and close to zero running time. Further analysis shows the adaptability of the proposed model to hurricanes of various intensities.",https://ieeexplore.ieee.org/document/9445634/,IEEE Transactions on Industrial Informatics,March 2022,ieeexplore
10.1109/ACCESS.2020.2968131,Resonance - An Intelligence Analysis Framework for Social Connection Inference via Mining Co-Occurrence Patterns Over Multiplex Trajectories,IEEE,Journals,"With the rapid development of the Internet of Things(IoT), in the last decades, law enforcement agencies have deployed extensive sensor networks for public safety purposes. Diverse kinds of trajectories from the sensor networks provide an unprecedented opportunity for intelligence analysis. The geographic co-movement pattern has rarely been used by the police force to infer social connections, although it has been prevalent in other fields. The previous studies have mainly focused on a singular form of trajectories with exact co-locations, and the spread of the co-locations is over-looked. In this paper, we propose a novel framework for detecting co-occurrence patterns over multiplex trajectories. Firstly, We constructed the foundation for the discovery of co-occurrence events, namely space-time resonance honeycomb. It consists of multiple polygonal zones over sensor networks. Secondly, we transform all trajectories into a series of space-time prisms, and co-location activities are recorded using a sliding window approach. Thirdly, we propose a novel feature: Geo-Spread, which captures the extent of the co-location spread. In the end, we combine multiple features and employ Random Forest to predict social connections. We conduct extensive experiments on both the public dataset and the real-world surveillance dataset. Experiment results on all datasets prove the effectiveness of the proposed framework by outperforming the state-of-the-art methods.",https://ieeexplore.ieee.org/document/8963942/,IEEE Access,2020,ieeexplore
10.1109/TNSE.2021.3054583,Resource-Constrained Neural Architecture Search on Edge Devices,IEEE,Journals,"The performance requirement of deep learning inevitably brings up with the expense of high computational complexity and memory requirements, to make it problematic for the deployment on resource-constrained devices. Edge computing, which distributedly organizes the computing node close to the data source and end-device, provides a feasible way to tackle the high-efficiency demand and substantial computational load. Whereas given edge device is resource-constrained and energy-sensitive, designing effective neural network architecture for specific edge device is urgent in the sense that deploys the deep learning application by the edge computing solution. Undoubtedly manually design the high-performing neural architectures is burdensome, let alone taking account of the resource-constraint for the specific platform. Fortunately, the success of Neural Architecture Search techniques come up with hope recently. This paper dedicates to directly employ multi-objective NAS on the resource-constrained edge devices. We first propose the framework of multi-objective NAS on edge device, which comprehensively considers the performance and real-world efficiency. Our improved MobileNet-V2 search space also strikes the scalability and practicality, so that a series of Pareto-optimal architectures are received. Benefits from the directness and specialization during search procedure, our experiment on JETSON NANO shows the comparable result with the state-of-the-art models on ImageNet.",https://ieeexplore.ieee.org/document/9336306/,IEEE Transactions on Network Science and Engineering,1 Jan.-Feb. 2022,ieeexplore
10.1109/ACCESS.2021.3061634,RnR: Retrieval and Reprojection Learning Model for Camera Localization,IEEE,Journals,"Camera localization is an essential technique in many applications, such as robot navigation, mixed reality, and unmanned vehicle. We are committed to solving the problem of predicting the 6-DoF pose of cameras from a single color image in a given three-Dimensional (3D) environment. In this paper, we proposed a robust learning model for it. Basically, our proposed methodology consists of two steps: image retrieval and space reprojection. The former is in charge of simultaneous localization and mapping based on pre-captured reference images that rely on the correspondence between pixel points and scene coordinates; whereas the latter carries out camera calibration between the 2D image plane and the 3D scene. Given a two-Dimensional (2D) image, the initial localization is accomplished rapidly by matching a reference image using Siamese networks. More precise localization is achieved by camera calibration between the 2D image and the 3D scene using a fully convolutional network. The experimental results on the public dataset show that our model is more robust and expandable than the previous methods. At the end of this paper, we also apply the system to Unmanned Aerial Vehicle (UAV) localization and achieve good results.",https://ieeexplore.ieee.org/document/9361658/,IEEE Access,2021,ieeexplore
10.1109/TIP.2006.877500,Robust and Efficient Image Alignment Based on Relative Gradient Matching,IEEE,Journals,"In this paper, we present a robust image alignment algorithm based on matching of relative gradient maps. This algorithm consists of two stages; namely, a learning-based approximate pattern search and an iterative energy-minimization procedure for matching relative image gradient. The first stage finds some candidate poses of the pattern from the image through a fast nearest-neighbor search of the best match of the relative gradient features computed from training database of feature vectors, which are obtained from the synthesis of the geometrically transformed template image with the transformation parameters uniformly sampled from a given transformation parameter space. Subsequently, the candidate poses are further verified and refined by matching the relative gradient images through an iterative energy-minimization procedure. This approach based on the matching of relative gradients is robust against nonuniform illumination variations. Experimental results on both simulated and real images are shown to demonstrate superior efficiency and robustness of the proposed algorithm over the conventional normalized correlation method",https://ieeexplore.ieee.org/document/1703584/,IEEE Transactions on Image Processing,Oct. 2006,ieeexplore
10.1109/JIOT.2018.2812210,SDCoR: Software Defined Cognitive Routing for Internet of Vehicles,IEEE,Journals,"The Internet of Vehicles (IoV) is a subapplication of the Internet of Things in the automotive field. Large amounts of sensor data require to be transferred in real-time. Most of the routing protocols are specifically targeted to specific situations in IoV. But communication environment of IoV usually changes in the space-time dimension. Unfortunately, the traditional vehicular networks cannot select the optimal routing policy when facing the dynamic environment, due to the lack of abilities of sensing the environment and learning the best strategy. Sensing and learning constitute two key steps of the cognition procedure. Thus, in this paper, we present a software defined cognitive network for IoV (SDCIV), in which reinforcement learning and software defined network technology are considered for IoV to achieve cognitive capability. To the best of our knowledge, this paper is the first one that can give the optimal routing policy adaptively through sensing and learning from the environment of IoV. We perform experiments on a real vehicular dataset to validate the effectiveness and feasibility of the proposed algorithm. Results show that our algorithm achieves better performance than several typical protocols in IoV. We also show the feasibility and effectiveness of our proposed SDCIV.",https://ieeexplore.ieee.org/document/8306876/,IEEE Internet of Things Journal,Oct. 2018,ieeexplore
10.1109/TCC.2019.2944823,SDN-Based Traffic Matrix Estimation in Data Center Networks through Large Size Flow Identification,IEEE,Journals,"Software defined networking (SDN) with separated control plane and data plane brings new opportunities for traffic measurement in data center networks. However, in the SDN-enabled switches, available TCAM (Ternary Content Addressable Memory) resources for traffic measurement are limited. Thus, it is necessary to utilize traffic matrix (TM) estimation to derive a hybrid network monitoring scheme through combining the partial direct measurement offered by SDN with some inference techniques. Although large size flows play an important role in improving TM estimation accuracy, directly monitoring each flow and finding out large size flows consume massive channel bandwidth resource between control plane and data plane. Therefore, in this paper, we identify large size flows from multiple historical TMs instead of monitoring each flow. First, we analyze multiple historical TMs and observe that origin-to-destination (OD) pair whose flow size is selected as large size flow at last time slot is most likely to be selected for per-flow monitoring at next time slot, so these OD pairs are identified by gradient boosting machine and are directly regarded as sampled OD pairs in order to reduce resource consumption. Then, we propose a greedy heuristic algorithm to solve SDN-enabled switch selection problem to best utilize the TCAM resources and guarantee that most of sampled OD pairs are measured in the flow table. We also present a source node prefix tree based bit merging aggregation (SPTBMA) scheme to design feasible forwarding rules to be inserted in TCAM of SDN-enabled switches and reserve more TCAM space for sampled OD pairs. Finally, the experimental results based on real traffic dataset demonstrate that our proposed scheme outperforms the existing algorithms in terms of improving TM estimation accuracy and overcoming limitation of TCAM resources.",https://ieeexplore.ieee.org/document/8854149/,IEEE Transactions on Cloud Computing,1 Jan.-March 2022,ieeexplore
10.1109/ACCESS.2020.2983003,SMOTEFUNA: Synthetic Minority Over-Sampling Technique Based on Furthest Neighbour Algorithm,IEEE,Journals,"Class imbalance occurs in classification problems in which the normalcases, or instances, significantly outnumber the abnormalinstances. Training a standard classifier on imbalanced data leads to predictive biases which cause poor performance on the class(es) with lower prior probabilities. The less frequent classes are often critically important events, such as system failure or the occurrence of a rare disease. As a result, the class imbalance problem has been considered to be of great importance for many years. In this paper, we propose a novel algorithm that utilizes the furthest neighbor of a candidate example to generate new synthetic samples. A key advantage of SOMTEFUNA over existing methods is that it does not have parameters to tune (such as K in SMOTE). Thus, it is significantly easier to utilize in real-world applications. We evaluate the benefit of resampling with SOMTEFUNA against state-of-the-art methods including SMOTE, ADASYN and SWIM using Naive Bayes and Support Vector Machine classifiers. Also, we provide a statistical analysis based on Wilcoxon Signed-rank test to validate the significance of the SMOTEFUNA results. The results indicate that the proposed method is an efficient alternative to the current methods. Specifically, SOMTEFUNA achieves better 5-fold cross validated ROC and precision-recall space performance.",https://ieeexplore.ieee.org/document/9045990/,IEEE Access,2020,ieeexplore
10.1109/TSC.2017.2777478,SOLAR: Services-Oriented Deep Learning Architectures-Deep Learning as a Service,IEEE,Journals,"Deep learning has been an emerging field of machine learning during past decades. However, the diversity and large scale data size have posed significant challenge to construct a flexible and high performance implementations of deep learning neural networks. In order to improve the performance as well to maintain the scalability, in this paper we present SOLAR, a services-oriented deep learning architecture using various accelerators like GPU and FPGA. SOLAR provides a uniform programming model to users so that the hardware implementation and the scheduling is invisible to the programmers. At runtime, the services can be executed either on the software processors or the hardware accelerators. To leverage the trade-offs between the metrics among performance, power, energy, and efficiency, we present a multitarget design space exploration. Experimental results on the real state-of-the-art FPGA board demonstrate that the SOLAR is able to provide a ubiquitous framework for diverse applications without increasing the burden of the programmers. Moreover, the speedup of the GPU and FPGA hardware accelerator in SOLAR can achieve significant speedup comparing to the conventional Intel i5 processors with great scalability.",https://ieeexplore.ieee.org/document/8119814/,IEEE Transactions on Services Computing,1 Jan.-Feb. 2021,ieeexplore
10.1109/ACCESS.2022.3152191,STBC Identification for Multi-User Uplink SC-FDMA Asynchronous Transmissions Exploiting Iterative Soft Information Feedback of Error Correcting Codes,IEEE,Journals,"With the advancement and widespread implementation of multiple-input multiple-output (MIMO) wireless communication systems over the last decade, space-time block coding (STBC) identification has become a critical task for intelligent radios. Previous examinations of STBC identification were focused on single-user transmissions over single-carrier and multi-carrier systems in combination with uncoded broadcasts. Practical systems, on the other hand, contain many users and employ error-correcting codes. For the first time in literature, this work explores the problem of STBC identification for multi-user uplink transmissions in single-carrier frequency division multiple access (SC-FDMA) systems. We take another step closer to real systems by addressing asynchronous transmissions and by conducting multi-user channel estimation. We also exploit the outputs of the channel decoder, which is usually used in many practical systems, to improve the identification and estimation processes. The mathematical analysis demonstrates that the maximum-likelihood (ML) solution of STBC identification, channel estimation, and synchronization can be executed by an iterative approach. The space-alternating generalized expectation-maximization (SAGE) algorithm is used to separate the overlaid signals arriving at the base-station (BS). The parameters under consideration for each user are then updated using an expectation-maximization (EM) processor. Simulation results show that the proposed architecture outperforms other identification methods published in the literature while maintaining a reasonable level of processing time.",https://ieeexplore.ieee.org/document/9715055/,IEEE Access,2022,ieeexplore
10.1109/ACCESS.2021.3102157,Scalable and Multifaceted Search and Its Application for Binary Malware Files,IEEE,Journals,"Malicious binary files are a serious threat to industrial information systems. Because of their large number, an automatic assistant tool becomes essential for analysis, and finding similar files would be a great help. In this paper, we present a fast, scalable, and multifaceted search scheme to find similar binary malware files. We use a content-defined chunking algorithm to convert a file into a feature set for the first time. The proposed scheme uses MinHash to reduce any feature set of any file to a fixed size, which significantly improves search accuracy, processing speed, and space utilization. We theoretically prove that the new scheme returns similar files in jaccard index order. Through implementation and experiments with 12 million malicious files, we confirm that the search speed is increased by 600%, space is reduced by 90%, and the accuracy is increased by 400% at least, compared with the state-of-the-art of Elasticsearch.",https://ieeexplore.ieee.org/document/9504570/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.3030214,Segment-Based Multiple-Base Compressed Addressing for Flexible JavaScript Heap Allocation,IEEE,Journals,"Many Internet-of-Things (IoT) systems use lightweight JavaScript engines to support easy programming in microcontrollers. Lightweight JavaScript engines use several techniques for memory optimization, such as static heap reservation and compressed addressing. Recent IoT systems also use several external libraries and a larger on-chip memory to support abundant functionalities, such as machine learning and connectivity. However, as the JavaScript heap space is not resizable owing to the memory optimizations, the JavaScript engine or an external library is prone to fail the memory allocation in these devices. To address this problem, we propose a flexible memory optimization technique, segment-based multiple-base compressed addressing (SMBCA), which compresses a pointer indicating a JavaScript object allocated to a resizable heap based on multiple base addresses. SMBCA comprises two components: a dynamic segment allocator (DSA) and a multiple-base compressed address translator (MBCAT). DSA dynamically allocates the JavaScript heap in segment units. Meanwhile, MBCAT converts a low-bitwidth address into a full-bitwidth address and vice versa, based on the multiple base addresses. To reduce the address compression overhead of MBCAT, we propose a software cache technique, reverse map cache (RMC). We found that the SMBCA reduces average memory usage by 43.9% compared to the existing lightweight JavaScript engines when running SunSpider benchmarks, V8 benchmarks, and real-world applications. We also showed that the RMC reduces the average address compression latency of MBCAT by 34.9% when running the SunSpider benchmarks.",https://ieeexplore.ieee.org/document/9220132/,IEEE Access,2020,ieeexplore
10.1109/TNN.2010.2072790,Self-Organizing MultiLayer Perceptron,IEEE,Journals,"In this paper, we propose an extension of a self-organizing map called self-organizing multilayer perceptron (SOMLP) whose purpose is to achieve quantization of spaces of functions. Based on the use of multilayer perceptron networks, SOMLP comprises the unsupervised as well as supervised learning algorithms. We demonstrate that it is possible to use the commonly used vector quantization algorithms (LVQ algorithms) to build new algorithms called functional quantization algorithms (LFQ algorithms). The SOMLP can be used to model nonlinear and/or nonstationary complex dynamic processes, such as speech signals. While most of the functional data analysis (FDA) research is based on B-spline or similar univariate functions, the SOMLP algorithm allows quantization of function with high dimensional input space. As a consequence, classical FDA methods can be outperformed by increasing the dimensionality of the input space of the functions under analysis. Experiments on artificial and real world examples are presented which illustrate the potential of this approach.",https://ieeexplore.ieee.org/document/5580080/,IEEE Transactions on Neural Networks,Nov. 2010,ieeexplore
10.1109/TKDE.2003.1161587,Semantic abstractions in the multimedia domain,IEEE,Journals,"Information searching by exactly matching content is traditionally a strong point of machine searching; this is not, however, how human memory works and is rarely satisfactory for advanced retrieval tasks in any domain-multimedia in particular, where the presentational aspects can be equally important to the semantic information content. A combined abstraction of the conceptual and presentational characteristics of multimedia applications, leading on the one hand to their conceptual structure (with classic semantics of the real-world modeled by entities, relationships, and attributes) and on the other to the presentational structure (including media type, logical structure, temporal synchronization, spatial (on the screen) ""synchronization"" and interactive behavior) is developed in this paper. Multimedia applications are construed as consisting of ""Presentational Units:"" elementary (a media object with play duration and screen position), and composite (recursive structures of PUs in the temporal, spatial, and logical dimensions). The fundamental concept introduced is that of Semantic Multimedia Abstractions (SMA): qualitative abstract descriptions of multimedia applications in terms of their conceptual and presentational properties at an adjustable level of abstraction. SMAs, which could be viewed as metadata, form an abstract space to be queried. A detailed study of possible abstractions (from multimedia applications to SMAs and SMA-to-SMA), a definition and query language for Semantic Multimedia Abstractions (SMA-L) and the corresponding SMA model (equivalent to extended OMT), as well as an implementation of a system capable of wrapping the presentational structure of XML-based documents complete this work, whose contribution lays in the classically fruitful boundary between AI, software engineering, and database research.",https://ieeexplore.ieee.org/document/1161587/,IEEE Transactions on Knowledge and Data Engineering,Jan.-Feb. 2003,ieeexplore
10.1109/TCSVT.2017.2718036,Semi-Supervised Cross-View Projection-Based Dictionary Learning for Video-Based Person Re-Identification,IEEE,Journals,"Video-based person re-identification (re-id) has attracted a lot of research interest. When facing dramatic growth in new pedestrian videos, existing video-based person re-id methods usually need large quantities of labeled pedestrian videos to train a discriminative model. In practice, labeling large quantities of pedestrian videos is a costly and time-consuming task, which will limit the application of these methods in the real environment. Therefore, it is valuable and necessary to investigate how to learn a discriminative re-id model by using limited labeled training pedestrian videos. In this paper, we propose a semi-supervised cross-view projection-based dictionary learning (SCPDL) approach for video-based person re-id. Specifically, SCPDL jointly learns a pair of feature projection matrices and a pair of dictionaries by integrating the information contained in labeled and unlabeled pedestrian videos. With the learned feature projection matrices, the influence of variations within each video to the re-id can be reduced. With the learned dictionary pair, pedestrian videos from two different cameras can be converted into coding coefficients in a common representation space, such that the differences between different cameras can be bridged. In the learning process, the labeled pedestrian videos are used to ensure that the learned dictionaries have favorable discriminability; the large quantities of unlabeled pedestrian videos are used to ensure that SCPDL can better capture the variations between pedestrian videos, such that the learned dictionaries can own stronger representative capability. Experiments on two public pedestrian sequence data sets (iLIDS-VID and PRID 2011) demonstrate the effectiveness of the proposed approach.",https://ieeexplore.ieee.org/document/7954687/,IEEE Transactions on Circuits and Systems for Video Technology,Oct. 2018,ieeexplore
10.1109/TASLP.2020.3037521,Semi-Supervised Multiple Source Localization Using Relative Harmonic Coefficients Under Noisy and Reverberant Environments,IEEE,Journals,"This article develops a semi-supervised algorithm to address the challenging multi-source localization problem in a noisy and reverberant environment, using a spherical harmonics domain source feature of the relative harmonic coefficients. We present a comprehensive research of this source feature, including (i) an illustration confirming its sole dependence on the source position, (ii) a feature estimator in the presence of noise, (iii) a feature selector exploiting its inherent directivity over space. Source features at varied spherical harmonic modes, representing unique characterization of the soundfield, are fused by the Multi-Mode Gaussian Process modeling. Based on the unifying model, we then formulate the mapping function revealing the underlying relationship between the source feature(s) and position(s) using a Bayesian inference approach. Another issue of the overlapped components is addressed by a pre-processing technique performing overlapped frame detection, which in turn reduces this challenging problem to a single source localization. It is highlighted that this data-driven method has a strong potential to be implemented in practice because only a limited number of labeled measurements is required. We evaluate this proposed algorithm using simulated recordings between multiple speakers in diverse environments, and extensive results confirm improved performance in comparison with the state-of-art methods. Additional assessments using real-life recordings further prove the effectiveness of the method, even at unfavorable circumstances with severe source overlapping.",https://ieeexplore.ieee.org/document/9259082/,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",2020,ieeexplore
10.1109/ACCESS.2021.3098121,Short-Term Electric Load Forecasting With Sparse Coding Methods,IEEE,Journals,"Short-term load forecasting is a key task for planning and stability of the current and future distribution grid, as it can significantly contribute to the management of energy market for ancillary services. In this paper we introduce the beneficial properties of applications of sparse representation and corresponding dictionary learning to the net load forecasting problem on a substation level. In this context, sparse representation theory can provide parsimonial predictive models, which become attractive mainly due to their ability to successfully model the input space in a self-learning manner, by interacting between theory, algorithms, and applications. Several techniques are implemented, incorporating numerous dictionary learning and sparse decomposition algorithms, and a hierarchical structured model is proposed. The concept of sparsity in each case is embedded throughout the utilization of different regularization forms which include the <i>l</i><sub>0</sub>, <i>l</i><sub>1</sub>, <i>l</i><sub>2</sub> and <i>l</i><sub>0</sub><sup>tree</sup> norms. The observed superiority of the proposed theory, especially the one which embeds the atoms and corresponding coefficients in a tree structure, stems from the construction of the dictionary so as to represent efficiently the ambient electricity signal space and the consequent extraction of sparse basis-vectors. The performance of each model is evaluated using real hourly load measurements from a high voltage/medium voltage (HV/MV) substation and compared with that of widely used machine learning methods. The provided analytical results, verify the effectiveness of hierarchical sparse representation in short-term load forecasting applications, in terms of common accuracy indices.",https://ieeexplore.ieee.org/document/9490196/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.3020799,Short-Term Industrial Load Forecasting Based on Ensemble Hidden Markov Model,IEEE,Journals,"Short-term load forecasting (STLF) for industrial customers has been an essential task to reduce the cost of energy transaction and promote the stable operation of smart grid throughout the development of the modern power system. Traditional STLF methods commonly focus on establishing the non-linear relationship between loads and features, but ignore the temporal relationship between them. In this paper, an STLF method based on ensemble hidden Markov model (e-HMM) is proposed to track and learn the dynamic characteristics of industrial customers consumption patterns in correlated multivariate time series, thereby improving the prediction accuracy. Specifically, a novel similarity measurement strategy of log-likelihood space is designed to calculate the log-likelihood value of the multivariate time series in sliding time windows, which can effectively help the hidden Markov model (HMM) to capture the dynamic temporal characteristics from multiple historical sequences in similar patterns, so that the prediction accuracy is greatly improved. In order to improve the generalization ability and stability of a single HMM, we further adopt the framework of Bagging ensemble learning algorithm to reduce the prediction errors of a single model. The experimental study is implemented on a real dataset from a company in Hunan Province, China. We test the model in different forecasting periods. The results of multiple experiments and comparison with several state-of-the-art models show that the proposed approach has higher prediction accuracy.",https://ieeexplore.ieee.org/document/9183956/,IEEE Access,2020,ieeexplore
10.1109/LRA.2020.3013848,Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?,IEEE,Journals,"Does progress in simulation translate to progress on robots? If one method outperforms another in simulation, how likely is that trend to hold in reality on a robot? We examine this question for embodied PointGoal navigation - developing engineering tools and a research paradigm for evaluating a simulator by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy), a library for seamless execution of identical code on simulated agents and robots - transferring simulation-trained agents to a LoCoBot platform with a one-line code change. Second, we investigate the sim2real predictivity of Habitat-Sim M. Savva et al., for PointGoal navigation. We 3D-scan a physical lab space to create a virtualized replica, and run parallel tests of 9 different models in reality and simulation. We present a new metric called Sim-vs-Real Correlation Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as used for the CVPR19 challenge is low (0.18 for the success metric), suggesting that performance differences in this simulator-based challenge do not persist after physical deployment. This gap is largely due to AI agents learning to exploit simulator imperfections - abusing collision dynamics to `slide' along walls, leading to shortcuts through otherwise non-navigable space. Naturally, such exploits do not work in the real world. Our experiments show that it is possible to tune simulation parameters to improve sim2real predictivity (e.g. improving SRCC<sub>Succ</sub> from 0.18 to 0.844) - increasing confidence that in-simulation comparisons will translate to deployed systems in reality.",https://ieeexplore.ieee.org/document/9158349/,IEEE Robotics and Automation Letters,Oct. 2020,ieeexplore
10.1109/TIP.2005.863105,Similarity-based online feature selection in content-based image retrieval,IEEE,Journals,"Content-based image retrieval (CBIR) has been more and more important in the last decade, and the gap between high-level semantic concepts and low-level visual features hinders further performance improvement. The problem of online feature selection is critical to really bridge this gap. In this paper, we investigate online feature selection in the relevance feedback learning process to improve the retrieval performance of the region-based image retrieval system. Our contributions are mainly in three areas. 1) A novel feature selection criterion is proposed, which is based on the psychological similarity between the positive and negative training sets. 2) An effective online feature selection algorithm is implemented in a boosting manner to select the most representative features for the current query concept and combine classifiers constructed over the selected features to retrieve images. 3) To apply the proposed feature selection method in region-based image retrieval systems, we propose a novel region-based representation to describe images in a uniform feature space with real-valued fuzzy features. Our system is suitable for online relevance feedback learning in CBIR by meeting the three requirements: learning with small size training set, the intrinsic asymmetry property of training samples, and the fast response requirement. Extensive experiments, including comparisons with many state-of-the-arts, show the effectiveness of our algorithm in improving the retrieval performance and saving the processing time.",https://ieeexplore.ieee.org/document/1593673/,IEEE Transactions on Image Processing,March 2006,ieeexplore
10.1109/ACCESS.2018.2880794,"Smart Space Concepts, Properties and Architectures",IEEE,Journals,"Smart spaces have been actively emerging recently, and researchers are working on developing and testing smart spaces in the real world. They facilitate smart applications that are adaptive to user preferences and contexts. In doing so they must satisfy applications' dynamically changing resource needs. These objectives are achievable by cooperation among connected devices and ubiquitous interaction. Smart space architecture designs in the literature are mostly application specific, their concepts and components defined based on the specific needs of one application. In this paper, we formally define general smart space concepts and architectural models rigorously and discuss related architectural components (both hardware and software) in detail. Based on a literature review we summarize the discriminating properties that a smart space must possess, and its basic components and services to realize these properties. We present a comparative analysis of the architectural designs proposed thus far. A comprehensive smart space architecture is proposed and its semantic interoperability is discussed in detail. In addition, we provide a case study of a smart lighting system, where the properties of smart spaces are analyzed. Finally, we provide a roadmap for future smart space development.",https://ieeexplore.ieee.org/document/8531616/,IEEE Access,2018,ieeexplore
10.1109/ACCESS.2021.3059665,Solving of Optimal Power Flow Problem Including Renewable Energy Resources Using HEAP Optimization Algorithm,IEEE,Journals,"This paper presents a novel endeavor to use the Heap optimization algorithm (HOA) to solve the problem of optimal power flow (OPF) in the electricity networks. The key objective is to optimize the cost of fuel of the conventional generators under the system limitations. Various scenarios are studied in a later stage considering the addition of the PV panel and/or wind farm with changing load curves during a typical day. The active output power of the generators is selected to be the OPF problem search space. The HOA is employed to get the best solution of the fitness function and provides the corresponding best solutions. The modeling of the heap-based optimizer (HBO) depends on three levels: the relation between the subordinates and the boss, the relation between the same level employees, and the contribution of the employee oneself. The validity of the proposed algorithm is tested for a variety of electric grids, the IEEE 30-bus, IEEE 57-bus and 118 bus networks. These networks are simulated under various scenarios. Real load curves, in this study, are considered to achieve a practical outcome. The simulation outcomes are evaluated and tested. The results indicate that the implemented HOA-based OPF methodology is flexible and applicable compared with that achieved by using the genetic algorithm.",https://ieeexplore.ieee.org/document/9354775/,IEEE Access,2021,ieeexplore
10.1109/TVCG.2007.1019,Space-Time Light Field Rendering,IEEE,Journals,"In this paper, we propose a novel framework called space-time light field rendering, which allows continuous exploration of a dynamic scene in both space and time. Compared to existing light field capture/rendering systems, it offers the capability of using unsynchronized video inputs and the added freedom of controlling the visualization in the temporal domain, such as smooth slow motion and temporal integration. In order to synthesize novel views from any viewpoint at any time instant, we develop a two-stage rendering algorithm. We first interpolate in the temporal domain to generate globally synchronized images using a robust spatial-temporal image registration algorithm followed by edge-preserving image morphing. We then interpolate these software-synchronized images in the spatial domain to synthesize the final view. In addition, we introduce a very accurate and robust algorithm to estimate subframe temporal offsets among input video sequences. Experimental results from unsynchronized videos with or without time stamps show that our approach is capable of maintaining photorealistic quality from a variety of real scenes.",https://ieeexplore.ieee.org/document/4293014/,IEEE Transactions on Visualization and Computer Graphics,July-Aug. 2007,ieeexplore
10.1109/TPS.2013.2276537,Space-Varying Templates for Real-Time Applications of Cellular Nonlinear Networks to Pattern Recognition in Nuclear Fusion,IEEE,Journals,"In this paper, a new methodology consisting of space-varying templates in cellular nonlinear networks (CNNs) for real-time visual pattern recognition in nuclear fusion devices is presented. The development of space-varying templates is a new upgrade, driven by the need to process different parts of the images in different ways. The new approach has been applied to the identification in real time of various objects present in the Joint European Torus videos of both infrared (IR) and visible cameras. IR videos are here used to detect hot spots and the regions of the walls in which dangerously high temperatures are reached, whereas visible cameras provide information about multifaceted asymmetric radiations from the edge, which are dangerous instabilities that can lead to disruptions. Their identification is particularly difficult because of their movement and their shape which is similar to other objects present in the frames. Therefore, in addition to space-varying template CNNs, quite sophisticated morphological operators have to be deployed and their outputs processed by machine learning tools, such as support vector machines. The implementation of the whole methodology was performed in a field-programmable gate array board, obtaining, in both applications, a final success rate close to 100% and a frame rate higher than 200 frames/s.",https://ieeexplore.ieee.org/document/6582663/,IEEE Transactions on Plasma Science,Sept. 2013,ieeexplore
10.1109/TAI.2021.3096489,Sparsity-Induced Graph Convolutional Network for Semisupervised Learning,IEEE,Journals,"The graph representation (GR) in a data space reveals the intrinsic information as well as the natural relationships of data, which is regarded as a powerful means of representation for solving the semisupervised learning problem. To effectively learn on a predefined graph with both labeled data and unlabeled data, the graph convolutional network (GCN) was proposed and has attracted a lot of attention due to its high-performance graph-based feature extraction along with its low computational complexity. Nevertheless, the performance of GCNs is highly sensitive to the quality of the graph, meaning with high probability, the GCNs will achieve poor performances on a badly defined graphs. In numerous real-world semisupervised learning problems, the graph connecting each entity in the data space implicitly exists so that there is no naturally predefined graph in these problems. To overcome the issues, in this article, we apply unified GR techniques and GCNs in a framework that can be implemented in semisupervised learning problems. To achieve this framework, we propose sparsity-induced graph convolutional network (SIGCN) for semisupervised learning. SIGCN introduces the sparsity to formulate significant relationships between instances by constructing a newly proposed <inline-formula><tex-math notation=""LaTeX"">$L_0$</tex-math></inline-formula>-based graph (termed as the sparsity-induced graph) before applying graph convolution to capture the high-quality features based on this graph for label propagation. We prove and demonstrate the feasibility of the unified framework as well as effectiveness in capturing features. Extensive experiments and comparisons were performed to show that the proposed SIGCN obtains a state-of-the-art performance in the semisupervised learning problem.",https://ieeexplore.ieee.org/document/9483578/,IEEE Transactions on Artificial Intelligence,Dec. 2021,ieeexplore
10.1109/TIP.2019.2946102,Spatiotemporal Knowledge Distillation for Efficient Estimation of Aerial Video Saliency,IEEE,Journals,"The performance of video saliency estimation techniques has achieved significant advances along with the rapid development of Convolutional Neural Networks (CNNs). However, devices like cameras and drones may have limited computational capability and storage space so that the direct deployment of complex deep saliency models becomes infeasible. To address this problem, this paper proposes a dynamic saliency estimation approach for aerial videos via spatiotemporal knowledge distillation. In this approach, five components are involved, including two teachers, two students and the desired spatiotemporal model. The knowledge of spatial and temporal saliency is first separately transferred from the two complex and redundant teachers to their simple and compact students, while the input scenes are also degraded from high-resolution to low-resolution to remove the probable data redundancy so as to greatly speed up the feature extraction process. After that, the desired spatiotemporal model is further trained by distilling and encoding the spatial and temporal saliency knowledge of two students into a unified network. In this manner, the inter-model redundancy can be removed for the effective estimation of dynamic saliency on aerial videos. Experimental results show that the proposed approach is comparable to 11 state-of-the-art models in estimating visual saliency on aerial videos, while its speed reaches up to 28,738 FPS and 1,490.5 FPS on the GPU and CPU platforms, respectively.",https://ieeexplore.ieee.org/document/8868103/,IEEE Transactions on Image Processing,2020,ieeexplore
10.1109/TIM.2021.3119138,Statistical <italic>n</italic>-Best AFD-Based Sparse Representation for ECG Biometric Identification,IEEE,Journals,"Electrocardiogram (ECG) biometric recognition as a personal identification method is receiving more and more attention because it can support live verification results. Compared with other biometric-based methods, it can provide higher security performance. The difficulty of the problem lies in how to stably extract ECG signal features and achieve real-time verification. In this study, a new type of sparse representation learning framework called statistical <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula>-best adaptive Fourier decomposition (SAFD) originated by Qian is adopted in ECG biometric identification. Adaptive Fourier decomposition (AFD) is a recently developed combination of transform-based signal decomposition and sparse representation method, which can adaptively select the atoms from a redundant dictionary through orthogonal processing. The advantage of the AFD-type methods is that each atom in the dictionary has a precise mathematical formula with good analytic properties. This characteristic is significantly distinguished it from other existing sparse representations, where the atoms learned are usually matrix data and cannot be described mathematically. The proposed SAFD extends the existing <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula>-best AFD from processing single signal to multi-signals and implements the <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula>-best AFD in the stochastic Hardy space. Therefore, the small number of learned atoms by SAFD is sufficient to capture internal structure and robustness of the signal and generate a discriminative representation that reflects the timefrequency characteristics of signals. It is very suitable for non-stationary signals like ECG. The proof of convergence of the algorithm is presented. Extensive experiments are conducted on five public databases collected in different realistic conditions, and an average identification accuracy of 98.0% is achieved. In addition, less than 1 ms for one matching process makes it possible to be implemented in real time. Experimental results demonstrate that the proposed method can achieve superior performance compared to other state-of-the-art ECG biometric identification methods.",https://ieeexplore.ieee.org/document/9565931/,IEEE Transactions on Instrumentation and Measurement,2021,ieeexplore
10.1109/JSTARS.2021.3111740,Structure-Aware Multikernel Learning for Hyperspectral Image Classification,IEEE,Journals,"Recently, the inclusion of spatial information has drawn increasing attention in hyperspectral image (HSI) applications due to its effectiveness in terms of improving classification accuracy. However, most of the techniques that include such spatial knowledge in the analysis are based on spatialspectral weak assumptions, i.e., all pixels in a spatial region are assumed to belong to the same class, and close pixels in spectral space are assigned the same label. This article proposes a novel structure-aware multikernel learning (SaMKL) method for HSI classification, which takes into account structural issues in order to effectively overcome the aforementioned weak assumptions and introduce a true multikernel learning process (based on multiple features derived from the original HSI), thus improving the spectral separability of such features. The proposed SaMKL method is composed of the following main steps. First, multiple (i.e., spectral, spatial, and textural) features are extracted from the original HSI based on various filtering operators. Then, a <inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>-peak density approach is designed to define superpixel regions that can properly capture the structural information of HSIs and overcome the aforementioned weak assumptions. Next, three sets of composite kernels are separately constructed to make full use of the spectral, spatial, and textural information. Meanwhile, these three sets of composite kernels are independently incorporated into a support vector machine classifier to obtain their corresponding classification results. Finally, majority voting is used as a simple and effective method to obtain the final classification labels. Experimental results on real HSI datasets indicate that the SaMKL outperforms other well-known and state-of-the-art classification approaches, in particular, when very limited labeled samples are available <i>a priori</i>.",https://ieeexplore.ieee.org/document/9535266/,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2021,ieeexplore
10.23919/JCIN.2019.8917873,TV White Space Spectrum Analysis Based on Machine Learning,PTP,Journals,"Exploration of TV white space (TVWS) is a promising solution to mitigate the spectrum shortage and provide opportunities for new applications. In this paper, we present a detailed analysis of spectrum utilisation over TVWS at different locations in London. Both short-term and long-term outdoor measurement campaigns are conducted over large scales to better understand the spectrum features and variations across multiple locations and time periods. Different from most fixed-location-only measurements, we also drive along the main streets of London with a portable moving node to measure the on-route spectrum density along with the corresponding geographical information, which allows us to study the features and variations of spectrum use through a continuous space. To better analyse the dynamic spectrum utilisation, a machine learning based analysis algorithm is developed over the real-world measurements. This approach allows us to characterise the similarity and variability in spectrum usage within and among different channels, locations, and time instances, which is critical for the secondary system deployment to efficiently exploit the white space.",https://ieeexplore.ieee.org/document/8917873/,Journal of Communications and Information Networks,June 2019,ieeexplore
10.1109/JIOT.2021.3051343,The Study of Urban Residentials Public Space Activeness Using Space-Centric Approach,IEEE,Journals,"With the advancement of the Internet of Things (IoT) and communication platform, large-scale sensor deployment can be easily implemented in an urban city to collect various information. To date, there are only a handful of research studies about understanding the usage of urban public spaces. Leveraging IoT, various sensors have been deployed in an urban residential area to monitor and study public space utilization patterns. In this article, we propose a data processing system to generate space-centric insights about the utilization of an urban residential region of multiple Points of Interests (PoIs) that consists of 190 000 m<sup>2</sup> real estate. We identify the activeness of each PoI based on the spectral clustering, and then study their corresponding static features, which are composed of transportation, commercial facilities, population density, along with other characteristics. Through the heuristic features inferring, the residential density and commercial facilities are the most significant factors affecting public place utilization.",https://ieeexplore.ieee.org/document/9321451/,IEEE Internet of Things Journal,"15 July15, 2021",ieeexplore
10.1109/TPAMI.2004.1262308,The writer independent online handwriting recognition system frog on hand and cluster generative statistical dynamic time warping,IEEE,Journals,"In this paper, we give a comprehensive description of our writer-independent online handwriting recognition system frog on hand. The focus of this work concerns the presentation of the classification/training approach, which we call cluster generative statistical dynamic time warping (CSDTW). CSDTW is a general, scalable, HMM-based method for variable-sized, sequential data that holistically combines cluster analysis and statistical sequence modeling. It can handle general classification problems that rely on this sequential type of data, e.g., speech recognition, genome processing, robotics, etc. Contrary to previous attempts, clustering and statistical sequence modeling are embedded in a single feature space and use a closely related distance measure. We show character recognition experiments of frog on hand using CSDTW on the UNIPEN online handwriting database. The recognition accuracy is significantly higher than reported results of other handwriting recognition systems. Finally, we describe the real-time implementation of frog on hand on a Linux Compaq iPAQ embedded device.",https://ieeexplore.ieee.org/document/1262308/,IEEE Transactions on Pattern Analysis and Machine Intelligence,March 2004,ieeexplore
10.1109/TAES.2002.1008975,Three-dimensional midcourse guidance using neural networks for interception of ballistic targets,IEEE,Journals,"A suboptimal midcourse guidance law is obtained for interception of free-fall targets in the three-dimensional (3D) space. Neural networks are used to approximate the optimal feedback strategy suitable for real-time implementation. The fact that the optimal trajectory in the 3D space does not deviate much from a vertical plane justifies the use of the two-dimensional (2D) neural network method previously studied. To regulate the lateral errors in the missile motion produced by the prediction error of the intercept point, the method of feedback linearization is employed. Computer simulations confirm the superiority of the proposed scheme over linear quadratic regulator guidance and proportional navigation guidance as well as its approximating capability of the optimal trajectory in the 3D space.",https://ieeexplore.ieee.org/document/1008975/,IEEE Transactions on Aerospace and Electronic Systems,April 2002,ieeexplore
10.1109/TIT.2021.3075415,To Split or not to Split: The Impact of Disparate Treatment in Classification,IEEE,Journals,"Disparate treatment occurs when a machine learning model produces different decisions for individuals based on a legally protected or sensitive attribute (e.g., age, sex). In domains where prediction accuracy is paramount, it could potentially be acceptable to fit a model which exhibits disparate treatment. To evaluate the effect of disparate treatment, we compare the performance of split classifiers (i.e., classifiers trained and deployed separately on each group) with group-blind classifiers (i.e., classifiers which do not use a sensitive attribute). We introduce the benefit-of-splitting for quantifying the performance improvement by splitting classifiers. Computing the benefit-of-splitting directly from its definition could be intractable since it involves solving optimization problems over an infinite-dimensional functional space. Under different performance measures, we (i) prove an equivalent expression for the benefit-of-splitting which can be efficiently computed by solving small-scale convex programs; (ii) provide sharp upper and lower bounds for the benefit-of-splitting which reveal precise conditions where a group-blind classifier will always suffer from a non-trivial performance gap from the split classifiers. In the finite sample regime, splitting is not necessarily beneficial and we provide data-dependent bounds to understand this effect. Finally, we validate our theoretical results through numerical experiments on both synthetic and real-world datasets.",https://ieeexplore.ieee.org/document/9415697/,IEEE Transactions on Information Theory,Oct. 2021,ieeexplore
10.1109/TPDS.2021.3116865,Topology-Aware Neural Model for Highly Accurate QoS Prediction,IEEE,Journals,"With the widespread deployment of various cloud computing and service-oriented systems, there is a rapidly increasing demand for collaborative quality-of-service (QoS) prediction. Existing QoS prediction methods have made great progress in modeling users and services as well as exploiting contexts of service invocations. However, they ignore the completion of service requests/responses relies on the underlying network topology and the complex interactions between Autonomous Systems. To tackle this challenge, we propose a topology-aware neural (TAN) model for collaborative QoS prediction. In the TAN model, the features of users, services, and intermediate nodes on the communication path are projected to a shared latent space as input features. To jointly characterize the invocation process, the path features and end-cross features are captured respectively through an explicit path modeling layer and an implicit cross-modeling layer. After that, a gating layer fuses and transmits these features to the prediction layer for estimating unknown QoS values. In this way, TAN provides a flexible framework that can comprehensively capture the invocation context for making accurate QoS prediction. Experimental results on two real-world datasets demonstrate that TAN significantly outperforms state-of-the-art methods on the tasks of response time, throughput, and reliability prediction. Also, TAN shows better extensibility of using auxiliary information.",https://ieeexplore.ieee.org/document/9555220/,IEEE Transactions on Parallel and Distributed Systems,1 July 2022,ieeexplore
10.1109/TCSII.2014.2387683,Toward an Ultralow-Power Onboard Processor for Tongue Drive System,IEEE,Journals,"The Tongue Drive System (TDS) is a new unobtrusive, wireless, and wearable assistive device that allows for real-time tracking of the voluntary tongue motion in the oral space for communication, control, and navigation applications. The latest TDS prototype appears as a wireless headphone and has been tested in human subject trials. However, the robustness of the external TDS (eTDS) in real-life outdoor conditions may not meet safety regulations because of the limited mechanical stability of the headset. The intraoral TDS (iTDS), which is in the shape of a dental retainer, firmly clasps to the upper teeth and resists sensor misplacement. However, the iTDS has more restrictions on its dimensions, limiting the battery size and consequently requiring a considerable reduction in its power consumption to operate over an extended period of two days on a single charge. In this brief, we propose an ultralow-power local processor for the TDS that performs all signal processing on the transmitter side, following the sensors. Assuming the TDS user on average issuing one command/s, implementing the computational engine reduces the data volume that needs to be wirelessly transmitted to a PC or smartphone by a factor of 1500, from 12 kb/s to ~8 b/s. The proposed design is implemented on an ultralow-power IGLOO nano field-programmable gate array (FPGA) and is tested on AGLN250 prototype board. According to our post-place-and-route results, implementing the engine on the FPGA significantly drops the required data transmission, while an application-specific integrated circuit (ASIC) implementation in a 65-nm CMOS results in a 15 power saving compared to the FPGA solution and occupies a 0.02-mm<sup>2</sup> footprint. As a result, the power consumption and size of the iTDS will be significantly reduced through the use of a much smaller rechargeable battery. Moreover, the system can operate longer following every recharge, improving the iTDS usability.",https://ieeexplore.ieee.org/document/7001588/,IEEE Transactions on Circuits and Systems II: Express Briefs,Feb. 2015,ieeexplore
10.1109/ACCESS.2020.3046730,Tracking In-Cabin Astronauts Using Deep Learning and Head Motion Clues,IEEE,Journals,"A person-following robot is under development for astronaut assistance on the Chinese Space Station. Real-time astronaut detection and tracking are the most important prerequisites for in-cabin flying assistant robots so that they can follow a specific astronaut and offer him/her assistance. In the limited space in the space station cabin, astronauts stand close to each other when working collaboratively; thus, large regions of their bodies tend to overlap in the image. In addition, because astronauts wear the same clothes most of the time, it is difficult to distinguish an individual astronaut using human body features. In this paper, we distinguish the astronauts by tracking their heads in the image. A deep learning model trained using big data is proposed for effective head detection. In addition, a motion model based on spatial clues is combined with the head detection results to track astronauts in the scene. A complete pipeline of the algorithm has been implemented and run efficiently on the Tegra X2 embedded AI microprocessor. A set of experiments were carried out and successfully validated the effectiveness of the proposed tracking algorithm. This algorithm is a step toward the implementation of robot assistants, especially in resource-limited environments.",https://ieeexplore.ieee.org/document/9305234/,IEEE Access,2021,ieeexplore
10.1109/LRA.2021.3051565,Tracking and Relative Localization of Drone Swarms With a Vision-Based Headset,IEEE,Journals,"We address the detection, tracking, and relative localization of the agents of a drone swarm from a human perspective using a headset equipped with a single camera and an Inertial Measurement Unit (IMU). We train and deploy a deep neural network detector on image data to detect the drones. A joint probabilistic data association filter resolves the detection problems and couples this information with the headset IMU data to track the agents. In order to estimate the drones' relative poses in 3D space with respect to the human, we use an additional deep neural network that processes image regions of the drones provided by the tracker. Finally, to speed up the deep neural networks' training, we introduce an automated labeling process relying on a motion capture system. Several experimental results validate the effectiveness of the proposed approach. The approach is real-time, does not rely on any communication between the human and the drones, and can scale to a large number of agents, often called swarms. It can be used to spatially task a swarm of drones and also employed without a headset for formation control and coordination of terrestrial vehicles.",https://ieeexplore.ieee.org/document/9324934/,IEEE Robotics and Automation Letters,April 2021,ieeexplore
10.1109/ACCESS.2020.3048683,Transfer Learning Based on Hybrid Riemannian and Euclidean Space Data Alignment and Subject Selection in Brain-Computer Interfaces,IEEE,Journals,"Transfer learning is a promising approach for reducing training time in a brain-computer interface (BCI). However, how to effectively transfer data from previous users to a new user poses a huge challenge. This paper presents a novel transfer learning approach that combines data alignment and source subject selection for motor imagery (MI) based BCIs. The former is achieved by a reference matrix from the regularization of the two reference matrices estimated in Riemannian and Euclidean space respectively, whereas the latter is implemented by a modified sequential forward floating-point search algorithm. The aligned training data from chosen source subjects are used for creating a classification model based on either spatial covariance matrices in Riemannian space or common spatial pattern algorithm in Euclidean space. The proposed algorithms were evaluated on two MI based BCI data sets with different subjects and compared with existing transfer learning algorithms with sole data alignment or subject selection. The experimental results show that the hybrid-space data alignment methods for reducing the differences among subjects significantly outperform two single-space alignment methods, and the source subject selection method can substantially enhance the similarity between source subjects and the target subject. The combination of the two methods achieves superior classification performance compared to either one. The proposed algorithms will greatly facilitate the real-world applications of MI based BCIs.",https://ieeexplore.ieee.org/document/9312161/,IEEE Access,2021,ieeexplore
10.1109/TII.2021.3078186,Trimming Feature Extraction and Inference for MCU-Based Edge NILM: A Systematic Approach,IEEE,Journals,"Nonintrusive load monitoring (NILM) enables the disaggregation of the global power consumption of multiple loads, taken from a single smart electrical meter, into appliance-level details. State-of-the-art approaches are based on machine learning methods and exploit the fusion of time- and frequency-domain features from current and voltage sensors. Unfortunately, these methods are compute-demanding and memory-intensive. Therefore, running low-latency NILM on low-cost resource-constrained microcontroller unit (MCU)-based meters is currently an open challenge. This article addresses the optimization of the feature spaces as well as the computational and storage cost reduction needed for executing state-of-the-art (SoA) NILM algorithms on memory- and compute-limited MCUs. We compare four supervised learning techniques on different classification scenarios and characterize the overall NILM pipeline's implementation on an MCU-based <i>Smart Measurement Node</i>. Experimental results demonstrate that optimizing the feature space enables edge MCU-based NILM with 95.15% accuracy, resulting in a small drop compared to the most accurate feature vector deployment (96.19%) while achieving up to 5.45 speedup and 80.56% storage reduction. Furthermore, we show that low-latency NILM relying only on current measurements reaches almost 80% accuracy, allowing a major cost reduction by removing voltage sensors from the hardware (HW) design.",https://ieeexplore.ieee.org/document/9426443/,IEEE Transactions on Industrial Informatics,Feb. 2022,ieeexplore
10.1109/TCCN.2021.3075770,Trustworthy and Context-Aware Distributed Online Learning With Autoscaling for Content Caching in Collaborative Mobile Edge Computing,IEEE,Journals,"Content caching is widely recognized a promising functionality to improve service performance in mobile edge computing (MEC). In the big data era, there are massive heterogeneous contents collected by the mobile devices, belonging to different users with specific context (e.g., hobby, environment, age, etc). However, local content caching without content popularity and context information in advance is not accurate enough. Especially, multiple large-scale contents cached in the local database bring high pressure to the process of content selection. Hence, to handle these important issues, we propose a context-aware distributed online learning algorithm for efficient content caching according to a novel tree-based and contextual multi-arm bandit theory for collaborative MEC in this paper. To guarantee the trustworthy collaboration, we introduce a trust evaluation factor to find reliable neighboring ENs. Moreover, our system extracts contextual information from users into the context space and builds up a content cover tree to maximize caching hit rates to satisfy users demands. Our simulation results based on a real-world dataset indicate that our proposal can achieve a balance between caching hit rates and time cost, and have a sublinear bound of cumulative regret. This verifies its superior caching-hits performance gain compared to the other related algorithms.",https://ieeexplore.ieee.org/document/9416273/,IEEE Transactions on Cognitive Communications and Networking,Dec. 2021,ieeexplore
10.1109/TKDE.2019.2936189,Truth Discovery by Claim and Source Embedding,IEEE,Journals,"Information gathered from multiple sources on the Web often exhibits conflicts. This phenomenon motivates the need of truth discovery, which aims to automatically find the true claim among multiple conflicting claims. Existing truth discovery methods are mainly based on iterative updates, optimization or probabilistic models. Although these methods have shown their own effectiveness, they have a common limitation. These methods do not model relationships between each pair of source and target such that they do not well capture the underlying interactions in the data. In this paper, we propose a new model for truth discovery, learning the representations of sources and claims automatically from the interactions between sources and targets. Our model first constructs a heterogenous network including source-claim, source-source and truth-claim relationships. It then embeds the network into a low dimensional space such that trustworthy sources and true claims are close. In this way, truth discovery can be conveniently performed in the embedding space. Moreover, our model can be implemented in both semi-supervised and un-supervised manners to deal with the label scarcity problem in practical truth discovery. Experiments on three real-world datasets demonstrate that our model outperforms existing state-of-the-art methods for truth discovery.",https://ieeexplore.ieee.org/document/8807324/,IEEE Transactions on Knowledge and Data Engineering,1 March 2021,ieeexplore
10.1109/TPAMI.2008.74,Twin Kernel Embedding,IEEE,Journals,"In most existing dimensionality reduction algorithms, the main objective is to preserve relational structure among objects of the input space in a low dimensional embedding space. This is achieved by minimizing the inconsistency between two similarity/dissimilarity measures, one for the input data and the other for the embedded data, via a separate matching objective function. Based on this idea, a new dimensionality reduction method called twin kernel embedding (TKE) is proposed. TKE addresses the problem of visualizing non-vectorial data that is difficult for conventional methods in practice due to the lack of efficient vectorial representation. TKE solves this problem by minimizing the inconsistency between the similarity measures captured respectively by their kernel gram matrices in the two spaces. In the implementation, by optimizing a nonlinear objective function using the gradient descent algorithm, a local minimum can be reached. The results obtained include both the optimal similarity preserving embedding and the appropriate values for the hyperparameters of the kernel. Experimental evaluation on real non-vectorial datasets confirmed the effectiveness of TKE. TKE can be applied to other types of data beyond those mentioned in this paper whenever suitable measures of similarity/dissimilarity can be defined on the input data.",https://ieeexplore.ieee.org/document/4479481/,IEEE Transactions on Pattern Analysis and Machine Intelligence,Aug. 2008,ieeexplore
10.1109/ACCESS.2018.2872751,UGV Navigation Optimization Aided by Reinforcement Learning-Based Path Tracking,IEEE,Journals,"The success of robotic, such as UGV systems, largely benefits from the fundamental capability of autonomously finding collision-free path(s) to commit mobile tasks in routinely rough and complicated environments. Optimization of navigation under such circumstance has long been an open problem: 1) to meet the critical requirements of this task typically including the shortest distance and smoothness and 2) more challengingly, to enable a general solution to track the optimal path in real-time outdoor applications. Aiming at the problem, this study develops a two-tier approach to navigation optimization in terms of path planning and tracking. First, a ropemodel has been designed to mimic the deformation of a path in axial direction under external force and the fixedness of the radial plane to contain a UGV in a collision-free space. Second, a deterministic policy gradient (DPG) algorithm has been trained efficiently on abstracted structures of an arbitrarily derived ropeto model the controller for tracking the optimal path. The learned policy can be generalized to a variety of scenarios. Experiments have been performed over complicated environments of different types. The results indicate that: 1) the rope model helps in minimizing distance and enhancing smoothness of the path, while guarantees the clearance; 2) the DPG can be modeled quickly (in a couple of minutes on an office desktop) and the model can apply to environments of increasing complexity under the circumstance of external disturbances without the need for tuning parameters; and 3) the DPG-based controller can autonomously adjust the UGV to follow the correct path free of risks by itself.",https://ieeexplore.ieee.org/document/8476521/,IEEE Access,2018,ieeexplore
10.1109/LRA.2021.3123374,Uncertainty for Identifying Open-Set Errors in Visual Object Detection,IEEE,Journals,"Deployed into an open world, object detectors are prone to open-set errors, false positive detections of object classes not present in the training dataset.We propose GMM-Det, a real-time method for extracting epistemic uncertainty from object detectors to identify and reject open-set errors. GMM-Det trains the detector to produce a structured logit space that is modelled with class-specific Gaussian Mixture Models. At test time, open-set errors are identified by their low log-probability under all Gaussian Mixture Models. We test two common detector architectures, Faster R-CNN and RetinaNet, across three varied datasets spanning robotics and computer vision. Our results show that GMM-Det consistently outperforms existing uncertainty techniques for identifying and rejecting open-set detections, especially at the low-error-rate operating point required for safety-critical applications. GMM-Det maintains object detection performance, and introduces only minimal computational overhead. We also introduce a methodology for converting existing object detection datasets into specific <i>open-set</i> datasets to evaluate open-set performance in object detection.",https://ieeexplore.ieee.org/document/9591346/,IEEE Robotics and Automation Letters,Jan. 2022,ieeexplore
10.1109/TPAMI.2016.2601608,Uniform Projection for Multi-View Learning,IEEE,Journals,"Multi-view learning aims to integrate multiple data information from different views to improve the learning performance. The key problem is to handle the unconformities or distortions among view-specific samples or measurements of similarity or dissimilarity. This paper models the view-specific samples as a nonlinear mapping of uniform but latent intact samples for all the views, and the view-specific dissimilarity matrices or similarity matrices are estimated in terms of the uniform latent one. Two methods are then developed for multi-view clustering. One makes use of uniform multidimensional scaling (UMDS) on multi-view dissimilarities or kernels. The other one uses a uniform class assignment (UCA) procedure that optimally extracts the cluster components contained in the view-specific similarity matrices. These two methods result in the same optimization model, subjected to some slightly different constraints. A first-order condition of solutions is given as a nonlinear eigenvalue problem, and a second order condition guarantees local optimality. The nonlinear eigenvalue problem is solved by an iterative algorithm via eigen-space updating, and its convergence is proven. Furthermore, a fast implementation of the algorithm is discussed, which adopts the strategy of restarting subspace extension. Numerical experiments on some real-world data sets provide good support to the proposed methods.",https://ieeexplore.ieee.org/document/7547930/,IEEE Transactions on Pattern Analysis and Machine Intelligence,1 Aug. 2017,ieeexplore
10.1109/ACCESS.2019.2932396,Unifying Structural Proximity and Equivalence for Network Embedding,IEEE,Journals,"The fundamental purpose of network embedding is to automatically encode each node in a network as a low-dimensional vector, while at the same time preserving certain characteristics of the network. Based on the nodes' embeddings, downstream network analytic tasks such as community mining, node classification, and link prediction, can then be easily implemented using traditional machine learning methods. In recent years, extensive network embedding methods have been proposed based on factorization, random walks, deep learning, and so on. However, most of them focus mainly on preserving the structural proximity of network nodes, where highly interconnected nodes in a network will be represented closely together in the embedded vector space. While in many real-world networks, existing studies have revealed that high-order organizations (e.g., network motifs and graphlets) may be related to specific network functions. In this case, nodes far apart but with a similar organization in a network (i.e., structural equivalence) may have similar network functions. Accordingly, in this paper, we present a hybrid embedding method that unifies both structural proximity and equivalence (SPaE) of a network. Specifically, we adopt the concept of graphlet degree vector (GDV) to measure structural equivalence between network nodes. Through carrying out experiments on both synthetic and real-world datasets, we evaluate the performance of the hybrid embedding method in tasks of node clustering, node classification, and visualization. The results demonstrate that the proposed SPaE method outperforms several state-of-the-art methods when the network analytic tasks are not merely related to structural proximity. Finally, we also conduct experiments to evaluate the flexibility, robustness, and parameter sensitivity of the hybrid embedding method.",https://ieeexplore.ieee.org/document/8784281/,IEEE Access,2019,ieeexplore
10.1109/TIP.2021.3092828,Unsupervised Deep Image Stitching: Reconstructing Stitched Features to Images,IEEE,Journals,"Traditional feature-based image stitching technologies rely heavily on feature detection quality, often failing to stitch images with few features or low resolution. The learning-based image stitching solutions are rarely studied due to the lack of labeled data, making the supervised methods unreliable. To address the above limitations, we propose an unsupervised deep image stitching framework consisting of two stages: unsupervised coarse image alignment and unsupervised image reconstruction. In the first stage, we design an ablation-based loss to constrain an unsupervised homography network, which is more suitable for large-baseline scenes. Moreover, a transformer layer is introduced to warp the input images in the stitching-domain space. In the second stage, motivated by the insight that the misalignments in pixel-level can be eliminated to a certain extent in feature-level, we design an unsupervised image reconstruction network to eliminate the artifacts from features to pixels. Specifically, the reconstruction network can be implemented by a low-resolution deformation branch and a high-resolution refined branch, learning the deformation rules of image stitching and enhancing the resolution simultaneously. To establish an evaluation benchmark and train the learning framework, a comprehensive real-world image dataset for unsupervised deep image stitching is presented and released. Extensive experiments well demonstrate the superiority of our method over other state-of-the-art solutions. Even compared with the supervised solutions, our image stitching quality is still preferred by users.",https://ieeexplore.ieee.org/document/9472883/,IEEE Transactions on Image Processing,2021,ieeexplore
10.1109/TIP.2018.2882155,Unsupervised Deep Video Hashing via Balanced Code for Large-Scale Video Retrieval,IEEE,Journals,"This paper proposes a deep hashing framework, namely, unsupervised deep video hashing (UDVH), for large-scale video similarity search with the aim to learn compact yet effective binary codes. Our UDVH produces the hash codes in a self-taught manner by jointly integrating discriminative video representation with optimal code learning, where an efficient alternating approach is adopted to optimize the objective function. The key differences from most existing video hashing methods lie in: 1) UDVH is an unsupervised hashing method that generates hash codes by cooperatively utilizing feature clustering and a specifically designed binarization with the original neighborhood structure preserved in the binary space and 2) a specific rotation is developed and applied onto video features such that the variance of each dimension can be balanced, thus facilitating the subsequent quantization step. Extensive experiments performed on three popular video datasets show that the UDVH is overwhelmingly better than the state of the arts in terms of various evaluation metrics, which makes it practical in real-world applications.",https://ieeexplore.ieee.org/document/8540456/,IEEE Transactions on Image Processing,April 2019,ieeexplore
10.1109/JIOT.2018.2868648,Unsupervised WiFi-Enabled IoT Device-User Association for Personalized Location-Based Service,IEEE,Journals,"A fundamental building block toward personalized location-based service and context-aware service in smart buildings is the knowledge about the identity and mobility of users in indoor environments. Conventional user identification systems require the deployment of dedicated infrastructure or the active user involvement. Motivated by the widespread usage of the WiFi-enabled mobile device (MD), e.g., people usually carry at least one MD in their daily lives, in this paper, we propose WinDUA, a WiFi-enabled nonintrusive device and user association scheme to infer user identity and mobility via a novel unsupervised association learning algorithm. First, we utilize our WiFi-based indoor positioning system to obtain the historical location data of each MD using only existing WiFi infrastructure in a nonintrusive manner. Then, we classify all the MDs into two categories: 1) static device (SD) and 2) mobile phone (MP), according to their location variations and overnight presences. Subsequently, we estimate the correct mapping between each SD and its user through hierarchical clustering and location similarity matching between its location and user's personal space. Finally, we make possible pairs of MP and SD according to their duration of coexistence as well as the historical location similarity to associate the owner of each MP. Real-world experiments are conducted in an office, verifying that WinDUA is able to associate the MD to the correct users in a nonintrusive and unsupervised manner.",https://ieeexplore.ieee.org/document/8454455/,IEEE Internet of Things Journal,Feb. 2019,ieeexplore
10.1109/ACCESS.2020.3008165,Using EEG and Deep Learning to Predict Motion Sickness Under Wearing a Virtual Reality Device,IEEE,Journals,"Virtual Reality (VR) research has been widely applied in many fields. VR promises to deliver the experience that is beyond the user's imagination. One of the advantages of VR is the feeling it gives of being there. VR can provide experiences impossible in the real world, such as flying, diving in deep water, exploring outer space, or living with dinosaurs. Despite the improvements in the software and hardware, the problem of motion sickness remains. We implement a deep learning model to train and predict motion sickness. A questionnaire is a well-known method to measure motion sickness. The weakness of the questionnaire is the measurement carried out after the user experiences motion sickness symptoms. By using the deep learning and EEG, the system will learn and classify motion sickness. The system learns the user's EEG pattern when they begin to feel the sickness symptoms. The system will be trained using deep learning to identify the sickness patterns in the future. By the EEG patterns, the system can predict the sickness symptoms before it occurs. Our model outperforms traditional models in loss values, accuracy, and F-measure metrics in Roller Coaster. With other datasets, our model also performs well. Our model can achieve 82.83% accuracy from the dataset. We also found that the time steps to predict motion sickness during 5 minute periods is a suitable configuration.",https://ieeexplore.ieee.org/document/9137130/,IEEE Access,2020,ieeexplore
10.1109/TITS.2020.2995856,Vehicle Trajectory Clustering Based on Dynamic Representation Learning of Internet of Vehicles,IEEE,Journals,"With the widely used Internet of Things, 5G, and smart city technologies, we are able to acquire a variety of vehicle trajectory data. These trajectory data are of great significance which can be used to extract relevant information in order to, for instance, calculate the optimal path from one position to another, detect abnormal behavior, monitor the traffic flow in a city, and predict the next position of an object. One of the key technology is to cluster vehicle trajectory. However, existing methods mainly rely on manually designed metrics which may lead to biased results. Meanwhile, the large scale of vehicle trajectory data has become a challenge because calculating these manually designed metrics will cost more time and space. To address these challenges, we propose to employ network representation learning to achieve accurate vehicle trajectory clustering. Specifically, we first construct the k-nearest neighbor-based internet of vehicles in a dynamic manner. Then we learn the low-dimensional representations of vehicles by performing dynamic network representation learning on the constructed network. Finally, using the learned vehicle vectors, vehicle trajectories are clustered with machine learning methods. Experimental results on the real-word dataset show that our method achieves the best performance compared against baseline methods.",https://ieeexplore.ieee.org/document/9115819/,IEEE Transactions on Intelligent Transportation Systems,June 2021,ieeexplore
10.1109/TBIOM.2019.2949364,Video Face Recognition Using Siamese Networks With Block-Sparsity Matching,IEEE,Journals,"Deep learning models for still-to-video FR typically provide a low level of accuracy because faces captured in unconstrained videos are matched against a reference gallery comprised of a single facial still per individual. For improved robustness to intra-class variations, deep Siamese networks have recently been used for pair-wise face matching. Although these networks can improve state-of-the-art accuracy, the absence of prior knowledge from the target domain means that many images must be collected to account for all possible capture conditions, which is not practical for many real-world surveillance applications. In this paper, we propose the deep SiamSRC network that employs block-sparsity for face matching, while the reference gallery is augmented with a compact set of domain-specific facial images. Prior to deployment, clustering based on row sparsity is performed on unlabelled faces captured in videos from the target domain. Cluster centers discovered in the capture condition space (defined by, e.g., pose, scale and illumination) are used as rendering parameters with an off-the-shelf 3D face model, and a compact set of synthetic faces are thereby generated for each reference still based on representative intra-class information from the target domain. For pair-wise similarity matching with query facial images, the SiamSRC exploits sparse representation-based classification with a block structure. Experimental results obtained with the videos from the Chokepoint and COX-S2V datasets indicate that the proposed SiamSRC network can outperform state-of-the-art methods for still-to-video FR with a single sample per person, with only a moderate increase in computational complexity.",https://ieeexplore.ieee.org/document/8883245/,"IEEE Transactions on Biometrics, Behavior, and Identity Science",April 2020,ieeexplore
10.1109/TPAMI.2020.3035130,VolterraNet: A Higher Order Convolutional Network With Group Equivariance for Homogeneous Manifolds,IEEE,Journals,"Convolutional neural networks have been highly successful in image-based learning tasks due to their translation equivariance property. Recent work has generalized the traditional convolutional layer of a convolutional neural network to non-euclidean spaces and shown group equivariance of the generalized convolution operation. In this paper, we present a novel higher order Volterra convolutional neural network (VolterraNet) for data defined as samples of functions on Riemannian homogeneous spaces. Analagous to the result for traditional convolutions, we prove that the Volterra functional convolutions are equivariant to the action of the isometry group admitted by the Riemannian homogeneous spaces, and under some restrictions, any non-linear equivariant function can be expressed as our homogeneous space Volterra convolution, generalizing the non-linear shift equivariant characterization of Volterra expansions in euclidean space. We also prove that second order functional convolution operations can be represented as cascaded convolutions which leads to an efficient implementation. Beyond this, we also propose a dilated VolterraNet model. These advances lead to large parameter reductions relative to baseline non-euclidean CNNs. To demonstrate the efficacy of the VolterraNet performance, we present several real data experiments involving classification tasks on spherical-MNIST, atomic energy, Shrec17 data sets, and group testing on diffusion MRI data. Performance comparisons to the state-of-the-art are also presented.",https://ieeexplore.ieee.org/document/9247263/,IEEE Transactions on Pattern Analysis and Machine Intelligence,1 Feb. 2022,ieeexplore
10.1109/ACCESS.2020.3009046,Wind Farm Layout Optimization Considering Obstacles Using a Binary Most Valuable Player Algorithm,IEEE,Journals,"Wind farms are developed and implemented in many places around the globe. Designing a wind farm is becoming more and more complex especially with the recent trend towards large farms. Finding the optimal locations of wind turbines inside a wind farm to reduce energy cost is a highly challenging task, as it requires the handling of conflicting criteria and depending on the number of turbines considered it can turn to a large scale-optimization problem. Therefore, the aim of this paper is to place efficiently wind turbines inside a given area considering all constraints. This problem formulated as an optimization problem is referred to as the wind farm layout optimization (WFLO) problem. This real-world problem is nonlinear and difficult to solve using classical optimization algorithms and it has to take into consideration wind scenarios, power curve and wake effects. For this purpose, a binary version of the most valuable player algorithm (MVPA) called BMVPA is developed and implemented. Furthermore, ten scenarios were investigated using different wind speeds, terrain sizes with and without obstacles. For the same terrain but including obstacles, it was found that the energy cost increased due to the presence of obstacles that could limit the search space and consequently reduces the number of available options. The empirical results obtained using BMVPA were compared with those obtained using other well-known algorithms like the binary particle swarm optimization and genetic algorithm. BMVPA showed better results in solving the WFLO problem than the comparative algorithms. The optimum design of the wind farm obtained will allow an efficient and economic exploitation of wind resource.",https://ieeexplore.ieee.org/document/9139477/,IEEE Access,2020,ieeexplore
10.1109/TGRS.2020.3017937,X-SVM: An Extension of C-SVM Algorithm for Classification of High-Resolution Satellite Imagery,IEEE,Journals,"The accurate land cover mapping of the Earth's surface using Earth observation data is one of the most studied, but yet the most challenging tasks of remote sensing field, particularly when it comes to urban areas. The large spectral variability of man-made structures, as well as the mixed pixel phenomenon, imposes the use of computational demanding techniques, which are not always effective for real case applications. Support vector machines (SVMs) are supervised learning models with associated learning algorithms, which are mainly used for classification and regression analysis. Specifically, a support vector classifier (SVC) constructs a hyperplane or a set of hyperplanes in a high-dimensional space, which separates the training data into different classes. These are then used to classify a whole image, or series of images. The current standard SVM algorithm for classification used by the most popular mapping software (e.g., ENVI, EnMAP) is the C-SVC. The parameterization of a C-SVC strongly affects the final classification result. Yet, there is no rule of thumb to choose the optimal parameters when classifying satellite imagery. Optimal parameterization totally depends on the training data, and to determine it for a specific case, a time-consuming trial-and-error process is inevitable. In this work, advancements for the C-SVC algorithm are proposed to enhance its performance when used to classify remote sensing data, eliminating the need for a part of manual parametrization, while ensuring increasing its performance.",https://ieeexplore.ieee.org/document/9180087/,IEEE Transactions on Geoscience and Remote Sensing,May 2021,ieeexplore
10.1109/TNNLS.2018.2844093,fpgaConvNet: Mapping Regular and Irregular Convolutional Neural Networks on FPGAs,IEEE,Journals,"Since neural networks renaissance, convolutional neural networks (ConvNets) have demonstrated a state-of-the-art performance in several emerging artificial intelligence tasks. The deployment of ConvNets in real-life applications requires powerefficient designs that meet the application-level performance needs. In this context, field-programmable gate arrays (FPGAs) can provide a potential platform that can be tailored to application-specific requirements. However, with the complexity of ConvNet models increasing rapidly, the ConvNet-to-FPGA design space becomes prohibitively large. This paper presents fpgaConvNet, an end-to-end framework for the optimized mapping of ConvNets on FPGAs. The proposed framework comprises an automated design methodology based on the synchronous dataflow (SDF) paradigm and defines a set of SDF transformations in order to efficiently navigate the architectural design space. By proposing a systematic multiobjective optimization formulation, the presented framework is able to generate hardware designs that are cooptimized for the ConvNet workload, the target device, and the application's performance metric of interest. Quantitative evaluation shows that the proposed methodology yields hardware designs that improve the performance by up to 6.65 over highly optimized graphics processing unit designs for the same power constraints and achieve up to 2.94 higher performance density compared with the state-ofthe-art FPGA-based ConvNet architectures.",https://ieeexplore.ieee.org/document/8401525/,IEEE Transactions on Neural Networks and Learning Systems,Feb. 2019,ieeexplore
10.1109/ACCESS.2020.3018270,iOceanSee: A Novel Scheme for Ocean State Estimation Using 3D Mobile Convolutional Neural Network,IEEE,Journals,"Ocean state estimation is a basic problem in the field of ocean engineering. Under the trend of data-driven, the development of intelligent ship decision-making, ocean energy system design and other aspects, are inseparable from the estimation of wave parameters in the ocean area. In recent years, researchers have developed remote sensing technology to monitor ocean waves. However, sensor-based methods all have a key limitation, which is high cost and fault worry. More importantly, one major limitation exists in current research: due to lack of change information and relying on a single feature of spatial data, the final predictive results are inaccurate. Adopting a 3D Convolutional Neural Network is a possible solution to improve the detection accuracy. Unfortunately, it cannot be deployed in the ocean environment due to lack of physical network connections. To resolve these issues, we develop a light-weight version of 3D Convolutional Neural Network, namely a low-cost, high-accuracy detection scheme to foresee ocean wave parameters using a 3D Mobile Convolutional Neural Network technique called iOceanSee in the marine environment. iOceanSee employs a mobile terminal composed of low-cost measuring equipment and non-interference (except light) device-an RGB camera to collect video data in real time. It extracts both space and time features through three-dimensional depthwise separable convolutions. More specifically, iOceanSee is able to capture the encoding motion information from multiple adjacent frames of the video, according to which period and height of the waves being evaluated. Our experimental results conclude that iOceanSee obtains comparable performance to 3D Convolutional Neural Network and outperforms other models in terms of measurement accuracy in the marine environments.",https://ieeexplore.ieee.org/document/9172055/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2019.2963159,mVideo: Edge Computing Based Mobile Video Processing Systems,IEEE,Journals,"Computer vision is widely used to detect anomalies in video processing systems for public safety. Applying Deep Neural Networks (i.e., DNNs) in computer vision can achieve a high detection accuracy but it requires a huge amount of computing power, storage space, and video data. Thus, DNNs-based video analytics is mostly deployed in the cloud with video data steaming from a set of stationary cameras. There are mainly three issues in this setting. First, steaming a huge amount of video data from cameras to cloud leads to high bandwidth consumption and latency. Second, when DNNs are deployed on resource-limited devices like edge nodes to reduce communication costs, it is hard to achieve a high detection accuracy. Third, stationary cameras can only collect a limited amount of video data that covers a small area, so it barely satisfies the needs of the real-time analytics in applications like public safety. We propose a mobile edge computing-based video stream processing platform, mVideo, which conducts video analytics making full use of resources at the collaborative edge and cloud nodes. On the mVideo, a mechanism is designed to partition a video analysis task based on available resources on the mobile edge node. Then, the edge nodes pre-process video data using a lightweight DNN model and upload the results to cloud nodes for further analysis. Thus mVideo not only collects video data that covers a large area, but also reduces the communication costs. To validate the proposed platform, a face recognition application is deployed on the mVideo prototype. Experimental results reveal that compared with the existing cloud computing model, mVideo reduces video data volume transmitted to the cloud nodes and power consumption by up to 99.5% and 96.2%, respectively. mVideo also improves the execution time by 90.0% to optimize mobile video analytics performance.",https://ieeexplore.ieee.org/document/8945324/,IEEE Access,2020,ieeexplore
10.1109/JSEN.2020.2991741,mm-Pose: Real-Time Human Skeletal Posture Estimation Using mmWave Radars and CNNs,IEEE,Journals,"In this paper, mm-Pose, a novel approach to detect and track human skeletons in real-time using an mmWave radar, is proposed. To the best of the authors' knowledge, this is the first method to detect &gt;15 distinct skeletal joints using mmWave radar reflection signals. The proposed method would find several applications in traffic monitoring systems, autonomous vehicles, patient monitoring systems and defense forces to detect and track human skeleton for effective and preventive decision making in real-time. The use of radar makes the system operationally robust to scene lighting and adverse weather conditions. The reflected radar point cloud in range, azimuth and elevation are first resolved and projected in Range-Azimuth and Range-Elevation planes. A novel low-size high-resolution radar-to-image representation is also presented, that overcomes the sparsity in traditional point cloud data and offers significant reduction in the subsequent machine learning architecture. The RGB channels were assigned with the normalized values of range, elevation/azimuth and the power level of the reflection signals for each of the points. A forked CNN architecture was used to predict the real-world position of the skeletal joints in 3-D space, using the radar-to-image representation. The proposed method was tested for a single human scenario for four primary motions, (i) Walking, (ii) Swinging left arm, (iii) Swinging right arm, and (iv) Swinging both arms to validate accurate predictions for motion in range, azimuth and elevation. The detailed methodology, implementation, challenges, and validation results are presented.",https://ieeexplore.ieee.org/document/9083948/,IEEE Sensors Journal,"1 Sept.1, 2020",ieeexplore
10.1109/DASC.1991.177200,Artificial expertise in systems engineering (aerospace systems),IEEE,Conferences,"The authors present some conceptual applications and examples for implementing artificial expertise in system development. Systems engineering optimization of computer technology applications can eliminate or redesign engineering processes such that the unified system function focuses on innovation, flexibility, speed, and quality. Artificial expertise for system engineering refers to the application of artificial intelligence expert systems and shared databases to promote the integration of cross-functional engineering groups through technical interchange and control mechanisms. Artificial expertise concepts have been combined with human information processing to develop real-time expert systems providing a foundation for requirements definition, system integration, and requirement verification. Through knowledge and processing function descriptions in terms of abstraction hierarchy, a system state of knowledge and organization has been achieved for specific domains.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/177200/,IEEE/AIAA 10th Digital Avionics Systems Conference,14-17 Oct. 1991,ieeexplore
10.1109/ITNG.2008.213,Developing an Aerospace System Software Using PBL and MDA,IEEE,Conferences,"This paper reports a problem-based learning - PBL approach for the development of real time embedded software for the aerospace sector, employing a commercial suite of tools that supports model-driven architecture - MDA projects. Using this approach, graduate and last-year undergraduate students in computer engineering of the Brazilian Aeronautical Institute of Technology (LTA) have developed an aerospace system prototype during an academic semester.",https://ieeexplore.ieee.org/document/4492700/,Fifth International Conference on Information Technology: New Generations (itng 2008),7-9 April 2008,ieeexplore
10.1109/ICMAE.2017.8038685,Using artificial intelligence based expert system for selection of design subcontractors: A case study in aerospace industry,IEEE,Conferences,"As one of the top expectations for type certification of an aircraft, Aviation Authorities (AA) regulate design organization to establish Design Assurance System (DAS). DAS is composed of design, independent monitoring and airworthiness functions in which these functions are specialized for aerospace industry. Besides, Design Organization Approval (DOA) is a milestone to establish a rigid Design Assurance System. By this way, design organization assures aircraft development life cycle by complying with aviation regulations. To meet requirements of Design Organization Approval, Design Organization transfers its authority and technical signatories to its subcontractors to improve effectiveness of the system. So, performance of design subcontractors shall be traceable and measurable to match capability requirements of main contractor. Thus, subcontractor evaluation is a long and complicated process; survey implementation could be misleading in some cases. The purpose of this study is to propose a novel tool to measure performance of a design subcontractor according to necessities of Design Assurance System. Up to now, there is no tool to evaluate aviation design subcontractors. With this tool, contractor firm can evaluate multiple criteria in a single run. AHP is used to prioritize criteria relative to each other one-by-one. Then, for subcontractor selection and subcontractor monitoring, Artificial Neural Network (ANN) is applied to optimize decision making process. Annual Actual Data is applied in AHP model to assess current performance score of subcontractor. To have a long term judgment of this system, the model shall be applied to a design subcontractor for more than once on fixed periods such as quarterly, yearly etc.",https://ieeexplore.ieee.org/document/8038685/,2017 8th International Conference on Mechanical and Aerospace Engineering (ICMAE),22-25 July 2017,ieeexplore
10.1109/FIE44824.2020.9273977,A Comprehensive Experiment to Enhance Multidisciplinary Engineering Ability via UAVs Visual Navigation,IEEE,Conferences,"This Research to Practice WIP presents a UAVs visual navigation based comprehensive experiment to enhance multidisciplinary engineering ability in Aerospace engineering education. In traditional courses, aerospace-related disciplines are independently distributed in different courses, and there is rarely a hands-on platform which includes signal processing, control theory, and artificial intelligence into Aerospace engineering. Facing this problem, this paper designs a multidisciplinary comprehensive experiment, aiming to provide a hand-on platform and flexible project-based program to students of aerospace engineering professions. First of all, in order to let the students understand actual aerospace problems, a multidisciplinary simulation platform containing UAVs and remote objects scenarios is constructed for them to explore in the experiments. Second, the content of the experiment is designed into three stages including data acquisition and processing, conceptual design and simulation, in-flight validation, during which the multidisciplinary engineering ability runs through the whole process of the activities. Finally, Project Oriented Design Based Learning is also introduced here to combine engineering design education with innovation and creativity. Through the project demonstration and presentation at the end of the experiment, the multidisciplinary engineering ability of each student can be effectively evaluated. The UVN comprehensive experiment enables students to work on real-world aerospace engineering problems through a hardware-software integration framework, which may greatly stimulate their curiosity and interest in autonomously learning. It also provides students unprecedented opportunities to immerse themselves in projects that cross disciplinary boundaries, improve their professional ability and enhance their exploration competence in aerospace areas.",https://ieeexplore.ieee.org/document/9273977/,2020 IEEE Frontiers in Education Conference (FIE),21-24 Oct. 2020,ieeexplore
10.1109/ICARM52023.2021.9536056,A Review of Bilateral Teleoperation Control Strategies with Soft Environment,IEEE,Conferences,"In the past two decades, bilateral teleoperation with haptic feedback has attracted great research and application interests in both robotics and other areas. Initially triggered by the need to handle dangerous and remote distance tasks such as nuclear materials manipulation and space exploration, bilateral teleoperation has found its way into other applications as a result of development of control theory, robotic technology (both hardware and software) and latest breakthrough in artificial intelligence and machine learning. Consequently, bilateral teleoperation is found facing new challenges brought by these new applications. One major and obvious change is the working environment for the slave manipulator: different from rigid or solid contact environments which are reasonably assumed in early applications in industrial, nuclear and aerospace applications, the slave environment is now more complex and often the objects in contact are much softer in term of stiffness and can not be described by simple elastic model if good teleoperation performance (accurate and transparent) is expected. In this paper, the research of bilateral teleoperation system considering soft environment in recent 20 years has been surveyed for the first time in literature, to the knowledge of the authors. Following the difference in real applications, in this review the definition of soft environment covers linear elastic environment with much lower stiffness than conventional industrial environment and nonlinear complex soft environment with/out time-varying characteristics. Accordingly, the surveyed control strategies and structures in recent literature to improve the stability and accuracy of bilateral teleoperation with soft environment are classified and explained. Finally, the main applications, current challenges and future perspectives of bilateral teleoperation with soft environment are discussed.",https://ieeexplore.ieee.org/document/9536056/,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),3-5 July 2021,ieeexplore
10.1109/IECON43393.2020.9254506,Adaptive Online Gated Recurrent Unit for Lithium-Ion Battery SOC Estimation,IEEE,Conferences,"The Li-ion batteries are commonly used for Electric Vehicles (EVs) and aerospace applications. One of the essential parameters in Li-ion batteries is state of charge (SOC) that shows the available energy in a battery. Various methods were proposed for SOC estimation. Since the battery has a nonlinear equations, it is important to use a method that does not require the system model. In the present study, a new Adaptive Online Gated Recurrent Unit (GRU) method is proposed for the State of Charge (SOC) estimation. It is a kind of deep Recurrent Neural Network(RNN) which solved the vanishing gradient problem in RNNs with GRU units. For Optimization a robust adaptive Online gradient learning method is used. This method is able to tune online the learning rate in the process. Adaptive GRU is a nondependent method from the nonlinear batteries model and simplifies the mathematical computation. The proposed technique is implemented on the real dataset of LifePO4 Li-ion batteries for finding SOC estimation. The exprimental result indicate that the Adaptive GRU method is more accurate than simple RNN.",https://ieeexplore.ieee.org/document/9254506/,IECON 2020 The 46th Annual Conference of the IEEE Industrial Electronics Society,18-21 Oct. 2020,ieeexplore
10.1109/NAECON.1989.40244,An American knowledge base in England: alternate implementations of an expert system flight status monitor,IEEE,Conferences,"A joint activity between the Dryden Flight Research Facility of the NASA Ames Research Center (Ames-Dryden) and the Royal Aerospace Establishment (RAE) on knowledge-based systems has been agreed. Under the agreement, a flight status monitor knowledge base developed at Ames-Dryden has been implemented using the real time AI (artificial intelligence) toolkit MUSE, which was developed in the UK. The background to the cooperation is described and the details of the flight status monitor and a prototype MUSE implementation are presented. It is noted that the capabilities of the expert-system flight status monitor data downlinked from the flight test aircraft and to generate information on the state and health of the system for the test engineers provides increased safety during flight testing of new systems. The expert-system flight status monitor provides the systems engineers with ready access to the large amount of information required to describe a complex aircraft system.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/40244/,Proceedings of the IEEE National Aerospace and Electronics Conference,22-26 May 1989,ieeexplore
10.1109/IECEC.1997.659200,An object oriented neuro-controller design for motor drive technologies,IEEE,Conferences,"This paper deals with the object-oriented model development of a neuro-controller design for permanent magnet (PM) DC motor drives. The system under study is described as a collection of interacting objects. Each object module describes the object behaviors, called methods. The characteristics of the object are included in its variables. The knowledge of the object exists within its variables, and the performance is determined by its methods. This structure maps well to the real world objects that comprise the system being modeled. A dynamic learning architecture that possesses the capabilities of simultaneous on-line identification and control is incorporated to enforce constraints on connections and control the dynamics of the motor. The control action is implemented ""on-line"", in ""real time"" in such a way that the predicted trajectory follows a specified reference model. A design example of controlling a PM DC motor on-line shows the effectiveness of the design tool. This will therefore be very useful in aerospace applications. It is expected to provide an innovative and novel software model for the rocket engine numerical simulator executive.",https://ieeexplore.ieee.org/document/659200/,IECEC-97 Proceedings of the Thirty-Second Intersociety Energy Conversion Engineering Conference (Cat. No.97CH6203),27 July-1 Aug. 1997,ieeexplore
10.1109/SNPD.2018.8441042,Analysis and Testing of Key Performance Indexes of Vxworks in Real-Time System,IEEE,Conferences,"Due to real-time systems' good real-time., predictability, scalability and high reliability, they are widely used in aerospace and medical fields, etc. VxWorks is an important realtime operating system. In order to fully test the real-time performance, this paper deeply analyzes the influencing factors of five key performance indicators such as task switching time, preemption time, interrupt latency time, message communication time and semaphore shuffling time. Based on these, a high precision system clock is used to insert timestamps at the entrances and exits of corresponding operations of each indicator and dynamically test the index parameters. According to the above test method, VxWorks6.8.3 real-time operating system performance is tested. The test results show that the real-time performance of VxWorks6.8.3 is strong, and it meets engineering's performance requirements.",https://ieeexplore.ieee.org/document/8441042/,"2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",27-29 June 2018,ieeexplore
10.1109/ROBOT.1986.1087518,Architecture and early experience with planning for the ALV,IEEE,Conferences,"This paper describes the software architecture and the initial algorithms that have proved to be effective for a real time robot planning system. The architecture is designed to incorporate planning technology from research on artificial intelligence while at the same time supporting the high performance decision making needed to control a fast-moving autonomous vehicle. The symbolic representation of the vehicle's plan is a key element in this architecture. Our initial algorithms use an especially efficient version of dynamic programming to find the best routes. The route is then translated into a symbolic plan. Replanning happens at several levels with the cost of replanning proportionate to the scope of the changes. This software is currently running in an environment which simulates the vehicle and perception systems, but it will be transferred to the DARPA Autonomous Land Vehicle built by Martin Marietta Denver Aerospace [Lowrie 86].",https://ieeexplore.ieee.org/document/1087518/,Proceedings. 1986 IEEE International Conference on Robotics and Automation,7-10 April 1986,ieeexplore
,BARTIN applied to visual inspection of axisymmetric engineering parts,IET,Conferences,"A visual inspection scheme for detecting flaws in axisymmetric engineering parts is described and shows very good performance. The scheme can be used for both recognition of parts and classifying flaws. The polygon transform, a compact representation of the edge information in images of axisymmetric objects gives the inspection system invariance with respect to location, scale, orientation, and illumination intensity. Polygon transform representations of the object provide the inputs to a BARTIN (Bayesian Real Time Network) that generates the inspection decisions. This implementation employs a distance measure related to Kullback-Leibler divergence to quantify the difference between sample polygon transforms, and demonstrates the parsimony and reliability of the BARTIN architecture. The latter enables inclusion of prior information in the form of probabilities, decision utilities, and engineering drawings. The results obtained from an application provided by British Aerospace gave two wrong (but safe) decisions in 625 test examples.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/263268/,1993 Third International Conference on Artificial Neural Networks,25-27 May 1993,ieeexplore
10.1109/C2I454156.2021.9689446,Challenges in Design and Development of Electron Beam Weld Predictor Software,IEEE,Conferences,"Electron beam welding (EBW) is high power density welding process used predominantly for fabrication of launch vehicles and spacecrafts made of aerospace grade materials. EBW process at LPSC-ISRO is a space qualified process and the cost and time involved in imbibing quality into the EBW process amounts to 65% of the total fabrication costs and 80 % of the total welding time. The weld quality is ensured by evaluating a pre-weld WTS (Weld Test Specimen), each time a weld is done.. The LPSC-ISRO-Bangalore, houses many variants of EBW machines and in order to tap, utilise the huge potential of empirical data available in-house, this software has been designed and developed to predict the EBW machine parameters for a given material and geometry details. The software developed is based on machine learning of the empirical data. The accuracy of prediction results have been validated using EBW experiments and presently the first version of the software is available for deployment in ISRO and continual improvement of prediction accuracies is ensured by enhancing the database with additional reference points. The software aids in reduction of total manufacturing cost and lead time as the WTS (Weld Test Specimen) requirements have been substantially reduced with aid of this software. Unlike software development for electrical and electronic systems, the challenges in design and development of software for mechanical systems (EBW machines in this design) commences with limited data availability in assessable form; data collection and assimilation are humongous as it is based on almost 20 nos of parameters related to EBW machine and weld quality and not all parameters are in measurable or assessable format. Despite this limitation, the software has been developed in two phases including feature extraction studies in phase2 thus enhancing the prediction accuracies achieved in phase1(a primitive and draft software design). Also the other challenges are input and output parameters for the software are in analog format &amp; knowledge based (interpretation needs skill); the empirical data is predominantly dependent on physics of EBW such as geometry, material properties and software accuracy to be validated by real-time physical EBW experiments. The challenges have been overcome by ensuring precision in empirical data collection, assimilation and cleansing and also feature extraction studies indicated the relevance in selection of input parameters. The selection of machine learning algorithm is also based on the applicability of the selected algorithm to address all the above challenges endured. Also the experimental results validated the prediction accuracies achieved via the Phase2 developed software. Despite the challenges endured, the indigenously designed, developed and validated EBW predictor software can predict EBW machine parameters (beam current and focus current) without any need to weld a WTS and hence, the software developed aids in reduction of the total manufacturing cost and also the lead time involved in realisation of launch vehicles and spacecrafts for various ISRO missions have been substantially reduced with induction of this EBW Predictor software.",https://ieeexplore.ieee.org/document/9689446/,"2021 2nd International Conference on Communication, Computing and Industry 4.0 (C2I4)",16-17 Dec. 2021,ieeexplore
10.1109/ICDSE50459.2020.9310145,Cockpit Display Graphics Symbol Detection for Software Verification Using Deep Learning,IEEE,Conferences,"In Software Development Life-cycle, Verification and Validation plays a very important role, especially in the case of Safety-Critical Industries like Aerospace. Display dashboard consists of multiple static and dynamic objects having affine transformation, graphics overlap, shadows and less inter symbol discriminative features compared to natural images. Manual Software graphics verification is an error-prone and time-consuming activity. In this paper, we propose a novel software graphics verification pipeline to verify graphics symbols and alphanumeric objects as per Software requirements. To the best of our knowledge, our proposed approach is the first study on deep learning-based graphics symbol detection from complex synthetic background which requires high model accuracy. We experiment using Single-shot Multibox Detector (SSD) and You Only Look Once (YOLO v2) to detect different Graphical symbols from display simulator real-time captured video frames. These detected objects are further classified based on their nature. Objects containing alphanumeric digits can be recognized using Optical Character Recognition and dynamic symbols are detected using object detection to infer other properties. Finally, all the extracted properties can be compared with test expectations to verify their correctness. The result shows superior accuracy of the SSD algorithm over other state-of-the-art object detection algorithms for detecting real-time graphics symbols.",https://ieeexplore.ieee.org/document/9310145/,2020 International Conference on Data Science and Engineering (ICDSE),3-5 Dec. 2020,ieeexplore
10.1109/RTEICT.2016.7808123,Data driven prognosis approach for safety critical systems,IEEE,Conferences,"Safety critical systems are being developed to improve the performance and cost effectiveness. The safety critical system are used in various domain such as aerospace domain, military, defense etc. In an aerospace domain there are many parameters affects the system environmental conditions, or hazards which cause many faults in the system which leads to failure. It is necessary to know before the system fails, so that necessary remedies can take to prevent the failure. The tool/software is needed to monitor the health management of safety critical systems. In this paper a prognostic technique is being used to mitigate the system failure. There are many techniques for the prognosis such as data driven technique, model based technique, and hybrid technique. This paper proposes implementation of the artificial neural network [ANN] based prognosis illustrates the use of data driven technique. The novelty of the proposed algorithm is that it uses formal techniques to develop a robust &amp; reliable prognostics algorithm. The approach developed will be demonstrated for gyro sensor a critical component in the aerospace domain. The ANN can train and classify real data from the gyro sensors, and it is implemented using high level interpreted language GNU-Octave. The cost function/error function is calculated for the trained ANN data and it is being observed that the values are converging to the minimum value. At last the system is classified as healthy, partially healthy, and unhealthy state of the system.",https://ieeexplore.ieee.org/document/7808123/,"2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)",20-21 May 2016,ieeexplore
10.23919/EATS52162.2021.9704856,Data-Driven Modeling of Dynamic Systems Based on Online Learning,IEEE,Conferences,"This paper presents modeling and prediction of the unknown dynamics from a variety of real-time operation data in complex physical processes or systems based on machine learning. More generally, this is a data-driven modeling approach based on physical knowledge at time of design and machine learning at runtime. The learning process may apply to the entire or partial system dynamics, while portions of the physical system dynamics may be known or given. The learned models together with the known physics-based equations can be widely used in control system design, for instance, as one component in a model predictive control (MPC) framework for integrated dynamic systems. Long short-term memory (LSTM) neural networks are chosen here, because they have additional stored states resulting from the past output, and the state storage can be internally controlled subject to the network status itself. Such controlled states can be regarded as gated states or memory blocks in a neural network, and they serve as key components of the LSTM neural network in controlling the information flow throughout the network. This paper provides technical details about the concept of a LSTM neural network that can be used to model and predict energy production in renewable sources, battery state of charge, engine fuel use (or energy efficiency), or energy demand of the payload in aerospace or energy system applications. The most compute-intensive part of the learning algorithm is solving a nonlinear optimization problem to minimize the total error between the predicted and actual outputs. After updating the learned model and parameters, the LSTM model implementation may perform certain computational steps in parallel, so as to accelerate the computing. The model learning and prediction processes can be implemented on a field programmable gate array (FPGA - a reconfigurable parallel computing device), for instance, on NIs CompactRIO modules or PXI FPGA modules. As an example, an application of the LSTM model is studied in this paper to learn and predict the varying power generation from intermittent sources based on historical and real-time operational data.",https://ieeexplore.ieee.org/document/9704856/,2021 AIAA/IEEE Electric Aircraft Technologies Symposium (EATS),11-13 Aug. 2021,ieeexplore
10.1109/AERO.2018.8396547,Data-driven quality prognostics for automated riveting processes,IEEE,Conferences,"Technologies based in robotics and automatics are reshaping the aerospace industry. Aircraft manufacturers and top-tier suppliers now rely on robotics to perform most of its operational tasks. Over the years, a succession of implemented mobile robots has been developed with the mission of automating important industrial processes such as welding, material handling or assembly procedures. However, despite the progress achieved, a major limitation is that the process still requires human supervision and an extensive quality control process. An approach to address this limitation is to integrate machine learning methods within the quality control process. The idea is to develop algorithms that can direct manufacturing experts towards critical areas requiring human supervision and quality control. In this paper we present an application of machine learning to a concrete industrial problem involving the quality control of a riveting machine. The proposal consists of an intelligent predictive model that can be integrated within the existing real time sensing and pre-processing sub-systems at the equipment level. The framework makes use of several data-driven techniques for pre-processing and feature engineering, combined with the most accurate algorithms, validated through k-folds cross validation technique which also estimates prediction errors. The model is able to classify the manufacturing process of the machine as nominal or anomalous according to a real-world data set of design requirements and operational data. Several machine learning algorithms are compared such as linear regression, nearest neighbor, support vector machines, decision trees, random forests and extreme gradient boost. Results obtained from the case study suggest that the proposed model produces accurate predictions which meet industrial standards.",https://ieeexplore.ieee.org/document/8396547/,2018 IEEE Aerospace Conference,3-10 March 2018,ieeexplore
10.1109/TAI.2000.889866,Debugging knowledge-based applications with a generic toolkit,IEEE,Conferences,"Knowledge refinement tools assist in the debugging and maintenance of knowledge based systems (KBSs) by attempting to identify and correct faults in the knowledge that account for incorrect problem-solving. Most refinement systems target a single shell and are able to refine only KBSs implemented in this shell. Our KRUSTWorks toolkit is unusual in that it provides refinement facilities that can be applied to a number of different shells, and is designed to be extensible to new shells. The paper outlines the components of the KRUSTWorks toolkit and how it is applied to faulty KBSs. It describes its application to two real aerospace KBSs implemented in CLIPS and POWER-MODEL to demonstrate its flexibility of application.",https://ieeexplore.ieee.org/document/889866/,Proceedings 12th IEEE Internationals Conference on Tools with Artificial Intelligence. ICTAI 2000,15-15 Nov. 2000,ieeexplore
10.1109/ISKE47853.2019.9170453,Design and Implementation of On-orbit Valuable Image Extraction for the TianZhi-1 Satellite,IEEE,Conferences,"Recently, software-defined satellite has become a research hotspot in the aerospace. Based on an advanced computing platform with open system architecture, researchers can upload software for specific tasks even the satellite has been launched into space. This paper we have designed an on-orbit application for China's first software-defined satellite TianZhi-1, which use Android smartphone as a system platform. Two main tasks are focused on our work, one is to reduce data redundancy and the other is to compress the size of the software. First, a light-weight and extensible framework is designed to support different image processing algorithms. Following this, we propose a three-step approach for on-orbit valuable image extraction, include image denoising, stitching, and salient object extraction. Experiments on the real satellite achieve outstanding results.",https://ieeexplore.ieee.org/document/9170453/,2019 IEEE 14th International Conference on Intelligent Systems and Knowledge Engineering (ISKE),14-16 Nov. 2019,ieeexplore
10.1109/AIAM54119.2021.00049,Design of Online Measurement System for Molten Pool Morphology in Selective Laser Melting (SLM) Process,IEEE,Conferences,"Selective Laser Melting (SLM), as the processing technology with the highest forming accuracy and the best processing quality in additive manufacturing, is widely used in aerospace fields, etc. Laser molten pool is the basic unit of SLM processing, and online monitoring of the laser molten pool morphology can reflect the SLM processing accuracy and process stability in real time. In this paper, an online measurement system for side-axis laser molten pool morphology is developed inside the SLM molding cabin. Temperature isolation and sealing protection is designed for the online measurement system under complex high-temperature environments. It uses high-speed COMS camera and image acquisition card equipped with FPGA chips to collect and transmit the molten pool image information in real time. The software design of the online measurement system includes the development of the camera SDK based on Visual Studio, the design of real-time image processing algorithms, the evaluation of the molten pool, etc., which aims to evaluate the image acquisition, transmission and storage of the molten pool. Through simulation analysis and experimental verification, it is proved that the system can allow on-line measurement of molten pool morphology.",https://ieeexplore.ieee.org/document/9724859/,2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture (AIAM),23-25 Oct. 2021,ieeexplore
10.1109/ITNG.2008.213,Developing an Aerospace System Software Using PBL and MDA,IEEE,Conferences,"This paper reports a problem-based learning - PBL approach for the development of real time embedded software for the aerospace sector, employing a commercial suite of tools that supports model-driven architecture - MDA projects. Using this approach, graduate and last-year undergraduate students in computer engineering of the Brazilian Aeronautical Institute of Technology (LTA) have developed an aerospace system prototype during an academic semester.",https://ieeexplore.ieee.org/document/4492700/,Fifth International Conference on Information Technology: New Generations (itng 2008),7-9 April 2008,ieeexplore
10.1109/RAAI52226.2021.9508033,Development of Gasoline-Electric Hybrid Propulsion Surveillance and Reconnaissance VTOL UAV,IEEE,Conferences,"Vertical Take-Off and Landing (VTOL) Unmanned Aerial Vehicles (UAV) have been a high potential topic in the aerospace industry during the last decades due to its multirotor and fixed-wing nature of the aircraft. Besides, having the ability to rapidly deploy from a tight airstrip and gathering Intelligence, Surveillance, and Reconnaissance (ISR) information is the best way to be one step ahead of the enemy. In this paper, we present the implementation and development of gasoline-electric hybrid propulsion VTOL Unmanned Aerial vehicle respectively. The Hybrid propulsion VTOL UAV offers image and real-time video transmission to the ground station with fully autonomous control to get the best view of the enemy from the sky. The gasoline-electric hybrid propulsion system provides long flight endurance with efficient power consumption. The fundamentals of the multirotor and the conventional fixed-wing aircraft present the theoretical background of the aircraft. The accomplished design consists of high-performance multirotor motors with an efficient gasoline engine. Furthermore, the control system architecture, avionics, and power distribution system presented with addressing cost-effective trending design techniques. The performance of the system has been improved using commercially off-the-shelf (COTS) hardware.",https://ieeexplore.ieee.org/document/9508033/,"2021 IEEE International Conference on Robotics, Automation and Artificial Intelligence (RAAI)",21-23 April 2021,ieeexplore
10.1109/ICVES.2009.5400189,Digital implementation of fuzzy logic controller for wide range speed control of brushless DC motor,IEEE,Conferences,"The brushless DC motors find wide applications such as in battery operated vehicles, wheel chairs, automotive fuel pumps, robotics, machine tools, aerospace and in many industrial applications due to their superior electrical and mechanical characteristics and its capability to operate in hazardous environment. Conventional controllers fail to yield desired performance in BLDC motor control systems due to the non-linearity arising out of variation in the system parameters and change in load. The main focus is now on the application of artificial intelligent techniques such as fuzzy logic to solve this problem. Another great challenge is to reduce the size and cost of the drive system without compromising the performance. In this paper, the design and digital implementation of fuzzy logic controller using a versatile ADUC812 microcontroller, and low-cost, compact, superior performance components are used in order to reduce the cost and size of the drive system. The experimental results are presented to prove the flexibility of the control scheme in real time.",https://ieeexplore.ieee.org/document/5400189/,2009 IEEE International Conference on Vehicular Electronics and Safety (ICVES),11-12 Nov. 2009,ieeexplore
10.1109/ICIAFS.2007.4544783,Dynamic power management of an embedded sensor network based on actor-critic reinforcement based learning,IEEE,Conferences,"Wireless sensor networks (WSNs) have gained tremendous popularity in recent years due to the wide range of applications envisioned - ranging from aerospace and defense to industrial and commercial. Although limited by communication and energy constraints, the low cost, small sensor nodes lend themselves to be deployed in large numbers to form a network with high spatial distribution. The overall effectiveness of the sensor network depends on how well the mutually contradicting objectives of conserving the limited on-board battery power and keeping the sensors awake for stimuli, are managed. In this paper, we have proposed an actor-critic based reinforcement learning mechanism that can be practically implemented on an embedded sensor with limited memory and processing power. Specifically, the contribution of this paper is the development of the value function (or critic/reinforcement function) that is implemented on each sensor node which aids in dynamic power scheduling based on different situations. The effectiveness of the proposed method has been demonstrated with real world experiments.",https://ieeexplore.ieee.org/document/4544783/,2007 Third International Conference on Information and Automation for Sustainability,4-6 Dec. 2007,ieeexplore
10.1109/IJCNN.1999.831156,Efficient training techniques for classification with vast input space,IEEE,Conferences,"Strategies to efficiently train a neural network for an aerospace problem with a large multidimensional input space are developed and demonstrated. The neural network provides classification for over 100,000,000 data points. A query-based strategy is used that initiates training using a small input set, and then augments the set in multiple stages to include important data around the network decision boundary. Neural network inversion and oracle query are used to generate the additional data, jitter is added to the query data to improve the results, and an extended Kalman filter algorithm is used for training. A causality index is discussed as a means to reduce the dimensionality of the problem based on the relative importance of the inputs.",https://ieeexplore.ieee.org/document/831156/,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),10-16 July 1999,ieeexplore
10.1109/LA-CCI47412.2019.9037045,Fault Tolerance Tool for Human and Machine Interaction &amp; Application to Civilian Aircraft,IEEE,Conferences,"Enhancing human-machine interaction is critical to aerospace applications. An essential requirement in safety critical systems is the clear need to guarantee trustworthiness of a system as well as V&amp;V (Verification and Validation). However, the current state of the art concerning decision support systems lacks effective tools in this area. The Coherence Function Package, introduced in this research, is a tool towards providing assurance that the action needed has the approval of both the human and the machine in terms of SAFETY. These algorithms shed light on the future of an Explainable Artificial Intelligence (XAI, [1]), that fosters a synergy between these two factors. This vital requirement that has been further underscored after the tragic events of the Boeing 737 Max 8 crashes [2]. Preliminary results show that the proposed approach is not only able to detect any errors in the system, it also assists in circumventing conflicts leading to incoherence and suggests a preferred solution in real-time.",https://ieeexplore.ieee.org/document/9037045/,2019 IEEE Latin American Conference on Computational Intelligence (LA-CCI),11-15 Nov. 2019,ieeexplore
10.1109/AERO.1999.794302,Integrating autonomous fault management with conventional flight software: a case study,IEEE,Conferences,"An evolutionary approach to on-board autonomy involves integrating autonomous processes with traditional spacecraft real-time flight software (FSW). Such an approach relies on layering autonomy onto existing architecture while making minimal changes to the existing FSW. This is distinctly different from a revolutionary approach, which calls for integrating the design of autonomy and FSW from the start. While a revolutionary approach may lead to a more optimal design, an evolutionary approach allows for reuse of proven FSW, facilitates the complicated process of software verification and validation, and accelerates acceptance by the aerospace community. Because intelligent autonomous agents are typically soft real-time processes while FSW typically meets hard real-time requirements, applying the evolutionary approach is not a straightforward task. One technique available for solving this problem is Rate Monotonic Analysis (RMA). RMA uses real-time scheduling theory to guarantee that all processes can be scheduled. It has been used successfully in real-time scheduling applications such as helicopter control systems. This paper presents a case study where RMA was used as a tool to resolve real-time scheduling and processing issues associated with adding autonomous fault isolation and recovery software to previously developed flight software. The combined software system was then successfully tested on a real-time testbed.",https://ieeexplore.ieee.org/document/794302/,1999 IEEE Aerospace Conference. Proceedings (Cat. No.99TH8403),7-7 March 1999,ieeexplore
10.1109/AERO.2002.1036133,Integrating model-based diagnostics with simulation for real time health monitoring,IEEE,Conferences,"In systems that contain continuous-valued variables, such as aerospace systems, faults can be sensitively detected and diagnosed by combining a real time simulation with a conventional model-based diagnostic engine. We present a unified approach in which the user builds a single model that contains a sufficiently detailed description of the system to support both simulation and automatic diagnosis. This approach is implemented in a modeling environment called CNModeler/spl trade/.",https://ieeexplore.ieee.org/document/1036133/,"Proceedings, IEEE Aerospace Conference",9-16 March 2002,ieeexplore
10.1109/IECON.2012.6389520,Matlab/Simulink software implementation and interfacing of a strap-down inertial attitude method,IEEE,Conferences,"The paper presents a software tool used to calculate the angular attitude of a vehicle starting from the readings of a strap-down gyro triad. Firstly, the problem statement is presented and the implied mathematical equations are shown. The theoretical model is based on the discretized form of a matrix differential equation solution, and focuses on its first six truncation order. In order to have an easier communication between the user and the software, especially in the off-line data processing applications, a Graphical User Interface is developed. The obtained software is validated by using an integrated INS/GPS navigation system as reference system, the inputs of our software being the outputs of the gyro sensors in the strap-down INS. The tool can be used in all strap-down inertial systems application fields like aerospace, naval and automotive navigation, robotics, as well as in medicine surgery. Also, the developed software can be used in the real time applications or in the off-line processing of the navigation inertial sensors recorded data.",https://ieeexplore.ieee.org/document/6389520/,IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society,25-28 Oct. 2012,ieeexplore
10.1109/OCEANS.1998.724376,Model development of an underwater manipulator for coordinated arm-vehicle control,IEEE,Conferences,"This paper presents research on the hydrodynamic modeling of a manipulator for an autonomous underwater scientific vehicle. The focus is on improving the modeling accuracy of the in-line hydrodynamic coupling between a two-link manipulator and a small, free-floating vehicle in order to achieve better control for coordinated motion of the combined system. Loads predicted using existing models for underwater arms were determined to be off by as much as 25% when applied to a real, two-link arm in a test tank. In this new approach, an experimentally-determined model has been developed that takes into account the 3D flow effects that have previously not been included. The end result is a model that provides accurate predictions for the joint torques of a two-link arm in a form simple enough to be implemented in algorithms for precision planning and control. This project is part of a joint program between the Aerospace Robotics Laboratory at Stanford University and the Monterey Bay Aquarium Research Institute.",https://ieeexplore.ieee.org/document/724376/,IEEE Oceanic Engineering Society. OCEANS'98. Conference Proceedings (Cat. No.98CH36259),28 Sept.-1 Oct. 1998,ieeexplore
10.1109/AUTEST.1989.81113,On the evaluation of real life test expert systems,IEEE,Conferences,"The authors consider the factors affecting the successful integration of expert systems (ESs) in testing and maintenance environments. The first factor considered has to do with the communication between the ES and the UUT (unit under test) expert and between the ES and the test technician. The authors discuss the importance of embedding in the expert system basic understanding of electronic terms and universal knowledge bases, i.e. knowledge which is not unique to a specific UUT. The second factor considered has to do with the communication between the ES and the ATE (automatic test equipment). It is claimed that artificial intelligence software will not penetrate the real-world test industry, unless it offers very smooth interfaces with test instrumentation and ATE. Several specific requirements are discussed. This work is based on extensive experience in introducing the AITEST expert system for electronic troubleshooting and service management to a wide variety of fields; including aerospace and military, automotive, computers and peripherals, communication and general instrumentation.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/81113/,IEEE Automatic Testing Conference.The Systems Readiness Technology Conference. Automatic Testing in the Next Decade and the 21st Century. Conference Record.,25-28 Sept. 1989,ieeexplore
10.1109/IJCNN.2013.6706957,Optimized neuro genetic fast estimator (ONGFE) for efficient distributed intelligence instantiation within embedded systems,IEEE,Conferences,"The Optimized Neuro Genetic Fast Estimator (ONGFE) is a software tool that allows for embedding system, subsystem, and component failure detection, identification, and prognostics (FDI&amp;P) capability by using Intelligent Software Elements (ISE) based upon Artificial Neural Networks (ANN). With an Application Programming Interface (API), highly innovative algorithms are compiled for efficient distributed intelligence instantiation within embedded systems. The original design had the purpose of providing a real time kernel to deploy health monitoring functions for Condition Based Maintenance (CBM) and Real Time Monitoring (RTM) systems in a broad variety of applications (such as aerospace, structural, and widely distributed support systems). The ONGFE contains embedded fast and on-line training for designing ANNs to perform several high performance FDI&amp;P functions. A key advantage of this technology is an optimization block based upon pseudogenetic algorithms which compensate for effects due to initial weight values and local minimums without the computational burden of genetic algorithms. The ONGFE also provides a synchronization block for communication with secondary diagnostic modules. The algorithms are designed for a distributed, scalar, and modular deployment. Based on this technology, a scheme for conducting sensor data validation has been embedded in Smart Sensors.",https://ieeexplore.ieee.org/document/6706957/,The 2013 International Joint Conference on Neural Networks (IJCNN),4-9 Aug. 2013,ieeexplore
10.1109/ICPHM.2019.8819386,Part Name Normalization,IEEE,Conferences,"Parts information plays a key role in prognostics and health management. However, expressions of parts often have a wide range of variations, spawned by typos, ad hoc abbreviations, acronyms, and incomplete names. Normalization of such terms is crucial for many applications. Part names post a major challenge also because they tend to be in the form of multi-word terms. In this paper, we propose a novel normalization method UNAMER (Unification and Normalization Analysis, Misspelling Evaluation and Recognition). It is a general method for identifying term variants, including multi-word term variants, and normalizing them under a canonical name. UNAMER does not rely on a predefined set of canonical terms, which is often hard to obtain. Given a term, UNAMER first identifies candidate variants by exploiting contextual information. It then uses a supervised machine learning model, trained using easy-to-generate examples, that leverages both contextual and lexical features to predict actual variants from the candidates. UNAMER further extends its capability to normalize multi-word parts, such as part names like `lt pnl', `letf pnl' and `lft panal' for `left panel' using a specialized linguistically motivated term alignment approach. UNAMER has been deployed in practical applications to normalize part names in the aerospace domain. We will use examples from these real-life applications to demonstrate and illustrate results from UNAMER.",https://ieeexplore.ieee.org/document/8819386/,2019 IEEE International Conference on Prognostics and Health Management (ICPHM),17-20 June 2019,ieeexplore
10.1109/UEMCON53757.2021.9666543,Real-Time Dynamic Object Recognition and Grasp Detection for Robotic Arm using Streaming Video: A Design for Visually Impaired Persons,IEEE,Conferences,"The use of robotic arms in industrial applications such as manufacturing, medical and aerospace industry has been steadily gaining popularity. Research is also ongoing into how to improve the lives of ordinary citizens using robotic arms. Often, it is difficult for physically impaired individuals to complete tasks such as obtaining an object from a shelf, opening a refrigerator door, obtaining food or drink from the refrigerator, and picking a needed item from a drawer. Thus, they must rely on a caregiver to aid them with such tasks or complete these tasks for them. In such cases, a robotic arm could potentially be used to assist the visually impaired individual to pick up items. Prior work has described a system that takes in audio commands from an individual, applies natural language processing to identify the action required by the user, and combines this with object recognition and grasp detection to identify the object in the fridge. However, the limitation in that work was the different components were not integrated and there were inefficiencies in the process of grasp point detection from images.This work builds on existing research by integrating the object recognition and grasp detection components. The work describes a technique to dynamically adjust the size of the object image sent to the grasp point detection module by extracting only the portion necessary for grasp detection from the stream of camera images. This approach of separately performing the object recognition, identifying the portion of the object suitable for grasping (i.e., the handle), and dynamically determining the buffer zone around the handle for use in the grasp detection module reduces the amount of data transmitted between steps and enables real-time object recognition and grasp identification. The integrated framework with object recognition using a pre-trained model and dynamic grasp point detection program was successfully tested with an Intel RealSense D455 camera.",https://ieeexplore.ieee.org/document/9666543/,"2021 IEEE 12th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",1-4 Dec. 2021,ieeexplore
10.1109/AERO47225.2020.9172719,Recurrent Neural Network Based Prediction to Enhance Satellite Telemetry Compression,IEEE,Conferences,"Because of the gradually increasing number of remote measured low and/or high frequency sampled parameters in space applications, aerospace mission operators have to make hard choices on which parameters at which sampling rates should be downlinked. On-board aerospace applications are characterized by limited storage and communication budgets, while lossless data compression schemes should be sufficient enough to enhance transmission efficiency and hence the whole aerospace mission. In this paper, a proposed two-stage lossless compression method for telemetry data is presented. The proposed method consists of a decorrelation stage and an entropy coding one. The Long-Short-Term Memory (LSTM) Recurrent Neural Network (RNN) is implemented as a predictor in the decorrelation stage of the proposed method, and an illustrative method of applying LSTM network for telemetry data samples prediction is presented and figured out. In experiments, different entropy coders: Rice codes, arithmetic method and Huffman algorithm are separately implemented at the second stage. The proposed method is tested by different real telemetry data sets of FUNcube satellite in frames of data words of 8-,10-,16-bits widths. Experimental results show that the proposed method improved compression efficiency based on a single stage of entropy coder: Rice codes, arithmetic code, and Huffman algorithm by a ratio up to: 98%, 21%, and 1.6%, respectively.",https://ieeexplore.ieee.org/document/9172719/,2020 IEEE Aerospace Conference,7-14 March 2020,ieeexplore
10.1109/DASC52595.2021.9594351,SEDA: A Self-Explaining Decision Architecture Implemented Using Deep Learning for On-Board Command and Control,IEEE,Conferences,"Machine learning (ML) is a powerful tool for solving stochastic optimization problems. The aerospace and defense sectors have a number of stochastic optimization problems that would benefit from the application of ML; however, people often have difficulties interpreting solutions arrived at via ML, which undermines trust, producing an obstacle to widespread adoption in these sectors. This paper introduces the Self-Explaining Decision Architecture (SEDA) for ML-based decision-making systems capable of generating intuitive explanations for their decisions in real time. SEDA makes use of a feature extraction subsystem and a sequence interpretation subsystem to identify patterns in data followed by a decision generation subsystem that determines appropriate actions based on those patterns. Internal state information from each of these subsystems is used to generate explanations of the systems decisions. Using this information to create explanations provides insight as to the data elements the system focused on when making decisions as well as the reasoning that was used. As a proof-of-concept, we present a first implementation of SEDA using start-of-the-art deep learning components including a combined convolutional neural network and long short-term memory network with attention mechanisms and demonstrate its use on both standard and custom datasets.",https://ieeexplore.ieee.org/document/9594351/,2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC),3-7 Oct. 2021,ieeexplore
10.1109/ICSAI.2012.6223663,Table of contents,IEEE,Conferences,The following topics are dealt with: control system theory; control system applications; human-machine interface; computer vision; robotics; computer control; power systems; energy systems; fuel cells; electrical vehicles; power electronics; wind energy; solar energy; nuclear energy; smart grids; power management; intelligent systems; pattern recognition; autonomous systems; knowledge engineering; computational intelligence; artificial intelligence; computer systems; distributed systems; grid computing; cloud computing; services computing; parallel computing; embedded systems; VLSI; nanocomputing; nanomaterials; aerospace engineering; biomedical engineering; biotechnology; computational sciences; ad hoc networking; sensor networking; wireless networking; Internet; system security; QoS; optical networks; communication theory; signal processing; video processing; image processing; remote sensing; computer graphics; animation; virtual reality; multimedia; data mining; bioinformatics; medical informatics; data engineering; and software engineering.,https://ieeexplore.ieee.org/document/6223663/,2012 International Conference on Systems and Informatics (ICSAI2012),19-20 May 2012,ieeexplore
10.1109/TAI.1992.246450,The G2 development and deployment environment,IEEE,Conferences,"G2 is an object-oriented development and deployment environment, combining rule-based and procedural reasoning, user interface graphics, database interface capabilities, dynamic simulation, and real-time execution in a single package. The applications of G2 typically have large knowledge bases, with tens of thousands of objects, and generally real-time execution is required. The online installations span the process industries, discrete manufacturing, aerospace, telecommunications, electric utilities, and others. Primarily these installations have been implemented directly by plant engineers, who have defined the knowledge contained in the applications using G2's structured natural language and active-object graphics. The design of G2 is described.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/246450/,Proceedings Fourth International Conference on Tools with Artificial Intelligence TAI '92,10-13 Nov. 1992,ieeexplore
10.1109/PHM.2016.7819785,The design of the prognostics and health management system for special vehicle,IEEE,Conferences,"With the rapid development of sensor technology on vehicles, more and more vehicles are checked from artificial detection to auto monitoring. A great number of real-time data acquired from sensors provides important foundation for PHM application on vehicles. Aiming at the special mining vehicle, it proposes the solution for real-time monitoring, fault diagnosis and health management based on the PHM theory. First, a kind of application service architecture is designed to describe how to implement the special mining vehicle PHM system and meet the needs of fault diagnosis and health management. Second, it describes each service module of the PHM platform system in detail, including inner functions and interaction. Third, it studies and proposes the models and methods of the various service modules, which can be used to implement the goals of the monitoring, diagnosis and health management from the view of the vehicle parameters. Finally, the Special Vehicle PHM System is developed based on the proposed models and methods, which is being successfully used in SanJiang Aerospace Group. The practical application result shows the proposed the design of PHM system can provides the service of fault diagnosis and health management for the special mining vehicles, which has great value for PHM implementation and promotion in the field of vehicles.",https://ieeexplore.ieee.org/document/7819785/,2016 Prognostics and System Health Management Conference (PHM-Chengdu),19-21 Oct. 2016,ieeexplore
10.1109/ETFA46521.2020.9212097,Towards Real-time Process Monitoring and Machine Learning for Manufacturing Composite Structures,IEEE,Conferences,"Components made from carbon fiber reinforced plastics (CFRP) offer attractive stability properties for the automotive or aerospace industry despite their light weight. To automate CFRP production, resin transfer molding (RTM) based on thermoset plastics is commonly applied. However, this manufacturing process has its shortcomings in quality and costs. The project CosiMo aims for a highly automated and cost-attractive manufacturing process using cheaper thermoplastic materials. In a thermoplastic RTM (T-RTM) process, the polymerization of -caprolactam to polyamide 6 is investigated using an intelligent mold tooling. Multiple sensor types integrated into the mold allow for tracking of process-relevant variables, such as material flow and polymerization state. In addition to monitoring the T-RTM process, a digital twin visualizes progress and makes predictions about issues and countermeasures based on machine learning.",https://ieeexplore.ieee.org/document/9212097/,2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),8-11 Sept. 2020,ieeexplore
10.1109/IMTC.1994.352108,Use of artificial neural networks for optimal sensing in complex structures analysis,IEEE,Conferences,"Among the advantages offered by the Artificial Neural Networks (ANNs), in the analysis and active control of structures characterized by high modal densities and complexity, it should be mentioned the possibility of optimizing the number and the position of sensors and actuators. This feature can result in a sensible reduction of cost in the analysis and control of large structures and could be of great advantage when some of the points under investigation of the structure are not physically accessible. The first step of research is the validation of the ANN approach to a satisfactory analysis of the structure. Previous papers have highlighted the encouraging results obtained from an ANN implementation for the description of a stiffened aluminum panel behaviour. In that case the ANN was trained with data obtained by a validated FEM (Finite Element Method) structural model. In the simulation phase the ANN estimated the behaviour of all the grid points with data referred only to a portion of points with interesting performances in terms of accuracy and computing time. The present paper describes a measurement system in which the ANN is trained with real data, experimentally obtained by accelerometers. The investigation was carried out considering a sinusoidal excitation, typically produced by the rotating engines of turboprob aircrafts. Real data allowed us to rest the ability of the ANN to learn the structural dynamics taking into the right account the influence of noise, not considered in the previous phase of the research. The experimental results highlight again the good ANN behaviour accuracy in the estimation of all the rest points considered for the analysis. The present research is the necessary premise for ANNs based active control applications in aeronautics and aerospace.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/352108/,Conference Proceedings. 10th Anniversary. IMTC/94. Advanced Technologies in I & M. 1994 IEEE Instrumentation and Measurement Technolgy Conference (Cat. No.94CH3424-9),10-12 May 1994,ieeexplore
10.1109/ICECS53924.2021.9665511,Using Hardware Performance Counters to support infield GPU Testing,IEEE,Conferences,"Graphics Processing Units (GPUs) have gained importance in several domains where a high computational effort is required (e.g., where Artificial Intelligence is used). At the same time, their adoption extended to domains (e.g., automotive, robotics, aerospace) where the effects of possible hardware faults can be extremely serious. Hence, it is crucial to identify methods allowing to quickly detect the occurrence of faults in a GPU while it works in the operational phase. In this paper we propose a method based on the adoption of hardware performance counters. We show that the method is able to detect permanent faults occurring in some critical modules, thus allowing to increase the overall reliability of the system. The method does not involve significant costs, since performance counters already exist in all real GPUs, e.g., to support silicon debug, and can be easily accessed in software.",https://ieeexplore.ieee.org/document/9665511/,"2021 28th IEEE International Conference on Electronics, Circuits, and Systems (ICECS)",28 Nov.-1 Dec. 2021,ieeexplore
10.1109/ICMAE.2017.8038685,Using artificial intelligence based expert system for selection of design subcontractors: A case study in aerospace industry,IEEE,Conferences,"As one of the top expectations for type certification of an aircraft, Aviation Authorities (AA) regulate design organization to establish Design Assurance System (DAS). DAS is composed of design, independent monitoring and airworthiness functions in which these functions are specialized for aerospace industry. Besides, Design Organization Approval (DOA) is a milestone to establish a rigid Design Assurance System. By this way, design organization assures aircraft development life cycle by complying with aviation regulations. To meet requirements of Design Organization Approval, Design Organization transfers its authority and technical signatories to its subcontractors to improve effectiveness of the system. So, performance of design subcontractors shall be traceable and measurable to match capability requirements of main contractor. Thus, subcontractor evaluation is a long and complicated process; survey implementation could be misleading in some cases. The purpose of this study is to propose a novel tool to measure performance of a design subcontractor according to necessities of Design Assurance System. Up to now, there is no tool to evaluate aviation design subcontractors. With this tool, contractor firm can evaluate multiple criteria in a single run. AHP is used to prioritize criteria relative to each other one-by-one. Then, for subcontractor selection and subcontractor monitoring, Artificial Neural Network (ANN) is applied to optimize decision making process. Annual Actual Data is applied in AHP model to assess current performance score of subcontractor. To have a long term judgment of this system, the model shall be applied to a design subcontractor for more than once on fixed periods such as quarterly, yearly etc.",https://ieeexplore.ieee.org/document/8038685/,2017 8th International Conference on Mechanical and Aerospace Engineering (ICMAE),22-25 July 2017,ieeexplore
10.1109/IECON.2012.6389251,ZnO crystalline nanowires array for application in gas ionization sensor,IEEE,Conferences,"The air monitoring becomes daily necessity not only in industrial environment and in aerospace applications but also in a living milieu as a consequence of the gas pollution. For detection of gaseous pollutants gas sensors are employed. In this work the successful p-type and n-type ZnO nanowires (NWs) were accomplished during electrochemical deposition. P-type ZnO NWs doped with Ag dopant were achieved with omitted post annealing procedure. Also, the novel gas ionization sensor (GIS) with integrated p-type ZnO NWs as the anode is proposed. P-type ZnO NWs-based gas sensor's characteristics compared with the gold NWs-based GIS, which was developed and reported by our group previously. It showed the comparable breakdown voltages in inert gas (Ar) atmosphere. P-type ZnO NWs-based GIS demonstrated good repeatability. The practical and low cost p-type ZnO NWs-based gas sensor presented in this article shows potential for future implementation in real world gas sensors' applications.",https://ieeexplore.ieee.org/document/6389251/,IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society,25-28 Oct. 2012,ieeexplore
10.1109/TAES.2019.2961824,Intrusion Detection System for the MIL-STD-1553 Communication Bus,IEEE,Journals,"MIL-STD-1553 is a military standard that defines the specification of a serial communication bus that has been implemented in military and aerospace avionic platforms for over 40 years. MIL-STD-1553 was designed for a high level of fault tolerance while less attention was paid to cyber security issues. Thus, as indicated in recent studies, it is exposed to various threats. In this article, we suggest enhancing the security of MIL-STD-1553 communication buses by integrating a machine learning-based intrusion detection system (IDS); such anIDS will be capable of detecting cyber attacks in real time. The IDS consists of two modules: 1) a remote terminal (RT) authentication module that detects illegitimately connected components and data transfers and 2) a sequence-based anomaly detection module that detects anomalies in the operation of the system. The IDS showed high detection rates for both normal and abnormal behavior when evaluated in a testbed using real 1553 hardware, as well as a very fast and accurate training process using logs from a real system. The RT authentication module managed to authenticate RTs with +0.99 precision and +0.98 recall; and detect illegitimate component (or a legitimate component that impersonates other components) with +0.98 precision and +0.99 recall. The sequence-based anomaly detection module managed to perfectly detect both normal and abnormal behavior. Moreover, the sequencebased anomaly detection module managed to accurately (i.e., zero false positives) model the normal behavior of a real system in a short period of time (~22 s).",https://ieeexplore.ieee.org/document/8946705/,IEEE Transactions on Aerospace and Electronic Systems,Aug. 2020,ieeexplore
10.1109/TIM.2018.2884450,Low-order Nonlinear Finite-Impulse Response Soft Sensors for Ionic Electroactive Actuators Based on Deep Learning,IEEE,Journals,"This paper introduces a soft sensor (SS) for the estimation of the deflection of a polymeric mechanical actuator. The actuator is based on ionic polymer-metal composites (IPMCs). Applications of IPMCs have been proposed in fields such as robotics, surgery, and aerospace, to mention the most interesting ones. In such application fields, both the complexity and the size of the actuating system are of chief importance. An SS can be, therefore, preferred to hardware measuring the actuator output, for estimating the actuator motion. Also, low-order models are of interest to limit the computational load, which can be a constraint in real-time applications. To this aim, several data-driven nonlinear finite-impulse response (NFIR) models have been investigated. Data, used for the model identification, have been acquired, in controlled environmental conditions, by using swept signals as the input to the IPMC actuator. Linear and nonlinear models, based on principal component analysis, shallow, and deep neural networks (NNs), have been investigated, for different model orders. The best results have been obtained by an SS based on a fifth-order NFIR model, implemented by a deep belief NN.",https://ieeexplore.ieee.org/document/8584087/,IEEE Transactions on Instrumentation and Measurement,May 2019,ieeexplore
10.1109/TII.2019.2954956,Toward New Retail: A Benchmark Dataset for Smart Unmanned Vending Machines,IEEE,Journals,"Deep learning is a popular direction in computer vision and digital image processing. It is widely utilized in many fields, such as robot navigation, intelligent video surveillance, industrial inspection, and aerospace. With the extensive use of deep learning techniques, classification and object detection algorithms have been rapidly developed. In recent years, with the introduction of the concept of unmanned retail, object detection, and image classification play a central role in unmanned retail applications. However, open-source datasets of traditional classification and object detection have not yet been optimized for application scenarios of unmanned retail. Currently, classification and object detection datasets do not exist that focus on unmanned retail solely. Therefore, in order to promote unmanned retail applications by using deep learning-based classification and object detection, in this article we collected more than 30000 images of unmanned retail containers using a refrigerator affixed with different cameras under both static and dynamic recognition environments. These images were categorized into ten kinds of beverages. After manual labeling, images in our constructed dataset contained 155153 instances, each of which was annotated with a bounding box. We performed extensive experiments on this dataset using ten state-of-the-art deep learning-based models. Experimental results indicate great potential of using these deep learning-based models for real-world smart unmanned vending machines.",https://ieeexplore.ieee.org/document/8908822/,IEEE Transactions on Industrial Informatics,Dec. 2020,ieeexplore
,An adaptive Neuro-Fuzzy control approach for motion control of a spacecraft maneuver,IEEE,Conferences,"This paper proposes an adaptive Neuro-Fuzzy control approach to predict the torque required to control the attitude and rate in a spacecraft maneuver. In the real world environment, the mathematical models of many complex systems are often not accurate, due to the presence of continuous disturbances that effect their dynamic equations, in addition to errors in parameter knowledge. Consequently, methods that rely less on precise mathematical models are often preferred. One such Adaptive Machine Learning Technique is proposed for motion control in spacecraft maneuver. The controller uses an inverse learning Adaptive Neuro-Fuzzy Inference System (ANFIS) model only to train itself from certain desired trajectories and tries to mimic the same in its response. Ideally, these training trajectories are obtained by directly measuring the spacecraft manuever response for various test inputs. Once the system is fully trained, the manuever is tested on a new trajectory with uncertain plant dynamics. However, for algorithm validation, trajectories generated through simulations based on mathematical models assumed to be reasonably accurate, can also be used for the training purpose. This approach is used for design and implementation of an ANFIS controller which is shown to work satisfactorily. Further possible developments of the method are outlined.",https://ieeexplore.ieee.org/document/6260276/,"2012 Proceedings of International Conference on Modelling, Identification and Control",24-26 June 2012,ieeexplore
10.1109/AERO.1997.577990,Integrating autonomy technologies into an embedded spacecraft system-flight software system engineering for new millennium,IEEE,Conferences,"Deep Space 1 (DS1) is the first deep-space mission of NASA's New Millennium technology validation program. The DS1 flight software will validate five autonomy technologies: 1) Planner/Scheduler, which receives ground or on-board requests for spacecraft activities and schedules them to resolve any resource conflicts or timing constraints; 2) Smart Executive, which expands planned activities into lower-level commands, deduces required hardware configurations or other actions, and provides detection and avoidance of constraint violations; 3) Mode Identification and Reconfiguration engine, which incorporates models of hardware and software behavior, detects discrepancies due to hardware or software failures, and requests recovery actions via the Smart Executive. 4) Autonomous Navigation, which determines the spacecraft trajectory from images of asteroids against the celestial sphere, and autonomously adjusts the trajectory to reach the target asteroid or comet. 5) Beacon Monitoring, which uses radio carrier modification and telemetry summarization to simplify ground monitoring of spacecraft health. Integration of these technologies into the spacecraft flight software architecture has presented a number of system engineering challenges, Some of these technologies were developed in a research-oriented, non-real-time, artificial intelligence organizational culture while spacecraft software is typically developed in a strong real-time, algorithmically-oriented culture. The Navigation technology has been developed in a ground-based environment. Integration of these different cultures and mutual education of the software team has been achieved. An early rapid prototype of an existing spacecraft design proved very valuable in educating the team members and in working out the development process.",https://ieeexplore.ieee.org/document/577990/,1997 IEEE Aerospace Conference,13-13 Feb. 1997,ieeexplore
10.1109/AERO47225.2020.9172454,Semi-Supervised Machine Learning for Spacecraft Anomaly Detection &amp; Diagnosis,IEEE,Conferences,"This paper describes Anomaly Detection via Topological-feature Map (ADTM), a data-driven approach to Integrated System Health Management (ISHM) for monitoring the health of spacecraft and space habitats. Developed for NASA Ames Research Center, ADTM leverages proven artificial intelligence techniques for rapidly detecting and diagnosing anomalies in near real-time. ADTM combines Self-Organizing Maps (SOMs) as the basis for modeling system behavior with supervised machine learning techniques for localizing detected anomalies. A SOM is a two-layer artificial neural network (ANN) that produces a low-dimensional representation of the training samples. Once trained on normal system behavior, SOMs are adept at detecting behavior previously not encountered in the training data. Upon detecting anomalous behavior, ADTM uses a supervised classification approach to determine a subset of measurands that characterize the anomaly. This allows it to localize faults and thereby provide extra insight. We demonstrate the effectiveness of our approach on telemetry data collected from a lab-stationed CubeSat (the LabSat) connected to software that gave us the ability to trigger several real hardware faults. We include an analysis and discussion of ADTM's performance on several of these fault cases. We conclude with a brief discussion of future work, which contains investigation of a hierarchical SOM-architecture as well as a Case-Based Reasoning module for further assisting astronauts in diagnosis and remediation activities.",https://ieeexplore.ieee.org/document/9172454/,2020 IEEE Aerospace Conference,7-14 March 2020,ieeexplore
10.5220/0005122004210428,Spacecraft solar arrays degradation forecasting with evolutionary designed ANN-based predictors,IEEE,Conferences,The problem of forecasting the degradation of spacecraft solar arrays is considered. The application of ANN-based predictors is proposed and their automated design with self-adaptive evolutionary and bio-inspired algorithms is suggested. The adaptation of evolutionary algorithms is implemented on the base of the algorithms' self-configuration. The island model for the bio-inspired algorithms cooperation is used. The performance of four developed algorithms for automated design of ANN-based predictors is estimated on real-world data and the most perspective approach is determined.,https://ieeexplore.ieee.org/document/7049803/,"2014 11th International Conference on Informatics in Control, Automation and Robotics (ICINCO)",1-3 Sept. 2014,ieeexplore
10.1109/AERO.2016.7500497,Statistical learning approach for spacecraft systems health monitoring,IEEE,Conferences,"The operations support technology continues to seek more efficient and effective ways to monitor for spacecraft operations and future low earth orbit exploration missions and beyond. This search for improvement has led to a significant movement to advance mission operations monitoring tools. Anomaly detection is an important field for the anticipation of spacecraft operations, working as an enabler of diagnostic and prognostic functions. This article discusses a new real application of a well-known data driven statistical software known as soft independent modeling for class analogy (SIMCA-P) developed by Umetrics to historical telemetry for attitude determination and control system (ADCS) of actual remote sensing spacecraft. Our design work is to detect anomalies from mission control center (MCC) through analyzing the telemetry readings received by MCC and respond to ADCS off-nominal situations by sending corrective actions tele-commands within real time to avoid critical and risky situations. We have implemented our approach on the engineering model of Egyptian satellite project Egypt-Sat1 and this model was our vehicle to simulate and verify the results. In conclusion, the analysis results provide a deep insight information and physical interpretation about the ADCS performance behavior.",https://ieeexplore.ieee.org/document/7500497/,2016 IEEE Aerospace Conference,5-12 March 2016,ieeexplore
10.1109/AERO.2015.7119180,Utilizing Artificial Intelligence to achieve a robust architecture for future robotic spacecraft,IEEE,Conferences,"This paper presents a novel failure-tolerant architecture for future robotic spacecraft. It is based on the Time and Space Partitioning (TSP) principle as well as a combination of Artificial Intelligence (AI) and traditional concepts for system failure detection, isolation and recovery (FDIR). Contrary to classic payload that is separated from the platform, robotic devices attached onto a satellite become an integral part of the spacecraft itself. Hence, the robot needs to be integrated into the overall satellite FDIR concept in order to prevent fatal damage upon hardware or software failure. In addition, complex dexterous manipulators as required for onorbit servicing (OOS) tasks may reach unexpected failure states, where classic FDIR methods reach the edge of their capabilities with respect to successfully detecting and resolving them. Combining, and partly replacing traditional methods with flexible AI approaches aims to yield a control environment that features increased robustness, safety and reliability for space robots. The developed architecture is based on a modular on-board operational framework that features deterministic partition scheduling, an OS abstraction layer and a middleware for standardized inter-component and external communication. The supervisor (SUV) concept is utilized for exception and health management as well as deterministic system control and error management. In addition, a Kohonen self-organizing map (SOM) approach was implemented yielding a real-time robot sensor confidence analysis and failure detection. The SOM features nonsupervized training given a typical set of defined world states. By compiling a set of reviewable three-dimensional maps, alternative strategies in case of a failure can be found, increasing operational robustness. As demonstrator, a satellite simulator was set up featuring a client satellite that is to be captured by a servicing satellite with a 7-DoF dexterous manipulator. The avionics and robot control were integrated on an embedded, space-qualified Airbus e.Cube on-board computer. The experiments showed that the integration of SOM for robot failure detection positively complemented the capabilities of traditional FDIR methods.",https://ieeexplore.ieee.org/document/7119180/,2015 IEEE Aerospace Conference,7-14 March 2015,ieeexplore
10.1109/CVPRW53098.2021.00227,A Monocular Pose Estimation Case Study: The Hayabusa2 Minerva-II2 Deployment,IEEE,Conferences,"In an environment of increasing orbital debris and remote operation, visual data acquisition methods are becoming a core competency of the next generation of spacecraft. However, deep space missions often generate limited data and noisy images, necessitating complex data analysis methods. Here, a state-of-the-art convolutional neural network (CNN) pose estimation pipeline is applied to the Hayabusa2 Minerva-II2 rover deployment; a challenging case with noisy images and a symmetric target. To enable training of this CNN, a custom dataset is created. The deployment velocity is estimated as 0.1908 m/s using a projective geometry approach and 0.1934 m/s using a CNN landmark detector approach, as compared to the official JAXA estimation of 0.1924 m/s (relative to the spacecraft). Additionally, the attitude estimation results from the real deployment images are shared and the associated tumble estimation is discussed.",https://ieeexplore.ieee.org/document/9523026/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),19-25 June 2021,ieeexplore
10.23919/ACC.1990.4791047,A Neural Net Approach to Space Vehicle Guidance,IEEE,Conferences,"The on-line implementation of numerical algorithms for solving the optimum trajectory/guidance problem for advanced space vehicles such as ALS, HLLV, AOTV, transatmospheric vehicles and interplanetary spacecraft is not possible due to their complexity. Hence, the current approach to the development of real-time guidance laws for these advanced space vehicles is to use approximation theory to obtain closed-loop guidance laws. Neural networks offer an alternative to the derivation and implementation of guidance laws. In this paper, we formulate the space vehicle guidance problem using a neural network approach and investigate the appropriate neural net architecture for modelling optimum guidance trajectories. In particular, we investigate the incorporation of a priori knowledge about the characteristics of the optimal guidance solution into the neural network architecture. The online classification performance of the developed network is demonstrated using a synthesized network trained with a data base of optimum guidance trajectories. Such a neural network based guidance approach can readily adapt to environment uncertainties such as those encountered by an AOTV during atmospheric maneuvers.",https://ieeexplore.ieee.org/document/4791047/,1990 American Control Conference,23-25 May 1990,ieeexplore
10.1109/AERO50100.2021.9438232,A Pipeline for Vision-Based On-Orbit Proximity Operations Using Deep Learning and Synthetic Imagery,IEEE,Conferences,"Deep learning has become the gold standard for image processing over the past decade. Simultaneously, we have seen growing interest in orbital activities such as satellite servicing and debris removal that depend on proximity operations between spacecraft. However, two key challenges currently pose a major barrier to the use of deep learning for vision-based on-orbit proximity operations. Firstly, efficient implementation of these techniques relies on an effective system for model development that streamlines data curation, training, and evaluation. Secondly, a scarcity of labeled training data (images of a target spacecraft) hinders creation of robust deep learning models. This paper presents an open-source deep learning pipeline, developed specifically for on-orbit visual navigation applications, that addresses these challenges. The core of our work consists of two custom software tools built on top of a cloud architecture that interconnects all stages of the model development process. The first tool leverages Blender, an open-source 3D graphics toolset, to generate labeled synthetic training data with configurable model poses (positions and orientations), lighting conditions, backgrounds, and commonly observed in-space image aberrations. The second tool is a plugin-based framework for effective dataset curation and model training; it provides common functionality like metadata generation and remote storage access to all projects while giving complete independence to project-specific code. Time-consuming, graphics-intensive processes such as synthetic image generation and model training run on cloud-based computational resources which scale to any scope and budget and allow development of even the largest datasets and models from any machine. The presented system has been used in the Texas Spacecraft Laboratory with marked benefits in development speed and quality. Remote development, scalable compute, and automatic organization of data and artifacts have dramatically decreased iteration time while increasing reproducibility and system comprehension. Diverse, high-fidelity synthetic images that more closely replicate the real environment have improved model performance against real-world data. These results demonstrate that the presented pipeline offers tangible benefits to the application of deep learning for vision-based on-orbit proximity operations.",https://ieeexplore.ieee.org/document/9438232/,2021 IEEE Aerospace Conference (50100),6-13 March 2021,ieeexplore
10.23919/CCC50068.2020.9188783,A Smartphone-Based Networked Control Platform: Design and Implementation,IEEE,Conferences,"The rapid development of embedded systems and IT makes electrical devices functional and tiny. This progress is especially reflected in the smartphone industry. In this paper, a novel applicable wireless control platform based on smartphones is proposed. The system is of high value due to the advantages of smartphones such as mobility, resourcefulness, tiny and etc. Mechanisms are designed in this paper to guarantee the real-timeness of the controller. To ease the controller implementation, code auto-generation technique is integrated to the platform, which is able to convert Simulink block diagrams to executable files for Android smartphones. In addition, protocols are designed to transfer data to remote workstations such that the control system can be supervised and monitored on-line. At last, the control platform is tested on a spacecraft simulator. The experimental results validate the effectiveness and reliability of the designed platform.",https://ieeexplore.ieee.org/document/9188783/,2020 39th Chinese Control Conference (CCC),27-29 July 2020,ieeexplore
10.1109/IROS.2014.6942859,A machine learning approach for real-time reachability analysis,IEEE,Conferences,"Assessing reachability for a dynamical system, that is deciding whether a certain state is reachable from a given initial state within a given cost threshold, is a central concept in controls, robotics, and optimization. Direct approaches to assess reachability involve the solution to a two-point boundary value problem (2PBVP) between a pair of states. Alternative, indirect approaches involve the characterization of reachable sets as level sets of the value function of an appropriate optimal control problem. Both methods solve the problem accurately, but are computationally intensive and do no appear amenable to real-time implementation for all but the simplest cases. In this work, we leverage machine learning techniques to devise query-based algorithms for the approximate, yet real-time solution of the reachability problem. Specifically, we show that with a training set of pre-solved 2PBVP problems, one can accurately classify the cost-reachable sets of a differentially-constrained system using either (1) locally-weighted linear regression or (2) support vector machines. This novel, query-based approach is demonstrated on two systems: the Dubins car and a deep-space spacecraft. Classification errors on the order of 10% (and often significantly less) are achieved with average execution times on the order of milliseconds, representing 4 orders-of-magnitude improvement over exact methods. The proposed algorithms could find application in a variety of time-critical robotic applications, where the driving factor is computation time rather than optimality.",https://ieeexplore.ieee.org/document/6942859/,2014 IEEE/RSJ International Conference on Intelligent Robots and Systems,14-18 Sept. 2014,ieeexplore
10.1109/CSCWD.2006.253127,An OBSM Method for Real Time Embedded System,IEEE,Conferences,"The traditional OBSM method in spacecraft and other real time embedded system is to re-compile the whole system after source code modification, and then reboot the target machine with the new version software. This hurts the availability of the system. Most of the new methods are based software or hardware redundancy. To reduce the cost and make the OBSM more conveniently, we classify OBSM, and patch the target system with different method according to different OBSM type, so the new solution can reduce the cost",https://ieeexplore.ieee.org/document/4019163/,2006 10th International Conference on Computer Supported Cooperative Work in Design,3-5 May 2006,ieeexplore
,An adaptive Neuro-Fuzzy control approach for motion control of a spacecraft maneuver,IEEE,Conferences,"This paper proposes an adaptive Neuro-Fuzzy control approach to predict the torque required to control the attitude and rate in a spacecraft maneuver. In the real world environment, the mathematical models of many complex systems are often not accurate, due to the presence of continuous disturbances that effect their dynamic equations, in addition to errors in parameter knowledge. Consequently, methods that rely less on precise mathematical models are often preferred. One such Adaptive Machine Learning Technique is proposed for motion control in spacecraft maneuver. The controller uses an inverse learning Adaptive Neuro-Fuzzy Inference System (ANFIS) model only to train itself from certain desired trajectories and tries to mimic the same in its response. Ideally, these training trajectories are obtained by directly measuring the spacecraft manuever response for various test inputs. Once the system is fully trained, the manuever is tested on a new trajectory with uncertain plant dynamics. However, for algorithm validation, trajectories generated through simulations based on mathematical models assumed to be reasonably accurate, can also be used for the training purpose. This approach is used for design and implementation of an ANFIS controller which is shown to work satisfactorily. Further possible developments of the method are outlined.",https://ieeexplore.ieee.org/document/6260276/,"2012 Proceedings of International Conference on Modelling, Identification and Control",24-26 June 2012,ieeexplore
10.1109/ICMA.2014.6885911,Attitude determination method and experimental research based on MEMS IMU and magnetoresistive sensor,IEEE,Conferences,"To evaluate and improve the performance of Micro-Electro-Mechanical Systems(MEMS) sensors and magnetoresistive sensor, an effective calibration method is implemented to evaluate the significant error sources such as bias, scale factor, and misalignment. These errors are estimated in virtue of static tests, rate tests. Furthermore, an intelligent attitude determination method of pointing a spinning spacecraft using only magnetic devices is proposed. The pitch angle of spinning spacecraft is estimated by artificial neural network on line. Real data test is performed to validate and evaluate the proposed approach. The experimental results show that this method is effective in calibrating the MEMS IMU and magnetoresistive sensor. The proposed intelligent attitude determination method by magnetometer data only can provide the high dynamic accuracy of less than 1.5 degree.",https://ieeexplore.ieee.org/document/6885911/,2014 IEEE International Conference on Mechatronics and Automation,3-6 Aug. 2014,ieeexplore
10.1109/eScience.2014.7,Automated Real-Time Classification and Decision Making in Massive Data Streams from Synoptic Sky Surveys,IEEE,Conferences,"The nature of scientific and technological data collection is evolving rapidly: data volumes and rates grow exponentially, with increasing complexity and information content, and there has been a transition from static data sets to data streams that must be analyzed in real time. Interesting or anomalous phenomena must be quickly characterized and followed up with additional measurements via optimal deployment of limited assets. Modern astronomy presents a variety of such phenomena in the form of transient events in digital synoptic sky surveys, including cosmic explosions (supernovae, gamma ray bursts), relativistic phenomena (black hole formation, jets), potentially hazardous asteroids, etc. We have been developing a set of machine learning tools to detect, classify and plan a response to transient events for astronomy applications, using the Catalina Real-time Transient Survey (CRTS) as a scientific and methodological testbed. The ability to respond rapidly to the potentially most interesting events is a key bottleneck that limits the scientific returns from the current and anticipated synoptic sky surveys. Similar challenge arise in other contexts, from environmental monitoring using sensor networks to autonomous spacecraft systems. Given the exponential growth of data rates, and the time-critical response, we need a fully automated and robust approach. We describe the results obtained to date, and the possible future developments.",https://ieeexplore.ieee.org/document/6972266/,2014 IEEE 10th International Conference on e-Science,20-24 Oct. 2014,ieeexplore
10.1109/RAST.2005.1512560,Autonomous onboard computer systems using real time trace models,IEEE,Conferences,"In the paper some particularities of the autonomous spacecraft computer systems are discussed. The ways to reach the autonomy are described - high reliability and adaptability of the system, reached by application of AI elements formal structural models (specification, verification and real time control) and dynamic reconfiguration. A method using trace models In real time for testing of system operations is described. The method is suitable for systems specified by process algebras (CSP - Communicating Sequential Processes, Timed CSP, ASM - Abstract State Machine, and TAM - Temporal Agent Model) where process tracts are used. The computing processes and these in the hardware are observed. Special control processes (tracers) are specified. They allow to control in real time the correspondence of the real system traces and preliminary computed traces (the formal specification) and send messages to the other subsystems when no correspondence (e.g. to specification and dynamic reconfiguration subsystems). The method allows finding in real, time the software and hardware incorrectly functioning (disparity of the traces) based on rigorous mathematical specification. In the paper are described: an autonomous control system structural model, an algorithm for real time traces control, software simulations (laboratory models) of autonomous systems, the faults and reaction of the tracer-processes, all based on CPPCSP - C++ Communicating Sequential Processes library. The application of trace models in real time allows find any system incorrectly operating and this is one of the ways for,increasing the onboard computer systeme reliability and to achieve full operation autonomy.",https://ieeexplore.ieee.org/document/1512560/,"Proceedings of 2nd International Conference on Recent Advances in Space Technologies, 2005. RAST 2005.",9-11 June 2005,ieeexplore
10.1109/AERO.2017.7943916,Classification of multi-failure mechanisms in space operations in using novel PLS-DA approach,IEEE,Conferences,"The article addresses an innovative statistical classification methodology to the real spacecraft telemetry based on statistical multivariate latent technique called projection to latent structure discriminant analysis (PLS-DA). The models are generated via using a well-known statistical multivariate software called soft independent modeling for class analogy (SIMCA-P) developed by Umetrics which is used for data exploration and classification. Models are used to detect and classify multi-failure mechanisms in the attitude determination and control subsystem (ADCS) of Egypt-Sat1 for the first time. Models taken altogether the ADCS two faults mechanisms for both angular velocity meters and reaction wheel malfunctions in one model, as well as to identify key contributors to inconsistent events autonomously which lead to characterize the spacecraft (ADCS) behavior. The analysis results lead to give a deep explanation to the system state-of-health (SOH) and characterize its operation behavior during the mission phases.",https://ieeexplore.ieee.org/document/7943916/,2017 IEEE Aerospace Conference,4-11 March 2017,ieeexplore
10.1109/AERO50100.2021.9438171,Considerations in the Deployment of Machine Learning Algorithms on Spaceflight Hardware,IEEE,Conferences,"Recent advances in artificial intelligence (AI) and machine learning (ML) have revolutionized many fields. ML has many potential applications in the space domain. Next generation space instruments are producing data at rates that exceed the capabilities of current spacecraft to store or transmit to ground stations. Deployment of ML algorithms onboard future spacecraft could perform processing of sensor data as it is gathered, reducing data volume and providing a dramatic increase in throughput of meaningful data. ML techniques may also be used to enhance the autonomy of space missions. However ML techniques have not yet been widely deployed in space environments, primarily due to limitations on the computational capabilities of spaceflight hardware. The need to verify that high-performance computational hardware can reliably operate in this environment delays the adoption of these technologies. Nevertheless, the availability of advanced processing capabilities onboard spacecraft is increasing. These platforms may not provide the processing power of terrestrial equivalents, but they do provide the resources necessary for deploying real-time execution of ML algorithms. In this paper, we present results exploring the implementation of ML techniques on computationally-constrained, high-reliability spacecraft hardware. We show two ML algorithms utilizing deep learning techniques which illustrate the utility of these approaches for space applications. We describe implementation considerations when tailoring these algorithms for execution on computationally-constrained hardware and present a workflow for performing these optimizations. We also present initial results on characterizing the trade space between algorithm accuracy, throughput, and reliability on a variety of hardware platforms with current and anticipated paths to spaceflight.",https://ieeexplore.ieee.org/document/9438171/,2021 IEEE Aerospace Conference (50100),6-13 March 2021,ieeexplore
10.1109/AERO.1997.574422,ESL: a language for supporting robust plan execution in embedded autonomous agents,IEEE,Conferences,"ESL (Execution Support Language) is a language for encoding execution knowledge in embedded autonomous agents. It is similar in spirit to RAPs (1989), RS (1983), and RPL Reactive Plan Language, and its design owes much to these systems. Unlike its predecessors, ESL aims for a more utilitarian point in the design space. ESL was designed primarily to be a powerful and easy-to-use tool, not to serve as a representation for automated reasoning or formal analysis (although nothing precludes its use for these purposes). ESL consists of several sets of loosely coupled features that can be composed in arbitrary ways. It is currently implemented as a set of extensions to Common Lisp, and is being used to build the executive component of a control architecture for an autonomous spacecraft.",https://ieeexplore.ieee.org/document/574422/,1997 IEEE Aerospace Conference,13-13 Feb. 1997,ieeexplore
10.1109/AERO.1999.794302,Integrating autonomous fault management with conventional flight software: a case study,IEEE,Conferences,"An evolutionary approach to on-board autonomy involves integrating autonomous processes with traditional spacecraft real-time flight software (FSW). Such an approach relies on layering autonomy onto existing architecture while making minimal changes to the existing FSW. This is distinctly different from a revolutionary approach, which calls for integrating the design of autonomy and FSW from the start. While a revolutionary approach may lead to a more optimal design, an evolutionary approach allows for reuse of proven FSW, facilitates the complicated process of software verification and validation, and accelerates acceptance by the aerospace community. Because intelligent autonomous agents are typically soft real-time processes while FSW typically meets hard real-time requirements, applying the evolutionary approach is not a straightforward task. One technique available for solving this problem is Rate Monotonic Analysis (RMA). RMA uses real-time scheduling theory to guarantee that all processes can be scheduled. It has been used successfully in real-time scheduling applications such as helicopter control systems. This paper presents a case study where RMA was used as a tool to resolve real-time scheduling and processing issues associated with adding autonomous fault isolation and recovery software to previously developed flight software. The combined software system was then successfully tested on a real-time testbed.",https://ieeexplore.ieee.org/document/794302/,1999 IEEE Aerospace Conference. Proceedings (Cat. No.99TH8403),7-7 March 1999,ieeexplore
10.1109/AERO.1997.577990,Integrating autonomy technologies into an embedded spacecraft system-flight software system engineering for new millennium,IEEE,Conferences,"Deep Space 1 (DS1) is the first deep-space mission of NASA's New Millennium technology validation program. The DS1 flight software will validate five autonomy technologies: 1) Planner/Scheduler, which receives ground or on-board requests for spacecraft activities and schedules them to resolve any resource conflicts or timing constraints; 2) Smart Executive, which expands planned activities into lower-level commands, deduces required hardware configurations or other actions, and provides detection and avoidance of constraint violations; 3) Mode Identification and Reconfiguration engine, which incorporates models of hardware and software behavior, detects discrepancies due to hardware or software failures, and requests recovery actions via the Smart Executive. 4) Autonomous Navigation, which determines the spacecraft trajectory from images of asteroids against the celestial sphere, and autonomously adjusts the trajectory to reach the target asteroid or comet. 5) Beacon Monitoring, which uses radio carrier modification and telemetry summarization to simplify ground monitoring of spacecraft health. Integration of these technologies into the spacecraft flight software architecture has presented a number of system engineering challenges, Some of these technologies were developed in a research-oriented, non-real-time, artificial intelligence organizational culture while spacecraft software is typically developed in a strong real-time, algorithmically-oriented culture. The Navigation technology has been developed in a ground-based environment. Integration of these different cultures and mutual education of the software team has been achieved. An early rapid prototype of an existing spacecraft design proved very valuable in educating the team members and in working out the development process.",https://ieeexplore.ieee.org/document/577990/,1997 IEEE Aerospace Conference,13-13 Feb. 1997,ieeexplore
10.1109/SOFTWAREMINING.2017.8100847,Mining temporal intervals from real-time system traces,IEEE,Conferences,"We introduce a novel algorithm for mining temporal intervals from real-time system traces with linear complexity using passive, black-box learning. Our interest is in mining nfer specifications from spacecraft telemetry to improve human and machine comprehension. Nfer is a recently proposed formalism for inferring event stream abstractions with a rule notation based on Allen Logic. The problem of mining Allen's relations from a multivariate interval series is well studied, but little attention has been paid to generating such a series from symbolic time sequences such as system traces. We propose a method to automatically generate an interval series from real-time system traces so that they may be used as inputs to existing algorithms to mine nfer rules. Our algorithm has linear runtime and constant space complexity in the length of the trace and can mine infrequent intervals of arbitrary length from incomplete traces. The paper includes results from case studies using logs from the Curiosity rover on Mars and two other realistic datasets.",https://ieeexplore.ieee.org/document/8100847/,2017 6th International Workshop on Software Mining (SoftwareMining),3-3 Nov. 2017,ieeexplore
10.1109/CCA.1993.348267,Multivariable neural network vibration control based on output feedback,IEEE,Conferences,"This paper presents a multivariable direct adaptive control concept for vibration suppression in flexible space structures. The adaptive controller is implemented by a combination of forward neural networks. Tuning of the controller gains (neural network synaptic weights) takes place in real-time and is performed by a nonlinear least squares algorithm. The control scheme is based on output rather than state feedback, an approach motivated from the fact that, in most applications, the system state is not readily available. The results are demonstrated by simulation using a high fidelity 6-input 6-output dynamic model of the testbed at the JPL/USAF-PL Large Spacecraft Control Laboratory.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/348267/,Proceedings of IEEE International Conference on Control and Applications,13-16 Sept. 1993,ieeexplore
10.1109/NAECON.2018.8556744,Onboard Image Processing for Small Satellites,IEEE,Conferences,"In general, the computational ability of spacecraft and satellites has lagged behind terrestrial computers by several generations. Moore's Law turns the supercomputers of yesterday into the laptops of today, but space computing remains relatively underpowered due to the harsh radiation environment and low risk-tolerance of most space missions. Space missions are generally low risk because of the high cost of components and launch. However, launch costs are drastically decreasing and innovations such as CubeSats are changing the risk equation. By accepting more risk and utilizing commercial of the shelf (COTS) parts, it is possible to cheaply build and launch extremely capable computing platforms into space. High performance satellites will be required for advanced interplanetary exploration due to latency challenges. The long transmission times between planets means satellites or robotic explorers need onboard processing to perform tasks in real-time. This paper explores one possible application that could be hosted onboard the next generation of high performance satellites, performing object classification on satellite imagery. Automation of satellite imagery processing is currently performed by servers or workstations on Earth, but this paper will show that those algorithms can be moved onboard satellites by using COTS components. First traditional computer vision techniques such as edge detection and sliding windows are used to detect possible objects on the open ocean. Then a modern neural network architecture is used to classify the object as a ship or not. This application is implemented on a Nvidia Jetson TX2 and measurements of the application's power use confirm that it fits within the Size Weight and Power (SWAP) requirements of SmallSats and possibly even CubeSats.",https://ieeexplore.ieee.org/document/8556744/,NAECON 2018 - IEEE National Aerospace and Electronics Conference,23-26 July 2018,ieeexplore
10.1109/AERO47225.2020.9172454,Semi-Supervised Machine Learning for Spacecraft Anomaly Detection &amp; Diagnosis,IEEE,Conferences,"This paper describes Anomaly Detection via Topological-feature Map (ADTM), a data-driven approach to Integrated System Health Management (ISHM) for monitoring the health of spacecraft and space habitats. Developed for NASA Ames Research Center, ADTM leverages proven artificial intelligence techniques for rapidly detecting and diagnosing anomalies in near real-time. ADTM combines Self-Organizing Maps (SOMs) as the basis for modeling system behavior with supervised machine learning techniques for localizing detected anomalies. A SOM is a two-layer artificial neural network (ANN) that produces a low-dimensional representation of the training samples. Once trained on normal system behavior, SOMs are adept at detecting behavior previously not encountered in the training data. Upon detecting anomalous behavior, ADTM uses a supervised classification approach to determine a subset of measurands that characterize the anomaly. This allows it to localize faults and thereby provide extra insight. We demonstrate the effectiveness of our approach on telemetry data collected from a lab-stationed CubeSat (the LabSat) connected to software that gave us the ability to trigger several real hardware faults. We include an analysis and discussion of ADTM's performance on several of these fault cases. We conclude with a brief discussion of future work, which contains investigation of a hierarchical SOM-architecture as well as a Case-Based Reasoning module for further assisting astronauts in diagnosis and remediation activities.",https://ieeexplore.ieee.org/document/9172454/,2020 IEEE Aerospace Conference,7-14 March 2020,ieeexplore
10.5220/0005122004210428,Spacecraft solar arrays degradation forecasting with evolutionary designed ANN-based predictors,IEEE,Conferences,The problem of forecasting the degradation of spacecraft solar arrays is considered. The application of ANN-based predictors is proposed and their automated design with self-adaptive evolutionary and bio-inspired algorithms is suggested. The adaptation of evolutionary algorithms is implemented on the base of the algorithms' self-configuration. The island model for the bio-inspired algorithms cooperation is used. The performance of four developed algorithms for automated design of ANN-based predictors is estimated on real-world data and the most perspective approach is determined.,https://ieeexplore.ieee.org/document/7049803/,"2014 11th International Conference on Informatics in Control, Automation and Robotics (ICINCO)",1-3 Sept. 2014,ieeexplore
10.1109/AERO.2016.7500497,Statistical learning approach for spacecraft systems health monitoring,IEEE,Conferences,"The operations support technology continues to seek more efficient and effective ways to monitor for spacecraft operations and future low earth orbit exploration missions and beyond. This search for improvement has led to a significant movement to advance mission operations monitoring tools. Anomaly detection is an important field for the anticipation of spacecraft operations, working as an enabler of diagnostic and prognostic functions. This article discusses a new real application of a well-known data driven statistical software known as soft independent modeling for class analogy (SIMCA-P) developed by Umetrics to historical telemetry for attitude determination and control system (ADCS) of actual remote sensing spacecraft. Our design work is to detect anomalies from mission control center (MCC) through analyzing the telemetry readings received by MCC and respond to ADCS off-nominal situations by sending corrective actions tele-commands within real time to avoid critical and risky situations. We have implemented our approach on the engineering model of Egyptian satellite project Egypt-Sat1 and this model was our vehicle to simulate and verify the results. In conclusion, the analysis results provide a deep insight information and physical interpretation about the ADCS performance behavior.",https://ieeexplore.ieee.org/document/7500497/,2016 IEEE Aerospace Conference,5-12 March 2016,ieeexplore
10.1109/ICSMC.1993.385064,Use of case-based reasoning techniques for intelligent computer-aided-design systems,IEEE,Conferences,"Reuse of designs is an important research direction for the future intelligent CAD systems. The main applications of such a research are various, from mechanical systems design (spacecraft, robot, ...) to software design. This paper will present a survey of the use of case-based reasoning (CBR) techniques for intelligent CAD systems in order to reuse designs or parts of designs. First, we will briefly resume some work issued from cognitive psychology, showing the importance of analogical-reasoning for design activities and then the origins of the CBR technology in AI. Second, we will then present the main systems using case-based reasoning for design activities followed by a comparative analysis between these systems. To conclude, we will indicate the main directions in CBR for design and will propose to adopt a cognitive approach from knowledge acquisition until the development of real design support systems.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/385064/,Proceedings of IEEE Systems Man and Cybernetics Conference - SMC,17-20 Oct. 1993,ieeexplore
10.1109/AERO.2015.7119180,Utilizing Artificial Intelligence to achieve a robust architecture for future robotic spacecraft,IEEE,Conferences,"This paper presents a novel failure-tolerant architecture for future robotic spacecraft. It is based on the Time and Space Partitioning (TSP) principle as well as a combination of Artificial Intelligence (AI) and traditional concepts for system failure detection, isolation and recovery (FDIR). Contrary to classic payload that is separated from the platform, robotic devices attached onto a satellite become an integral part of the spacecraft itself. Hence, the robot needs to be integrated into the overall satellite FDIR concept in order to prevent fatal damage upon hardware or software failure. In addition, complex dexterous manipulators as required for onorbit servicing (OOS) tasks may reach unexpected failure states, where classic FDIR methods reach the edge of their capabilities with respect to successfully detecting and resolving them. Combining, and partly replacing traditional methods with flexible AI approaches aims to yield a control environment that features increased robustness, safety and reliability for space robots. The developed architecture is based on a modular on-board operational framework that features deterministic partition scheduling, an OS abstraction layer and a middleware for standardized inter-component and external communication. The supervisor (SUV) concept is utilized for exception and health management as well as deterministic system control and error management. In addition, a Kohonen self-organizing map (SOM) approach was implemented yielding a real-time robot sensor confidence analysis and failure detection. The SOM features nonsupervized training given a typical set of defined world states. By compiling a set of reviewable three-dimensional maps, alternative strategies in case of a failure can be found, increasing operational robustness. As demonstrator, a satellite simulator was set up featuring a client satellite that is to be captured by a servicing satellite with a 7-DoF dexterous manipulator. The avionics and robot control were integrated on an embedded, space-qualified Airbus e.Cube on-board computer. The experiments showed that the integration of SOM for robot failure detection positively complemented the capabilities of traditional FDIR methods.",https://ieeexplore.ieee.org/document/7119180/,2015 IEEE Aerospace Conference,7-14 March 2015,ieeexplore
10.1109/AERO47225.2020.9172453,Utilizing Reinforcement Learning to Autonomously Mange Buffers in a Delay Tolerant Network Node,IEEE,Conferences,"In order to effectively communicate with Earth from deep space there is a need for network automation similar to that of the Internet. The existing automated network protocols, such as TCP and IP, cannot work in deep space due to the assumptions under which they were designed. Specifically, protocols assume the existence of an end-to-end path between the source and destination for the entirety of a communication session and the path being traversable in a negligible amount of time. In contrast, a Delay Tolerant Network is a set of protocols that allows networking in environments where links suffer from high-delay or disruptions (e.g. Deep Space). These protocols rely on different assumptions such as time synchronization and suitable memory allocation. In this paper, we consider the problem of autonomously avoiding memory overflows in a Delay Tolerant Node. To that end, we propose using Reinforcement Learning to automate buffer management given that we can easily measure the relative rates of data coming in and out of the DTN node. In the case of detecting overflow, we let the autonomous agent choose between three actions: slowing down the client, requesting more resources from the Deep Space Network, or selectively dropping packets once the buffer nears capacity. Furthermore, we show that all of these actions can be realistically implemented in real-life operations given current and planned capabilities of Delay Tolerant Networking and the Deep Space Network. Similarly, we also show that using Reinforcement Learning for this problem is well suited to this application due to the number of possible states and variables, as well as the fact that large distances between deep space spacecraft and Earth prevent human-in-the-loop intervention.",https://ieeexplore.ieee.org/document/9172453/,2020 IEEE Aerospace Conference,7-14 March 2020,ieeexplore
10.1109/ICCSCE.2011.6190480,[Front cover],IEEE,Conferences,The following topics are dealt with: output membership function; fuzzy logic controller; self adaptive neuro-fuzzy control; FES-assisted paraplegics indoor rowing exercise; indoor empirical path loss prediction model; 2.4 GHz 802.11N network; moving vehicle detection; RGB removal shadow segmentation; modeling virtual driving environment; regressive linear prediction; speech signals; wheel acceleration/deceleration; integrated stability control systems; nonlinear PID control; multiple input-single output model; MPPT; electronically tunable voltage-mode MIMO universal filter; high input impedance voltage-mode universal filter; OTA; parallel distributed fuzzy LQR controller; double-pendulum-type overhead cranes; ANFIS based modeling; overtaking maneuver trajectory; ASTER satellite data; spline interpolation technique; sea bed logging method; evolutionary normal-boundary intersection method; heterogeneous wireless sensor network; high precision laser tracker system; contactless position measurement; semantic Web ontologies; social network sites; Malay text-to-speech system; allophone synthesis; dynamic programming approach; pipelined optical bus systems; mainstream software sharing platform; oil &amp; gas exploration and development; remotely operated vehicle control system; surrogate modeling; model based sensor fault tolerant control system; micromachined thermal conductivity sensor; dual stack IPv4/IPv6 testbed; malware detection; digital evidence container; security convergence; global stability; continuous time delayed linear system; LMI based approach; robust adaptive controller; observer design; discrimination electrical power; multistage centrifugal compressors; remote controlled HD videoconference system; knowledge management; wave generator system; STATCOM; blood cell image segmentation; wireless controller area ;ARM microcontroller; ECG signals based mental stress assessment; wavelet transform; genetic-optimized neuro-fuzzy inference system; meshless local Petrov-Galerkin method; power plant automation; SCADA systems:; energy potential detection; autarkic smart object design; liquid level control; focal epileptic seizure forecasting; artificial neural networks; patch antennas arrayanti-swing control; double-pendulum-type overhead crane; distributed fuzzy LQR controller; genetic fuzzy rule set selection;force control; SMA actuated gripper; self tuning fuzzy PID controller; multivariable system; recurrent diagonal neural network; customized mobile learning management system; multi-flow rate mode selection; pneumatic dispensing valve system; clonal selection based artificial immune system algorithm; adaptive nonlinear PID controller; nonholonomic mobile robot; data compression technique; global solar radiation;1.8GHz electromagnetic field exposure; electro-hydraulic actuator; Methyl Tert-butyl Ether production; reactive distillation; robust control design; spacecraft attitude systems; level drum process control training system; time-delayed feedback control; chaotic T-S fuzzy systems; photovoltaic panels Perturbation; MPPT Method; island-mode doubly-fed induction generator; reactive power control; active power control; contactless optical sensor system; automatic column-based data object clustering; multilingual databases; mycobacterium tuberculosis detection; intelligent software agents; auditory wavelet packet filters; multiple intersections traffic signal timing optimization; overlapping vehicle tracking; adaptive particle filter; endoscopic image compression; double density discrete wavelet transform; Malaysian English accents identification; AR modeling techniques; online news management; double weight codes amplitude coding optical CDMA system network; output feedback sliding mode control; chaotic trajectory tracking; electro-hydraulic actuator system; discrete sliding mode control; retentive backtracking bit ; anti-collision algorithm; RFID systems; humanoid robot NAO; face detection technique; robotic assistive therapy; fractional order PI controller; modular general purpose controller board; biologically inspired robot; bundle branch blocks; multilayered perceptron network; helical antenna prototype; wireless power transmission; optimal control; nonlinear inverted pendulum dynamical system; 1.8 GHz radio frequency signal radiation effects; WiFi electromagnetic radiation; MRI brain classification; principal component analysis; electromagnetic generator; speed sensorless field oriented control; parallel- connected dual PMSM; solar radiation data analysis; Daubechies wavelets; dual-power PV-grid energy system; thermal energy storage system; single ortho-rectified high resolution satellite imagery; TopoMap revision; free swinging shank; hemiplegics; scanning resolution and laser scanner.,https://ieeexplore.ieee.org/document/6190480/,"2011 IEEE International Conference on Control System, Computing and Engineering",25-27 Nov. 2011,ieeexplore
10.1109/ACCESS.2021.3064928,Deep Space Network Scheduling via Mixed-Integer Linear Programming,IEEE,Journals,"NASAs Deep Space Network (DSN) is a globally-spanning communications network responsible for supporting the interplanetary spacecraft missions of NASA and other international users. The DSN is a highly utilized asset, and the large demand for its services makes the assignment of DSN resources a daunting computational problem. In this paper we study the DSN scheduling problem, which is the problem of assigning the DSNs limited resources to its users within a given time horizon. The DSN scheduling problem is oversubscribed, meaning that only a subset of the activities can be scheduled, and network operators must decide which activities to exclude from the schedule. We first formulate this challenging scheduling task as a Mixed-Integer Linear Programming (MILP) optimization problem. Next, we develop a sequential algorithm which solves the resulting MILP formulation to produce valid schedules for large-scale instances of the DSN scheduling problem. We use real world DSN data from week 44 of 2016 in order to evaluate our algorithms performance. We find that given a fixed run time, our algorithm outperforms a simple implementation of our MILP model, generating a feasible schedule in which 17% more activities are scheduled by the algorithm than by the simple implementation. We design a non-MILP based heuristic to further validate our results. We find that our algorithm also outperforms this heuristic, scheduling 8% more activities and 20% more tracking time than the best results achieved by the non-MILP implementation.",https://ieeexplore.ieee.org/document/9373338/,IEEE Access,2021,ieeexplore
10.1109/JPROC.2018.2849003,SMC: Satisfiability Modulo Convex Programming,IEEE,Journals,"The design of cyber-physical systems (CPSs) requires methods and tools that can efficiently reason about the interaction between discrete models, e.g., representing the behaviors of cyber components, and continuous models of physical processes. Boolean methods such as satisfiability (SAT) solving are successful in tackling large combinatorial search problems for the design and verification of hardware and software components. On the other hand, problems in control, communications, signal processing, and machine learning often rely on convex programming as a powerful solution engine. However, despite their strengths, neither approach would work in isolation for CPSs. In this paper, we present a new satisfiability modulo convex programming (SMC) framework that integrates SAT solving and convex optimization to efficiently reason about Boolean and convex constraints at the same time. We exploit the properties of a class of logic formulas over Boolean and nonlinear real predicates, termed monotone satisfiability modulo convex formulas, whose satisfiability can be checked via a finite number of convex programs. Following the lazy satisfiability modulo theory (SMT) paradigm, we develop a new decision procedure for monotone SMC formulas, which coordinates SAT solving and convex programming to provide a satisfying assignment or determine that the formula is unsatisfiable. A key step in our coordination scheme is the efficient generation of succinct infeasibility proofs for inconsistent constraints that can support conflict-driven learning and accelerate the search. We demonstrate our approach on different CPS design problems, including spacecraft docking mission control, robotic motion planning, and secure state estimation. We show that SMC can handle more complex problem instances than state-of-the-art alternative techniques based on SMT solving and mixed integer convex programming.",https://ieeexplore.ieee.org/document/8428633/,Proceedings of the IEEE,Sept. 2018,ieeexplore
