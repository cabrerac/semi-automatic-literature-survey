doi,type,publication,publisher,publication_date,database,title,url,abstract,domain,id,status
cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,to_check,semantic_scholar,,2018-01-01 00:00:00,semantic_scholar,comparing human-robot proxemics between virtual reality and the real world,https://www.semanticscholar.org/paper/cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,"Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of Human-Robot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. Comparing Human-Robot Proxemics between Virtual Reality and the Real World Rui Li KTH Royal Institute of Technology Stockholm, Sweden Rui3@kth.se ABSTRACT Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other.Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. INTRODUCTION Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI) [1][2][3][4]. VR has been used to test teleoperation and collect demonstration data to train machine learning algorithms, which showcased the effectiveness of learning visuomotor skills using data collected by consumer-grade devices [1]. VR teleoperation systems were proposed to crowdsource robotic demonstrations at scale [2]. A VR simulation framework was also proposed to replace the physical robot, as VR can enable high level abstraction in embodiment and multimodal interaction [3]. VR has also been used as a rapid prototyping tool to design in-vehicle interactions and interfaces for self-driving cars, which showed the evocation to genuine responses from test participants [4]. Compared to other HRI experiment methods, VR as an emerging interactive media provides unique advantages. VR HRI has the potential of having higher immersion and fidelity than picture based HRI, video-based HRI and simulated HRI. In situations where the perception of the robot is challenging, compared to on-screen viewing, VR display showed significant improvement on collaborative tasks [5]. When comparing VR HRI to the physical, realworld interaction (Live HRI), there is a trade-off between the two. VR experiences still cannot replace physical experiences due to system limitation, and limited interaction modalities etc. [6]. For example, system limitations such as limited field of view and low display resolution could reduce immersion and presence of the VR experience, resulting in different behaviors from Live experiments. Limited interaction modalities, such as the absence of touch, means that the participant could not feel the robot or even go through the robot, which could potentially break the entire interaction. Figure 1: Photograph of the Live experiment setting However, with the help of the distribution of consumer-grade VR devices and online crowdsourcing platforms, VR HRI has the potential to gain massive data for training robotic behavior and studying HRI related issues. Data collection through VR can also reduce noise and improve the data quality [1], which help to ease data processing and algorithm training. Furthermore, VR HRI experiments can test concepts and interactions without physical robots, making it more resource efficient and less expensive than Live HRI. Less hardware also means that the experiment will be less cumbersome to set up, easier to be reproduced and to ensure experiment quality. In this study, HRI Proxemics (the preferred personal space between a human and a robot) was compared to give a better justification and more basic understanding of the relationship between Live and VR. Proxemics preferences rely on lower level intuition [7], therefore, reflect the differences in the perceptions between Live and VR better. Compared to other HRI subject such as conversational (audio) or gaze behavior (visual), which are more modality dependent, proxemics can give a comprehensive understanding of the human responses. In addition, variations of modalities in VR can greatly influence human perception. For example, a higher visual familiarity of the physical environment in VR can decrease the effect of distance distortion [8]. Auditory inputs play another important role in VR, the addition of spatial sound can increase the sense of presence in VR and provide sound localization [9]. Thus, this work also compares VR settings with variance in modalities to evaluate the impacts of visual familiarity and spatial sound on VR HRI experiments. A 2 x 3 mixed design experiment was conducted to evaluate the differences between Live and VR HRI, as well as the influence of visual familiarity and spatial sound in VR. For the Live HRI, the pepper robot from Softbank Robotics was used (Figure 1). In the VR HRI, a 3D model of the same robot was used. To measure visual familiarity, the VR scene was created in Blender based on a 3D scan of the physical lab. The spatial sound was created by enabling the movement of the physical robot, due to the difficulties of engineering spatial sound. The interaction was implemented in Unity. As an objective measurement for proxemics preference, the minimum comfort distance (MCD) was measured. In addition, for the psychological perception of the experience, the feeling of presence was measured with the SUS questionnaire. For the perception of the robot, two relevant factors, competence and discomfort was measured with the ROSAS questionnaire.",autonomous vehicle,1,not included
59e10d1d4cd454635914cfd0ac5160a318fd0473,to_check,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,ub09 session 9,https://www.semanticscholar.org/paper/59e10d1d4cd454635914cfd0ac5160a318fd0473,"In the domain of Wireless Sensor Networks (WSN), providing an effective security solution to protect the motes and their communications is challenging. Due to the hard constraints on performance, storage and energy consumption, normal network-security related techniques cannot be applied. Focusing on the ""Intrusion Detection"" problem, we propose a realworld application of our WSN Intrusion Detection System (WIDS). WIDS exploits the Weak Process Models to classify potential security issues in the WSN and to notify the operators when an attack tentative is detected. In this demonstration, we show how our IDS works, how it detects some basic attacks and how the IDS can evolve to fullfil the needs of secure WSN deployments. Download Paper (PDF) UB09.2 RESCUE: EDA TOOLSET FOR INTERDEPENDENT ASPECTS OF RELIABILITY, SECURITY AND QUALITY IN NANOELECTRONIC SYSTEMS DESIGN Authors: Cemil Cem Gürsoy1, Guilherme Cardoso Medeiros2, Junchao Chen3, Nevin George4, Josie Esteban Rodriguez Condia5, Thomas Lange6, Aleksa Damljanovic5, Raphael Segabinazzi Ferreira4, Aneesh Balakrishnan6, Xinhui Anna Lai1, Shayesteh Masoumian7, Dmytro Petryk3, Troya Cagil Koylu2, Felipe Augusto da Silva8, Ahmet Cagri Bagbaba8 and Maksim Jenihhin1 1Tallinn University of Technology, EE; 2Delft University of Technology, NL; 3IHP, DE; 4BTU Cottbus-Senftenberg, DE; 5Politecnico di Torino, IT; 6IROC Technologies, FR; 7Intrinsic ID B.V., NL; 8Cadence Design Systems GmbH, DE Abstract The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF)The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF) UB09.3 ASAM: AUTOMATIC SYNTHESIS OF ALGORITHMS ON MULTI CHIP/FPGA WITH COMMUNICATION CONSTRAINTS Authors: Amir Masoud Gharehbaghi, Tomohiro Maruoka, Yukio Miyasaka, Akihiro Goda, Amir Masoud Gharehbaghi and Masahiro Fujita, The University of Tokyo, JP Abstract Mapping of large systems/computations on multiple chips/multiple cores needs sophisticated compilation methods. In this demonstration, we present our compiler tools for multi-chip and multi-core systems that considers communication architecture and the related constraints for optimal mapping. Specifically, we demonstrate compilation methods for multi-chip connected with ring topology, and multi-core connected with mesh topology, assuming fine-grained reconfigurable cores, as well as generalization techniques for large problems size as convolutional neural networks. We will demonstrate our mappings methods starting from data-flow graphs (DFGs) and equations, specifically with applications to convolutional neural networks (CNNs) for convolution layers as well as fully connected layers. Download Paper (PDF) UB09.4 HEPSYCODE-MC: ELECTRONIC SYSTEM-LEVEL METHODOLOGY FOR HW/SW CO-DESIGN OF MIXED-CRITICALITY EMBEDDED SYSTEMS Authors: Luigi Pomante1, Vittoriano Muttillo1, Marco Santic1 and Emilio Incerto2 1Università degli Studi dell'Aquila DEWS, IT; 2IMT Lucca, IT Abstract Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF)Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF) UB09.5 CS: CRAZYSQUARE Authors: Federica Caruso1, Federica Caruso1, Tania Di Mascio1, Alessandro D'Errico1, Marco Pennese2, Luigi Pomante1, Claudia Rinaldi1 and Marco Santic1 1University of L'Aquila, IT; 2Ministry of Education, IT Abstract CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF)CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF) UB09.6 LABSMILING: A SAAS FRAMEWORK, COMPOSED OF A NUMBER OF REMOTELY ACCESSIBLE TESTBEDS AND RELATED SW TOOLS, FOR ANALYSIS, DESIGN AND MANAGEMENT OF LOW DATA-RATE WIRELESS PERSONAL AREA NETWORKS BASED ON IEEE 802.15.4 Authors: Carlo Centofanti, Luigi Pomante, Marco Santic and Walter Tiberti, University of L'Aquila, IT Abstract Low data-rate wireless personal area networks (LR-WPANs) are constantly increasing their presence in the fields of IoT, wearable, home automation, health monitoring. The development, deployment and testing of SW based on IEEE 802.15.4 standard (and derivations, e.g. 15.4e), require the exploitation of a testbed as the network grows in complexity and heterogeneity. This demo shows LabSmiling: a SaaS framework which connects testbeds deployed in a real-world-environment and the related SW tools that make available a meaningful (but still scalable) number of physical devices (sensor nodes) to developers. It provides a comforta",autonomous vehicle,2,not included
10.23919/cisti.2017.7975750,to_check,2017 12th Iberian Conference on Information Systems and Technologies (CISTI),IEEE,2017-06-24 00:00:00,ieeexplore,uav simulator for grown-up people quality of life enhancement,https://ieeexplore.ieee.org/document/7975750/,This paper presents the development of a virtual reality simulator for the management of a UAV (Unmanned Aerial Vehicle) focused on improving the quality of life of grown-up people. The present research has collected characteristics of gestures and physical movements from users made by other related research in order to study the same interaction within a virtual world. Through this research a smaller number of gestures were created improving the user learning curve without affecting the usability. The following implementation uses a client-server architecture composed of 2 Raspberry Pi devices and a Smartphone acting as a server the communication between them was achieved by employing Bluetooth Low Energy technology. The immersive virtual experience is accomplished by using Unity 3D and Google VR tools that allowed the design and display of a playful virtual environment as an approach to promote physical and cognitive skills such as spatial thinking and hand-eye coordination. By performing maneuvers through an aerial circuit filled with obstacles the proposed UAV simulator encourages motor and mental activity while the user is being entertained. The result is the improvement of the user quality of life by avoiding cognitive and physical sedentarism.,autonomous vehicle,3,not included
10.3390/books978-3-03897-092-7,to_check,core,'MDPI AG',2018-01-01 00:00:00,core,uav or drones for remote sensing applications (volume 1),,"The rapid development and growth of unmanned aerial vehicles (UAVs) as a remote sensing platform, as well as advances in the miniaturization of instrumentation and data systems, have resulted in an increasing uptake of this technology in the environmental and remote sensing science communities. Although tough regulations across the globe may still limit the broader use of UAVs, their use in precision agriculture, ecology, atmospheric research, disaster response biosecurity, ecological and reef monitoring, forestry, fire monitoring, quick response measurements for emergency disaster, Earth science research,volcanic gas sampling, monitoring of gas pipelines, mining plumes, humanitarian observations and biological/chemo-sensing tasks continues to increase. This Special Issue provides a forum for high-quality peer-reviewed papers that broaden the awareness and understanding of UAV developments, applications of UAVs for remote sensing, and associated developments in sensor technology, data processing and communications, and UAV system design and sensing capabilities. This topic encompasses many algorithms and process flows and tools, including: robust vehicle detection in aerial images based on cascaded convolutional neural networks; a stereo dual-channel dynamic programming algorithm for UAV image stitching, as well as seamline determination based on PKGC segmentation for remote sensing image mosaicking; the implementation of an IMU-aided image stacking algorithm in digital cameras; the study of multispectral characteristics at different observation angles, rapid three-dimensional reconstruction for image sequence acquired from UAV cameras; comparisons of Riegl Ricopter UAV Lidar-derived canopy height and DBH with terrestrial Lidar; vision based target finding and inspection of a ground target using a multirotor UAV system; a localization framework for real-time UAV autonomous landing using an on-ground deployed visual approach; curvature continuous and bounded path planning for fixed-wing UAVs; the calculation and identification of the aerodynamic parameters for small-scaled fixed-wing UAVs Several wildfire and agricultural applications of UAVS including: deep learning-based wildfire identification in UAV imagery; postfire vegetation survey campaigns; secure utilization of beacons; and UAVS used in emergency response systems for building fire hazards; observing spring and fall phenology in a deciduous forest with aerial drone imagery; the design and testing of a UAV mapping system for agricultural field surveying; artificial neural network to predict vine water status spatial variability using multispectral information obtained from an unmanned aerial vehicle; automatic hotspot and sun glint detection in UAV multispectral images obtained via uncooled thermal camera calibration and optimization of the photogrammetry process for UAV applications in agriculture; olive yield forecast tool based on the tree canopy geometry using UAS imagery; spatial scale gap filling downscaling method for applications in precision agriculture; automatic co-registration algorithm to remove canopy shaded pixels in UAV-borne thermal images to improve the estimation of crop water stress on vineyards; methodologies for improving plant pest surveillance in vineyards and crops using UAV-based hyperspectral and spatial data; UAV-assisted dynamic clustering of wireless sensors and networks for crop health monitoring. Several applications of UAVS in the fields of environment and conservation including the following: the automatic detection of pre-existing termite mounds through UAS and hyperspectral imagery; aerial mapping of forests affected by pathogens using UAVs; hyperspectral sensors and artificial i ntelligence; coral reef and coral bleaching monitoring; invasive grass and vegetation surveys in remote arid lands. UAVs are also utilized in many other applications: vicarious calibration of SUAS microbolometer temperature imagery for the estimation of radiometric land surface temperature; the documentation of hiking trails in alpine areas; the detection of nuclear sources by UAV teleoperation using a visuo-haptic augmented reality interface; the design of a UAV-embedded microphone array system for sound source localization in outdoor environments; the monitoring of concentrated solar power plants, accuracy analysis of a dam model from drone surveys, mobile sensing and actuation infrastructure, UAV-based frameworks for river hydromorphological characterization; online aerial terrain mapping for ground robot navigation",autonomous vehicle,4,not included
944ad40d87fce6566211eeca78fe0b08eee1e34b,to_check,semantic_scholar,Frontiers in Robotics and AI,2020-01-01 00:00:00,semantic_scholar,trustable environmental monitoring by means of sensors networks on swarming autonomous marine vessels and distributed ledger technology,https://www.semanticscholar.org/paper/944ad40d87fce6566211eeca78fe0b08eee1e34b,"The article describes a highly trustable environmental monitoring system employing a small scalable swarm of small-sized marine vessels equipped with compact sensors and intended for the monitoring of water resources and infrastructures. The technological foundation of the process which guarantees that any third party can not alter the samples taken by the robot swarm is based on the Robonomics platform. This platform provides encrypted decentralized technologies based on distributed ledger tools, and market mechanisms for organizing the work of heterogeneous multi-vendor cyber-physical systems when automated economical transactions are needed. A small swarm of robots follows the autonomous ship, which is in charge of maintaining the secure transactions. The swarm implements a version of Reynolds' Boids model based on the Belief Space Planning approach. The main contributions of our work consist of: (1) the deployment of a secure sample certification and logging platform based on the blockchain with a small-sized swarm of autonomous vessels performing maneuvers to measure chemical parameters of water in automatic mode; (2) the coordination of a leader-follower framework for the small platoon of robots by means of a Reynolds' Boids model based on a Belief Space Planning approach. In addition, the article describes the process of measuring the chemical parameters of water by using sensors located on the vessels. Both technology testing on experimental vessel and environmental measurements are detailed. The results have been obtained through real world experiments of an autonomous vessel, which was integrated as the “leader” into a mixed reality simulation of a swarm of simulated smaller vessels.The design of the experimental vessel physically deployed in the Volga river to demonstrate the practical viability of the proposed methods is shortly described.",autonomous vehicle,5,included
57165b0eb61847bec87cdd6df7a4eb37bd92fc59,to_check,semantic_scholar,Surgical innovation,2020-01-01 00:00:00,semantic_scholar,commercially available head-mounted displays are unsuitable for augmented reality surgical guidance: a call for focused research for surgical applications,https://www.semanticscholar.org/paper/57165b0eb61847bec87cdd6df7a4eb37bd92fc59,"Recent advances in portable computational units, optics, and photonics devices have enabled the scientific community to open many new fronts in biomedical research, with the development of innovative augmented reality (AR) applications exploiting the potentialities offered by head-mounted display (HMD) technology. Such technology has reached the maturity to be translated into commercial products, and published works on HMDs provide glimpses of how AR will disrupt the surgical field, allowing for an ergonomic, intuitive, and 3-dimensional fruition of preoperative and intraoperative information. Nowadays several commercial HMDs, such as Microsoft HoloLens, Meta or Magic Leap, integrate tracking and registration technology, and the deployment of software development kits has reduced technical complexity of custom application development, allowing for a wide range of users to easily create AR applications and attracting researchers to explore their potentialities for the implementation of surgical navigators. The above-mentioned HMDs are designed following an optical see-through (OST) approach, which augments the natural view through the projection of virtual reality information on semitransparent displays in front of the user’s eyes. The OST approach fits well in the surgical domain as it offers an instantaneous full-resolution view of the real world, allowing the natural synchronization of visual and proprioceptive information, and a complete situation awareness. Ongoing research is aimed at the goal of providing a device “conceived as a transparent interface between the user and the environment, a personal and mobile window that fully integrates real and virtual information.”1 Commercial companies are rapidly improving HMD ergonomic aspects, for example, HoloLens 2 features an improved field of view (52° diagonal), which includes eye tracking, and offers more comfortable wearability. However, maximizing surgical accuracy remains a challenge for manufacturers and researchers. Together with ergonomics, the achievement of precision objectives must be addressed to develop a visor suitable for guiding surgical operations, not to mention compliance with medical device regulations. An increasing number of research studies propose the use of commercial HMDs to guide surgical interventions.2 To the best of our knowledge, these works are principally focused on the need to strengthen virtual/real patient registration (eg, use of an external localization system),3 improve virtual content stability,4 and solve calibration issues, and they underestimate the contribution of perceptual issues to the user accuracy. One of the largest obstacles to obtain a perceptually correct augmentation is the inability to render proper focus cues in HMDs; indeed, the majority of systems offers the AR content at a fixed focal distance, failing to stimulate natural eye accommodation and retinal blur effects.5 Our recent work2 suggests to avoid the use of existing HMD-OST, which are not specifically designed for performing tasks in peripersonal space (<1 m), to guide manual tasks requiring a high level of precision, since perceptual issues, particularly “focal rivalry” (ie, inability to see simultaneously in focus the virtual and real content), can affect user performance.5 Most commercial systems (HoloLens, Lumus, Meta, Ora2) indeed have a fixed focal plane at 2 m or more (often infinite). Thus, during manual tasks, virtual content is 903197 SRIXXX10.1177/1553350620903197Surgical InnovationCarbone et al editorial2020",autonomous vehicle,6,not included
506e55521ddd1442ae6eae58534aa971946acc3f,to_check,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,distributed heterogeneous tracking for augmented reality,https://www.semanticscholar.org/paper/506e55521ddd1442ae6eae58534aa971946acc3f,"Augmented reality (AR) is a technique in which a user’s view of the real world is enhanced or augmented with additional information generated from a computer model (Azuma et al., 2001). The enhancement may consist of virtual artifacts to be fitted into the environment or a display of non-geometric information about existing real objects. Mobile AR (MAR) systems implement this interaction paradigm in an environment in which the user moves, possibly over wide areas (Feiner, MacIntyre, Hoellerer, & Webster, 1997). This is in contrast to non-mobile AR systems that are utilized in limited spaces such as a computer-aided surgery or by a technician’s aid in a repair shop. There are a number of challenges to implementing successful AR systems. These include a proper calibration of the optical properties of cameras and display systems (Tuceryan et al., 1995; Tuceryan, Genc, & Navab, 2002), and an accurate registration of threedimensional objects with their physical counterparts and environments (Breen, Whitaker, Rose, & Tuceryan, 1996; Whitaker, Crampton, Breen, Tuceryan, & Rose, 1995). In particular, as the observer (or an object of interest) moves over time, the 3D graphics need to be properly updated so that the realism of the resulting scene and/or alignment of necessary objects and graphics are maintained. Furthermore, this has to be done in real time and with high accuracy. The technology that allows this real-time update of the graphics as users and objects move is a tracking system that measures the position and orientation of the tracked objects (Koller et al., 1997). The ability to track objects, therefore, is one of the big challenges in MAR systems. This article describes a software framework for realizing such a distributed tracking environment by discovering independently deployed, possibly heterogeneous trackers and fusing the data from them while roaming over a wide area. In addition to the MAR domain, this kind of a tracking capability would also be useful in other domains such as robotics and locationaware applications. The novelty of this research lies in the amalgamation of the theoretical principles from the domains of AR/VR, data fusion, and the distributed software systems to create a sensor-based, wide-area tracking environment. BACKGROUND",autonomous vehicle,7,included
26960f55ddb24b2338a5eae012c63cd13abb7bdd,to_check,semantic_scholar,2017 IEEE International Symposium on Parallel and Distributed Processing with Applications and 2017 IEEE International Conference on Ubiquitous Computing and Communications (ISPA/IUCC),2017-01-01 00:00:00,semantic_scholar,performance evaluation for wifi dcf networks from theory to testbed,https://www.semanticscholar.org/paper/26960f55ddb24b2338a5eae012c63cd13abb7bdd,"Distributed Coordination Function (DCF) is a basic MAC protocol used in the world-wide WiFi networks and plays a key role in determining the network performance, especially in situations with a large number of users and high-density Access Point (AP) deployed. To achieve a better understanding of the real-world performance of 802.11 DCF networks, we have constructed an emulation platform and a prototype testbed for performance evaluation. The design and implementation of these two platforms are discussed in this paper. The key DCF parameters, i.e., the initial contention window size ($CW_{min}$) and the maximum contention window size ($CW_{max}$), are tuneable so that we are able to study the impact of these DCF parameters on the network performance. These experiment results are compared against with a recently proposed unified analytical framework to examine the model assumptions and system performance bottlenecks. Our results demonstrate that by adapting the values of $CW_{min}$ based on WiFi traffic load, the maximal network throughput can be achieved, and the optimal value of $CW_{min}$ varies when the network size changes. As a reality check, the emerging software defined WiFi network architecture can be optimized for performance enhancement guided by this unified performance model.",autonomous vehicle,9,not included
83c19e91c197218df688172968455ff9d4efc7fe,to_check,semantic_scholar,,2017-01-01 00:00:00,semantic_scholar,enhancing liveness testing for transferring data packets through using automatic test packet generation,https://www.semanticscholar.org/paper/83c19e91c197218df688172968455ff9d4efc7fe,"-Networks are getting bigger and more complex, yet administrators rely on incomplete tools such as and to debug problems. We propose an automated and systematic approach for testing and rectify networks called “Automatic evaluates Package Generation” (ATPG). ATPG reads router configurations and generates a device-independent model. The model is used to generate a minimum set of test packets to minimally exerting every link in the network or maximally exerting every rule in the network. Test packets are sent periodically, and detected failures trigger a separate mechanism to localize the revoke. ATPG can detect both functional and renderings problems. ATPG complements but goes beyond earlier work in static checking for which cannot detect liveness or performance faults or fault localization which only localize revoke given liveness results. We describe our prototype ATPG implementation and results on two real-world data sets: Stanford University’s backbone network and Internet. We find that a small number of test packets suffice to test all rules in these networks. A sending 4000 test packet 10 times per second consumes less than 1% of link capacity. ATPG code and the datasets are publicly available. Keyword: ATPG, liveness, Networks. ________________________________________________________________________________________________________ I.INTRODUCTION Networking is the word fundamentally cogitates to computers and their property. It is very often used in the world of computers and their use in different connections. The term networking express the link between two or more computers and their tendency, with the vital purpose of sharing the data stored in the computers, with each other. The networks between the computing tendencies are very public these days due to the launch of assorted hardware and computer software which aid in making the activity much more convenient to build and use. Fig: 1.1Structure of Networking between the different computers The discuss about Figure: 1.1 Structure of Networking between the different computer. Its main process of share the Internet to different things and devices. General Network Techniques When computers communicate on a network, they send out information packets without knowing if anyone is listening. Computers in a network all have an attached to the network and that is called to be attached to a network bus. What one computer sends out will reach the other computer on the local area network. © 2017 IJEDR | Volume 5, Issue 1 | ISSN: 2321-9939 IJEDR1701073 International Journal of Engineering Development and Research (www.ijedr.org) 476 Fig: 1.2 the clear idea about the networking functions The discus about figure: 1.2 clear ideas of network function and different computers to be able to distinguish between each other, every computer have unique ID called MAC-address Media Access Control Address. This address is not only unique on your network but unique for all tendencies that can be aquiline up to a network. The MAC-address is tied to the hardware and has nothing to do with IP-addresses. Since all computers on the network graduate inversion that is sent out from all other computers the MACaddresses is primarily used by the computers to filter out incoming network traffic that is addressed to the scratcher computer. When a computer expostulation with another computer on the network, it sends out both the other computers MAC-address and the MACaddress of its own. In that way the receiving computer will not only realize that this parcel is for me but also, who sent this data packet so a return response can be sent to the sender. Ethernet network as delineate here, all computers hear all network aggregation since they are attached to the same bus. This network structure is called multi-drop. One problem with this network structure is that when you have, let say ten computers on a network and they expostulation attendance and due to that they sends out there data packets randomly, collisions occur when two or more computers sends data at the same time. When that happens data gets imperfect and has to be resent. On a network that is heavy loaded even the resent packets collide with other packets and have to be resent again. In reality this soon takes affect an information problem. If respective computers communicate with each other at high speed they may not be able to utilize more than 25% of the total network information measure since the rest of the bandwidth is used for regressive antecedently corrupted packets. The way to minimize this problem is to use network switches. II.RELATED NETWORK Detecting the occurrence and location of performance anomalies is critical to ensuring the effective operation of network infrastructures. In this paper we present a framework for detecting and apposition performance anomalies based on using an active investigate measurement infrastructure deployed on the periphery of a network. Our framework has three components: an algorithm for detection performance oddball on a path, an algorithm for environs which paths to probe at a given time in order to detect performance anomalies where a path is defined as the set of links between two sampling nodes, and an algorithm for designation the links that are causing an identified anomaly on a path The path selection algorithm is designed to enable a interchange between insure that all links in a network are of times monitored to detect performance anomalies, while minimizing probing overhead.[1] This paper, we develop failure-resilient techniques for monitoring link detain and imbecility in a Service Provider or endeavor IP network. Our two-phased approach attempts to minimize both the monitoring infrastructure costs as well as the additional aggregation due to probe messages. In the first phase, we compute the particular point of a minimal set of monitoring stations such © 2017 IJEDR | Volume 5, Issue 1 | ISSN: 2321-9939 IJEDR1701073 International Journal of Engineering Development and Research (www.ijedr.org) 477 that all network links are covered, even in the bigness of several link reverting. Afterwards, in the second phase, we compute a minimal set of probe messages that are transmitted by the stations to measure link delays and isolate network faults these approximation ratios are provably very close to the best possible bounds for any algorithm. [2]We present a new symbolic execution tool, KLEE, capable of automatically induce tests that wangle high coverage on a diverse set of complex and environmentallyintense programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment position on millions of UNIX systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage on average over 90% per tool. [3] The emergence of Open Flow-capable switches enables exciting new network functionality, at the risk of programming errors that make communication less reliable. The centralized programming model, where a single accountant program manages the network, seems to reduce the likelihood of bugs. However, the system is inherently scattered and asynchronous, with events happening at different switches and end hosts, and inevitable delays affecting communication with the controller. In this paper, we present economic, regular techniques for testing unqualified controller programs. Our NICE tool applies model checking to explore the state space of the entire system the controller, the switches, and the hosts. [4] Network performance tomography, characteristics of the network interior, such as link loss and packet latency, is inferred from unrelated end-to-end measurements. Most work to date is based on employ packet level correlations. However, these methods are often limited in scope-multicast is not widely deployed-or require deployment of additional hardware or software system. Some recent work has been successful in reaching a less detailed goal: identifying the lossiest network links using only unrelated end-toend sampling. In this paper, we abstract the properties of network performance that allow this to be done and exploit them with a quick and simple deduction algorithm that, with high likely, separate the worst performing links. [5] III. SYSTEM ANALAYS Automatic Test Packet Generation (ATPG) framework that self-loading generates a minimal set of collection to test the liveness of the underlying topology and the congruence between data plane state and redundancy description. The tool can also automatically generate packets to test performance assertions such as packet latency. It can also be specialized to generate a minimal set of packets that merely test every link for network liveness.  A survey of network operators revealing common failures and root causes.  A test packet generation algorithm.  A revoke localization algorithm to isolate faulty devices and rules.  ATPG use cases for functional and performance testing.  Evaluation of a prototype ATPG system using rule sets collected from the Stanford and Internet2 backbones.",autonomous vehicle,10,not included
2b4cfc0b672aafc526bda6f7e35cf204ece0d34c,to_check,semantic_scholar,,,semantic_scholar,ac 2009-1203: a novel interdisciplinary sensor networks laboratory,https://www.semanticscholar.org/paper/2b4cfc0b672aafc526bda6f7e35cf204ece0d34c,"Today, networks of legacy and newer sophisticated sensors and actuators that combine reconfigurable gigascale semiconductor technology with emerging micro-mechanical systems (MEMS) and nanotechnology subsystems (i.e. bio-systems/chemical/fluidics/photonics/ etc) are being designed and deployed in almost every area of technology that impacts human endeavor and commerce. These smart sensors/actuators are being networked together through: either standards based or industry specific, proprietary, wired networks or newly emerging wireless networking technologies. Presently, at the twoand four-year college level, technicians and technologists in a wide variety of impacted disciplines are not receiving an adequate education about: fundamental sensor theory, basic sensor operation, sensor system deployment planning, appropriate data-transport and networking connectivity schemes, applications software, and impending system maintenance support needs of these increasingly more sophisticated and complex, smart sensor/actuator based systems. This paper will report on the development, organization, and use of a novel interdisciplinary sensor networks laboratory. The heart of the laboratory is a dedicated data-network (SensorNet) that emulates a wide area network or WAN. The SensorNet WAN nodes and other network access points allow for the interconnection of numerous types of industry specific and standard “area networks” typically utilized for the gathering of sensor data and directing other sensor functions, as well as, the associated PC’s and servers used to direct the sensor systems and warehouse the gathered data. This laboratory environment lends itself to real world case-study and problem-based type student-centered learning experiences that can be themselves integrated into established fields of technology that do not normally include this type of activity as part of the field’s traditional educational experience at the undergraduate level. I. Overview Although it is not uncommon for several different technology fields to converge together, it is somewhat unexpected to observe such an amalgamation rapidly triggering other technologic innovations that have widespread potential to change our relationship with the environment and our daily endeavors. However, this is just what is happening today. Only a short time ago, the Internet, the result of a convergence of several technologies, spawned the development of what is commonly known as the “Information Economy.” Today another innovative and important convergence of technologies has recently gained critical mass and recognition by business and industry, government, academia, and professional societies. It is the deployment of intricate systems involving complex sensors with embedded (ambient) intelligence and advanced actuators coupled with modern data-transport and networking technologies and applicationenabling software with data fusion capabilities. This rapidly evolving convergence of technologies, which allows us to implement sensor systems that gather in situ (remote), realtime, statistically relevant information and interpret it in new and novel ways, has already started to transform automation and process control systems. The technology of networked sensor systems has the very genuine potential to significantly impact almost every aspect of human endeavor by increasing system efficiency, reducing energy consumption, permitting the real-time monitoring of the “health” of the nation’s infrastructure and environment, and improving public health and safety. Applications are limitless! P ge 1.77.3 On a global level, the NSF has been calling this “grand convergence,” cyberinfrastructure. One may find many references to this concept, forecasts of potential future applications, reports on inprogress test projects such as HPWREN, NIMS, and ROADnet, and potential research funding opportunities on the NSF’s Web site [1] . However, most of this current, enthusiastic attention and promotion of cyberinfrastructure by the NSF is aimed at senior, graduate-level research institutions. Not surprisingly, most of the NSF’s recent Requests for Proposals (RFPs) in this area have been targeted at basic research about wireless sensor networks and systems and applications of these systems to infrastructure and environmental monitoring and other technology areas. While many applications of networked sensor systems are yet to be even thought of, the reality is that they are being deployed today and will continue to proliferate for many years to come until they eventually become as commonplace as a typical public utility like electricity. This paper describes aspects of an NSF funded CCLI project (DUE 0736888), titled, “The Sensor Networks Education Project” (SNEP) that seeks to develop materials and a model teaching laboratory that will be useful for other faculty and organizations at the twoand even the four-year college level to emulate. This project looks at this evolving convergence on a more practical level and speaks to the lack of engineering technology faculty expertise and teaching materials needed to infuse the newly recognized, exponentially growing knowledge base of networked sensor technology into the curricula and hence into the skill-sets of today’s twoand four-year technical college graduates – the technicians and technologists of tomorrow. This is the community of workers that will most likely deal with the design, deployment, updating, and maintenance of these systems. Today, networks of legacy and newer sophisticated sensors that combine reconfigurable gigascale semiconductor technology with emerging micro-electromechanical systems (MEMS) and nanotechnology subsystems [2] (i.e. bio-systems/chemical/molecular/photonic) are being designed and deployed in almost every area of technology that impacts human endeavor and commerce (i.e. Aerospace, Agriculture, Automotive, Biomedical, Building Automation, Energy Exploration and Production, Environmental Monitoring, Healthcare, Homeland Security, Industrial Automation, Infrastructure Monitoring, Information Technology, Manufacturing, Military, Pharmaceutical, Telecomm, Transportation, Weather Forecasting, etc). These sensors are being networked together through: either standards based or industry proprietary wired networks or emerging wireless networking technologies. Presently, at the twoand four-year college level, technologists and technicians in a wide variety of impacted disciplines are not receiving an adequate education about: fundamental sensor theory, basic sensor operation, sensor system deployment planning, appropriate data-transport and networking connectivity schemes, applications software, and impending system maintenance support needs of these increasingly more sophisticated sensor based systems. Recently, there has been a great deal of public dialogue about the out-sourcing of American manufacturing jobs and the effect of this reality on the nation’s future. Dealing with an ever increasing base of physical sensor networks in all areas of endeavor will not be something that can be done through a call to a help desk located in a foreign country. The apparent curriculum shortcoming regarding these topics within today’s associate and bachelors degree technology oriented programs is primarily due to the extremely rapid evolution and convergence of several key areas of electronics, computer, and MEMS technology (i.e. embedded processing, smart P ge 1.77.4 sensors/actuators, wired and wireless networking, etc), the lack of appropriate up-to-date educational materials, and a lack of appropriate faculty expertise in this rapidly expanding and remarkably cross-disciplinary field. II. Project Overview Over its two-year life-span, this CCLI Phase I project has as its primary goals the creation and testing of interdisciplinary student-centered learning materials primarily designed for a “field laboratory” type environment, the dissemination of these materials, and the development of faculty expertise in the multi-disciplinary field of networked sensors and modern active-learner teaching techniques. To accomplish these goals the project will: (1) develop and deploy a model, innovative, replicate-able, multi-interdisciplinary, case-study and problem-based oriented, networked, distributed sensor laboratory, (2) develop basic and advanced instructional materials and standard and “hybrid” laboratory activities related to the sensor laboratory that can be utilized for introductory courses in sensor technology or more advanced courses in networked sensor systems for use by both twoand four-year technology programs, (3) develop several prototype multifaceted educational modules that integrate traditional science and math based theory, practical real-world laboratory exercises, and science based, high-resolution, interactive simulation software, applicable to several of the major technology areas employing networked sensor technology (i.e. building automation and infrastructure monitoring and industrial automation), and (4) provide on-going local, regional, and national dissemination of these developed materials and laboratory experiences through hands-on faculty workshops and webbased distribution technologies including the National Science Digital Library (NSDL). In addition, for the duration of the project, continuous on-going professional development in the principles and applications of student-centered and active learner techniques will be provided to the recruited college faculty that will take part in the project. Research has shown that long term professional development programs are more effective than short-term workshops. For this project to be successful, the participating faculty must learn how to effectively integrate content and pedagogy in a way that actively engages students in individual and collaborative problem solving, analysis, synthesis, critical thinking, reasoning, and skillfully applying knowledge in real-w",autonomous vehicle,11,not included
d70a02826a51efc7d133f688a820e73e6a1a1b44,to_check,semantic_scholar,SSW,2010-01-01 00:00:00,semantic_scholar,semantic high level querying in sensor networks,https://www.semanticscholar.org/paper/d70a02826a51efc7d133f688a820e73e6a1a1b44,"The quick development and deployment of sensor technology within the general frame of the Internet of Things poses relevant opportunity and challenges. The sensor is not a pure data source, but an entity (Semantic Sensor Web) with associated metadata and it is a building block of a “worldwide distributed” real time database, to be processed through real-time queries. Important challenges are to achieve interoperability in connectivity and processing capabilities (queries) and to apply “intelligence” and processing capabilities as close as possible to the source of data. This paper presents the extension of a general architecture for data integration in which we add capabilities for processing of complex queries and discuss how they can be adapted to, and used by, an application in the Semantic Sensor Web, presenting a pilot study in environment and health domains. 1 Background and Motivation The rapid development and deployment of sensor technology involves many different types of sensors, both remote and in situ, with such diverse capabilities as range, modality, and manoeuvrability. It is possible today to utilize networks with multiple sensors to detect and identify objects of interest up close or from a great distance. Connected Objects – or the Internet of Things – is expected to be a significant new market and encompass a large variety of technologies and services in different domains. Transport, environmental management, health, agriculture, domestic appliances, building automation, energy efficiency will benefit of real-time reality mining, personal decision support capabilities provided by the growing information shadow (i.e. data traces) of people, goods and objects supplied by the huge data available from the emerging sensor Web [1]. Vertical applications can be developed to connect to and communicate with objects tailored for specific sub domains, service enablement to face fragmented connectivity, device standards, application information protocols etc. and device management. Building extending connectivity, connectivity tailored for object communication – with regards to business model, service level, billing etc, are possible exploitation areas of the Internet Connected Objects. Important challenges are to achieve interoperability in connectivity and processing capabilities (queries, etc.), to distribute “intelligence” and processing capabilities as close as possible to the source of data (the Giordani I., Toscani D., Archetti F. and Cislaghi M.. Semantic High Level Querying in Sensor Networks. DOI: 10.5220/0003116600720084 In Proceedings of the International Workshop on Semantic Sensor Web (SSW-2010), pages 72-84 ISBN: 978-989-8425-33-1 Copyright c 2010 SCITEPRESS (Science and Technology Publications, Lda.) sensor or mobile device), in order to avoid massive data flows and bottlenecks on the connectivity side. The sensor is not a pure data source, but an entity (Semantic Sensor Web) with associated domain metadata, capable of autonomous processing and it is a building block of a “worldwide distributed” real time database, to be processed through realtime queries. The vision of the Semantic Sensor Web promises to unify the real and the virtual world by integrating sensor technologies and Semantic Web technologies. Sensors and their data will be formally described and annotated in order to facilitate the common integration, discovery and querying of information. Since this semantic information ultimately needs to be communicated by the sensors themselves, one may wonder whether existing techniques for processing, querying and modeling sensor data are still applicable under this increased load of transmitted data. In the following of this paper we introduce the state of the art in data querying over network of data providers. In Sect. 2 we present the software architecture of a data integration system in which we added complex query processing features. Sect. 3 introduces the case study in which we deployed our system: the study of short term effect of air pollution on health. Sect. 4 presents the detailed implementation of the querying features together with results on real data sets. Finally, Sect 5 presents the conclusions and future work. 1.1 State of the Art This paper stems from the work presented in [12], in which is presented a software system aimed at forecasting the demand of patient admissions on health care structures due to environmental pollution. The target users of this decision sup-port tool are health care managers and public administrators, which need help in resource allocation and policies implementation. The key feature of that system was the algorithmic kernel, to perform time series analysis through Autoregressive Hidden Markov Models (AHMM) [7]. The scenario in which the system has been deployed is the research project LENVIS1, which is aimed to create a network of services for data and information sharing based on heterogeneous and distributed data sources and modeling. One of the innovations brought by LENVIS is the “service oriented business intelligence”, i.e. an approach to Business Intelligence in which the information presented to the user comes from data processing that is performed online, i.e. data are extracted under request of the applications, and on the basis of data availability, i.e. data are exchanged through web services, which does not guarantee response time neither availability. Such a complex environment, in which data sources are distributed over the internet, is common to several problems and has been faced by different approaches. One of them is that of [13], in which “monitoring queries” continuously collect data about spatially-related physical phenomena. An algorithm, called Adaptive Pocket Driven Trajectories, is used to select data collection paths based on the spatial layout of sen1 LENVIS Localised environmental and health information services for all. FP7-ICT-2007-2. Project number 223925. www.lenvis.eu 73",autonomous vehicle,12,not included
0c177e2385aad71584840a21b23ebd4b58c00132,to_check,semantic_scholar,,2012-01-01 00:00:00,semantic_scholar,service virtualization: reality is overrated,https://www.semanticscholar.org/paper/0c177e2385aad71584840a21b23ebd4b58c00132,"Software drives innovation and success in todays business world. Yet critical software projects consistently come in late, defective, and way over budget. So whats the problem? Get ready for a shock, because the answer to the problem is to avoid reality altogether. A new IT practice and technology called Service Virtualization (SV) is industrializing the process of simulating everything in our software development and test environments. Yes, fake systems are even better than the real thing for most of the design and development lifecycle, and SV is already making a huge impact at some of the worlds biggest companies. Service Virtualization:Reality Is Overratedis the first book to present this powerful new method for simulating the behavior, data, and responsiveness of specific components in complex applications. By faking out dependency constraints, SV delivers dramatic improvements in speed, cost, performance, and agility to the development of enterprise application software. Writing for executive and technical readers alike, SV inventor John Michelsen and Jason English capture lessons learned from the first five years of applying this game-changing practice in real customer environments. Other industriesfrom aviation to medicinealready understand the power of simulation to solve real-world constraints and deliver new products to market better, faster, and cheaper. Now its time to apply the same thinking to our software. For more information, see servicevirtualization.com. What youll learnYou will learn why, when, where, and how to deploy service virtualization (SV) solutions to mitigate or eliminate the constraints of an unavailable or unready service system by simulating its dependent components in order to deliver better enterprise software faster and at lower cost. In particular, you will learn step-by-step why, when, where, and how to deploy the following SV solutions: shift-left infrastructure availability performance readiness test scenario management Who this book is for This book is not only for IT practitioners on engineering, testing, and environments teams engaged in the development and delivery of enterprise software, but also for executives of companies in all sectors who need to understand and implement emergent opportunities to improve the time to market and overall competitiveness of any outward-facing business strategy that has a software application component. Table of ContentsForeword by Burt Klein Chapter 1. Introduction Service Virtualization Briefly Defined Key Practices Enabled by SV Shift-Left Infrastructure Availability Performance Readiness Test Scenario Management Navigating This Book Chapter 2. The Business Imperative: Innovate or Die Consumers Have No Mercy Business Demands Agile Software Delivery Increased Change and Complexity for IT Simulation Is Not Just for Other Industries Chapter 3. How We Got Here From Monolithic to Composite Apps Todays Complex Service Environments From Waterfall to Agile Development Chapter 4. Constraints: The Enemy of Agility Unavailable Systems and Environments Conflicting Delivery Schedules Data Management and Volatility Third Party Costs and Control Chapter 5. What is Service Virtualization? The Opposite of Server Virtualization Creation of a Virtual Service Maintaining Virtual Services What Kinds of Things You Can virtualize Virtual Service Environments (VSEs) Chapter 6. Where to Start with SV? Pick a Hairy Problem Identify Stakeholders Set Real Value Goals for Releases Avoid Inappropriate Technologies Chapter 7. Capabilities of Service Virtualization Technology Live-Like Development Environment Automation Eliminates Manual Stubbing and Maintenance Enables Parallel Dev and Test No more Availability Problem Platform-Neutrality Chapter 8. Best Practice #1: Shift-Left Reducing Wait Time Early Component and System Testing Define SV from Capture Define Incomplete SV from Requirements Expected Results Customer Example Chapter 9. Best Practice #2: Infrastructure Availability Finding Over-Utilized Resources Virtualizing Mainframes Avoiding Big IT Outlays Expected Results Customer Example Chapter 10. Best Practice #3: Performance Readiness Virtualizing Performance Environments Informing Performance from Production Expected Results Customer Example Chapter 11. Best Practice #4: Test Scenario Management Managing Big Data Shielding Teams from Volatility Massively Parallel Regression Testing Expected Results Customer Example Chapter 12. Rolling out Service Virtualization Who Pays for Service Virtualization? Overcoming Organizational Challenges Who Manages a VSE? Should I Have More Than One? Key Skills and Roles in a Virtual IT World Chapter 13. Service Virtualization in the DevTest Cloud Constraints of Cloud Dev and Test Achieving Elastic Cloud Environments Chapter 14. Assessing the Value Key Metrics for Success Areas for Improvement Chapter 15. Conclusion The Industrialized Software Supply Chain Innovate and Thrive Whats Next for SV? Glossary About the Authors",autonomous vehicle,13,not included
eebc85dcbbf0b5904316256d57cf7e9c3488d024,to_check,semantic_scholar,,2012-01-01 00:00:00,semantic_scholar,e-science approaches in molecular science,https://www.semanticscholar.org/paper/eebc85dcbbf0b5904316256d57cf7e9c3488d024,"Computer simulations of the properties of processes and materials are becoming increasingly necessary in several technological and environmental studies. This implies a growing demand of computing resources that severely exploits computational environments in terms of sustainability and reliability of the infrastructure.
 The developments in computing hardware and software, in particular the deployment of world-wide reliable Grid Computing infrastructures, the adoption of innovative computing approaches like the General Purpose Graphic Processing Unit (GPGPU) Computing and the High Performance Network environments, stimulate the exploitation of new approaches and methodologies in Computational Sciences. Furthermore the advances made in the World Wide Web, allow the implementation of Web sites from which the simulation of elementary chemical processes at molecular level is performed combining various techniques and computational approaches which are executed on High Throughput Computing (HTC) and/or High Performance Computing (HPC) platforms.
 The ubiquity of information and computing resources has impacted on the researchers? productivity, in a similar way the same technologies impacted everyone?s daily life. The E-science technologies facilitate the exchange of information among researchers, enhance the collaborative work and increase the quality of dissemination of results. Several European initiatives are devoted to facilitate researchers? work and to establish networks among researchers of the various disciplines, enabling some European research groups to reach leading positions in their disciplines. We have got the support of the EU COST (COllaboration in Science and Technology) Initiative in two Actions devoted to the facilitation of adoption of Grid and Distributed Computing technologies in Molecular and Matter Sciences. In particular I participated to the COST D23 Action: Metachem - Metalaboratories for Complex Computational Applications in Chemistry (2000-2005), and I coordinated the Working Group: Simbex: a metalaboratory for the a priori simulation of crossed molecular Beam Experiments. Furthermore I participated to the COST D37 Action: Grid Computing in Chemistry: GRIDCHEM (2006-2009), and I coordinated the Working Group: ELAMS: E-science and Learning Approaches in Molecular Science.
 The outcomes of both COST Actions contributed significantly to establish an active group of Computational Chemistry and Molecular and Matter Science laboratories which adopted the Grid Computing as an innovative computing paradigm for performing massive computational campaigns.
 Since 2004 the researchers of such laboratories joined the Virtual Organization (VO) CompChem established on the EGEE Grid Infrastructure, the largest distributed computing environment ever established worldwide, and coordinated by CERN (Conseil Europeen pour la Recherche Nucleaire). I served as VO Manager since the VO was established, under the coordination of Prof. Antonio Lagana, Department of Chemistry, University of Perugia.
 The present thesis covered a long period of research work focused on implementing some e-science instruments to the computational chemistry community, in particular the community of users belonging to the COMPCHEM Virtual Organization active in the EGEE/EGI European Grid Initiative.
 The Thesis describes some tools and approaches the author adopted to provide innovative tools to the Computational Chemistry community based on two main pillars:
1. approaches for running large computational campaigns on Grid Infrastructures 2. adopting virtual reality techniques for making more intuitive the interaction with nanoscale computing approaches and simplifying the definition of the initial conditions of the molecular simulations
 The research work originated 10 research papers, several of them produced as a joint work with European laboratories interested in the implementation of e-science tools for a smart, curious and demanding community like the Computational Chemistry one.
 The author has been able to provide a useful view of the molecular world through the use of virtual reality techniques, combined with the most advanced Web technologies, in particular using the ISO standard X3D for the 3D visualization and interaction with a virtual world. These innovative tools enabled the researchers to set up the environment for carrying out complex molecular simulations (as in the case of the Dl-Poly software package) in a intuitive and visual way. Once defined the species interacting in the considered molecular system, represented in a virtual world, the system produce the input file for the simulation and the Dl-Poly program may be launched, possibly on a Grid infrastructure to take benefit of the powerful available computational resources.
 In chapters 1, 2 and 3 the various steps toward the implementation of an a-priori molecular simulator on the EGEE/EGI Grid, for the COMPCHEM VO users, are reported.
 Molecular Virtual Reality applications are really useful as e-learning support tools for Chemistry students. To this purpose we implemented a Learning Management System based on a semantic web approach, described in chapter 7, and an assessment system, described in chapter 8, which have been used several times to assess the competences of students participating to the Erasmus Mundus Master of Science in Theoretical Chemistry and Computational Modeling. The system enables the coordinators of the Master to monitor the progresses made by the students an a daily basis.
 The author has shown how it is possible to use virtual reality approaches to describe a chemical experiment at both human and molecular level using a virtual reality approach. To this end a multi-scale virtual reality approach has been adopted to deal with the description of the physical environment,HVR. The main features of the virtual reality representation of the experiments and the potentiality of associating VRML and X3D with Java engine calculator are outlined in chapter 4.
 In chapter 5 an X3D Molecular Virtual Reality environment in which the researcher is able to interact with it by using immersive devices and dynamic gestures is described. By using the gestures the researcher is able to modify the composition of the molecular system by adding or subtracting functions and the molecular properties of the new species are evaluated in real time by invoking a Web Service implementing the simulation environment. This has required the assemblage of an innovative approach coupling the management of immersive devices with Web Services and molecular dynamics packages.
 In chapter 6 the author has presented an X3D Molecular Virtual Reality environment which makes usage of the most recent and powerful HTML and Web technologies. The approach implemented takes into account the modern approaches followed in implementing Social Networking environments and showed how useful these approaches are also in implementing scientific environments. We think such type of work is important also in consideration of how our lifestyle is changing, thanks to the ubiquity of the information, the availability of an increasing availability of storage and computing power. The social networking showed us how deep may be the impact of the computing and networking facility in the daily life and similarly the computational science, and the computational chemistry in particular, has to reshape the classical approaches and methodologies in order to gain advantage of the modern computing platforms and the powerfulness of the networking, distributed and mobile environments.",autonomous vehicle,14,not included
44ce98492713648fef9446f779de56029f432763,to_check,semantic_scholar,,2006-01-01 00:00:00,semantic_scholar,effectiveness of collaborative learning in online teaching,https://www.semanticscholar.org/paper/44ce98492713648fef9446f779de56029f432763,"This paper describes how e-learning is becoming popular and used as an alternative means of solving problems in education. E-learning is usually used in distance learning and may be used to replace conventional classroom teaching. Many educational institutions use Internet for collaborative learning in a distributed educational process. It has been known that traditional communications media can be replaced by electronic communication for the whole educational process and in particular, to assess the role of collaborative learning in a distributed education environment. It has been found that a distributed educational process naturally supports collaborative learning environments in which students and tutors interact and provide essential support for students studying at a distance. The tutor’s feedback to students help the learning process and there is indication that tutors are happy to work in the new environment. It is therefore suggested that “blended” online teaching – a combination of the use of the Internet as a medium of instruction and tutors to do face-to-face teaching via a collaborative learning approach – may be implemented to achieve enhanced distance-student performance. INTRODUCTION There is a number of definitions for e-learning. For example, Soekartawi et al. (2002) defined elearning as: ‘... a generic term for all technologically supported learning using an array of teaching and learning tools as phone bridging, audio and videotapes, teleconferencing, satellite transmissions, and the more recognized web-based training or computer aided instruction also commonly referred to as online courses...’. Today, e-learning has become an extremely popular alternative means of delivering educational services worldwide mainly because it is seen as a means of resolving significant educational problems that cannot be solved by conventional means. E-learning is particularly used in open and distance learning (ODL) as it can provide flexible education for those who cannot attend regular schooling particularly because they are unable to leave their work to attend regular conventional classes. Additionally, in an archipelagic country like Indonesia, where the bulk of the population is spread over thousands of islands, educational services are made more accessible through ODL. Most ODL programmes in Southeast Asia, particularly in Indonesia, deploy any one or a combination of the following media: print, radio, television, audiocassettes, videocassettes and computers. Generally, the course materials of ODL programmes in the region are largely print-based. It is likely that this will continue for a long time. However, by increasing the use of computers and information and communication technology (ICT), the role of the printed-based course materials will be replaced gradually by e-learning. MOJIT Effectiveness Of Collaborative Learning In Online Teaching* 69 The development of ICT is moving rapidly because of its pertinent role in providing education to the masses. In fact, by effectively deploying ICT, the educational institutions concerned can cope with, and cater to, the expanding student population. In general, it has been observed that the use of ICT in education in Indonesia has resulted in relatively successful outcomes. There have been numerous problems, however, but there is one significant issue that may be cited here because of its implications to future endeavours in education in Indonesia and in the Southeast Asian region as well. This issue has direct influence over the learning process. It is called transactional distance. The physical separation of teacher and student is no longer a problem given the developments in ICT. However, with ICT use, transactional distance can easily result in a misunderstanding and miscomprehension of the concepts to be learned. In fact, it can even lead to the mal-education of people, following a lack of appropriate communication between learner and teacher. If there is no communication, however, this transactional distance between learner and teacher becomes wider. THE NEW LEARNING PARADIGM In 1995, the International Council on Distance Education (ICDE) conducted what it called an anecdotal, worldwide survey to determine the nature, reality and pace of the shift in the learning paradigm (ICDE, 1996). The survey noted the following clear signs: o A shift from objective knowledge to constructed knowledge. o A shift from an industrial-based to a knowledge-based society. o A shift in education missions from providing instruction to providing learning. o A shift in technologically-mediated procedures of communication and learning. o A shift from “current college and university models to as yet undetermined structures.” Over the last five years, the fifth observation of ICDE has become very clear. The “undetermined structure” at the time of the ICDE survey has come to be known as the virtual learning structure. Experts made similar observations from the ICTsector at about the same time period (Soekartawi et al., 2002). The importance of the virtual campus in the Indonesian context has been reported by Soekartawi (2002, 2002a, 2003) and Haryono & Alatas (2002). According to Garmer & Firestone (1996), due to the development of ICT, the paradigm for learning is shifting away from the traditional notion that ‘‘knowledge” is transferred from teacher to student within the confines of the classroom where the focus of the teaching-learning process before was the teacher; now, it is shifting to the learner. A new understanding of learning places the learner at the centre of the learning process, with the teacher serving an important supporting role in facilitating the process (Garmer & Firestone, 1996). In this new paradigm, successful learning is measured by the individual’s ability to apply appropriate tools and information to solve problems in real life. There is a challenge in this new concept of learning. It is necessary to unlearn old habits and notions of how learning should be structured and to develop new habits of instruction that motivate learners to take greater control over their own education (Garmer & Firestone, 1996). This is what generally known by the “first concern” which should be to understand the learner’s motivations and goals in order that we can expand opportunities for learning. This new paradigm has critical implications for the use of ICT (e-learning) in education, particularly for ODL. For one, it requires that the learner take on more responsibility and autonomy in his learning. This is something that the learner may not be comfortable with or prepared to assume completely at this time. For another, teachers will have to give up control over the learning process and take on a new role similar to that of educational coaches who spend more time on the sidelines MOJIT Effectiveness Of Collaborative Learning In Online Teaching* 70 watching and maybe making plans for a more effective learning environment. The new paradigm has also highlighted an important issue. There is a need for all learners to upgrade their skills, particularly skills to learn on their own. This largely requires an ability to seek, understand and use information, which, in turn, requires the ability to use technology. In today’s world, for example, one must be computer literate to gain access to information and new knowledge. COLLABORATIVE LEARNING One of the critical factors in online learning is the quality issue. Many efforts can be used to meet this issue, among them obtaining feedback evaluation from the student. Further, a crucial aspect of successful distance education is the quality of feedback on student assignments. The electronic assignment handling system pioneered in these trials has now been adopted by the Open University at large and will serve a big number of students. Collaborative learning, therefore, can be used to improve the issue of quality in distance learning. Collaborative learning is an essential ingredient in the recipe to create an ""effective learning environment"" as it provides learners with the opportunity to discuss, argue, negotiate and reflect upon existing beliefs and knowledge. The learner is ""...involved in constructing knowledge through a process of discussion and interaction with learning peers and experts...."" Harasim (in Thomas, 1999, p.51). To facilitate collaboration so that personal knowledge can be constructed, there needs to be a purpose for the collaboration and the purpose needs to be meaningful to the learner. Thus, it is important that an appropriate context is set for the collaborative activity, for example, assigning a ""real world"" task for learners or a problem to which all learners can relate. In addition to setting the context, there needs to be a vehicle through which collaboration can take place. In traditional faceto-face educational settings, collaboration mostly occurs through conversation, that is, individuals interacting with one another via the use of language. Therefore, in terms of creating an effective learning environment, four attributes surface as being paramount: • Providing opportunities to foster personal construction of knowledge • Setting an appropriate context for learning to create the opportunities. • Facilitating collaboration amongst learners. • Using conversation to facilitate collaboration. Petre (1998) argued that while superficially it might appear that distance learning/education is the domain of the distance educator, the aims of educational institutions are similar in ideology; according to Thomas (1999), only the implementations differ. However, what distance educators brings to this arena is the experience of how to interact with students and tutors who are geographically and temporally remote from a campus as well as from one another. The ultimate objective is to diminish and even remove the barriers that remoteness erects. In general, the learning environment in the Open University can be described",autonomous vehicle,15,not included
e61e75ec370207ed0dca2e81307e8901d09c0493,to_check,semantic_scholar,,2013-01-01 00:00:00,semantic_scholar,enabling customer experience and front-office transformation through business process engineering,https://www.semanticscholar.org/paper/e61e75ec370207ed0dca2e81307e8901d09c0493,"In the past, the scope of business processes has been circumscribed to the industrialization of enterprise operations. Indeed, Business Process Management (BPM) has focused on relatively mature operations, with the goal of improving performance through automation. However, in today’s world of customer-centricity and individualized services, the richest source of economic value-creation comes from enterprise-customer contacts beyond transactions. Consequently, process has recently moved out of its traditional court and is becoming prevalent in less traditional competences such as marketing operations, customer-relationship management, campaign creation and monitoring, brand management, sales and advisory services, multi-channel management, service innovation and management life-cycle, among others. These competences host customerenterprise co-creation activities characterized by innovation, human creativity, and new technologies. Above all, these work-practices call for continuous differentiation, instead of “pouring concrete” on emerging business processes. While BPM will continue to make important contributions to the factory of enterprises, Business Process Engineering (BPE) is chartered to provide a holistic approach to new opportunities related to the life-cycle of enterprise customers and the transformation of so-called Front-Office Operations. More broadly, Business Process Engineering fosters a new space for the multidisciplinary study of process, integrating individuals, information and technology, and it does so with the goal of engineering (i.e., designing and running) innovative enterprise operations to serve customers and improve their experiences. Furthermore, given past challenges in the Back-Office, it is imperative that managers focus on processes in the Front-Office where the software industry has jumped into with solutions that bury key processes within applications, thus making differentiation and agility very difficult. BPE stresses the critical importance of the integration of Information and Behavior and it is this goal that links it with Business Informatics: the information process in organizations and society. Since behavior and information are complementary and inseparable domains of concern, current approaches to decision making based on data-only evidence should be reexamined holistically: it may be catastrophic to explicate or predict the behavior of organizations or individuals meaningfully by insisting on the ongoing divorce across the two domains. In particular, Business Informatics and Business Process Engineering offer an opportunity to address potential benefits of “big data” and “business analytics” beyond the IT domain. Having IEEE lead these directions means an opportunity for stimulating new research and practice on the most fundamental problems that enterprises and customers face today in dealing with each other. Keywords— business process engineering; customer experience; business process management; business informatics; enterprise engineering I. PROCESS IS OUT OF THE INDUSTRIALIZATION BOX Business process has been at the center of the stage in both research and industry for several decades. Under the brand of Business Process Management (BPM), business process has attracted a great deal of attention from many practitioners and scholars. BPM has been defined as the analysis, design, implementation, optimization and monitoring of business processes [70], [219], [79], [229], [230]. In [266], Van der Aalst defined some targets of BPM: ‘ ... supports business processes using methods, techniques, and software to design, enact, control and analyze operational processes involving humans, organizations, applications, documents and other sources of information” . While the above definitions are quite comprehensive and broad, in reality most BPM research and industry activity has grwon upon the motivation of reducing operating costs through automation, optimization and outsourcing. There are a several schools of thought and practice (such as lean, lean sixsigma, and others [172], [6], [4], [5]) and a myriad of related literature in the last 40 years that serve to illustrate the focus on cost contention. Around the middle of the past decade, T. Davenport stated in a celebrated Harvard Business Review paper [54] that processes were being “analyzed, standardized, and quality checked”, and that this phenomenon was happening for all sort of activities, stated in Davenport’s own terms: “from making a mouse trap to hiring a CEO”. The actual situation is that industry investment and consequential research have stayed much more on “trapping the mouse” than in differentiating customer services through innovative and more intelligent processes, let alone hiring CEOs. This may be explained partly from Davenport’s own statements: “Process standards could revolutionize how businesses work. They could dramatically increase the level and breadth of outsourcing and reduce the number of processes that organizations decide to perform for themselves” (bold face is added here for emphasis). With the advent of different technologies such as mobile, cloud, social media, and related capabilities that have empowered consumers, the classical approach and scope of business process have begun to change quickly. Organizations are adopting new operating models [100] that will drastically affect the way processes are conceived and deployed. As stated by many authors in the last four decades, business process work is supposed to cover all competences in an organization, irrespective of the specific skills from human beings participating in such operations. However, in an unpublished inspection of about 1,300 papers conducted by 1 Van der Aalst excludes strategy processes from BPM, a remarkable point that will be revisited in more depth later in this paper. the author and some of his collaborators 2 , most process examples shown in the literature deal with rather simple forms of coordination of work, mostly exhibiting a flow structure and addressing administrative tasks (like those captured in early works on office information systems). Furthermore, the examples provided usually deal with rather idealized operations, probably offered as simple examples with the only purpose of illustrating theoretical or foundational research results. Thus, radically simplified versions of “managing an order”, “approving a form”, “processing a claim”, “paying a provider”, “delivering an order” etc. are among the most popular examples of processes found in the literature. The lack of public documentation of substantial collections of real-world processes is remarkable. The authors in [106] both confirmed the dominant focus on simple business processes and also suggested potential practical consequences of related research: “... there is a growing and very active research community looking at process modeling and analysis, reference models, workflow flexibility, process mining and process-centric service-oriented architecture (SOA). However, it is clear that existing approaches have problems dealing with the enormous challenges real-life BPM projects are facing [ ... ] Conventional BPM research seems to focus on situations with just a few isolated processes ...”. Of course, the list of available real-world processes would be a lot richer if one included the set defined by enterprise packaged applications [219]. However, this comprehensive collection is proprietary because it constitutes a key piece of intellectual capital coming from software vendors or integrators in the industry. The traditional focus on process has also raised much controversy. At the S-BPM ONE Conference in 2010, a keynote speaker [176] remarked: “Let me be as undiplomatic as I possibly can be without being offensive [...] The academic community is as much to blame [...] as the vendors of BPM systems, who continue to reduce the task of managing business processes to a purely technological and automation-oriented level”. While other authors in the same conference debated “who is to blame” very animatedly [78], [234] it is important to highlight that the statement from Olbrich (in bold face above for emphasis) reinforces that BPM has mostly followed the obsession of automation and optimization by means of Information Technology. A detailed inspection of the extant literature confirms that business process work has been devoted to a rather small fraction of the actual variety and complexity found in enterprise behavior. This behavior enacts many valuegenerating capabilities that organizations cultivate based on skills provided by their own workforces and through rich interactions with other enterprise stakeholders, particularly customers. The following points offer a simplified summary: 2 At the time of this publication in IEEE, the mentioned work still remains unpublished. The co-authors are L. Flores and V. Becker both from IBM. (1) Business process research in Computer Science has been traditionally focused on certain classes of enterprise operations, mostly involving simple coordination mechanisms across tasks. This type of coordination and the overall behavior represented in underlying models reflect very much an “assembly line” where work is linearly synchronized to deliver a desired artifact or outcome. Simplicity of the choreography is ensured by removing any form of overhead in communication when moving from one stage to the next. Unlike other more complex business processes, many software applications do have this simplified structure. In fact, a trend since the early 2000’s is to separate the specific application logic from the coordination / choreography needed across modules, and both of them from the actual data contained in a data-base management system. Different foundations and a plethora of languages have been created to capture this semantics of coordination such as Business Process Modeling Notation (BPMN), Business Process Execution Languag",autonomous vehicle,16,not included
635c01ca9f63bac692d2b9e4e6f4bdaf9aef4ce7,to_check,semantic_scholar,,2003-01-01 00:00:00,semantic_scholar,tactical insertion mission planning and rehearsal using virtual reality simulation,https://www.semanticscholar.org/paper/635c01ca9f63bac692d2b9e4e6f4bdaf9aef4ce7,"Systems Technology, Inc. (STI) has developed a versatile new system for parachute mission planning and rehearsal, combining the validated technology of STI's PC-based PARASIM parachute simulation system with real-time interactive networking, powerful scene generation graphics tools, and terrain-correlated wind fields. This Tactical Insertion Mission Planning and Rehearsal Simulator (TIMPARS) was developed under the SBIR program, funded by the US Special Operations Command (SOCOM). The TIMPARS system rests on four cornerstones: the PARASIM  simulation software, the real-time interactive network, the scene generation toolkit, and the terrain-correlated wind generation module. These elements combine to produce a system with which users can utilize geospecific terrain data and imagery to recreate a real-world site as a simulation scene, input actual or forecasted wind speeds and directions at altitude above the chosen location to generate a terrain-correlated wind field specific to the simulation scene, and then plan and rehearse a mission in a real-time simulation environment with multiple live participants interacting in the same virtual space. BACKGROUND STI's original parachute simulator was developed for use by smokejumpers, US Forestry Service airborne firefighters. Designed to teach round and ramair canopy control, this early version employed rudimentary graphics with a fixed monitor; users stood before the display monitor and pulled simple toggle lines to maneuver in the simulation. Despite the austere configuration, this version provided the minimum cues required to teach parachute flight safely at low cost. In 1996, STI launched a major development effort to incorporate new photo- realistic graphics and head-mounted display/virtual reality technology into the simulator. Subsequent development efforts produced malfunctions procedures software, riser controls, harness switches, and additional simulator improvements. The implementation of these enhanced simulators by the US Marine Corps (USMC) and the Military Freefall School in Yuma, AZ, resulted in a drastic drop in the rate of training injuries. In particular, the USMC First Force Reconnaissance Company experienced a 75% reduction in main canopy cutaways after implementing the enhanced simulator in the MC-5 static line deployed ram-air parachute system (SLDRAPS) transition course at Camp Pendleton, CA. TIMPARS PROJECT",autonomous vehicle,17,not included
0fb400e80dcda882fc293327c5c12e59fd18af93,to_check,semantic_scholar,IWUC,2006-01-01 00:00:00,semantic_scholar,on-demand loading of pervasive-oriented applications using mass-market camera phones,https://www.semanticscholar.org/paper/0fb400e80dcda882fc293327c5c12e59fd18af93,"Camera phones are the first realistic platform for the development of pervasive computing applications: they are personal, ubiquitous, and the builtin camera can be used as a context-sensing equipment. Unfortunately, currently available systems for pervasive computing, emerged from both academic and industrial research, can be adopted only on a small fraction of the devices already deployed or in production in the next future. In this paper we present an extensible programming infrastructure that turns mass-market phones into a platform for pervasive computing. 1 Mobile phone: a platform for pervasive computing Pervasive computing tries to make M. Weiser’s vision [1] a reality by saturating the environment with computing and communication devices: the most of the infrastructure is often invisible and supports user’s activities with an interaction model that is strongly human-centric. Today, almost fifteen years later, despite significant progresses in both hardware and software technologies, this vision is still not completely realizable or economically convenient. Supporting the interaction between users and the environment can be greatly simplified if we relax the interaction model and include a personal device as the access medium. Mobile phones are the most obvious candidates: they are in constant reach of their users, have wireless connectivity capabilities, and are provided with increasing computing power [2]. Even better results can be achieved with those phones that are equipped with a camera. Instead of manually getting information or editing configurations, users can point physical objects to express their will of using them: taking a picture of the objects would suffice to setup the link with the offered services. Relaying on an image acquisition device does not impose a strict limit to the share of possible users, since an always growing number of commercially available mobile phone is equipped with an integrated camera: according to recent studies [3], over 175 million camera phones were shipped in 2004 and, by the end of the decade, the global population of camera phones is expected to surpass 1 billion. ? This work is partially supported by the Italian Ministry for Education and Scientific Research (MIUR) in the framework of the FIRB-VICOM project. However, the acquisition of context-related information through images is not a trivial task, especially with resource-constrained devices. To ease the recognition process, objects can be labeled with visual tags readable by machines. Once decoded, visual tags either directly provide information about the resource they are attached to or, if the amount of information is too large, they act as resource identifiers that can be used to gather information from the network. In this paper, we describe the design and the implementation of POLPO 1 (Polpo is On-demand Loading of Pervasive-Oriented applications), a software system that turns mass-market phones into a platform for the development of pervasive applications. With POLPO, a phone with a built-in camera and compatible with the Java 2 Micro Edition (J2ME) platform is able to get context information by decoding visual tags attached to real-world objects. POLPO supports dynamic loading and installation of custom applications used to interact with the desired resources. 2 Background and contribution In this section we summarize the most relevant solutions based on visual tags and the contribution of our system in this field. Cybercode [4] is a visual tagging system based on a two-dimensional barcode technology. The system has been used to develop several augmented reality applications where the physical world is linked to the digital space trough the use of visual tags. Cybercode is one of the first systems where visual tags can be recognized by low-cost CCD or CMOS cameras, without the need for separate and dedicated readers. Each Cybercode symbol is able to encode 24 or 48 bits of information. The system has been tested with notebook PCs and PDAs. In [5] the author presents a system that turns camera-phones into mobile sensors for two-dimensional visual tags. By recognizing a visual tag, the device can determine the coded value, as well as additional parameters, such as the viewing angle of the camera. The system includes a movement detection scheme which enables to use the mobile phone as a mouse (this is achieved by associating a coordinate scheme to visual tags). The communication capability of the mobile phone is used to retrieve information related to the selected tag and to interact with the corresponding resource. Tag recognition and motion detection algorithms were implemented in C++ for Symbian OS. The Mobile Service Toolkit (MST) [6] is a client-server framework for developing site-specific services that interact with users’ smart phones. Services are advertised by means of machine-readable visual tags, which encode the Bluetooth device address of the machine that hosts the service (Internet protocols addressing could be supported as well). Visual tags also include 15 bits of application-specific data. Once the connection has been established, MST servers can request personal information to the client to provide personalized services. Site-specific services can push user interfaces, expressed with a markup language similar to WML (Wireless Markup Language), to smart phones. MST also provide thin-client functionality: servers can push arbitrary graphics 1 The Italian name for the octopus vulgaris, a cephalopod of the order octopoda, probably the most intelligent of the invertebrates. to the phone’s display which in turn forwards all keypress events to the server. The client-side is written in C++ and requires Symbian OS. A similar approach is described in [7], where the authors propose an architecture for a platform that supports ubiquitous services. Real-world objects are linked to services on the network through visual tags based on geometric invariants that do not depend on the viewing direction [8]. But differently from other solutions, image processing does not take place on the user’s device: pictures are sent to a server where they are elaborated and converted into IDs. Instead of using two-dimensional barcodes, an alternative way of performing object recognition is the one based on radio frequency identification (RFID): small tags, attached to or incorporated into objects, that respond to queries from a reader. However this solution, that can be useful in many pervasive computing scenarios, is not particularly suitable when the interaction is mediated by mobile phones, that lack the capability of reading RFIDs. In our opinion, currently available solutions present two major drawbacks: i) they are limited to specific hw/sw platforms (i.e. Symbian OS), excluding most of the models of mobile phones already shipped and in production in the near future; ii) the software needed to interact with the environment is statically installed onto the mobile phone and cannot be dynamically expanded, e.g. to interact with new classes of resources. We designed and developed a system for pervasive computing based on visual tags that overcomes these constraints as follows. Compatibility with J2ME The system runs on devices compatible with the J2ME platform. This environment is quite limited in terms of both memory and execution speed, but also extremely popular (nearly all mobile phones produced). This required the implementation of a pure Java decoder of visual tags for the J2ME environment. Downloadable applications Our system is based on the idea that the interaction with a given class of resources, e.g. printers, public displays, etc., takes place through a custom application. New custom applications can be downloaded from the network and installed onto the user’s device as needed. This brings two advantages: i) the classes of resources that can be used do not have to be known a priori; ii) the user’s device, that is resource constrained, includes only the software needed to interact with the services actually used. The J2ME platform comprises two configurations, few profiles, and several optional packages. The J2ME configurations identify different classes of devices: the Connected Device Configuration (CDC) is a framework that supports the execution of Java application on embedded devices such as network equipment, set-top boxes, and personal digital assistants; the Connected Limited Device Configuration (CLDC) defines the Java runtime for resource constrained devices as mobile phones and pagers. Our systems runs on top of the version 1.1 of the CLDC, that provides support for floating point arithmetics (unavailable in version 1.0). The adopted profile is the Mobile Information Device Profile (MIDP) that, together with CLDC, provides a complete Java application environment for mobile phones. 3 System architecture POLPO requires that physical resources are labeled with visual tags, and that a program providing access to POLPO functionalities is installed onto the user’s device. This program has the following primary functions: – Decoding of visual tags. The image captured with the built-in camera is processed to extract the data contained into the visual tag. – Management of custom applications. The program downloads and installs the custom application required to interact with a resource. Usually, resources of the same kind share the same custom application (i.e., a single application is used to interact with all printers, another is used with public displays, etc). – Management of user’s personal data. In many cases, applications need information about the user to provide customized services. For this reason, the software installed on mobile phones includes a module that manages user’s personal data and stores them into the persistent memory. Managed data comprise user’s name, telephone number, email address, homepage, etc. Each resource is identified and described by a Data Matrix visual tag. Da",autonomous vehicle,18,not included
ee2be26c5e2bfab3885adc0e6c70e4583435bd3e,to_check,semantic_scholar,,2003-01-01 00:00:00,semantic_scholar,how to harness the grid with ogsa - tutorial proposal,https://www.semanticscholar.org/paper/ee2be26c5e2bfab3885adc0e6c70e4583435bd3e,"Summary and Conclusion As a conclusion to this tutorial, we will share lessons learnt from our experience developing with OGSA and highlight the limitations as well as the benefits of deploying OGSA middleware. In particular, we will examine the behaviour of the crystal polymorph prediction system operating in a Grid environment and consider how effectively the implementation exploits available resources. We also discuss practical and political considerations that arise from “real world” Grid environments where technical arguments are often compromised by the needs and preferences of different users, organizations and domain administrators. Conduct of tutorial Delivery The tutorial will consist mostly of a talk support by a Powerpoint presentation. We also intend to illustrate some concepts with live software demonstrations: the first outlining the process of creating a simple OGSA service using the Globus Toolkit 3.0 (GT3) from simple interface description through to stub generation and wrapping an implementation using the delegation model; the second demonstrating the crystal polymorph application running in a simulated Grid environment. This will enable participants to experience Grid middleware from a developer’s perspective and lend some reality to the concepts and mechanisms the tutorial covers.",autonomous vehicle,19,not included
9566723d1b3075cd14cabc8519a24a7eaa00cd5a,to_check,semantic_scholar,,2008-01-01 00:00:00,semantic_scholar,"using simulation tools for embedded software development class # 410 , embedded systems conference , silicon valley 2008",https://www.semanticscholar.org/paper/9566723d1b3075cd14cabc8519a24a7eaa00cd5a,"ion vs. Detail A key insight in building simulations is that you must always make a trade-off between simulator detail and the scope of the simulated system. Looking at some extreme cases, you cannot use the same level of abstraction when simulating the evolution of the universe on a grand scale as when simulating protein folding. You can always trade execution time for increased detail or scope, but assuming you want a result in a reasonable time scale, compromises are necessary. A corollary to the abstraction rule is that simulation is a workload that can always use maximum computer performance (unless it is limited by the speed of interaction from the world or users). A faster computer or less detailed model lets you scale up the size of the system simulated or reduce simulation run times. In general, if the processor in your computer is not loaded to 100%, you are not making optimal use of simulation. The high demands for computer power used to be a limiting factor for the use of simulation, requiring large, expensive, and rare supercomputers to be used. Today, however, even the cheapest PC has sufficient computation power to perform relevant simulations in reasonable time. Thus, the availability of computer equipment is not a problem anymore, and simulation should be a tool considered for deployment to every engineer in a development project. Simulating the Environment Simulation of the physical environment is often done for its own sake, without regard for the eventual use of the simulation model by embedded software developers. It is standard practice in mechanical and electrical engineering to design with computer aided tools and simulation. For example, control engineers developing control algorithms for physical systems such as engines or processing plants often build models of the controlled system in tools such as MatLab/Simulink and Labview. These models are then combined with a model of the controller under development, and control properties like stability and performance evaluated. From a software perspective, this is simulating the specification of the embedded software along with the controlled environment. For a space probe, the environment simulation could comprise a model of the planets, the sun, and the probe itself. This model can be used to evaluate proposed trajectories, since it is possible to work through missions of years in length in a very short time. In conjunction with embedded computer simulations, such a simulator would provide data on the attitude and distance to the sun, the amount of power being generated from solar panels, and the positions of stars seen by the navigation sensors. When the mechanical component of an embedded system is potentially dangerous or impractical to work with, you absolutely want to simulate the effects of the software before committing to physical hardware. For example, control software for heavy machinery or military vehicles are best tested in simulation. Also, the number of physical prototypes available is fairly limited in such circumstances, and not something every developer will have at their desk. Such models can be created using modeling tools, or written in C or C++ (which is quite popular in practice). In many cases, environment simulations can be simple data sequences captured from a real sensor or simply guessed by a developer. It should be noted that a simulated environment can be used for two different purposes. One is to provide “typical” data to the computer system simulation, trying to mimic the behavior of the final physical system under normal operating conditions. The other is to provide “extreme” data, corresponding to boundary cases in the system behavior, and “faulty” data corresponding to broken sensors or similar cases outside normal operating conditions. The ability to inject extreme and faulty cases is a key benefit from simulation. Simulating the Human User Interface The human interface portion of an embedded device is often also simulated during its development. For testing user interface ideas, rapid prototyping and simulation is very worthwhile and can be done in many different ways. One creative example is how the creator of the original Palm Pilot used a wooden block to simulate the effect of carrying the device. Instead of building complete implementations of the interface of a TV, mobile phone, or plant control computer, mockups are built in specialized user interface (UI) tools, in Visual Studio GUI builder on a PC, or even PowerPoint or Flash. Sometimes such simulations have complex behaviors implemented in various scripts or even simple prototype software stacks. Only when the UI design is stable do you commit to implementing it in real code for your real device, since this typically implies a greater programming effort. In later phases of development, when the hardware user interface and most of the software user interface is done, a computer simulation of a device needs to provide input and output facilities to make it possible to test software for the device without hardware. This kind of simulation runs the gamut from simple text consoles showing the output from a serial port to graphical simulations of user interface panels where the user can click on switches, turn knobs, and watch feedback on graphical dials and screens. A typical example is Nokia’s Series 60 development kit, which provides a virtual mobile phone with a keypad and small display. Another example is how virtual PC tools like VmWare and Parallels map the display, keyboard, and mouse of a PC to a target system. In consumer electronics, PC peripherals are often used to provide live test data approximating that of a real system. For example, a webcam is a good test data generator for a simulated mobile phone containing a camera. Even if the optics and sensors are different, it still provides something better than static predetermined images. Same for sound capture and playback – you want to hear the sound the machine is making, not just watch the waveform on a display. Simulating the Network Most embedded computers today are connected to one or more networks. These networks can be internal to a system; for example, in a rack-based system, VME, PCI, PCI Express, RapidIO, Ethernet, IC, serial lines, and ATM can be used to connect the boards. In cars, CAN, LIN, FlexRay, and MOST buses connect body electronics, telematics, and control systems. Aircraft control systems communicate over special bus systems like MIL-STD-1553, ARINC 429, and AFDX. Between the external interfaces of systems, Ethernet running internet standards like UDP and TCP is common. Mobile phones connect to headsets and PCs over Bluetooth, USB, and IR, and to cellular networks using UMTS, CDMA2000, GSM, and other standards. Telephone systems have traffic flowing using many different protocols and physical standards like SS7, SONET, SDH, and ATM. Smartcards connect to card readers using contacts or contact-less interfaces. Sensor nodes communicate over standard wireless networks or lower-power, lower-speed interfaces like Zigbee. Thus, existing in an internal or external network is a reality for most embedded systems. Due to the large scale of a typical network, the network part is almost universally simulated to some extent. You simply cannot test a phone switch or router inside its real deployment network, so you have to provide some kind of simulation for the external world. You don’t want to test mobile phone viruses in the live network for very practical reasons. Often, many other nodes on the network are being developed at the same time. Or you might just want to combine point simulations of several networked systems into a single simulated network. Network simulation can be applied at many levels of the networking stack. The picture below shows the most common levels at which network simulation is being performed. The two levels highlighted in green are the ones that are most useful for embedded software work on a concrete target model. Physical signalling Bit stream Packet transmission Network protocol Application protocol High-level application actions Analog signals, bit errors, radio modeling Clocked zeros and ones, CAN with contention, Ethernet with CSMA model Ethernet packets with MAC address, CAN packets, serial characters, VME data read/write TCP/IP etc. FTP, DHCP, SS7, CANopen Load software, configure node, restart Hardware/software boundary r r / ft r r The most detailed modeling level is the physical signal level. Here, the analog properties of the transmission medium and how signals pass through it is modeled. This makes it possible to simulate radio propagation, echoes, and signal degradation, or the electronic interference caused by signals on a CAN bus. It is quite rarely used in the setting of developing embedded systems software, since it complex and provides more details than strictly needed. Bit stream simulation looks at the ones and zeroes transmitted on a bus or other medium. It is possible to detect events like transmission collisions on Ethernet and the resulting back-off, priorities being clocked onto a CAN bus, and signal garbling due to simultaneous transmissions in radio networks. An open example of such a simulator is the VMNet simulator for sensor networks. Considering the abstraction levels for computer system simulation discussed below, this is at an abstraction level similar to cycle-accurate simulation. Another example is the simulation of the precise clock-by-clock communication between units inside a system-on-a-chip. Packet transmission passes entire packets around, where the definition of a packet depends on the network type. In Ethernet, packets can be up to 65kB large, while serial lines usually transmit single bytes in each “packet”. It is the network simulation equivalent of transaction-level modeling, as discussed below for computer boards. The network simulation has no knowledge of the meaning of the packets. It just passes opaqu",autonomous vehicle,20,not included
cdd8028ef1569a9c7191e806839ed2bb261efdb9,to_check,semantic_scholar,,2002-01-01 00:00:00,semantic_scholar,title: an integrated design environment to evaluate power/performance tradeoffs for sensor network applications,https://www.semanticscholar.org/paper/cdd8028ef1569a9c7191e806839ed2bb261efdb9,"Networks of inexpensive, low-power sensing nodes that can monitor the environment, perform limited processing on the samples, and detect events of interest in a collaborative fashion are fast becoming a reality. Examples of such monitoring and detection include target tracking based on acoustic signatures and line-of-bearing estimation, climate control, intrusion detection, etc. The advances in low-power radio technology are making wireless communication within sensor networks an attractive option. However, it is typically difficult or impossible to replenish energy resources available to a portable sensor node, once it is deployed. Maximizing the life of sensor nodes is an overriding priority, and different energy optimization techniques are being developed to addresses computation/communication tradeoffs. A large number of research efforts are focusing on different aspects of the general problem of designing efficient sensor network-based systems where the metrics to measure efficiency vary from system to system. With technological advancements such as silicon-based radios expected to become a reality in a few years, designers of sensor network-based systems will be faced with an extremely large set of design decisions. Each choice will affect the overall system performance in ways that might not always be cleanly modeled. In addition to the research challenges in design and optimization, the practical aspects of designing real-world sensor networks will become equally important. For example, the ability of the design framework to allow rapid specification and evaluation of a particular network configuration is crucial for a more exhaustive exploration of the design space. A design environment for future sensor networks should provide tools and formal methodologies that will allow designers to model, analyze, optimize, and simulate such systems. In the context of our work, design and optimization of a sensor network application involves determining the task allocation to different sensor nodes and the inter-node communication mechanism. Design of the sensor node hardware itself is also an area of active research. However, we assume that a set of node architectures is already available to our enduser, and the design problem is restricted to using the available hardware (with flexibilities, if any, such as dynamic voltage scaling) to efficiently implement the target application. We take a simple, seven-node wireless sensor network for acoustic detection [5] (Automatic Target Recognition) as the case study and demonstrate (i) a modeling and simulation methodology for a class of sensor networks, and (ii) a software framework that implements our methodology. Our formal application model is illustrated in Fig. 1. We use a data flow graph representation to model the computing tasks and their data dependencies. The end-to-end application consists of two types of such data flow graphs: the first type denotes the processing that has to be performed for each sample before it is ready to be ‘fused’ with results from other sensors, and the second type represents the computing involved in data fusion. Specifically , in our case study, a Fast Fourier Transform (FFT) operation is the only task that is performed on each block of sampled data. The outputs of FFTs from all seven nodes are provided as an input to the collaborative computing part, which consists of delay and sum beamforming (BF), and lineof-bearing (LOB) estimation. The result of collaborative computation in such a cluster model of sensor networks has to be transmitted to some observer. This is accomplished by designating one of the nodes as the cluster-head, which could be equipped with more powerful communication facilities than other sensor nodes. All communication within our cluster is one-hop, and processing of a particular data sample (FFT/BF/LOB) occurs either on its home node, or the cluster-head, or partly on both. Simulating a completely specified instance of the above class of sensor networks involves many challenges. None of the existing network simulators to our knowledge models the internal architecture of the processing nodes in the network. This is because the focus of most network simulators is on protocol development and empirical analysis. Except in areas such as high-speed router design, the node internals have little or no impact on decisions related to protocol design. Also, processor simulators do not model the environment outside the chip boundary. Therefore, to obtain detailed and accurate performance estimates for the entire system, we propose a technique to automatically generate network scenarios based on results from low-level node simulations. The network simulator is configured using the generated scenarios, and the individual simulation results are merged and presented to the end-user as a whole. Such a ‘horizontal’ simulation is accomplished through the use of a central data repository for model information, which means that the simulators never have to directly interact with each other. The simulators we integrate provide estimates about energy consumption, thereby assisting in a power/performance analysis of a specific system configuration. Our design framework facilitates multi-granular simulation, i.e., simulating the same system configuration by using simulation models at different levels of abstraction. Typically, coarse-grained models provide rapid estimates, but need to make approximations about system behavior that might not be very accurate. For such a scenario, we demonstrate a form of analytical model refinement (see Fig. 2), i.e. the data from low-level simulations can be automatically processed to ‘distill’ parameter values used by high-level simulators. Naturally, the exact processing has to be specified by someone with knowledge of the analytical model semantics. Our design environment provides the following capabilities to the user: • To graphically describe the target application, node architecture, network configuration, and task-to-node mapping. • To change (reconfigure) the system model to explore alternate designs. Some of the parameters that can be manipulated by the designer include receive/transmit power of the radio, voltage/frequency setting of the processor, cluster geometry, propagation models, etc. • To automatically simulate a design using a coarse system model. • To automatically configure and execute low-level simulators for the node (Wattch [3]) and the network (ns-2 [4]) and obtain system-wide energy and latency estimates. • To automatically update high-level model parameters using low-level simulation statistics. • To graphically visualize simulation results and facilitate (manual) identification of power/performance bottlenecks in the design. This work is an illustration of the general approach of the MILAN [1] project. A modeling and simulation framework based on the first version of the 7-node ATR system model was implemented in [2]. The primary focus of that work was a prototype demonstration of simulator integration and model refinement. Therefore, the system model itself lacked generality. Also, we use a relatively more detailed version of the high-level estimator implemented for [2]. This work represents a significant step towards the ultimate goal of a design environment for automatic optimization and synthesis of sensor network applications.",autonomous vehicle,21,not included
336d84f6e7a7a398c70e924b4677c1fee7f3b81d,to_check,semantic_scholar,,2001-01-01 00:00:00,semantic_scholar,the advantages of micro simulation in traffic modelling with reference to the n4 platinum toll road,https://www.semanticscholar.org/paper/336d84f6e7a7a398c70e924b4677c1fee7f3b81d,"Micro simulation has been used to a limited extend in the past in South Africa, despite major advantages of this tool above static modelling and it’s popularity oversees. The main advantages are dynamic modelling and visual interpretation of the traffic conditions. This tool is ideal to test geometric designs, traffic controls and a variety of traffic management measures. These include incident and congestion management, road works, ramp metering, VMS, etc. It is an extremely suitable tool to use when low cost solutions must be found because of severely limited infrastructure resources. Times that micro simulation was not able to calculate and show reliable traffic situations is over, various traffic simulation models have developed and have reached high quality standards. Micro simulation is about to gain a real market share all around the world; South Africa is following. Modelling toll plazas at interchanges on the N4 Platinum Toll Road is used to illustrate the advantages of micro simulation. Geometric design options, measures effecting toll throughput and traffic control options were evaluated in this example as well as the estimation of the expected life span of various options within a congested network. The package used in this study is AIMSUN2, an advanced micro simulation package widely used internationally that can interact with TRANSYT, SCOOT, EMME/2 and SATURN. AIMSUN2 has been applied to traffic impact analysis, traffic control measures, HOV-lanes, tolling and geometric design within the last three years in South Africa. It has also been used successfully to convey results of investigations to nontechnical people. 1. MICRO SIMULATION 1.1 Background on the development of micro simulation The microscopic traffic simulation models are based on the reproduction of the traffic flows simulating the behavior of the individual vehicles, this not only enables them to capture the full dynamics of time dependent traffic phenomena, but also to deal with behavioral models accounting for drivers’ reactions. The underlying hypothesis is that the dynamics of a stream of traffic is the result of a series of drivers’ attempts to regulate their speed and acceleration accordingly with information received. The driver’s actions resulting from the interpretation of the information received will consist on the control of the acceleration (braking and accelerating), the control of heading (steering) and the decision of overtaking the precedent vehicle either to increase the speed or to position themselves in the right lane to perform a maneuver (i.e. a turning). The origins of microscopic traffic simulation can be traced back to the early stages of digital computers. Although the basic principles were set up many years ago, with the seminal work of, among others, Robert Hermann and the General Motors Group in the early fifties, the computing requirements made them impractical until hardware and software developments made them affordable even on today’s laptop computers. Most of the currently existing microscopic traffic simulators are based on the family of carfollowing, lane changing and gap acceptance models to model the vehicle’s behavior. Carfollowing models are a form of stimulus-response model, where the response is the reaction of the driver (follower) to the motion of the vehicle immediately preceding him (the leader) in the traffic stream. The response of the follower is to accelerate or decelerate in proportion to the magnitude of the stimulus at time t after a reaction time T. The generic form of the conceptual model is: response (t+T) = sensitivity * stimulus (t) Among the most used models are Helly’s model (1), implemented in SITRA-B+, (2), Herman’s model (3), or its improved version by Wicks (4), implemented in MITSIM, (5), the psycho-physical model of Wiedemann, (6), used in VISSIM (7), or the ad hoc version of Gipps (8), used in AIMSUN2 (9, 10). Other microscopic simulators such as INTEGRATION (11) and PARAMICS employ heuristic or other modeling not publicly available in analytic form. A common drawback of most of these models is that the model parameters are global i.e. constant for the entire network whereas it is well know that driver’s behavior is affected by traffic conditions. Therefore a more realistic car-following modeling for microscopic simulation should account for local behavior. This implies that some of the model parameters must be local depending on local geometric and traffic conditions. 1.2 What micro simulation is and the advantages thereof compared to static models The deployment of Intelligent Traffic Systems (ITS) requires support of complementary studies clearly showing the feasibility of the systems and what benefits should be expected from their operation. The large investments required have to be justified in a robust way. That means feasibility studies that validate the proposed systems, assess their expected impacts and provide the basis for sound cost benefit analyses. Microscopic traffic simulation has proven to be a useful tool to achieve these objectives. This is not only due to its ability to capture the full dynamics of time dependent traffic phenomena, but also being capable of dealing with behavioral models accounting for drivers’ reactions when exposed to ITS systems. The advent of ITS has created new objectives and requirements for micro-simulation models. Quoting from Deliverable D3 of the European Commission Project SMARTEST [12]: “The objective of micro-simulation models is essentially, from the model designers point of view, to quantify the benefits of Intelligent Transportation Systems (ITS), primarily Advanced Traveler Information Systems (ATIS) and Advanced Traffic Management Systems (ATMS). Micro-simulation is used for evaluation prior to or in parallel with on-street operation. This covers many objectives such as the study of dynamic traffic control, incident management schemes, real-time route guidance strategies, adaptive intersection signal controls, ramp and mainline metering, etc. Furthermore some models try to assess the impact and sensitivity of alternative design parameters”. The analysis of traffic systems and namely ITS systems, is beyond the capabilities of traditional static transport planning models. Microscopic simulation is then the suitable analysis tool to achieve the required objectives. An example from a real case study where microscopic simulation was used to complement static modeling will help us to understand better how both levels may help the decisionmaker. The city of San Sebastian, in the North of Spain, completed recently a new urban freeway connecting two separated neighborhoods. Figure 1 shows the typical result of the planning study with an close up of the Amara neighborhood. The road network and the demand were modeled using the EMME/2 package. The figure displays the expected impacts of the new infrastructure highlighting in green the average flow reduction due to the redistribution of flows enabled by the new paths on the network, and in red the increase of flows attracted by these paths. A significant discharge in the level of congestion in the main road network was the foreseen impact of the new infrastructure, but the access to the new freeway in the East-West direction shows some undesirable side effects in the neighborhood (Amara) inside the rectangle. The solutions to these problems demands a close up to the subnetwork and take decisions at the level of traffic control and traffic management schemes, not excluding even a partial reshaping of part of the street network. This type of decision requires a more detailed modeling, able of reproducing in a very accurate way the traffic conditions, accounting for the interactions between the vehicular flows and the infrastructure, and obviously including the influence of the traffic lights, objective that can only be achieved by a microscopic traffic simulation model. Figure 1: Expected impact of the new infrastructure in San Sebastian with an close up of the Amara neighborhood Figure 2 displays the corresponding model built with the AIMSUN2 traffic simulation software, the EMME/2 sub model has been built automatically from the AIMSUN2 model by means of an interface between both systems. Figure 2: AIMSUN2 micro simulation model of the Amara neighborhood The type of information that micro simulation can provide for a further analysis is beyond the capabilities of traditional static models. The average flows from sections to sections turning movements) for the allowed movements at selected intersections in the model, speeds and delays for every simulated time interval can be obtained. The dynamic analysis for a time period is completed with values for other traffic variables or indicators of the quality of service as number of stops, time delayed at stops, average queue lengths, etc. Figure 3 provide a further insight on the capacity of analysis provided by dynamic simulation software. The graphic in this figure describes the evolution over time of average flows. The same type of graph can be produced for average queue lengths on a subset of selected sections in the model. Figure 3: The evolution of average flows over time 1.3 The ease of model building and data input (AIMSUN2) The recent evolution of the microscopic simulators has taken advantages of the state-of-theart in the development of object-oriented simulators, and graphical user interfaces, as well as the new trends in software design and the available tools that support it adapted to traffic modeling requirements. A proper achievement of the basic requirements of a microscopic simulator implies building models as close to reality as possible. The closer the model is to reality the more data demanding it become. This has been traditionally the main barrier Section Volumes (Veh/h) 0 200 400 600 80",autonomous vehicle,22,not included
79dd4e21811c1399a4525d82e16c8fbf23db3d51,to_check,semantic_scholar,Encyclopedia of Database Systems,1993-01-01 00:00:00,semantic_scholar,human-computer interaction,https://www.semanticscholar.org/paper/79dd4e21811c1399a4525d82e16c8fbf23db3d51,"Contents Foreword Preface to the third edition Preface to the second edition Preface to the first edition Introduction Part 1 Foundations Chapter 1 The human 1.1 Introduction 1.2 Input-output channels Design Focus: Getting noticed Design Focus: Where's the middle? 1.3 Human memory Design Focus: Cashing in Design Focus: 7 +- 2 revisited 1.4 Thinking: reasoning and problem solving Design Focus: Human error and false memories 1.5 Emotion 1.6 Individual differences 1.7 Psychology and the design of interactive systems 1.8 Summary Exercises Recommended reading Chapter 2 The computer 2.1 Introduction Design Focus: Numeric keypads 2.2 Text entry devices 2.3 Positioning, pointing and drawing 2.4 Display devices Design Focus: Hermes: a situated display 2.5 Devices for virtual reality and 3D interaction 2.6 Physical controls, sensors and special devices Design Focus: Feeling the road Design Focus: Smart-Its - making sensors easy 2.7 Paper: printing and scanning Design Focus: Readability of text 2.8 Memory 2.9 Processing and networks Design Focus: The myth of the infinitely fast machine 2.10 Summary Exercises Recommended reading Chapter 3 The interaction 3.1 Introduction 3.2 Models of interaction Design Focus: Video recorder 3.3 Frameworks and HCI 3.4 Ergonomics Design Focus: Industrial interfaces 3.5 Interaction styles Design Focus: Navigation in 3D and 2D 3.6 Elements of the WIMP interface Design Focus: Learning toolbars 3.7 Interactivity 3.8 The context of the interaction Design Focus: Half the picture? 3.9 Experience, engagement and fun 3.10 Summary Exercises Recommended reading Chapter 4 Paradigms 4.1 Introduction 4.2 Paradigms for interaction 4.3 Summary Exercises Recommended reading Part 2 Design process Chapter 5 Interaction design basics 5.1 Introduction 5.2 What is design? 5.3 The process of design 5.4 User focus Design Focus: Cultural probes 5.5 Scenarios 5.6 Navigation design Design Focus: Beware the big button trap Design Focus: Modes 5.7 Screen design and layout Design Focus: Alignment and layout matter Design Focus: Checking screen colors 5.8 Iteration and prototyping 5.9 Summary Exercises Recommended reading Chapter 6 HCI in the software process 6.1 Introduction 6.2 The software life cycle 6.3 Usability engineering 6.4 Iterative design and prototyping Design Focus: Prototyping in practice 6.5 Design rationale 6.6 Summary Exercises Recommended reading Chapter 7 Design rules 7.1 Introduction 7.2 Principles to support usability 7.3 Standards 7.4 Guidelines 7.5 Golden rules and heuristics 7.6 HCI patterns 7.7 Summary Exercises Recommended reading Chapter 8 Implementation support 8.1 Introduction 8.2 Elements of windowing systems 8.3 Programming the application Design Focus: Going with the grain 8.4 Using toolkits Design Focus: Java and AWT 8.5 User interface management systems 8.6 Summary Exercises Recommended reading Chapter 9 Evaluation techniques 9.1 What is evaluation? 9.2 Goals of evaluation 9.3 Evaluation through expert analysis 9.4 Evaluation through user participation 9.5 Choosing an evaluation method 9.6 Summary Exercises Recommended reading Chapter 10 Universal design 10.1 Introduction 10.2 Universal design principles 10.3 Multi-modal interaction Design Focus: Designing websites for screen readers Design Focus: Choosing the right kind of speech Design Focus: Apple Newton 10.4 Designing for diversity Design Focus: Mathematics for the blind 10.5 Summary Exercises Recommended reading Chapter 11 User support 11.1 Introduction 11.2 Requirements of user support 11.3 Approaches to user support 11.4 Adaptive help systems Design Focus: It's good to talk - help from real people 11.5 Designing user support systems 11.6 Summary Exercises Recommended reading Part 3 Models and theories Chapter 12 Cognitive models 12.1 Introduction 12.2 Goal and task hierarchies Design Focus: GOMS saves money 12.3 Linguistic models 12.4 The challenge of display-based systems 12.5 Physical and device models 12.6 Cognitive architectures 12.7 Summary Exercises Recommended reading Chapter 13 Socio-organizational issues and stakeholder requirements 13.1 Introduction 13.2 Organizational issues Design Focus: Implementing workflow in Lotus Notes 13.3 Capturing requirements Design Focus: Tomorrow's hospital - using participatory design 13.4 Summary Exercises Recommended reading Chapter 14 Communication and collaboration models 14.1 Introduction 14.2 Face-to-face communication Design Focus: Looking real - Avatar Conference 14.3 Conversation 14.4 Text-based communication 14.5 Group working 14.6 Summary Exercises Recommended reading Chapter 15 Task analysis 15.1 Introduction 15.2 Differences between task analysis and other techniques 15.3 Task decomposition 15.4 Knowledge-based analysis 15.5 Entity-relationship-based techniques 15.6 Sources of information and data collection 15.7 Uses of task analysis 15.8 Summary Exercises Recommended reading Chapter 16 Dialog notations and design 16.1 What is dialog? 16.2 Dialog design notations 16.3 Diagrammatic notations Design Focus: Using STNs in prototyping Design Focus: Digital watch - documentation and analysis 16.4 Textual dialog notations 16.5 Dialog semantics 16.6 Dialog analysis and design 16.7 Summary Exercises Recommended reading Chapter 17 Models of the system 17.1 Introduction 17.2 Standard formalisms 17.3 Interaction models 17.4 Continuous behavior 17.5 Summary Exercises Recommended reading Chapter 18 Modeling rich interaction 18.1 Introduction 18.2 Status-event analysis 18.3 Rich contexts 18.4 Low intention and sensor-based interaction Design Focus: Designing a car courtesy light 18.5 Summary Exercises Recommended reading Part 4 Outside the box Chapter 19 Groupware 19.1 Introduction 19.2 Groupware systems 19.3 Computer-mediated communication Design Focus: SMS in action 19.4 Meeting and decision support systems 19.5 Shared applications and artifacts 19.6 Frameworks for groupware Design Focus: TOWER - workspace awareness Exercises Recommended reading Chapter 20 Ubiquitous computing and augmented realities 20.1 Introduction 20.2 Ubiquitous computing applications research Design Focus: Ambient Wood - augmenting the physical Design Focus: Classroom 2000/eClass - deploying and evaluating ubicomp 20.3 Virtual and augmented reality Design Focus: Shared experience Design Focus: Applications of augmented reality 20.4 Information and data visualization Design Focus: Getting the size right 20.5 Summary Exercises Recommended reading Chapter 21 Hypertext, multimedia and the world wide web 21.1 Introduction 21.2 Understanding hypertext 21.3 Finding things 21.4 Web technology and issues 21.5 Static web content 21.6 Dynamic web content 21.7 Summary Exercises Recommended reading References Index",autonomous vehicle,23,not included
f77d59740dfeb10e9b650ec8b1baba91fca70279,to_check,semantic_scholar,,2011-01-01 00:00:00,semantic_scholar,navibeam: indoor assistance and navigation for shopping malls through projector phones,https://www.semanticscholar.org/paper/f77d59740dfeb10e9b650ec8b1baba91fca70279,"We present our concept of an indoor assistance and navigation system for pedestrians that leverages projector phones. Digitally enhanced guides have many advantages over traditional paper-based indoor guides, most of all that they can be aware of their current context and display dynamic information. That is why particularly shopping malls recently started deploying indoor assistance applications for mobile phones, which also include support for navigation. Moreover, as we show in the paper, projected interfaces offer additional distinct advantages over static guides and even traditional or augmented reality mobile applications. We describe five concepts for a shopping mall indoor assistance system based on projector phones, comprising support for shop selection, precise way finding, “virtual fitting” of clothes, and context-aware and ambient advertisements. We then apply the concepts to a typical shopping scenario, where users wear the phone at their belt and constantly project the interface in front of them. Expected benefits of our system are that people find their way quicker, easier, and less distracted from their usual shopping experience. Finally, we discuss the technical feasibility of our envisioned implementation and research questions we are particularly interested in. INTRODUCTION Navigation and location-based services for pedestrians recently gained a lot of attention and are becoming rapidly adopted. Very popular among these are applications for location-based places recommendations and turn-by-turn navigation. While these applications mostly focus on outdoor navigation, less attention has been paid to the opportunities for providing indoor assistance with mobile devices. Especially in large complex buildings, e.g. shopping centers, in most cases static signs, You-Are-Here maps, or paper flyer maps are still the only available navigation assistance for visitors. Preliminary observations and interviews we conducted in nearby shopping centers show that, at least in Germany, available navigational aids are still as insufficient as Levine reported them to be almost 30 years ago [8]. Despite a lot of research projects that explored indoor assistance over the last decade, it was not before the beginning of 2011 that we saw the first mobile shopping applications reaching consumer markets, such as the Sam’s Club mobile application [1], which provides indoor navigation to specific items and/or shops in some selected American shopping malls. Similarly, some shopping centers in Asia introduced mobile AR applications for shopping assistance [2]. In our research group we study future application areas of projector phones, i.e. mobile phones with integrated projectors (see [11] for a detailed survey). We found projector phones to provide some distinguished advantages for indoor navigation assistance, e.g., that interaction can be handsfree and the projection can serve as ambient display, thereby not contradicting the traditional shopping experience. Further that the surrounding world can be directly augmented, freeing the user from mapping between mobile display and real world and that the output space is much larger than on mobile displays. And finally, that bystander can see and attach to the projected output. RELATED WORK We present relevant work dealing with mobile shopping, recommender systems, indoor positioning and navigation. One of the first works on digital mobile shopping assistants has been done by Asthana et al. [3]. They presented main usage scenarios, e.g., telling people where to find certain products or informing them on discounts and special offers. Similar can be recognized in aforementioned mobile shopping applications and as well in recently filed patents, e.g. from Apple Inc. (US 2010/0198626 A1, US 2010/0191578 A1), which include navigation, service interactions (e.g., parking tickets), and support for social networking. Yang et al. [15] developed a location-aware recommender system. It learns a customer’s interests from previously visited product websites and continuously presents a list of products around the customer’s current location, that are likely to interest him. The software also takes into account the distance to shops and is able to learn customer’s preference between highly interesting products and far distances. Butz et al. [4] present a hybrid indoor navigation system that is able to adapt route instructions to different output devices (screen resolution, device resources) and based on the precision of available location information. Results from [7,14] indicate that intelligent fusion of infrastructure techniques, e.g. GPS, GSM triangulation, and sensors like accelerometers, magnetometers, and gyroscopes, enables precise indoor location tracking with current commercially available smartphones in unaltered environments. Kray et al. [6] explore the design space of routing instructions for pedestrians on mobile devices. Narzt et al. present a specific mobile application for augmented reality (AR) [10]. Alternative systems for pedestrian navigation are the Rotating Compass by Rukzio et al. [12], showing personalised navigation information on public displays and the CabBoots system by Frey [5], which guides users by means of tactile output in the shoes. Aforementioned techniques, with the exception of the last two, rely on holding a handset device. However, we feel strongly inclined that holding a device in hand for a longer time does not fit well the traditional shopping experience. Negative side effects, e.g., arm fatigue, regular switching of the field of view, do not allow using the shopping assistance application as constant companion. CONCEPT In our envisioned prototype, people wear their projector phone on their belt, projecting a display right in front of their feet (Figure 1). In the following we present five concepts for mobile shopping assistance that are enabled by projector phones. Later we apply these concepts to a typical shopping scenario. Shadow Interaction Since our concept expects people to wear the projector fixed to the belt most of the time, direct interaction with the mobile device would not be sufficient as the only interaction technique. Audio is not an alternative because of the noisy environment in a shopping mall. This leaves users with the option to directly interact with the projection, either by feet or hands (or gaze in the future). In preliminary studies we discovered that foot-based interaction is not well suited due to the fact that foot movement inherently involves movement of the body at the same time, which makes interaction cumbersome. We found interacting with the shadow of a finger in front of the projection a much more promising solution. Figure 2 shows the stroke of the resulting shadow on the projection. With the tip of the shadow, all points on the projection can be reached. Items should be highlighted once the shadow reaches their bounding box to give adequate feedback to the user. Although the tracking of the finger’s height to enable traditional press/release behaviors would be possible, this would require the user to maintain a complex mental model of different finger height levels. Instead, our concept builds on the fact, that the tip of the index finger can be moved well without changing the shape of the rest of the hand or even the middle knuckle of the index finger. Thus, to select an item, a user moves the index finger to point on the desired item and then bends the index finger and unbends it again. Alternatively, one could use the finger’s dwell time as in Microsoft’s Kinect. To the best of our knowledge, shadow interaction with projections has not been reported before. Radar of Recommendations Another concept that is particularly useful with projected navigation is a radar of recommendations. Building on [15], we want to constantly show and update a personal radar of products the user might be interested in (Figure 2). Based on the information available from accounts with online stores (e.g., Amazon) and items recently explored with our system (see fourth concept), users see offers of nearby stores in front of them and can select these items to start a navigation in the middle of the circle. The size of an item conveys the expected interest of the user, the distance from the middle depicts the walking distance (not air path) from the user’s current location. The size of the radius of interest (distance to shops and items) can be adjusted by slightly changing the angle of the projector, similarly to looking further ahead. Different from [15], the projection provides a much larger output space and the radar serves as ambient display in the user’s periphery. Projected (augmented) Navigation When the user selected an item or shop he is interested in, the assistance system starts a projected navigation. In outdoor navigation, turn-by-turn navigation is still the most prominent, though we have seen augmented reality been used in research [10]. Especially for indoor navigation, where there are more tight and subsequent turns or small decision spaces (take the left stairs up, not the right stairs down) our experience is that turn-by-turn navigation does not work well. Therefore, we want the user not to follow Figure 1: The projector is worn at different locations on the belt (left and middle) and can optionally be taken into hand to change the angle of the projection (right). Figure 2: The user interacts with his radar of recommendations through the shadow of his finger. The orange outline shows the shadow, the left red circle the fingertip that, for clicking, can be changed independently of the finger’s middle knuckle (right circle) or the rest of the hand’s shape. this type of directions, but instead simply follow a blue line projected in front of her (see Figure 3). Since the projector phone is spatially-aware, both in terms of location and orientation, the blue line is projected as a static augmentation of the real world, i",autonomous vehicle,24,not included
59304bc73b6d24f18d23404e0d408c462b66c4d0,to_check,semantic_scholar,Softwaretechnik-Trends,2008-01-01 00:00:00,semantic_scholar,simulationsbasierte analyse und entwicklung von peer-to-peer-systemen,https://www.semanticscholar.org/paper/59304bc73b6d24f18d23404e0d408c462b66c4d0,"Peer-to-Peer (P2P) systems are distributed systems composed of up to millions of functionally equivalent entities (peers), which form P2P overlay networks on top of physical networks to communicate. The functionality of a peer is implemented by a P2P application which de nes the behavior of the whole P2P system. The equivalence of peers is realized by providing client functionality as well as server functionality. Implementing a P2P system with speci ed behavior is a di cult task because the behavior depends on many factors, such as the used P2P search methods and the underlying physical network. Some factors cannot be taken into account completely because of their complexity or unknown or not understood parts. For instance, the prospective user behavior may only be estimated based on observed data. When engineering complex, dynamic software systems such as P2P systems, simulation is often used to analyze the properties of these systems based on simulation models in an early development phase. With simulations in natural sciences, the separation of reality and (simulation) model is clear: the reality exists in nature, while the model exists as software within some computer system. When simulating software systems, this separation is not so obvious: the simulated model is itself a software system. With P2P systems, for instance, a simpli ed P2P system is modeled and simulated for predicting properties of real P2P systems. The new software engineering contribution of this work is the Peer Software Engineering (PeerSE) method, which allows a controlled transition from simulation models to real-world software systems. The method starts with a comparative analysis of simulation models for P2P systems and proceeds iteratively toward the experimental implementation in a laboratory setting and nally a real-world P2P system deployed in a target environment. The method includes a simulation model for P2P systems and a tool supporting the execution of simulation and laboratory experiments. Simulation is an essential part of the PeerSE method used to identify and to compare models ful lling given requirements. When an appropriate model has been found, model components can be reused and further re ned to implement a laboratory P2P system. To allow for a controlled transition of model components to laboratory components, the results of simulation and laboratory experiments are directly compared using the same metrics. The applicability of the PeerSE method has been successfully evaluated by analyzing and realizing a P2P system for distributed software development.",autonomous vehicle,25,not included
4dd4afbb17999bdf9e218001e3a6ae2252c10f8f,to_check,semantic_scholar,Defense + Security,2017-01-01 00:00:00,semantic_scholar,visualizing uas-collected imagery using augmented reality,https://www.semanticscholar.org/paper/4dd4afbb17999bdf9e218001e3a6ae2252c10f8f,"One of the areas where augmented reality will have an impact is in the visualization of 3-D data. 3-D data has traditionally been viewed on a 2-D screen, which has limited its utility. Augmented reality head-mounted displays, such as the Microsoft HoloLens, make it possible to view 3-D data overlaid on the real world. This allows a user to view and interact with the data in ways similar to how they would interact with a physical 3-D object, such as moving, rotating, or walking around it. A type of 3-D data that is particularly useful for military applications is geo-specific 3-D terrain data, and the visualization of this data is critical for training, mission planning, intelligence, and improved situational awareness. Advances in Unmanned Aerial Systems (UAS), photogrammetry software, and rendering hardware have drastically reduced the technological and financial obstacles in collecting aerial imagery and in generating 3-D terrain maps from that imagery. Because of this, there is an increased need to develop new tools for the exploitation of 3-D data. We will demonstrate how the HoloLens can be used as a tool for visualizing 3-D terrain data. We will describe: 1) how UAScollected imagery is used to create 3-D terrain maps, 2) how those maps are deployed to the HoloLens, 3) how a user can view and manipulate the maps, and 4) how multiple users can view the same virtual 3-D object at the same time.",autonomous vehicle,26,not included
10.1109/icdmw.2019.00123,to_check,2019 International Conference on Data Mining Workshops (ICDMW),IEEE,2019-11-11 00:00:00,ieeexplore,implementation of mobile-based real-time heart rate variability detection for personalized healthcare,https://ieeexplore.ieee.org/document/8955523/,"The ubiquity of wearable devices together with areas like internet of things, big data and machine learning have promoted the development of solutions for personalized healthcare that use digital sensors. However, there is a lack of an implemented framework that is technically feasible, easily scalable and that provides meaningful variables to be used in applications for translational medicine. This paper describes the implementation and early evaluation of a physiological sensing tool that collects and processes photoplethysmography data from a wearable smartwatch to calculate heart rate variability in real-time. A technical open-source framework is outlined, involving mobile devices for collection of heart rate data, feature extraction and execution of data mining or machine learning algorithms that ultimately deliver mobile health interventions tailored to the users. Eleven volunteers participated in the empirical evaluation that was carried out using an existing mobile virtual reality application for mental health and under controlled slow-paced breathing exercises. The results validated the feasibility of implementation of the proposed framework in the stages of signal acquisition and real-time calculation of heart rate variability (HRV). The analysis of data regarding packet loss, peak detection and overall system performance provided considerations to enhance the real-time calculation of HRV features. Further studies are planned to validate all the stages of the proposed framework.",health,27,included
10.1109/access.2021.3103680,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,human centric digital transformation and operator 4.0 for the oil and gas industry,https://ieeexplore.ieee.org/document/9509417/,"Working at an oil and gas facility, such as a drilling rig, production facility, processing facility, or storage facility, involves various challenges, including health and safety risks. It is possible to leverage emerging digital technologies such as smart sensors, wearable or mobile devices, big data analytics, cloud computing, extended reality technologies, robotic systems, and drones to mitigate the challenges faced by oil and gas workers. While these technologies are not new to the oil and gas industry, most of its existing digital transformation initiatives follow business or process-centric approaches, in which the critical driver of the technology adoption is the enhancement of production, efficiency, and revenue. As a result, they may not address the challenges faced by the workers. As oil and gas workers are among the essential assets in the oil and gas industry, it is vital to address the challenges faced by these workers. This paper proposes a human-centric digital transformational framework for the oil and gas industry to deploy existing digital technologies to enhance their workers' health, safety, and working conditions. The paper outlines the critical challenges faced by oilfield workers, introduces a system architecture to implements a human-centric digital transformation, discusses the opportunities of the proposed framework, and summarizes the key impediment for the proposed framework.",health,28,not included
10.1109/icci-cc.2017.8109760,to_check,2017 IEEE 16th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),IEEE,2017-07-28 00:00:00,ieeexplore,early diagnosis of mild cognitive impairment: a case study in approaches to inductive-logic programming,https://ieeexplore.ieee.org/document/8109760/,"Recent rapid advances in data collection routines in clinical science have led to a trend of storing patient data in a heterogeneous database. The lack of existing computing tools to enable operability to use machine learning on these heterogeneous data sources is a barrier to the healthcare sciences. Healthcare data is usually complex and highly context-dependent, and it requires modern computational tools to handle the complexity of such data. This study sought to utilize the data collected from virtual reality (VR)-based software and a leap-motion device used for learning in mild cognitive impairment (MCI) cases to enable early detection of MCI by analyzing the classification rules for errors (action slips) based on finger-action transitions when performing instrumental activities of daily living (IADL). Finger motion was recorded as a time-series database. An induction technique known as Inductive-Logic Programming (ILP), which uses logical and clausal language to represent the training data, was then used to discover a concise classification rule using logical programming. We were able to generate rules on how action transitions of the finger in the experiments were related to the pattern of micro-errors that indicate the difference of error regarding the length of the no-motion state of the finger.",health,29,not included
10.1109/roman.1995.531965,to_check,Proceedings 4th IEEE International Workshop on Robot and Human Communication,IEEE,1995-07-07 00:00:00,ieeexplore,the hyper hospital-virtual reality based medical system on the computer network-its concept and user-configurable virtual world creating system,https://ieeexplore.ieee.org/document/531965/,"We have been developing a novel medical care system which is constructed on an electronic or computerized information network using virtual reality as the principal human interface. The major purpose of the hyper hospital is to restore humane interactions between patients and various medical caretakers by making a much closer contact between them in the real medical scene than that in current conventional medical practice. In the present study, we discuss the most fundamental part of the development of the virtual reality system for the medical use, that is, system software for the creation of the virtual world which can be defined and modified by users of various levels, not only by the medical caretakers, but particularly by patients.",health,30,not included
10.1007/s10916-021-01783-y,to_check,Journal of Medical Systems,Springer,2021-11-02 00:00:00,springer,machine learning for health: algorithm auditing & quality control,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10916-021-01783-y,"Developers proposing new machine learning for health (ML4H) tools often pledge to match or even surpass the performance of existing tools, yet the reality is usually more complicated. Reliable deployment of ML4H to the real world is challenging as examples from diabetic retinopathy or Covid-19 screening show. We envision an integrated framework of algorithm auditing and quality control that provides a path towards the effective and reliable application of ML systems in healthcare. In this editorial, we give a summary of ongoing work towards that vision and announce a call for participation to the special issue  Machine Learning for Health: Algorithm Auditing & Quality Control in this journal to advance the practice of ML4H auditing.",health,31,not included
10.1016/j.cmpb.2011.02.007,to_check,Computer Methods and Programs in Biomedicine,scopus,2012-09-01,sciencedirect,information system analysis of an e-learning system used for dental restorations simulation,https://api.elsevier.com/content/abstract/scopus_id/84863864660,"The goal of using virtual and augmented reality technologies in therapeutic interventions simulation, in the fixed prosthodontics (VirDenT) project, is to increase the quality of the educational process in dental faculties, by assisting students in learning how to prepare teeth for all-ceramic restorations. Its main component is an e-learning virtual reality-based software system that will be used for the developing skills in grinding teeth, needed in all-ceramic restorations. The complexity of the domain problem that the software system dealt with made the analysis of the information system supported by VirDenT necessary. The analysis contains the following activities: identification and classification of the system stakeholders, description of the business processes, formulation of the business rules, and modelling of business objects. During this stage, we constructed the context diagram, the business use case diagram, the activity diagrams and the class diagram of the domain model. These models are useful for the further development of the software system that implements the VirDenT information system.",health,32,not included
10.1016/j.ejvs.2011.03.032,to_check,European Journal of Vascular and Endovascular Surgery,scopus,2011-08-01,sciencedirect,efficient implementation of patient-specific simulated rehearsal for the carotid artery stenting procedure: part-task rehearsal,https://api.elsevier.com/content/abstract/scopus_id/79960953215,"Objective(s)
                  Patient-specific simulated rehearsal (PsR) is a technological advance within the domain of endovascular virtual reality (VR) simulation. It allows incorporation of patient-specific computed tomography Digital Imaging and Communications in Medicine (CT DICOM) data into the simulation and subsequent rehearsal of real patient cases. This study aimed to evaluate whether a part-task rehearsal (PTr) of a carotid artery stenting procedure (CAS) on a VR simulator is as effective as a full-task (FTr) preoperative run through.
               
                  Methods
                  Medical trainees were trained in the CAS procedure and randomised to a PTr or FTr of a challenging CAS case (Type-II arch). PTr consisted of 30min of repeated catheterisations of the common carotid artery (CCA). Thereafter, both groups performed the CAS procedure in a fully functional simulated operating suite (SOS) with an interventional team. Technical performances were assessed using simulator-based metrics and expert ratings. Other aspects of performance were assessed using the Non-Technical Skills for Surgeons (NOTSS) scoring.
               
                  Results
                  Twenty trainees were evenly randomised to either PTr or FTr. No differences in performance were seen except for the total time the embolic protection device (EPD) was deployed (9.4min for the PT vs. 8.1min for the FT, p
                     =0.02). Total time (26.3 vs. 25.5min, p
                     =0.94), fluoroscopy time (15.8 vs. 14.4min, p
                     =0.68), number of roadmaps (10.5 vs. 11.0, p
                     =0.54), amount of contrast (53.5 vs. 58.0ml, p
                     =0.33), time to deploy the EPD (0.9 vs. 0.8min, p
                     =0.31) and time to catheterise the CCA (9.2 vs. 8.9min, p=0.94) were similar. Qualitative performances as measured by expert ratings (score 24 vs. 24, p
                     =0.49) and NOTSS (p
                     >0.05 for all categories) were also comparable.
               
                  Conclusions
                  Part- and full-task rehearsals are equally effective with respect to the operative performance of a simulated CAS intervention. This finding makes a patient-specific rehearsal more efficient and may increase the feasibility of implementation of this technology into medical practice.",health,33,not included
10.3390/ani11041008,to_check,core,'MDPI AG',2021-01-01 00:00:00,core,digital twins in livestock farming,,"Artificial intelligence (AI), machine learning (ML) and big data are consistently called upon to analyze and comprehend many facets of modern daily life. AI and ML in particular are widely used in animal husbandry to monitor both the animals and environment around the clock, which leads to a better understanding of animal behavior and distress, disease control and prevention, and effective business decisions for the farmer. One particularly promising area that advances upon AI is digital twin technology, which is currently used to improve efficiencies and reduce costs across multiple industries and sectors. In contrast to a model, a digital twin is a digital replica of a real-world entity that is kept current with a constant influx of data. The application of digital twins within the livestock farming sector is the next frontier and has the potential to be used to improve large-scale precision livestock farming practices, machinery and equipment usage, and the health and well-being of a wide variety of farm animals. The mental and emotional states of animals can be monitored using recognition technology that examines facial features, such as ear postures and eye white regions. Used with modeling, simulation and augmented reality technologies, digital twins can help farmers to build more energy-efficient housing structures, predict heat cycles for breeding, discourage negative behaviors of livestock, and potentially much more. As with all disruptive technological advances, the implementation of digital twin technology will demand a thorough cost and benefit analysis of individual farms. Our goal in this review is to assess the progress toward the use of digital twin technology in livestock farming, with the goal of revolutionizing animal husbandry in the future. View Full-Tex",health,34,not included
5e171bf6603a03fbfdf3434abcd29496a2327100,to_check,semantic_scholar,Multimodal Technol. Interact.,2021-01-01 00:00:00,semantic_scholar,a learning analytics conceptual framework for augmented reality-supported educational case studies,https://www.semanticscholar.org/paper/5e171bf6603a03fbfdf3434abcd29496a2327100,"The deployment of augmented reality (AR) has attracted educators’ interest and introduced new opportunities in education. Additionally, the advancement of artificial intelligence has enabled educational researchers to apply innovative methods and techniques for the monitoring and evaluation of the teaching and learning process. The so-called learning analytics (LA) discipline emerged with the promise to revolutionize traditional instructional practices by introducing systematic and multidimensional ways to improve the effectiveness of the instructional process. However, the implementation of LA methods is usually associated with web-based platforms, which offer direct access to learners’ data with minimal effort or adjustments. On the other hand, the complex nature of immersive technologies and the diverse instructional approaches which are utilized in different scientific domains have limited the opportunities for research and development in this direction. Within these research contexts, we present a conceptual framework that describes the elements of an LA process tailored to the information that can be gathered from the use of educational applications, and further provide an indicative case study for AR-supported educational interventions. The current work contributes by elucidating and concretizing the design elements of AR-supported applications and provides researchers and designers with guidelines on how to apply instructional strategies in (augmented) real-world projects.",health,35,not included
10.1109/icccn52240.2021.9522281,to_check,2021 International Conference on Computer Communications and Networks (ICCCN),IEEE,2021-07-22 00:00:00,ieeexplore,realization of an intrusion detection use-case in onap with acumos,https://ieeexplore.ieee.org/document/9522281/,"With Software-Defined Networking and Machine Learning/Artificial Intelligence (ML/AI) reaching new paradigms in their corresponding fields, both academia and industry have exhibited interests in discovering unique aspects of intelligent and autonomous communication networks. Transforming such intentions and interests to reality involves software development and deployment, which has its own story of significant evolution. There has been a notable shift in the strategies and approaches to software development. Today, the divergence of tools and technologies as per demand is so substantial that adapting a software application from one environment to another could involve tedious redesign and redevelopment. This implies enormous effort in migrating existing applications and research works to a modern industrial setup. Additionally, the struggles with sustainability maintenance of such applications could be painful. Concerning ML/AI, the capabilities to train, deploy, retrain, and re-deploy AI models as quickly as possible will be crucial for AI-driven network systems. An end-to-end workflow using unified open-source frameworks is the need of the hour to facilitate the integration of ML/AI models into the modern software-driven virtualized communication networks. Hence, in our paper, we present such a prototype by demonstrating the journey of a sample SVM classifier from being a python script to be deployed as a micro-service using ONAP and Acumos. While illustrating various features of Acumos and ONAP, this paper intends to make readers familiar with an end-to-end workflow taking advantage of the integration of both open-source platforms.",industry,36,included
10.1109/proc.1984.12819,to_check,Proceedings of the IEEE,IEEE,1984-01-01 00:00:00,ieeexplore,the anticipated impact of supercomputers on finite-element analysis,https://ieeexplore.ieee.org/document/1457087/,"The supercomputers of the 1980's have already impacted large-scale computation. This paper discusses the status and anticipated impact of supercomputers on finite-element analysis which is the primary tool for structural analysis and is also very useful in other areas of engineering analysis. The initial impact has been the significant reduction in turnaround time for large problems and the corresponding opportunity to solve heretofore unsolvable problems. In these cases, emphasis has been placed on employing already-proven computing software which was modifed to take advantage of vector processing and other forms of parallel operations. This trend is expected to continue because the established usage base of commercially available programs is not likely to be quickly dislodged. The near term will see the further use of design optimization, broader use of nonlinear mechanics, and a closer link between designers and analysts because of improved computer turnaround. The economy of scale suggests that solution techniques will be performed not only faster but cheaper than is possible with scalar processors which will further encourage the analysis of larger, more complex structures. The supercomputers of the future are expected to offer additional challenges to today's application systems. A primary factor in this will be the effective use of multiprocessors. Additional influence is expected as Artificial Intelligence matures to the point where Expert Systems become a reality for selected engineering and scientific disciplines. In order to effectively compete, today's software companies must address the possibility of significant changes in the architecture and methodology currently embodied in today's systems. Improved packaging, most likely in the form of pre- and postprocessors, will be necessary to provide industry- or technology-specific systems solutions.",industry,37,not included
10.1109/isie45063.2020.9152441,to_check,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),IEEE,2020-06-19 00:00:00,ieeexplore,deployment of a smart and predictive maintenance system in an industrial case study,https://ieeexplore.ieee.org/document/9152441/,"Industrial manufacturing environments are often characterized as being stochastic, dynamic and chaotic, being crucial the implementation of proper maintenance strategies to ensure the production efficiency, since the machines' breakdown leads to a degradation of the system performance, causing the loss of productivity and business opportunities. In this context, the use of emergent ICT technologies, such as Internet of Things (IoT), machine learning and augmented reality, allows to develop smart and predictive maintenance systems, contributing for the reduction of unplanned machines' downtime by predicting possible failures and recovering faster when they occur. This paper describes the deployment of a smart and predictive maintenance system in an industrial case study, that considers IoT and machine learning technologies to support the online and real-time data collection and analysis for the earlier detection of machine failures, allowing the visualization, monitoring and schedule of maintenance interventions to mitigate the occurrence of such failures. The deployed system also integrates machine learning and augmented reality technologies to support the technicians during the execution of maintenance interventions.",industry,38,included
10.1109/tase.2019.2938316,to_check,IEEE Transactions on Automation Science and Engineering,IEEE,2020-04-01 00:00:00,ieeexplore,semiautomatic labeling for deep learning in robotics,https://ieeexplore.ieee.org/document/8844069/,"In this article, we propose an augmented reality semiautomatic labeling (ARS), a semiautomatic method which leverages on moving a 2-D camera by means of a robot, proving precise camera tracking, and an augmented reality pen (ARP) to define initial object bounding box, to create large labeled data sets with minimal human intervention. By removing the burden of generating annotated data from humans, we make the deep learning technique applied to computer vision, which typically requires very large data sets, truly automated and reliable. With the ARS pipeline, we created two novel data sets effortlessly, one on electromechanical components (industrial scenario) and other on fruits (daily-living scenario) and trained two state-of-the-art object detectors robustly, based on convolutional neural networks, such as you only look once (YOLO) and single shot detector (SSD). With respect to conventional manual annotation of 1000 frames that takes us slightly more than 10 h, the proposed approach based on ARS allows to annotate 9 sequences of about 35 000 frames in less than 1 h, with a gain factor of about 450. Moreover, both the precision and recall of object detection is increased by about 15% with respect to manual labeling. All our software is available as a robot operating system (ROS) package in a public repository alongside with the novel annotated data sets. Note to Practitioners-This article was motivated by the lack of a simple and effective solution for the generation of data sets usable to train a data-driven model, such as a modern deep neural network, so as to make them accessible in an industrial environment. Specifically, a deep learning robot guidance vision system would require such a large amount of manually labeled images that it would be too expensive and impractical for a real use case, where system reconfigurability is a fundamental requirement. With our system, on the other hand, especially in the field of industrial robotics, the cost of image labeling can be reduced, for the first time, to nearly zero, thus paving the way for self-reconfiguring systems with very high performance (as demonstrated by our experimental results). One of the limitations of this approach is the need to use a manual method for the detection of objects of interest in the preliminary stages of the pipeline (ARP or graphical interface). A feasible extension, related to the field of collaborative robotics, could be used to exploit the robot itself, manually moved by the user, even for this preliminary stage, so as to eliminate any source of inaccuracy.",industry,39,not included
10.23919/jcn.2020.100039,to_check,Journal of Communications and Networks,KICS,2020-12-01 00:00:00,ieeexplore,special issue on 6g wireless systems,https://ieeexplore.ieee.org/document/9321190/,"While 5G is currently being deployed around the globe, research on 6G is under way aiming at addressing the coming challenges of drastic increase of wireless data traffic and support of other usage scenarios. 6G is expected to extend 5G capabilities even further. Higher bitrates (up to Tbps) and lower latency (less than 1ms) will allow introducing new services — such as pervasive edge intelligence, ultra-massive machine-type communications, extremely reliable low-latency communications, holographic rendering and high-precision communications — and meet more stringent requirements, especially in the following dimensions: energy efficiency; intelligence; spectral efficiency; security, secrecy and privacy; affordability; and customization. Artificial intelligence approaches and techniques, such as machine learning (of which deep learning and reinforcement learning are specific examples), and machine reasoning (which includes planning, scheduling, knowledge representation and reasoning, search and optimization), are the new fundamental enablers to operate networks more efficiently, enhance the overall end user experience and provide innovative service applications. Quantum Optics Computing (QOC) and Quantum Key Distribution (QKD) are almost ready for industrial applications. In particular, massive Internet of Things (mIoT), Industrial IoT (IloT), fully automated robotic platforms (which include control, perception, sensors and actuators, as well as the integration of other techniques into cyber-physical systems), vehicles and multisensory extended reality are examples of the new data-demanding applications, which will impose new performance targets and motivate 6G design and deployment.",industry,40,not included
10.1007/978-3-030-85867-4_4,to_check,Business Process Management: Blockchain and Robotic Process Automation Forum,Springer,2021-01-01 00:00:00,springer,airpa: an architecture to support the execution and maintenance of ai-powered rpa robots,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-85867-4_4,"Robotic Process Automation (RPA) has quickly evolved from automating simple rule-based tasks. Nowadays, RPA is required to mimic more sophisticated human tasks, thus implying its combination with Artificial Intelligence (AI) technology, i.e., the so-called intelligent RPA. Putting together RPA with AI leads to a challenging scenario since (1) it involves professionals from both fields who typically have different skills and backgrounds, and (2) AI models tend to degrade over time which affects the performance of the overall solution. This paper describes the AIRPA project, which addresses these challenges by proposing a software architecture that enables (1) the abstraction of the robot development from the AI development and (2) the monitor, control, and maintain intelligent RPA developments to ensure its quality and performance over time. The project has been conducted in the Servinform context, a Spanish consultancy firm, and the proposed prototype has been validated with reality settings. The initial experiences yield promising results in reducing AHT (Average Handle Time) in processes where AIRPA deployed cognitive robots, which encourages exploring the support of intelligent RPA development.",industry,41,included
http://arxiv.org/abs/1811.02213v1,to_check,arxiv,arxiv,2018-11-06 00:00:00,arxiv,"hybrid approach to automation, rpa and machine learning: a method for
  the human-centered design of software robots",http://arxiv.org/abs/1811.02213v1,"One of the more prominent trends within Industry 4.0 is the drive to employ
Robotic Process Automation (RPA), especially as one of the elements of the Lean
approach. The full implementation of RPA is riddled with challenges relating
both to the reality of everyday business operations, from SMEs to SSCs and
beyond, and the social effects of the changing job market. To successfully
address these points there is a need to develop a solution that would adjust to
the existing business operations and at the same time lower the negative social
impact of the automation process.
  To achieve these goals we propose a hybrid, human-centered approach to the
development of software robots. This design and implementation method combines
the Living Lab approach with empowerment through participatory design to
kick-start the co-development and co-maintenance of hybrid software robots
which, supported by variety of AI methods and tools, including interactive and
collaborative ML in the cloud, transform menial job posts into higher-skilled
positions, allowing former employees to stay on as robot co-designers and
maintainers, i.e. as co-programmers who supervise the machine learning
processes with the use of tailored high-level RPA Domain Specific Languages
(DSLs) to adjust the functioning of the robots and maintain operational
flexibility.",industry,42,not included
http://arxiv.org/abs/1908.01862v1,to_check,arxiv,arxiv,2019-08-05 00:00:00,arxiv,semi-automatic labeling for deep learning in robotics,http://arxiv.org/abs/1908.01862v1,"In this paper, we propose Augmented Reality Semi-automatic labeling (ARS), a
semi-automatic method which leverages on moving a 2D camera by means of a
robot, proving precise camera tracking, and an augmented reality pen to define
initial object bounding box, to create large labeled datasets with minimal
human intervention. By removing the burden of generating annotated data from
humans, we make the Deep Learning technique applied to computer vision, that
typically requires very large datasets, truly automated and reliable. With the
ARS pipeline, we created effortlessly two novel datasets, one on
electromechanical components (industrial scenario) and one on fruits
(daily-living scenario), and trained robustly two state-of-the-art object
detectors, based on convolutional neural networks, such as YOLO and SSD. With
respect to the conventional manual annotation of 1000 frames that takes us
slightly more than 10 hours, the proposed approach based on ARS allows
annotating 9 sequences of about 35000 frames in less than one hour, with a gain
factor of about 450. Moreover, both the precision and recall of object
detection is increased by about 15\% with respect to manual labeling. All our
software is available as a ROS package in a public repository alongside the
novel annotated datasets.",industry,43,not included
10.1016/j.procir.2020.04.135,to_check,Procedia CIRP,scopus,2020-01-01,sciencedirect,application of artificial intelligence to an electrical rewinding factory shop,https://api.elsevier.com/content/abstract/scopus_id/85091693237,"The evolution of artificial intelligence (AI) and big data resulted in the full potential realization of technologies through convergence. Tremendous acceptance, adoption and implementation of the United Nations Sustainable Development Goals (SDG) Agenda 2030, has resulted in original equipment manufacturers (OEM) developing various designs of rotary machines in a bid to improve energy efficiency, with more improvements expected in the coming decade. An effective technique to manage energy efficiency in the smart grid is through integration of demand side management, inclusive of optimization of rewinding of an electric motor in a machine shop. This paper aims to conceptualize application of AI and augmented reality (AR) towards process visibility of remanufacturing rotary machine stators by robotic vision. SLT is the triangulation methodology used in laser scanning for 3D modelling, and instantaneous condition assessment of the core. A pre-defined robotic path is used towards identification of features for range image acquisition. Therefore, the potential of industry 4.0 in resuscitation of end-of-life products through service remanufacturers by RE in a rewinding shop are presented.",industry,44,not included
10.1016/j.procir.2018.01.036,to_check,Procedia CIRP,scopus,2018-01-01,sciencedirect,"intuitive robot programming through environment perception, augmented reality simulation and automated program verification",https://api.elsevier.com/content/abstract/scopus_id/85061975291,"The increasing complexity of products and machines as well as short production cycles with small lot sizes present great challenges to production industry. Both, the programming of industrial robots in online mode using hand-held control devices or in offline mode using text-based programming requires specific knowledge of robotics and manufacturer-dependent robot control systems. In particular for small and medium-sized enterprises the machine control software needs to be easy, intuitive and usable without time-consuming learning steps, even for employees with no in-depth knowledge of information technology. To simplify the programming of application programs for industrial robots, we extended a cloud-based, task-oriented robot control system with environment perception and plausibility check functions. For the environment perception a depth camera and pointcloud processing hardware were installed. We detect objects located in the robot’s workspace by pointcloud processing with ROS and the PCL and add them to the augmented reality user interface of the robot control. The combination of process knowledge from task-oriented application programming and information about available workpieces from automated image processing enables a plausibility check and verification of the robot program before execution. After a robot program has been approved by the plausibility check, it is tested in an augmented reality simulation for collisions with the detected objects before deployment to the physical robot hardware. Experiments were carried out to evaluate the effectiveness of the developed extensions and confirmed their functionality.",industry,45,not included
10.28991/esj-2021-01308,to_check,core,'Ital Publication',2021-10-01 00:00:00,core,utilization of alkin-wp-based digital library evaluation software as evaluation tool of digital library effectiveness,https://core.ac.uk/download/478595370.pdf,"One source of learning in universities is a digital library. In the era of industry 4.0, most universities have implemented digital libraries in supporting the learning process. However, the reality shows that digital library management is still ineffective. Therefore, the implementation of digital libraries needs to be evaluated for determining the digital library effectiveness used as learning resources in supporting the learning process in universities. Many evaluation tools are used to evaluate the effectiveness of digital libraries but have not provided accurate recommendation results to support decision-making. This research presents an innovation in the form of an evaluation tool that can be used to evaluate the digital library effectiveness in universities. That evaluation tool is called the Alkin-WP-based digital library evaluation software. This software is a desktop platform that contains aspects of measuring the digital library effectiveness by referring to the components of the Alkin evaluation model and the WP (Weighted Product) method. This research aimed to show the effectiveness level of the utilization of Alkin-WP-based digital library evaluation software. This research method was R & D (Research & Development) which refers to the ten development stages of the Borg and Gall model. In this research, development was focused only on a few stages, included: usage trials, final product revision, dissemination, and implementation. The subjects involved in assessing the implementation/utilization of the Alkin-WP-based digital library evaluation software were 35 people, in the usage trials were six people, in product revision were three people, and at the stage of dissemination were 15 people. The tools used to collect data were questionnaires and interview guidelines. The data analysis technique used was descriptive quantitative. The effectiveness level of utilizing the Alkin-WP-based digital library evaluation software was 88.34%. It showed that the evaluation software had effective. The impact of this research results on the scientific field of educational evaluation is being able to show the existence of a new evaluation tool based on educational evaluation and artificial intelligence. That evaluation tool can easier for library heads to make policies for revamping digital library services based on accurate recommendations. Doi: 10.28991/esj-2021-01308 Full Text: PD",industry,46,not included
"[{'title': none, 'identifiers': ['1227-0954', 'issn:1227-0954']}]",to_check,core,,2016-12-01 00:00:00,core,????????????????????????,Public Law Perspectives on Personal Information Protection in the Era of Automated Vehicle,"4??? ???????????? ????????? ???????????????????????? ?????? ???????????? ????????? ????????? ???????????? ??????????????? ????????? ????????? ????????? ??????????????? ???????????? ????????? ???????????? ???????????? ??????????????? ??????. ?????? ?????????????????? ??????????????? ?????? ??????(innovation)?????????, ?????? ??????????????? ????????? ????????????????????? ????????? ?????? ????????? ?????? ????????? ????????? ???????????? ?????? ????????????????????????????????? ????????? ????????? ???????????? ?????????. ??????????????? ????????? ?????? ???????????? ?????? &lsquo;????????????&rsquo;??? &lsquo;???????????????&rsquo;(posthuman)??? ?????????, ????????? &lsquo;??????&rsquo;(human) ????????? ????????? ???????????? ????????? ??????????????? ?????? ????????? ??????????????? ????????? ???????????? ???????????? ????????? ?????? ????????? ?????? ?????? ????????? ?????? ???????????? ?????? ????????? ?????? ????????? ????????? ????????? ???????????? ??????.
???????????????????????? ??????????????? ???????????? ????????????, ???????????? ??? ?????? ????????? ????????? ??????????????? ?????? ????????????. ?????? ????????? ????????? ???????????? ???????????? ??? ?????? ??????, ?????????????????? ????????? ????????? ??????????????? ????????? ????????????. ????????? ????????? ??? ?????? ???????????? ???????????? ?????? ???????????? ??? ??? ?????????, ?????? ???????????????????????? ?????? ????????? ??????????????? ???????????? ????????? ??? ?????????, ???????????? ???????????? ????????? ???????????? ???????????? ?????? ????????? ????????? ???????????? ????????? ???????????? ?????? ?????? ????????????, ?????? ???????????? ????????? ????????? ?????? ??????????????? ?????? ????????? ??? ?????? ????????? ???????????? ??????, ?????? ??? ????????? ??????????????? ???????????? ?????? ????????? ???????????? ???????????? ????????? ????????? ????????? ????????? ???????????? ??????&middot;???????????? ????????? ???????????? ??????.
????????? ???????????? ???????????????????????? ??? ??????????????? ??????????????? ?????? ????????? ????????? ??????(device)??? ??????????????? ?????? ?????????, ???????????????????????? ?????? ????????? ????????? ????????? ???????????? ?????? ??????????????? ???????????? ???????????????, ????????? ?????????????????? ???????????? ?????? ????????????(connected) ?????? ??? ????????? ??????????????? ????????????????????????????????? ???????????? ?????? ?????????, ???????????? ????????? ????????? ????????? ???????????? ???????????? ?????? ????????? ????????? ???????????? ????????? ??? ??? ??????. ????????? ???????????? ?????? ????????? ????????? ????????? ?????????????????? ??????????????? ????????? ????????? ????????? ????????? ???????????? ????????? ??????????????? ????????? ??????????????? ??????????????? ??????????????? ?????????????????? ????????? ????????? ???????????? ??????. ?????? ???????????? ?????????????????? ??????????????? ??? ?????? ??????, ????????????, ??????????????? ????????? ?????? ??????, ????????? ?????? ??????????????? ?????????, ????????????????????? ???????????? ??????????????? ????????? ????????????, ?????? ??????????????? ????????????, ???????????? ????????? ???????????? ????????? ????????? ?????? ????????? ????????? ??????????????? ???????????? ??? ??????. ????????? ????????? ?????? ????????? ??? ???????????? ????????? ????????? ?????? ???????????? ?????? ????????? ????????? ??? ??????, ??????????????? ??????????????? ??????????????? ?????? ??????????????? ????????? ??? ?????? ?????????, ????????? ????????? ?????? ?????? ???????????? ????????? ???????????? ????????? ????????? ????????? ????????? ?????? ???????????? ??????????????? ????????? ????????? ????????????.
????????? ???????????? ????????? ??????????????? ???????????? ??????????????? ??????. ??????????????? ?????? ????????? ????????? ????????? ????????? ???????????? ????????? ??????????????? ??? ??????????????? ????????? ????????? ???????????? ???????????? ???????????????????????? ????????? ????????? ????????? ?????? ????????? ????????? ?????????  ????????? ????????? ????????? ????????? ??? ?????? ?????????.


Concept of Automated Vehicle in the era of the fourth industrial revolution is transforming from the `vehicle` that had its focal point in the hardware to the `portable computer` that has its focus in the software that processes broad range of digital information. The phenomenon in one aspect is innovation through scientific technology. However on the other hand, it is raising humanistic, philosophical and legal questions about how human life will change according to the technological and industrial transition. Age of `Artificial Intelligence` and `Posthuman` emerging through the development of technology is creating new concept of human and bringing forward the issue of new danger that is occurring when reason and physical abilities thought as the essence of `human` is being substituted by scientific technology and computer. The advocates of automated vehicle assert that it provides more safety and convenience to human. Such argument cannot be completely be false, and I wish to reach the same conclusion eventually. However, as other process of introducing new technology like nuclear energy tell us, to consider whether automated vehicle is more convenient and better than the previous vehicle system and whether there are no new danger that new technology is bringing about or overlooked elements of danger is severely important. Therefore, if certain danger is expected, technological prevention should be devised as a first step, and for the sections where technology and industry is not self-regulating, it is legislations and policies` role to lead and force to a human-friendly direction. Automated vehicle that is being discussed nowadays can be neutral itself and seem to be no more than a convenient device. However, since each automated vehicle is activated by entering massive amount of digital information and since it collects, stores, and distributes a welter of personal information when connected to Internet of Things(IoT), whole new danger from the previous era is created in terms of personal information and privacy. The ironic reality of surveillance society since the modern times that has assured freedom of an individual on the one hand and has reinforced utilitarian society control on the grounds of fully protecting one`s right and expanding welfare on the other lies beneath this near future. Especially, personal information is more being exposed and distributed and collection and surveillance of digital information is becoming easier in the information age. Automated vehicle system cannot be free from such exposure and danger of violating since it is operated with geolocation, video, and communication information as well. Moreover, since violation of personal information can lead to violation of privacy and human dignity, and damage the basic order of liberal democracy, arranging the frame of public regulation relating to personal information protection by researching U.S. and European Union legislation and seems more than necessary. Being awake is what sovereign needs in the digital era. Likewise, to secure right to personal information, we should remember to turn the monitoring lamp on in advance not only as an individual, but also as a group in order to obtain convenience and prevent danger of new era of automated vehicle.??? ????????? ??????????????? ?????????????????????????????? ???????????????(HY-2014??????)",industry,47,not included
ecf8a33465099bfc5aa5a4b0bc0ad3861ad35018,to_check,semantic_scholar,Proceedings,2021-01-01 00:00:00,semantic_scholar,digitalized development methodology for the continuous vehicle product life cycle,https://www.semanticscholar.org/paper/ecf8a33465099bfc5aa5a4b0bc0ad3861ad35018,"The automotive industry is undergoing an unprecedented transformation. From new E/E architectures to central vehicle computers, new developments are emerging to deal with increasing complexity and a growing number of functions. As well as making increasing use of affordable consumer IT technologies, vehicle software is evolving into a living object that receives a continuous stream of updates and even functional enhancements. This is creating a new world of entirely new business models in which conventional development methods are reaching their limits. A vehicle exists both as a physical object in the real world and as a digital model, or “digital twin”, each of which is closely synchronized with the other. Development and operations cycles are increasingly merging in a way that would be impossible without virtualization and modern development approaches such as continuous deployment. Bosch and its subsidiary ETAS are shaping this new world. They offer a Software-in-the-Loop (SiL) suite that links the various elements of virtualization to the required development tools in a modular and flexible manner. Coordinated interactions between the key elements of the SiL framework ensure the quality and usability of the virtual world elements. These include, for example, virtual ECUs that guarantee realistic behavior, a powerful simulation and integration tool, virtual networks, and validation through the monitoring of system behavior in the field. Virtualization at the complete-vehicle level is no longer a vision, but a reality that is already used by OEMs.",industry,48,not included
8f9975f5799a333bd5397b437f195a3a9ce6c34c,to_check,semantic_scholar,,2010-01-01 00:00:00,semantic_scholar,real-world service interaction with enterprise systems in dynamic manufacturing environments,https://www.semanticscholar.org/paper/8f9975f5799a333bd5397b437f195a3a9ce6c34c,"The factory of the future will be heavily based on internet and web technologies. A new generation of devices with embedded hardware and software will feature greatly improved storage, computing, and networking capabilities. This will lead to a system landscape of millions of networked devices that is heterogeneous with respect to functionality but features standard interfaces. This new breed of devices will not only be able to store and report information about themselves and their physical surroundings, but execute more computations and local logic. They will form collaborative peer-to-peer networks and also connect to central systems. By eliminating media breaks, e.g. by replacing manual data entry with a direct connection to devices, this “internet of things” will feature end-to-end connectivity, making the models of the real world, as they exist in business systems, follow reality more precisely and with shorter delay. This will change the way we design, deploy and use services at all layers of the system, be it the device, line, plant, or company level or even between collaborating organizations. This chapter describes an architecture for effective integration of the services from the internet of things with enterprise services. We describe the case of centrally managing a population of devices that are located at different sites, including dynamic discovery of devices and the services they offer, near real-time cross-site interaction, interaction with business processes and distributed system management.",industry,49,not included
10.1109/icas.2009.2,to_check,2009 Fifth International Conference on Autonomic and Autonomous Systems,IEEE,2009-04-25 00:00:00,ieeexplore,[title page iii],https://ieeexplore.ieee.org/document/4976566/,The following topics are dealt with: real-time chain-structured synchronous dataflow; memory requirement formal determination; linear singular descriptor differential system; execution-optimized paths; greedy strategy; load trend evaluation; self-managed P2P streaming; context-aware ambient assisted living application; self-adaptive distributed model; autonomic systems; wireless sensor networks; topology control; learning based method; self-recovery method; mobile data sharing; heterogeneous QoS resource manager; component-based self-healing; NGN mobility; interactive user activity; .NET Windows service agent technology; agent based Web browser; resource-definition policies; autonomic computing; autonomic system administration; automatic database performance tuning; knowledge management; adaptive reinforcement learning; VoIP services; autonomic RSS: distributed virtual reality simulations; virtual machines resources allocation; multi-lier distributed systems; network I/O extensibility; virtual keyboards; self-configuring smart homes; legged underwater vehicles; particle filters; reusable semantic components; multi-agent systems; fixed-wing unmanned aerial vehicles; fuzzy inference system; robot swarms; mobile robots; optimization architecture; autonomous unmanned helicopter landing system design; heterogeneous multi-database environments; autonomic software license management system; Web server crashes prediction; laser range finder; video quality; wireless networks; ITU-T G.1030; open IMS core; context-aware data mining methodology; supply chain finance cooperative systems; autonomous pervasive environments; distributed generic stress tool; dynamic adaptive systems; multisensory media effects and user preference.,smart cities,50,not included
10.1007/978-3-030-28925-6_1,to_check,3rd EAI International Conference on IoT in Urban Space,Springer,2020-01-01 00:00:00,springer,cityflow: supporting spatial-temporal edge computing for urban machine learning applications,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-28925-6_1,"A growing trend in smart cities is the use of machine learning techniques to gather city data, formulate learning tasks and models, and use these to develop solutions to city problems. However, although these processes are sufficient for theoretical experiments, they often fail when they meet the reality of city data and processes, which by their very nature are highly distributed, heterogeneous, and exhibit high degrees of spatial and temporal variance. In order to address those problems, we have designed and implemented an integrated development environment called CityFlow that supports developing machine learning applications. With CityFlow, we can develop, deploy, and maintain machine learning applications easily by using an intuitive data flow model. To verify our approach, we conducted two case studies: deploying a road damage detection application to help monitor transport infrastructure and an automatic labeling application in support of a participatory sensing application. These applications show both the generic applicability of our approach, and its ease of use; both critical if we wish to deploy sophisticated ML based applications to smart cities.",smart cities,51,included
http://arxiv.org/abs/2004.06049v2,to_check,arxiv,arxiv,2020-04-09 00:00:00,arxiv,"a prospective look: key enabling technologies, applications and open
  research topics in 6g networks",http://arxiv.org/abs/2004.06049v2,"The fifth generation (5G) mobile networks are envisaged to enable a plethora
of breakthrough advancements in wireless technologies, providing support of a
diverse set of services over a single platform. While the deployment of 5G
systems is scaling up globally, it is time to look ahead for beyond 5G systems.
This is driven by the emerging societal trends, calling for fully automated
systems and intelligent services supported by extended reality and haptics
communications. To accommodate the stringent requirements of their prospective
applications, which are data-driven and defined by extremely low-latency,
ultra-reliable, fast and seamless wireless connectivity, research initiatives
are currently focusing on a progressive roadmap towards the sixth generation
(6G) networks. In this article, we shed light on some of the major enabling
technologies for 6G, which are expected to revolutionize the fundamental
architectures of cellular networks and provide multiple homogeneous artificial
intelligence-empowered services, including distributed communications, control,
computing, sensing, and energy, from its core to its end nodes. Particularly,
this paper aims to answer several 6G framework related questions: What are the
driving forces for the development of 6G? How will the enabling technologies of
6G differ from those in 5G? What kind of applications and interactions will
they support which would not be supported by 5G? We address these questions by
presenting a profound study of the 6G vision and outlining five of its
disruptive technologies, i.e., terahertz communications, programmable
metasurfaces, drone-based communications, backscatter communications and
tactile internet, as well as their potential applications. Then, by leveraging
the state-of-the-art literature surveyed for each technology, we discuss their
requirements, key challenges, and open research problems.",smart cities,52,not included
http://arxiv.org/abs/1404.1905v1,to_check,arxiv,arxiv,2014-04-04 00:00:00,arxiv,developing a 21st century global library for mathematics research,http://arxiv.org/abs/1404.1905v1,"Developing a 21st Century Global Library for Mathematics Research discusses
how information about what the mathematical literature contains can be
formalized and made easier to express, encode, and explore. Many of the tools
necessary to make this information system a reality will require much more than
indexing and will instead depend on community input paired with machine
learning, where mathematicians' expertise can fill the gaps of automatization.
This report proposes the establishment of an organization; the development of a
set of platforms, tools, and services; the deployment of an ongoing applied
research program to complement the development work; and the mobilization and
coordination of the mathematical community to take the first steps toward these
capabilities. The report recommends building on the extensive work done by many
dedicated individuals under the rubric of the World Digital Mathematical
Library, as well as many other community initiatives. Developing a 21st Century
Global Library for Mathematics envisions a combination of machine learning
methods and community-based editorial effort that makes a significantly greater
portion of the information and knowledge in the global mathematical corpus
available to researchers as linked open data through a central organizational
entity-referred to in the report as the Digital Mathematics Library. This
report describes how such a library might operate - discussing development and
research needs, role in facilitating discover and interaction, and establishing
partnerships with publishers.",smart cities,53,not included
10.1016/j.jnca.2020.102596,to_check,Journal of Network and Computer Applications,scopus,2020-06-01,sciencedirect,on the classification of fog computing applications: a machine learning perspective,https://api.elsevier.com/content/abstract/scopus_id/85082445495,"Currently, Internet applications running on mobile devices generate a massive amount of data that can be transmitted to a Cloud for processing. However, one fundamental limitation of a Cloud is the connectivity with end devices. Fog computing overcomes this limitation and supports the requirements of time-sensitive applications by distributing computation, communication, and storage services along the Cloud to Things (C2T) continuum, empowering potential new applications, such as smart cities, augmented reality (AR), and virtual reality (VR). However, the adoption of Fog-based computational resources and their integration with the Cloud introduces new challenges in resource management, which requires the implementation of new strategies to guarantee compliance with the quality of service (QoS) requirements of applications.
                  In this context, one major question is how to map the QoS requirements of applications on Fog and Cloud resources. One possible approach is to discriminate the applications arriving at the Fog into Classes of Service (CoS). This paper thus introduces a set of CoS for Fog applications which includes, the QoS requirements that best characterize these Fog applications. Moreover, this paper proposes the implementation of a typical machine learning classification methodology to discriminate Fog computing applications as a function of their QoS requirements. Furthermore, the application of this methodology is illustrated in the assessment of classifiers in terms of efficiency, accuracy, and robustness to noise. The adoption of a methodology for machine learning-based classification constitutes a first step towards the definition of QoS provisioning mechanisms in Fog computing. Moreover, classifying Fog computing applications can facilitate the decision-making process for Fog scheduler.",smart cities,54,not included
10.1016/j.procs.2019.09.007,to_check,Procedia Computer Science,scopus,2019-01-01,sciencedirect,increase the interest in learning by implementing augmented reality: case studies studying rail transportation.,https://api.elsevier.com/content/abstract/scopus_id/85073117730,"Learn a subject, for some people, might be an uninteresting and boring activity, especially when the subject to learn are difficult subjects to understand. Many methods used to change learning activities become more enjoyable and interested. This study proposed a new method in learning activities, by applied augmented reality technology in the learning process. The case study used in this paper are implementation the augmented reality in studied subjects related to train technology. In this study, author implement augmented reality on learning material, combines real and virtual things in one media, in this case a mobile device. The impact of implementation of augmented studied, at the end of experiment, author can conclude when implement augmented reality technology in learning material helps the learning process and increasing the impressive and fun factor in learning process and make the learning process more interested. Implementation of Augmented Reality in learning material gives more information about the object being studied, information about on shapes, textures, and provide more visualization for the object.",smart cities,55,not included
10.1109/bicits51482.2021.9509919,to_check,2021 1st Babylon International Conference on Information Technology and Science (BICITS),IEEE,2021-04-29 00:00:00,ieeexplore,optical impairment compensation in fiber communication systems based on artificial intelligence: a comprehensive survey,https://ieeexplore.ieee.org/document/9509919/,"The global demand for high-speed communication has increased dramatically over the past few years when data beginning to dominating of the traffic according to the Cisco Visual Networking Index (VNI). Data traffic is triple between 2014 and 2020, mainly, due to developing applications that consume bandwidth such as cloud services, HD video, high quality of real-time video transmission, virtual- augmented reality (VR-AR), online- games (video games), exchange of multimedia via smartphones and the more like. In fact, in 2020, more than a million minutes of multimedia (video) content is transiting the IP network every second according to the VNI; and the demands will exceed the capability of the current (core) internet backbone systems, in which optical communications are the main infrastructure. In this paper, the focus was on reviewing the mechanisms used for the most important and most effective techniques used to increase the capacity of optical transmission systems, namely Nonlinear Compensation (NLC) which work to reduce the nonlinear impairments that represent the main intrinsic challenges and the main capacity limitations facing the optical systems. The traditional NLC techniques were determined based on the approximate solution of the Nonlinear Schrodinger Equation (NLSE) through Digital Back Propagation (DBP), or Split- step Fourier Method (SSFM). however, their implementation demands excessive signal processing resources, and high-level accurate knowledge. A completely new approach that uses artificial intelligence (AI) algorithms to identify and solve these impairments has been studied in this paper. Traditional NLC techniques are reviewed in the first part to mitigation the nonlinearities and estimate the quality of transmission (QoT). Whereas in the second part, we review the uses of AI techniques that have been studied in applications related to monitoring performance, reduce nonlinearity, and quantify QoT. Finally, this paper presents a summary with a conclusion and outlook for development and challenges in optical fiber communication systems where AI is predictable to represent a hot major role in the near future.",multimedia,56,not included
10.1109/aivr46125.2019.00024,to_check,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),IEEE,2019-12-11 00:00:00,ieeexplore,a live storytelling virtual reality system with programmable cartoon-style emotion embodiment,https://ieeexplore.ieee.org/document/8942322/,"Virtual reality (VR) is a promising new medium for immersive storytelling. While previous research works on VR narrative have tried to engage audiences through nice scenes and interactivity, the emerging live streaming shows the role of a presenter, especially the conveyance of emotion, for promoting audience involvement and enjoyment. In this paper, to lower the requirement of emotion embodiment, we borrow experience from cartoon animation and comics, and propose a novel cartoon-style hybrid emotion embodiment model to increase a storyteller's presence during live performance, which contains an avatar with six basic emotions and auxiliary multimodal display to enhance emotion expressing. We further design and implement a system to teleoperate the embodiment model in VR for live storytelling. In particular, 1) we design a novel visual programming tool that allows users to customize emotional effects based on the emotion embodiment model; 2) we design a novel face tracking module to map presenters' emotional states to the avatar in VR. Our lightweight web-based implementation also makes the application very easy to use. We conduct two preliminary qualitative studies to explore the potential of the hybrid model and the storytelling system, including interviews with three experts and a workshop study with local secondary school students. Results show the potential of the VR storytelling system for education.",multimedia,57,not included
10.1109/icaie53562.2021.00156,to_check,2021 2nd International Conference on Artificial Intelligence and Education (ICAIE),IEEE,2021-06-20 00:00:00,ieeexplore,a review on the application of virtual reality technology in ideological and political teaching,https://ieeexplore.ieee.org/document/9534508/,"In recent years, the application of virtual reality technology in different fields has attracted the attention of society and academic circles. Some scholars connect virtual reality technology with ideological and political teaching, so as to further explore the new ideas of ideological and political teaching. This paper analyzes the meaning, characteristics and functions of virtual reality technology, and puts forward its current difficulties in ideological and political courses. At the same time, this paper also points out the implementation principles, approaches and strategies of virtual reality technology in the ideological and political curriculum teaching, in order to enrich the teaching methods in the ideological and political curriculum, and lay a research foundation for improving the educational quality of the ideological and political curriculum.",multimedia,58,not included
10.1109/ecai.2017.8166500,to_check,"2017 9th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",IEEE,2017-07-01 00:00:00,ieeexplore,system using a hybrid application for virtual reality 3d drawing,https://ieeexplore.ieee.org/document/8166500/,"Smart devices have many sensors that capture the user data and use it for device control, improving the user experience, or send it through a computer network in order to be processed. The interaction between user and applications, especially in virtual reality, is made via motion and gestures with information offered by sensors such as accelerometer, compass or gyroscope. In order to reduce the complexity of developing this type of applications, it is possible to write hybrid applications that allow the programmer to code using web technologies then deploy and run the resulted applications natively on multiple operating systems. This paper presents the implementation of a virtual reality drawing hybrid application, created with the Telerik Mobile platform, which reads the compass and accelerometer data and sends the information to a server that creates a 3D representation using the Processing Development Environment.",multimedia,59,not included
10.1109/vr.2019.8798186,to_check,2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),IEEE,2019-03-27 00:00:00,ieeexplore,virtual reality and photogrammetry for improved reproducibility of human-robot interaction studies,https://ieeexplore.ieee.org/document/8798186/,"Collecting data in robotics, especially human-robot interactions, traditionally requires a physical robot in a prepared environment, that presents substantial scalability challenges. First, robots provide many possible points of system failure, while the availability of human participants is limited. Second, for tasks such as language learning, it is important to create environments that provide interesting' varied use cases. Traditionally, this requires prepared physical spaces for each scenario being studied. Finally, the expense associated with acquiring robots and preparing spaces places serious limitations on the reproducible quality of experiments. We therefore propose a novel mechanism for using virtual reality to simulate robotic sensor data in a series of prepared scenarios. This allows for a reproducible dataset that other labs can recreate using commodity VR hardware. We demonstrate the effectiveness of this approach with an implementation that includes a simulated physical context, a reconstruction of a human actor, and a reconstruction of a robot. This evaluation shows that even a simple “sandbox” environment allows us to simulate robot sensor data, as well as the movement (e.g., view-port) and speech of humans interacting with the robot in a prescribed scenario.",multimedia,60,not included
10.1109/vrw50115.2020.00132,to_check,2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),IEEE,2020-03-26 00:00:00,ieeexplore,"[dc] quality, presence, and emotions in virtual reality communications",https://ieeexplore.ieee.org/document/9090552/,"This doctoral thesis looks for the identification and evaluation of the factors that allow to improve the QoE of a remote client in telepresence and virtual reality scenarios. Specifically, quality and socioemotional concepts such as social and spatial presence, empathy, and emotions of being in a completely different place, as well as communicate and interact with people who are in that place. The main goals of my research are the analysis of the methodologies to evaluate video quality and socioemotional concepts, the implementation of additional tools using ML techniques to improve the QoE, and finally, experiments in real use cases.",multimedia,61,not included
10.1109/access.2020.3022644,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,dynamic visual communication image framing of graphic design in a virtual reality environment,https://ieeexplore.ieee.org/document/9187835/,"This paper explores dynamic visual communication image framing for graphic design based on virtual reality algorithms; it defines corresponding feature representations by delineating layers of pixels, elements, relationships, planes, and applications; and it investigates methods for quantifying geometric features, perceptual features, and style features. The contents include extraction methods for element colors, calculation methods for layout perceptual features and color-matching perceptual features, and pairwise comparison methods for style features. By overfitting the distribution of geometric features in the data, the model can predict the probability density distribution of features such as element position and color under specific conditions to support the generation of flat images. To construct a prediction model, the sampling method of features, the model optimization method, and the data learning strategy are investigated. This thesis involves the design and implementation of a lossless/near-lossless compression system for high-frame-rate gaze camera image data, which is faced with the technical problems of high fidelity and strong real-time and reliable compression. The image single-frame lossless/near-loss-free compression ratio is generally low, and the compression ratio can be improved by using the correlation between image frames. In this paper, we study the application of lossless compression between image frames, the efficient computing structure of FPGA, and an onboard compression system.",multimedia,62,not included
10.1093/iwc/iwv010,to_check,Interacting with Computers,OUP,2015-09-01 00:00:00,ieeexplore,neuro-fuzzy physiological computing to assess stress levels in virtual reality therapy,https://ieeexplore.ieee.org/document/8155481/,"This paper reports the design and assessment of a neuro-fuzzy model to support clinicians during virtual reality therapy. The implemented model is able to automatically recognize the perceived stress levels of the patients by analyzing physiological and behavioral data during treatment. The model, consisting of a self-organizing map and a fuzzy-rule-based module, was trained unobtrusively recording electrocardiogram, breath rate and activity during stress inoculation provided by the exposure to virtual environments. Twenty nurses were exposed to sessions simulating typical stressful situations experienced at their workplace. Four levels of stress severity were evaluated for each subject by gold standard clinical scales administered by trained personnel. The model's performances were discussed and compared with the main machine learning algorithms. The neuro-fuzzy model shows better performances in terms of stress level classification with 83% of mean recognition rate.RESEARCH HIGHLIGHTS Stress levels were predicted on the basis of physiological computing using a neuro-fuzzy model during virtual reality therapy. Features were extracted from ECG and respiration obtaining high accuracy and optimization of computational costs. The neuro-fuzzy model shows better performance than the more frequently adopted classifiers. This approach may enhance the use of physiological computing for stress treatment in clinical practice.",multimedia,63,not included
10.1109/ccwc.2019.8666471,to_check,2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC),IEEE,2019-01-09 00:00:00,ieeexplore,"a uav with autonomy, pattern recognition for forest fire prevention, and ai for providing advice to firefighters fighting forest fires",https://ieeexplore.ieee.org/document/8666471/,"The design of a long endurance UAV powered by solar energy, with autonomy flying over a pre specified forest area equipped with LiDAR which includes R, G, B and Infrared or near infrared bands to take clear and detail video of every part of the forest in order to recognize legal or illegal camp fires, dry areas providing hazardous conditions, upload temperature, humidity and other ground sensor data, helping to characterize the degree of forest fire vulnerability using pattern recognition, and communicating this information to firefighters either upon request, or as alarm events. The airplane electronic hardware software uses computational photography and virtual reality to create a detail 3-D forest video real time. It communicates all this information to a ground station real time. Although the aircraft has autonomy, trained pilots in the ground station can override the autonomy and fly the aircraft. In case of forest fire the aircraft electronic hardware software system can compute the exact area affected, the fire velocity and speed, the wind direction, and provide advice to the firefighters regarding the optimal way of fighting the fire. Often time forest fires destroy the communication infrastructure, so the airplane has a router to enable firefighters to exchange, text and voice information.",multimedia,64,not included
10.1109/ssst.1994.287869,to_check,Proceedings of 26th Southeastern Symposium on System Theory,IEEE,1994-03-22 00:00:00,ieeexplore,a knowledge-based software reuse environment for program development,https://ieeexplore.ieee.org/document/287869/,"The knowledge-based software reuse environment (KBSRE) for program development assists the user to familiarize himself with the domain application environment, to locate partially matched components from the reusable component library, to understand the life-cycle knowledge of a component, and to decompose a component when its subcomponents are available. The knowledge base was created by the frame based component representation method and a set of decomposition rules and sibling rules. The components are used to generate the new software system by component composition methods. This KBSRE is an open system which allows the implementor to modify the knowledge base, decomposition rules, and sibling rules with minimum efforts. Development of a virtual reality application is shown as an example.&lt;<ETX>&gt;</ETX>",multimedia,65,not included
10.1109/isscc.2019.8662397,to_check,2019 IEEE International Solid- State Circuits Conference - (ISSCC),IEEE,2019-02-21 00:00:00,ieeexplore,an 879gops 243mw 80fps vga fully visual cnn-slam processor for wide-range autonomous exploration,https://ieeexplore.ieee.org/document/8662397/,"Simultaneous localization and mapping (SLAM) estimates an agent's trajectory for all six degrees of freedom (6 DoF) and constructs a 3D map of an unknown surrounding. It is a fundamental kernel that enables head-mounted augmented/virtual reality devices and autonomous navigation of micro aerial vehicles. A noticeable recent trend in visual SLAM is to apply computationand memory-intensive convolutional neural networks (CNNs) that outperform traditional hand-designed feature-based methods [1]. For each video frame, CNN-extracted features are matched with stored keypoints to estimate the agent's 6-DoF pose by solving a perspective-n-points (PnP) non-linear optimization problem (Fig. 7.3.1, left). The agent's long-term trajectory over multiple frames is refined by a bundle adjustment process (BA, Fig. 7.3.1 right), which involves a large-scale (~120 variables) non-linear optimization. Visual SLAM requires massive computation (&gt;250GOP/s) in the CNN-based feature extraction and matching, as well as datadependent dynamic memory access and control flow with high-precision operations, creating significant low-power design challenges. Software implementations are impractical, resulting in 0.2s runtime with a ~3GHz CPU+ GPU system with &gt;100MB memory footprint and &gt;100W power consumption. Prior ASICs have implemented either an incomplete SLAM system [2,3] that lacks estimation of ego-motion or employed a simplified (non-CNN) feature extraction and tracking [2,4,5] that limits SLAM quality and range. A recent ASIC [5] augments visual SLAM with an off-chip high-precision inertial measurement unit (IMU), simplifying the computational complexity, but incurring additional power and cost overhead.",multimedia,66,not included
10.1109/robot.2000.844768,to_check,Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),IEEE,2000-04-28 00:00:00,ieeexplore,application of automatic action planning for several work cells to the german ets-vii space robotics experiments,https://ieeexplore.ieee.org/document/844768/,"Experiences in space robotics show, that the user normally has to cope with a huge amount of data. So, only robot and mission specialists are able to control the robot arm directly in teleoperation mode. By means of an intelligent robot control in cooperation with virtual reality methods, it is possible for non-robot specialists to generate tasks for a robot or an automation component intuitively. Furthermore, the intelligent robot control improves the safety of the entire system. The on-ground robot control and command station for the robot arm ERA onboard the satellite ETS-VII builds on a new resource-based action planning approach to manage robot manipulators and other automation components. In the case of ERA, the action planning system also takes care of the ""real"" robot onboard the satellite and the ""virtual"" robot in the simulation system. By means of the simulation system, the user can plan tasks ahead as well as analyze and visualize different strategies. The paper describes the mechanism of resource-based action planning, its application to different work cells, the practical experiences gained from the implementation for the on-ground robot control and command station for the robot arm ERA developed in the GETEX project as well as the services it provides to support VR-based man machine interfaces.",multimedia,67,included
10.1109/aivr50618.2020.00083,to_check,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),IEEE,2020-12-18 00:00:00,ieeexplore,eye tracking data collection protocol for vr for remotely located subjects using blockchain and smart contracts,https://ieeexplore.ieee.org/document/9319118/,"Eye tracking data collection in the virtual reality context is typically carried out in laboratory settings, which usually limits the number of participants or consumes at least several months of research time. In addition, under laboratory settings, subjects may not behave naturally due to being recorded in an uncomfortable environment. In this work, we propose a proof-of-concept eye tracking data collection protocol and its implementation to collect eye tracking data from remotely located subjects, particularly for virtual reality using Ethereum blockchain and smart contracts. With the proposed protocol, data collectors can collect high quality eye tracking data from a large number of human subjects with heterogeneous socio-demographic characteristics. The quality and the amount of data can be helpful for various tasks in datadriven human-computer interaction and artificial intelligence.",multimedia,68,not included
10.1109/hpca51647.2021.00016,to_check,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),IEEE,2021-03-03 00:00:00,ieeexplore,heterogeneous dataflow accelerators for multi-dnn workloads,https://ieeexplore.ieee.org/document/9407116/,"Emerging AI-enabled applications such as augmented and virtual reality (AR/VR) leverage multiple deep neural network (DNN) models for various sub-tasks such as object detection, image segmentation, eye-tracking, speech recognition, and so on. Because of the diversity of the sub-tasks, the layers within and across the DNN models are highly heterogeneous in operation and shape. Diverse layer operations and shapes are major challenges for a fixed dataflow accelerator (FDA) that employs a fixed dataflow strategy on a single DNN accelerator substrate since each layer prefers different dataflows (computation order and parallelization) and tile sizes. Reconfigurable DNN accelerators (RDAs) have been proposed to adapt their dataflows to diverse layers to address the challenge. However, the dataflow flexibility in RDAs is enabled at the cost of expensive hardware structures (switches, interconnects, controller, etc.) and requires per-layer reconfiguration, which introduces considerable energy costs. Alternatively, this work proposes a new class of accelerators, heterogeneous dataflow accelerators (HDAs), which deploy multiple accelerator substrates (i.e., sub-accelerators), each supporting a different dataflow. HDAs enable coarser-grained dataflow flexibility than RDAs with higher energy efficiency and lower area cost comparable to FDAs. To exploit such benefits, hardware resource partitioning across sub-accelerators and layer execution schedule need to be carefully optimized. Therefore, we also present Herald, a framework for co-optimizing hardware partitioning and layer scheduling. Using Herald on a suite of AR/VR and MLPerf workloads, we identify a promising HDA architecture, Maelstrom, which demonstrates 65.3% lower latency and 5.0% lower energy compared to the best fixed dataflow accelerators and 22.0% lower energy at the cost of 20.7% higher latency compared to a state-of-the-art reconfigurable DNN accelerator (RDA). The results suggest that HDA is an alternative class of Pareto-optimal accelerators to RDA with strength in energy, which can be a better choice than RDAs depending on the use cases.",multimedia,69,included
10.1109/aivr46125.2019.00057,to_check,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),IEEE,2019-12-11 00:00:00,ieeexplore,live emoji: a live storytelling vr system with programmable cartoon-style emotion embodiment,https://ieeexplore.ieee.org/document/8942384/,"We introduce a novel cartoon-style hybrid emotion embodiment model for live storytelling in virtual reality (VR). It contains an avatar with six basic emotions and an auxiliary multimodal display to enhance the expression of emotions. We further design and implement a system to teleoperate the embodiment model in VR for live storytelling. Specifically, 1) we design a novel visual programming tool that allows users to customize emotional effects based on the emotion embodiment model; 2) we design a novel face tracking module to map presenters' emotional states to the avatar in VR. Our web-based implementation makes the application easy to use. This is an accompanying paper extracted from [1] for the demo session in IEEE AIVR 2019.",multimedia,70,not included
10.1109/ismar50242.2020.00033,to_check,2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR),IEEE,2020-11-13 00:00:00,ieeexplore,optical gaze tracking with spatially-sparse single-pixel detectors,https://ieeexplore.ieee.org/document/9284794/,"Gaze tracking is an essential component of next generation displays for virtual reality and augmented reality applications. Traditional camera-based gaze trackers used in next generation displays are known to be lacking in one or multiple of the following metrics: power consumption, cost, computational complexity, estimation accuracy, latency, and form-factor. We propose the use of discrete photodiodes and light-emitting diodes (LEDs) as an alternative to traditional camera-based gaze tracking approaches while taking all of these metrics into consideration. We begin by developing a rendering-based simulation framework for understanding the relationship between light sources and a virtual model eyeball. Findings from this framework are used for the placement of LEDs and photodiodes. Our first prototype uses a neural network to obtain an average error rate of 2.67° at 400 Hz while demanding only 16 mW. By simplifying the implementation to using only LEDs, duplexed as light transceivers, and more minimal machine learning model, namely a light-weight supervised Gaussian process regression algorithm, we show that our second prototype is capable of an average error rate of 1.57° at 250 Hz using 800 mW.",multimedia,71,not included
10.1109/aivr50618.2020.00022,to_check,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),IEEE,2020-12-18 00:00:00,ieeexplore,photorealistic avatars to enhance the efficacy of selfattachment psychotherapy,https://ieeexplore.ieee.org/document/9319121/,"We have designed, developed, and tested an Immersive virtual reality (VR) platform to practice the protocols of Self-attachment psychotherapy. We made use of customized photorealistic avatars for the implementation of both the high-end version (based on Facebook's Oculus) and the low-end version (based on Google's cardboard) of our platform. Under the Selfattachment therapeutic framework, the causes of mental disorders such as chronic anxiety and depression are traced back to the individual's insecure attachment with their primary caregiver during childhood and their subsequent problems in affect regulation. The conventional approach (without VR) to Selfattachment requires that the individual uses their childhood photographs to recall their childhood memories and then imagine that the child that they were is present with them. They thus establish a compassionate relationship with their childhood self and then, using love songs and dancing, create an affectional bond with them. Their adult self subsequently role plays a good parent and interacts with their imagined childhood self to perform various developmental and re-parenting activities. The goal is to enhance their capacities for self-regulation of emotion, which can lead them into earning secure attachment. It is hypothesized that our immersive virtual reality platform - which enables the users to interact with their customized 3D photorealistic childhood avatar - offers either a better alternative or at least a complementary visual tool to the conventional imaginal approach to Self-attachment. The platform was developed in Unity 3D, a cross-platform game engine, and takes advantage of the itSeez3D Avatar SDK for generating a customized photorealistic 3D avatar head from a 2D childhood image of the user. The platform also offers facial and body animations for some of the basic emotional states such as Happy, Sad, Scared and Joyful and it allows modifications to the avatar body (height/ width) and clothing color. A study to compare the use of the avatar-based approach (VR) to Self-attachment with the conventional photo-based approach showed promising results. Almost 85% of the participants reported that their photorealistic childhood avatar in VR was more relatable than their childhood photos. Both low-end and high-end VR based approaches were unanimously reported to be more effective than the conventional imaginal approach. Participants reported that the high-end version of the VR platform was more realistic and immersive than the low-end mobile VR version.",multimedia,72,not included
10.1109/icaie53562.2021.00145,to_check,2021 2nd International Conference on Artificial Intelligence and Education (ICAIE),IEEE,2021-06-20 00:00:00,ieeexplore,research on improving students’ interest in learning based on citespace.5.7.r2,https://ieeexplore.ieee.org/document/9534593/,"To comprehensively understand the research situation and research trend in the field of ""learning interest"", this paper uses CiteSpace.5.7.R2 information visualization software as a research tool, selects 2400 literature data from the Web of Science core database from 1975 to 2021 and 984 literature data from the CNKI database from 1900 to 2021 for visual analysis, respectively from the aspects of national cooperation, institutional cooperation, keyword co-occurrence, co-citation network, etc. The results show that the current international research hotspot is the interest development model, the application of virtual reality technology, and so forth. The research hotspot in China is teaching reform. Finally, the paper puts forward the prospect of the research in the field of ""learning interest"" in China: strengthening the cooperation among colleges and universities; using science and technology in teaching, such as virtual reality, to improve students’ interest in learning.",multimedia,73,not included
10.1109/cecnet.2012.6202239,to_check,"2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet)",IEEE,2012-04-23 00:00:00,ieeexplore,research on the safety awareness training software for commercial vehicles drivers,https://ieeexplore.ieee.org/document/6202239/,"The feasibility for safety awareness training on commercial vehicle drivers with the application of automotive driving simulator was analyzed in this study, the virtual reality based safety awareness training methodology was also proposed. The reference and specific contents for training courses design was firstly illustrated. The explanation of main architecture planning and implementation methods was two important parts in this study. The implementation effect of this software was then presented. The results of feedbacks from the users who were in the practical application of the system indicate that this software system can improve the safety awareness in driving. The prospect for future research and application direction were finally discussed.",multimedia,74,not included
10.1109/vr.2004.1310053,to_check,IEEE Virtual Reality 2004,IEEE,2004-03-31 00:00:00,ieeexplore,resolving object references in multimodal dialogues for immersive virtual environments,https://ieeexplore.ieee.org/document/1310053/,"This paper describes the underlying concepts and the technical implementation of a system for resolving multi-modal references in virtual reality (VR). In this system the temporal and semantic relations intrinsic to referential utterances are expressed as a constraint satisfaction problem, where the propositional value of each referential unit during a multimodal dialogue updates incrementally the active set of constraints. As the system is based on findings of human cognition research it also regards, e.g., constraints implicitly assumed by human communicators. The implementation takes VR related real-time and immersive conditions into account and adapts its architecture to well known scene-graph based design patterns by introducing a so-called reference resolution engine. Regarding the conceptual work as well as regarding the implementation, special care has been taken to allow further refinements and modifications to the underlying resolving processes on a high level basis.",multimedia,75,not included
10.1109/icra.2013.6631400,to_check,2013 IEEE International Conference on Robotics and Automation,IEEE,2013-05-10 00:00:00,ieeexplore,robust real-time visual odometry for dense rgb-d mapping,https://ieeexplore.ieee.org/document/6631400/,"This paper describes extensions to the Kintinuous [1] algorithm for spatially extended KinectFusion, incorporating the following additions: (i) the integration of multiple 6DOF camera odometry estimation methods for robust tracking; (ii) a novel GPU-based implementation of an existing dense RGB-D visual odometry algorithm; (iii) advanced fused realtime surface coloring. These extensions are validated with extensive experimental results, both quantitative and qualitative, demonstrating the ability to build dense fully colored models of spatially extended environments for robotics and virtual reality applications while remaining robust against scenes with challenging sets of geometric and visual features.",multimedia,76,not included
10.1109/icrom.2015.7367807,to_check,2015 3rd RSI International Conference on Robotics and Mechatronics (ICROM),IEEE,2015-10-09 00:00:00,ieeexplore,virtual arm in simulink environment for reaching movement simulation in the transverse plane,https://ieeexplore.ieee.org/document/7367807/,"Different pieces of software can be used to simulate virtual movements, such as SIMM-SD/Fast or LifeModAdams. Complexities of using multiple pieces of software and lack of enough knowledge of their concepts to use them professionally has made it inevitable for most researchers to use common and helpful software, MATLAB/SIMULINK. In recent years the use of virtual reality in reaching movement in order to assess the performance and improve the efficiency of models has been of interest to researchers. Because simulation results are presented in an environment very close to real-world. This paper deal with implementation of a virtual arm model in reaching movement in MATLAB/SIMULINK toolbox. The skeletal part of the virtual arm model consists of two joints and two links on the transverse plane. The implemented-muscle model is based on Zajac muscle-tendon model where Static and dynamic characteristics muscles and tendons are justified. Various MATLAB toolboxes such as neural networks, fuzzy, optimization, signal processing and system identification have made it possible to analyze the virtual arm model in these sectors.",multimedia,77,not included
10.1109/cbs.2018.8612261,to_check,2018 IEEE International Conference on Cyborg and Bionic Systems (CBS),IEEE,2018-10-27 00:00:00,ieeexplore,semg-based torque estimation using time-delay ann for control of an upper-limb rehabilitation robot,https://ieeexplore.ieee.org/document/8612261/,"Robotic-assisted rehabilitation of the upper limb following neurological injury can achieve best possible functional recovery when patients are engaged in the therapy. However, implementation of active training is still difficult as it's challenging to detect human motion intention online and impose corresponding robot control. This paper introduces a novel upper-limb rehabilitation robot, and proposes a sEMG-driven (sEMG: surface Electromyography) torque estimation model based on artificial neural networks (ANN). The robot has three DOFs, of which the first two DOFs adopt a planar parallel structure, and the wrist module has an exoskeleton form. In this study, we design an impedance controller and an admittance controller for the first two DOFs and the wrist module, respectively. Specifically, for the first two DOFs, the assistance/resistance force at the end-effector was controlled according to its motions and desired interaction impedance; for the wrist module, an sEMG armband was used to collect 8 channels of sEMG signals from the forearm muscles, and a time-delay ANN model was developed to estimate the wrist pronation/supination torque, based on which the wrist rotation was controlled according to the human motion intention. To overcome the overfitting problem, besides the experimental samples of wrist rotation, both resting and co-contraction samples were collected for training. Finally, combining with the design of a virtual reality game and force fields, the proposed methods were implemented and tested experimentally on the upper-limb rehabilitation robot.",multimedia,78,included
10.1109/aivr.2018.00018,to_check,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),IEEE,2018-12-12 00:00:00,ieeexplore,a compensation method of two-stage image generation for human-ai collaborated in-situ fashion design in augmented reality environment,https://ieeexplore.ieee.org/document/8613637/,"In this paper, we consider a human-AI collaboration task, fashion design, in augmented reality environment. In particular, we propose a compensation method of two-stage image generation neural network for generating fashion design with progressive users' inputs. Our work is based on a recent proposed deep learning model, pix2pix, that can successfully transform an image from one domain into another domain, such as from line drawings to color images. However, the pix2pix model relies on the condition that input images should come from the same distribution, which is usually hard for applying it to real human computer interaction tasks, where the input from users differs from individual to individual. To address the problem, we propose a compensation method of two-stage image generation. In the first stage, we ask users to indicate their design preference with an easy task, such as tuning clothing landmarks, and use the input to generate a compensation input. With the compensation input, in the second stage, we then concatenate it with the real sketch from users to generate a perceptual better result. In addition, to deploy the two-stage image generation neural network in augmented reality environment, we designed and implemented a mobile application where users can create fashion design referring to real world human models. With the augmented 2D screen and instant feedback from our system, users can design clothing by seamlessly mixing the real and virtual environment. Through an online experiment with 46 participants and an offline use case study, we showcase the capability and usability of our system. Finally, we discuss the limitations of our system and further works on human-AI collaborated design.",multimedia,79,included
10.1109/icmla51294.2020.00193,to_check,2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA),IEEE,2020-12-17 00:00:00,ieeexplore,an embedded deep learning system for augmented reality in firefighting applications,https://ieeexplore.ieee.org/document/9356175/,"Firefighting is a dynamic activity, in which numerous operations occur simultaneously. Maintaining situational awareness (i.e., knowledge of current conditions and activities at the scene) is critical to the accurate decision-making necessary for the safe and successful navigation of a fire environment by firefighters. Conversely, the disorientation caused by hazards such as smoke and extreme heat can lead to injury or even fatality. This research implements recent advancements in technology such as deep learning, point cloud and thermal imaging, and augmented reality platforms to improve a firefighter's situational awareness and scene navigation through improved interpretation of that scene. We have designed and built a prototype embedded system that can leverage data streamed from cameras built into a firefighter's personal protective equipment (PPE) to capture thermal, RGB color, and depth imagery and then deploy already developed deep learning models to analyze the input data in real time. The embedded system analyzes and returns the processed images via wireless streaming, where they can be viewed remotely and relayed back to the firefighter using an augmented reality platform that visualizes the results of the analyzed inputs and draws the firefighter's attention to objects of interest, such as doors and windows otherwise invisible through smoke and flames.",multimedia,80,included
10.1109/iscaie.2018.8405457,to_check,2018 IEEE Symposium on Computer Applications & Industrial Electronics (ISCAIE),IEEE,2018-04-29 00:00:00,ieeexplore,augmented reality enhanced computer aided learning for young children,https://ieeexplore.ieee.org/document/8405457/,"Learning to write can be exhausting for young children. In Traditional teaching, children with a different learning abilities are taught with the same rubric. This, in turn, impacts children that need extra attention to catch up with their pairs, which leads children to suffer right from the early learning stages. Traditional teaching methods also are so rigid that makes them unable to automatically identify those children with less abilities and in need of extra work. Hence, with the rapid development of ICT, an innovative learning methods are sought to be important to allow children to be taught with different rubrics. The aim of this research is to improve learning process for pre-school children via introducing Augmented Reality (AR) in the process which, in turn, simplify the learning process as well as identifying children abilities. The research introduces gamification to the process in order to ease the burden on children. Furthermore, we are trying to involve both school as well home to be part of the educational cycle that makes parents to be part of the learning/educational process of their young children. Augmented reality combined with pleasing sound make the learning more interactive and enjoyable. The outcome of this research also helps parents to keep track of their children's learning. The paper also describes the deployment of the application in a local schools as a pilot study so teachers can get feedback on student's learning curve and to fine tune the work further.",multimedia,81,not included
10.1109/iccsai53272.2021.9609795,to_check,2021 1st International Conference on Computer Science and Artificial Intelligence (ICCSAI),IEEE,2021-10-28 00:00:00,ieeexplore,rtr ar photo booth: the real-time rendering augmented reality photo booth,https://ieeexplore.ieee.org/document/9609795/,"The use of Photo Booth at several events has become a means of documentation such as selfies and wefies, However, in some applications, the technology has not been utilized properly. This study aims to describe how the application of Augmented Reality technology to be applied to the form of photography services in the form of a photo booth with real-time rendering techniques from Spark AR and the use of the cloud. We propose a creation schema to implement augmented reality in photobooths for marketing purposes in exhibitions or events. The implementation description will include how augmented reality technology is used, development methods, and references on how to use it until it is ready for use. The performance measurement was also carried out using Frame Rate Per Second (FPS) on two different device configurations in several experiments. The results show that the proposed photobooth can run up to more than 60 FPS or at above standard performance.",multimedia,82,not included
10.1109/educon46332.2021.9454149,to_check,2021 IEEE Global Engineering Education Conference (EDUCON),IEEE,2021-04-23 00:00:00,ieeexplore,using augmented reality in programming learning: a systematic mapping study,https://ieeexplore.ieee.org/document/9454149/,"Coding skills have become the new language of communication for the tech world. At an educational level, applying the concepts and logic of programming is a complex task for the student. The investigation of this problem was carried out with the intention of knowing if there are tools that could help the student understand programming by using augmented reality technology. For this purpose, a systematic mapping study was carried out to identify, filter and classify the information through a query applied in different ways of research. As a result, 34 articles were selected and classified. The main results show that: a) programming learning is not limited in terms of the student's age; b) Augmented reality has potential advantages in programming learning; c) Due to the extensive content of the programming, applications focused on specific topics were found according to the level of studies; d) The software used in the development of AR applications, mostly uses Unity with Vuforia; and, e) Augmented reality contributes to different learning techniques and styles that improve the way information is perceived and visualized. In conclusion, augmented reality technology has proven to have positive consequences in the programming learning process, providing a starting point for the development of a tool that contributes to the programming learning based on the characteristics found and analyzed in this document.",multimedia,83,not included
10.23919/mipro.2019.8756928,to_check,"2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",IEEE,2019-05-24 00:00:00,ieeexplore,utilizing apple’s arkit 2.0 for augmented reality application development,https://ieeexplore.ieee.org/document/8756928/,"When it comes to practical augmented reality applications, mobile platform tools are the most deserving. Thanks to the nature of mobile devices and their everyday usage, the ideal basis for this kind of content has inadvertently formed itself. Consequently, within the iOS development environment, Apple's Xcode program enables application development using the ARKit library which delivers a host of benefits. Amongst the plethora of advantages, this paper focuses on utilizing features such as the ability to measure distances between two points in space, horizontal and vertical plane detection, the ability to detect three-dimensional objects and utilize them as triggers, and the consolidated implementation of ARKit and MapKit libraries in conjunction with the Google Places API intended for displaying superimposed computer-generated content on iOS 11 and later iterations of Apple's mobile operating system.",multimedia,84,not included
10.1109/compsac51774.2021.00171,to_check,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE,2021-07-16 00:00:00,ieeexplore,a model to helping the construction of creative service-based software,https://ieeexplore.ieee.org/document/9529361/,"With the advent of the Service Oriented Architecture (SOA) in system design, various domain knowledges are included in a service-based application, such as the design of Artificial Intelligence (AI) or augmented reality (AR) systems. While merging one or multiple domains into computation systems, the computation systems can be widely applied in various domain usages with novelty, useful, and surprising properties, which are defined as systems of creative computing. In creative computing, several theoretical evaluation metrics and verification approaches have been proposed for system design in several domains. However, a solid practical design environment for creative service-based systems is rarely considered in current researches. In this paper, we propose a model for creative service software development based on semantic web, which is applied in two phases: (1) requirement specification and (2) service design. In order to bridge the knowledge gap between domain experts and software engineers, and provide a machine-readable format for creative computing, two sub-models, Requirement Specification and Service Structure Models, are constructed in both phases, sequentially. After the latter sub-model is validated, the creative service software is well-constructed based on the services definition and composition represented by the model.",multimedia,85,not included
10.1109/miltechs.2017.7988855,to_check,2017 International Conference on Military Technologies (ICMT),IEEE,2017-06-02 00:00:00,ieeexplore,challenges in the implementation of autonomous systems into the battlefield,https://ieeexplore.ieee.org/document/7988855/,"The era of a system with some level of autonomous capacity is here for long time, however the deployment of fully autonomous system in military domain is still challenging. The main limitation in the AS operationalization are mentioned and proposals to these challenges are discussed. The ontology of the AS operationalization is elaborated and associations among the main terms is enlisted. The modeling and simulation experimental framework is designed to create a testing and verification environment for AS deployment. The key factor is to include standards of the robotic family and modeling and simulation as well. The standardized scenarios approach for AS deployment is mentioned. The perception of the AS is discussed and the idea of the augmented reality for autonomous system is introduced and related to the human behavior.",multimedia,86,not included
10.1109/fpl.2011.94,to_check,2011 21st International Conference on Field Programmable Logic and Applications,IEEE,2011-09-07 00:00:00,ieeexplore,pattern compression of fast corner detection for efficient hardware implementation,https://ieeexplore.ieee.org/document/6044867/,"This paper shows stream-oriented FPGA implementation of the machine-learned Features from Accelerated Segment Test (FAST) corner detection, which is used in the parallel tracking and mapping (PTAM) for augmented reality (AR). One of the difficulties of compact hardware implementation of the FAST corner detection is a matching process with a large number of corner patterns. We propose corner pattern compression methods focusing on discriminant division and pattern symmetry for rotation and inversion. This pattern compression enables implementation of the corner pattern matching with a combinational circuit. Our prototype implementation achieves real-time execution performance with 7-9% of available slices of a Virtex-5 FPGA.",multimedia,87,not included
10.1109/ccece.2012.6335012,to_check,2012 25th IEEE Canadian Conference on Electrical and Computer Engineering (CCECE),IEEE,2012-05-02 00:00:00,ieeexplore,"realtime hdr (high dynamic range) video for eyetap wearable computers, fpga-based seeing aids, and glasseyes (eyetaps)",https://ieeexplore.ieee.org/document/6335012/,"Realtime video HDR (High Dynamic Range) is presented in the context of a seeing aid designed originally for task-specific use (e.g. electric arc welding). It can also be built into regular eyeglasses to help people see better in everyday life. Our prototype consists of an EyeTap (electric glasses) welding helmet, with a wearable computer upon which are implemented a set of image processing algorithms that implement realtime HDR (High Dynamic Range) image processing together with applications such as mediated reality, augmediatedTM, and augmented reality. The HDR video system runs in realtime and processes 120 frames per second, in groups of three frames or four frames (e.g. a set of four differently exposed images captured every thirtieth of a second). The processing method, for implementation on FPGAs (Field Programmable Gate Arrays), achieves a realtime performance for creating HDR video using our novel compositing methods, and runs on a miniature self-contained battery-operated head-worn circuit board, without the need for a host computer. The result is an essentially self-contained miniaturizable hardware HDR camera system that could be built into smaller eyeglass frames, for use in various wearable computing and mediated/ aug-mediated reality applications, as well as to help people see better in their everyday lives.",multimedia,88,not included
10.1109/icspis.2017.8311599,to_check,2017 3rd Iranian Conference on Intelligent Systems and Signal Processing (ICSPIS),IEEE,2017-12-21 00:00:00,ieeexplore,3ds max to fem for building thermal distribution: a case study,https://ieeexplore.ieee.org/document/8311599/,"The complication of building constructions, with irregular geometry, different building materials, variable morphology, alterations and damages, poses numerous challenges in the digital modeling and simulation of structural performances under different types of actions. Most of the research is focused on importing Three Dimension (3D) geometry data in a Finite Element Method (FEM) applications. This paper presents an innovative two-step methodology (3ds MAX-to-FEM) able to convert a Three Dimension Studio Modeling, Animation &amp; Rendering Software/Autodesk (3ds MAX) file into a FEM for structural simulation. In this study, the 3ds MAX file is a large building, has been carried out with an accurate survey that integrates geometrical aspects, element interconnections, and architectural considerations. Then it is turned into COMSOL Multiphysics environment and tested thermal simulation with a geometric rationalization which preserves irregularities and anomalies, such as verticality deviation and variable thickness. After setting material properties, loads, and boundary conditions, the structural simulation is run with a detailed model that respects the uniqueness and authenticity of the building. A real case study is illustrated and discussed to prove that a rigorous 3ds MAX to FEM workflow allows the generation of an accurate practical methodology for 3D visualization and simulation for thermal distribution operation in COMSOL. Structural simulation was carried out with a 3D mesh derived from the 3ds MAX file in order to take into consideration the geometrical irregularity of a building. COMSOL Multiphysics is a software tool uses artificial intelligent and soft computing for doing computations with high speed and accuracy, and computes and shows different types of phenomenon together. In this study simulation results compare with the reality results. Here, the advantages and disadvantages of the proposed approach are illustrated.",multimedia,89,not included
10.1109/icra40945.2020.9197465,to_check,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,deepracer: autonomous racing platform for experimentation with sim2real reinforcement learning,https://ieeexplore.ieee.org/document/9197465/,"DeepRacer is a platform for end-to-end experimentation with RL and can be used to systematically investigate the key challenges in developing intelligent control systems. Using the platform, we demonstrate how a 1/18th scale car can learn to drive autonomously using RL with a monocular camera. It is trained in simulation with no additional tuning in the physical world and demonstrates: 1) formulation and solution of a robust reinforcement learning algorithm, 2) narrowing the reality gap through joint perception and dynamics, 3) distributed on-demand compute architecture for training optimal policies, and 4) a robust evaluation method to identify when to stop training. It is the first successful large-scale deployment of deep reinforcement learning on a robotic control agent that uses only raw camera images as observations and a model-free learning method to perform robust path planning. We open source our code and video demo on GitHub<sup>2</sup>.",multimedia,90,included
10.1109/snpd.2012.99,to_check,"2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",IEEE,2012-08-10 00:00:00,ieeexplore,evaluation of realism of dynamic sound space using a virtual auditory display,https://ieeexplore.ieee.org/document/6299338/,"We can perceive a sound position from binaural signals using mainly head-related transfer functions (HRTFs). Using the theorem presented herein, we can display a sound image to a specific position in virtual auditory space by HRTFs. However, HRTF is defined commonly in a free-field, and a virtual sound image is perceived as a dry source without reflection, reverberation, or ambient noise. Therefore, the virtual sound space might be unnatural. The authors developed a software-based virtual auditory display (VAD) that outputs audio signals for a set of headphones with a three-dimensional position sensor. The VAD software can display a dynamic virtual auditory space that is responsive to a listener's head movement. Subjective evaluations were conducted to clarify the relation between the perceived reality of virtual sound space and ambient sound. Evaluation results of the reality of the virtual sound space displayed by the VAD software are introduced.",multimedia,91,not included
http://arxiv.org/abs/2009.10679v1,to_check,arxiv,arxiv,2020-09-22 00:00:00,arxiv,"an embedded deep learning system for augmented reality in firefighting
  applications",http://arxiv.org/abs/2009.10679v1,"Firefighting is a dynamic activity, in which numerous operations occur
simultaneously. Maintaining situational awareness (i.e., knowledge of current
conditions and activities at the scene) is critical to the accurate
decision-making necessary for the safe and successful navigation of a fire
environment by firefighters. Conversely, the disorientation caused by hazards
such as smoke and extreme heat can lead to injury or even fatality. This
research implements recent advancements in technology such as deep learning,
point cloud and thermal imaging, and augmented reality platforms to improve a
firefighter's situational awareness and scene navigation through improved
interpretation of that scene. We have designed and built a prototype embedded
system that can leverage data streamed from cameras built into a firefighter's
personal protective equipment (PPE) to capture thermal, RGB color, and depth
imagery and then deploy already developed deep learning models to analyze the
input data in real time. The embedded system analyzes and returns the processed
images via wireless streaming, where they can be viewed remotely and relayed
back to the firefighter using an augmented reality platform that visualizes the
results of the analyzed inputs and draws the firefighter's attention to objects
of interest, such as doors and windows otherwise invisible through smoke and
flames.",multimedia,92,included
http://arxiv.org/abs/1906.05925v1,to_check,arxiv,arxiv,2019-06-13 00:00:00,arxiv,deep learning development environment in virtual reality,http://arxiv.org/abs/1906.05925v1,"Virtual reality (VR) offers immersive visualization and intuitive
interaction. We leverage VR to enable any biomedical professional to deploy a
deep learning (DL) model for image classification. While DL models can be
powerful tools for data analysis, they are also challenging to understand and
develop. To make deep learning more accessible and intuitive, we have built a
virtual reality-based DL development environment. Within our environment, the
user can move tangible objects to construct a neural network only using their
hands. Our software automatically translates these configurations into a
trainable model and then reports its resulting accuracy on a test dataset in
real-time. Furthermore, we have enriched the virtual objects with
visualizations of the model's components such that users can achieve insight
about the DL models that they are developing. With this approach, we bridge the
gap between professionals in different fields of expertise while offering a
novel perspective for model analysis and data interaction. We further suggest
that techniques of development and visualization in deep learning can benefit
by integrating virtual reality.",multimedia,93,not included
http://arxiv.org/abs/1912.06321v2,to_check,arxiv,arxiv,2019-12-13 00:00:00,arxiv,"sim2real predictivity: does evaluation in simulation predict real-world
  performance?",http://arxiv.org/abs/1912.06321v2,"Does progress in simulation translate to progress on robots? If one method
outperforms another in simulation, how likely is that trend to hold in reality
on a robot? We examine this question for embodied PointGoal navigation,
developing engineering tools and a research paradigm for evaluating a simulator
by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy),
a library for seamless execution of identical code on simulated agents and
robots, transferring simulation-trained agents to a LoCoBot platform with a
one-line code change. Second, we investigate the sim2real predictivity of
Habitat-Sim for PointGoal navigation. We 3D-scan a physical lab space to create
a virtualized replica, and run parallel tests of 9 different models in reality
and simulation. We present a new metric called Sim-vs-Real Correlation
Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as
used for the CVPR19 challenge is low (0.18 for the success metric), suggesting
that performance differences in this simulator-based challenge do not persist
after physical deployment. This gap is largely due to AI agents learning to
exploit simulator imperfections, abusing collision dynamics to 'slide' along
walls, leading to shortcuts through otherwise non-navigable space. Naturally,
such exploits do not work in the real world. Our experiments show that it is
possible to tune simulation parameters to improve sim2real predictivity (e.g.
improving $SRCC_{Succ}$ from 0.18 to 0.844), increasing confidence that
in-simulation comparisons will translate to deployed systems in reality.",multimedia,94,included
http://arxiv.org/abs/1510.03727v1,to_check,arxiv,arxiv,2015-10-13 00:00:00,arxiv,semanticpaint: a framework for the interactive segmentation of 3d scenes,http://arxiv.org/abs/1510.03727v1,"We present an open-source, real-time implementation of SemanticPaint, a
system for geometric reconstruction, object-class segmentation and learning of
3D scenes. Using our system, a user can walk into a room wearing a depth camera
and a virtual reality headset, and both densely reconstruct the 3D scene and
interactively segment the environment into object classes such as 'chair',
'floor' and 'table'. The user interacts physically with the real-world scene,
touching objects and using voice commands to assign them appropriate labels.
These user-generated labels are leveraged by an online random forest-based
machine learning algorithm, which is used to predict labels for previously
unseen parts of the scene. The entire pipeline runs in real time, and the user
stays 'in the loop' throughout the process, receiving immediate feedback about
the progress of the labelling and interacting with the scene as necessary to
refine the predicted segmentation.",multimedia,95,not included
http://arxiv.org/abs/2010.12570v3,to_check,arxiv,arxiv,2020-10-23 00:00:00,arxiv,"eye tracking data collection protocol for vr for remotely located
  subjects using blockchain and smart contracts",http://arxiv.org/abs/2010.12570v3,"Eye tracking data collection in the virtual reality context is typically
carried out in laboratory settings, which usually limits the number of
participants or consumes at least several months of research time. In addition,
under laboratory settings, subjects may not behave naturally due to being
recorded in an uncomfortable environment. In this work, we propose a
proof-of-concept eye tracking data collection protocol and its implementation
to collect eye tracking data from remotely located subjects, particularly for
virtual reality using Ethereum blockchain and smart contracts. With the
proposed protocol, data collectors can collect high quality eye tracking data
from a large number of human subjects with heterogeneous socio-demographic
characteristics. The quality and the amount of data can be helpful for various
tasks in data-driven human-computer interaction and artificial intelligence.",multimedia,96,not included
http://arxiv.org/abs/1911.01562v1,to_check,arxiv,arxiv,2019-11-05 00:00:00,arxiv,"deepracer: educational autonomous racing platform for experimentation
  with sim2real reinforcement learning",http://arxiv.org/abs/1911.01562v1,"DeepRacer is a platform for end-to-end experimentation with RL and can be
used to systematically investigate the key challenges in developing intelligent
control systems. Using the platform, we demonstrate how a 1/18th scale car can
learn to drive autonomously using RL with a monocular camera. It is trained in
simulation with no additional tuning in physical world and demonstrates: 1)
formulation and solution of a robust reinforcement learning algorithm, 2)
narrowing the reality gap through joint perception and dynamics, 3) distributed
on-demand compute architecture for training optimal policies, and 4) a robust
evaluation method to identify when to stop training. It is the first successful
large-scale deployment of deep reinforcement learning on a robotic control
agent that uses only raw camera images as observations and a model-free
learning method to perform robust path planning. We open source our code and
video demo on GitHub: https://git.io/fjxoJ.",multimedia,97,included
http://arxiv.org/abs/1208.6057v1,to_check,arxiv,arxiv,2012-08-30 00:00:00,arxiv,"self-paced brain-computer interface control of ambulation in a virtual
  reality environment",http://arxiv.org/abs/1208.6057v1,"Objective: Spinal cord injury (SCI) often leaves affected individuals unable
to ambulate. Electroencephalogramme (EEG) based brain-computer interface (BCI)
controlled lower extremity prostheses may restore intuitive and able-body-like
ambulation after SCI. To test its feasibility, the authors developed and tested
a novel EEG-based, data-driven BCI system for intuitive and self-paced control
of the ambulation of an avatar within a virtual reality environment (VRE).
  Approach: Eight able-bodied subjects and one with SCI underwent the following
10-min training session: subjects alternated between idling and walking
kinaesthetic motor imageries (KMI) while their EEG were recorded and analysed
to generate subject-specific decoding models. Subjects then performed a
goal-oriented online task, repeated over 5 sessions, in which they utilised the
KMI to control the linear ambulation of an avatar and make 10 sequential stops
at designated points within the VRE.
  Main results: The average offline training performance across subjects was
77.2 +/- 9.5%, ranging from 64.3% (p = 0.00176) to 94.5% (p = 6.26*10^-23),
with chance performance being 50%. The average online performance was 8.4 +/-
1.0 (out of 10) successful stops and 303 +/- 53 sec completion time (perfect =
211 sec). All subjects achieved performances significantly different than those
of random walk (p < 0.05) in 44 of the 45 online sessions.
  Significance: By using a data-driven machine learning approach to decode
users' KMI, this BCIVRE system enabled intuitive and purposeful self-paced
control of ambulation after only a 10-minute training. The ability to achieve
such BCI control with minimal training indicates that the implementation of
future BCI-lower extremity prosthesis systems may be feasible.",multimedia,98,not included
10.1016/j.entcom.2021.100404,to_check,Entertainment Computing,scopus,2021-05-01,sciencedirect,using gestural emotions recognised through a neural network as input for an adaptive music system in virtual reality,https://api.elsevier.com/content/abstract/scopus_id/85100077108,"In this article, a head gesture recognition system is developed in order to identify emotional inputs and provide them to an adaptive music system (LitSens) in virtual reality applications, improving virtual presence in the process. Two iterations of this system, both founded on neural networks, are presented: the first one is based on a multi-layer perceptron, whereas the second one consists of a hybrid one-dimensional convolutional neural network. In both cases, the system is able to recognise fear by analysing head gestures. Whereas the first implementation is quicker when recognising this emotion, the second one is slower, but much more accurate, which makes it a better option overall for soundtrack adaptation. An experiment is then detailed, aimed towards validating the behaviour of a gestural recogniser when detecting fear in players. The results achieved through this validation are generally positive, but evince the need for an improvement in terms of system responsiveness.",multimedia,99,not included
10.1016/j.surg.2020.09.040,to_check,Surgery (United States),scopus,2021-05-01,sciencedirect,the future surgical training paradigm: virtual reality and machine learning in surgical education,https://api.elsevier.com/content/abstract/scopus_id/85097383738,"Surgical training has undergone substantial change in the last few decades. As technology and patient complexity continues to increase, demands for novel approaches to ensure competency have arisen. Virtual reality systems augmented with machine learning represents one such approach. The ability to offer on-demand training, integrate checklists, and provide personalized, surgeon-specific feedback is paving the way to a new era of surgical training. Machine learning algorithms that improve over time as they acquire more data will continue to refine the education they provide. Further, fully immersive simulated environments coupled with machine learning analytics provide real-world training opportunities in a safe atmosphere away from the potential to harm patients. Careful implementation of these technologies has the potential to increase access and improve quality of surgical training and patient care and are poised to change the landscape of current surgical training. Herein, we describe the current state of virtual reality coupled with machine learning for surgical training, future directions, and existing limitations of this technology.",multimedia,100,not included
10.1016/b978-0-12-815503-5.00008-5,to_check,Infrastructure Computer Vision,scopus,2019-01-01,sciencedirect,the future,https://api.elsevier.com/content/abstract/scopus_id/85093476644,"This chapter discusses the future of Infrastructure Computer Vision (ICV). It starts first with an overview of how ICV progressed from the early 1980s up until 2019 and the types of issues addressed given the toolsets at our disposal. The section then continues with an exploration of the current trends in ICV. We attempt to group individual efforts together into trends and provide some context on what enables them, what they make possible, and what the future of those trends might be. We eventually focus on deep learning, consumer-grade ICV applications offered by start-ups, advances in software and hardware development, the new wave of robotics, advances in multi-spectral imaging, and a general note about the cycle of trends using mixed reality as an example. The section then discusses a set of predictions on what might the future look like 10 years from now, and what drivers might facilitate or hinder the realization of those predictions.",multimedia,101,not included
10.1016/b978-0-12-816176-0.00045-4,to_check,Handbook of Medical Image Computing and Computer Assisted Intervention,scopus,2019-01-01,sciencedirect,challenges in computer assisted interventions,https://api.elsevier.com/content/abstract/scopus_id/85082596227,"Challenges in design, implementation, clinical evaluation, and deployment of computer assisted intervention solutions are manifold. Some of these challenges will be discussed in this chapter.
               Computer assistance in both surgical procedures and radiology interventions aim at augmenting the clinicians with the overall objective of providing better clinical outcome. Multimodal imaging, robotics, artificial intelligence, and augmented reality play a major role in computer assisted interventions. After a brief analysis of the state-of-the-art and practice in this field, we discuss the challenges in design and development, as well as translation and deployment of the technology, from research projects motivated by clinical needs to solutions routinely used within clinical setups. We also consider the required training of surgeons and the surgical team as a major component for smooth and successful translation. We present simulation as an important tool not only for the design and development of computer assisted intervention solutions but also in their fast and smooth translation into daily practice.",multimedia,102,not included
10.1002/nme.6127,to_check,core,An augmented reality platform for interactive aerodynamic design and analysis,2019-01-01 00:00:00,core,https://core.ac.uk/download/303798217.pdf,'Wiley',"While modern CFD tools are able to provide the user with reliable and accurate simulations, there is a strong need for interactive design and analysis tools. State-of-the-art CFD software employs massive resources in terms of CPU time, user interaction, and also GPU time for rendering and analysis. In this work, we develop an innovative tool able to provide a seamless bridge between artistic design and engineering analysis. This platform has three main ingredients: computer vision to avoid long user interaction at the pre-processing stage, machine learning to avoid costly CFD simulations, and augmented reality for an agile and interactive post-processing of the results",multimedia,103,not included
https://core.ac.uk/download/322366596.pdf,to_check,core,'Lviv State University of Life Safety',2018-12-31 00:00:00,core,hard-soft-технологія інформаційного супроводу  процесу моделювання теплотворення/теплоспоживання  в двигуні внутрішнього згоряння,10.32447/20784643.18.2018.01,"Deterministic and, in a certain sense, ""linear"" interpretation of the world often leads to the recognition of the fact that the more accurate model we need, the more complex it must be (as in case of a formalized reproduction of the real system, or the implementation of the desired system properties in the process of formal synthesis of something new). Instead, following the principle of synergy leads to the conviction that there is always a certain model of optimal complexity e.g. in the synthesis of the new system, and in the analysis of real system peculiarities. However, the model of reality could be a part of this reality that is included to the carefully structured formal description.  Since we cannot penetrate into the working space of the serial engine while testing, we should use a test engine of a special construction when the working space corresponds to the laws of similarity and this engine will serve as a model of the working space of the serial engine.
&nbsp;
&nbsp;
The study illustrates the effectiveness of hard-soft technology while investigating the peculiarities of heat generation and heat consumption in the internal combustion engine, which will combine mathematic and algorithmic means of modelling as well as the means of real simulation. The necessity of hard-soft technology introduction arises from the excessive complexity of thermal phenomena occurring in the internal combustion engine (ICE), and the inability to fully subordinate these phenomena to existing analytical models.
The combination of original and analytical properties, reality and virtual reality while modelling the processes in internal combustion engines allows us to substantially improve the quality of information in the process of design and engine construction. Taking this into consideration, there are some natural grounds to apply principles of heuristic self-organization, self-learning, means of the neural networks, etc. in the design implementation.
The study demonstrates the example of modelling the real working space of ICE with the forced start that serves as a supplement to the mathematical algorithmic two-zone model of heat generation / heat consumption / heat extraction.
The basic information that can be obtained by means of hard-soft technology in the framework of, for example, the two-zone model of the work process in the gasoline engine, is the variability with the change in the angle of rotation of the crankshaft of the engine: absolute pressure (indicative diagram); absolute temperature; heat transmitted inside the cylinder between zones; coefficient of excess air; coefficient of heat transfer; intensity of heat extraction in the process of combustion of fuel; intensity of heat transfer through the walls of the cylindeДетерміністичне і в певному сенсі «лінійне» трактування світу часто веде до визнання того, що чим точнішою потрібна його модель, тим складнішою вона має бути (як у разі формалізованого відтворення реальної системи, так і у разі втілення бажаних системних властивостей у процесі формалізованого синтезу чогось нового). Натомість дотримання принципу синергетичності веде до переконання, що завжди існує якась модель оптимальної складності — і тоді, коли йдеться про синтез нової системи, і тоді, коли провадиться аналіз властивостей реальної системи. Але ж моделлю реальності може слугувати також і якась частина цієї реальності, долучена до ретельно структурованого формального опису. Оскільки дослідними засобами проникнути в робочий простір серійного двигуна нема змоги, то доводиться використовувати дослідний двигун особливої конструкції, робочий простір якого відповідає законам подібності і слугуватиме моделлю-аналогом робочого простору серійного двигуна.
Мета роботи — обґрунтувати ефективність hard-soft-технології дослідження особливостей теплотворення і теплоспоживання в двигуні внутрішнього згоряння, яка б системно поєднувала в собі засоби математичного й алгоритмічного моделювання та засоби натурного симулювання. Необхідність впровадження hard-soft-технології випливає з надмірної складності теплових явищ, що перебігають у двигуні внутрішнього згоряння, та неможливості уповні підпорядкувати ці явища існуючим аналітичним модельним уявленням.
Поєднання натурності та аналітичності, реальності та віртуальності в моделюванні процесів у двигунах внутрішнього згоряння дозволяє принципово підвищити якість інформаційного забезпечення процесу проектування й конструювання двигунів. При цьому виникають природні підстави для втілення у моделювання принципів евристичної самоорганізації, самонавчання, засобів штибу нейронних мереж тощо.
Наводиться приклад формування реального робочого простору двигуна внутрішнього згоряння з примусовим запаленням, покликаного доповнити математично-алгоритмічну двозонну модель теплотворення/теплоспоживання/тепловідведення.
Основною інформацією, яку можна добувати засобами hard-soft-технології в рамках, приміром, двозонної моделі робочого процесу в бензиновому двигуні, є змінюваність зі зміною кута повороту колінчастого вала двигуна: абсолютного тиску (індикаторна діаграма); абсолютної температури; теплоти, що пересилається всередині циліндра між зонами; коефіцієнта надлишку повітря; коефіцієнта тепловіддачі; інтенсивності тепловиділення у процесі згоряння палива; інтенсивність тепловідведення через стінки циліндра",multimedia,104,not included
https://core.ac.uk/download/326015561.pdf,to_check,core,'MDPI AG',2018-01-01 00:00:00,core,semantic fusion for natural multimodal interfaces using concurrent augmented transition networks,10.3390/mti2040081,"Semantic fusion is a central requirement of many multimodal interfaces. Procedural methods like finite-state transducers and augmented transition networks have proven to be beneficial to implement semantic fusion. They are compliant with rapid development cycles that are common for the development of user interfaces, in contrast to machine-learning approaches that require time-costly training and optimization. We identify seven fundamental requirements for the implementation of semantic fusion: Action derivation, continuous feedback, context-sensitivity, temporal relation support, access to the interaction context, as well as the support of chronologically unsorted and probabilistic input. A subsequent analysis reveals, however, that there is currently no solution for fulfilling the latter two requirements. As the main contribution of this article, we thus present the Concurrent Cursor concept to compensate these shortcomings. In addition, we showcase a reference implementation, the Concurrent Augmented Transition Network (cATN), that validates the concept’s feasibility in a series of proof of concept demonstrations as well as through a comparative benchmark. The cATN fulfills all identified requirements and fills the lack amongst previous solutions. It supports the rapid prototyping of multimodal interfaces by means of five concrete traits: Its declarative nature, the recursiveness of the underlying transition network, the network abstraction constructs of its description language, the utilized semantic queries, and an abstraction layer for lexical information. Our reference implementation was and is used in various student projects, theses, as well as master-level courses. It is openly available and showcases that non-experts can effectively implement multimodal interfaces, even for non-trivial applications in mixed and virtual reality",multimedia,105,not included
10.5167/uzh-144415,to_check,core,"321via.ch : design di una piattaforma per lo sviluppo di competenze linguistiche, interlinguistiche e interculturali dell’italiano in Svizzera",2017-05-31 00:00:00,core,https://core.ac.uk/download/154256365.pdf,,"Italian is one of the four national languages of Switzerland and, with German and French, its official language. Although protected by law, in the last two decades Italian in schools has suffered a number of attacks that highlighted the need for specific language policies and relative pedagogical implementation.

This project is located in the latter sphere of action and aims to fill a gap in the landscape of educational offers for teaching Italian in Switzerland. Following the paradigm of “gestaltungsorientierte Forschung”, and more specifically the design-based research approach, this thesis examines the phases of conception, design, development, implementation and assessment of the web-based training 321via.ch – a learning platform conceived to develop Italian language skills at A1-B2 level on the one hand and interlinguistic and intercultural competences specific to the Swiss Italian reality on the other. The end result consists of a technical solution (a learning content management system with a highly flexible authoring tool) and an educational solution, consisting of some thirty learning objects and about 200 learning tasks.

The summative assessment that concludes this thesis focusses on the impact of the first solution on both the editorial team and university students of Italian Language Education and of the second on a sample of Italian teachers and high school students: data collected allows for an evaluation of the respondents’ perception of 321via.ch usefulness as a teaching or learning tool and their willingness to refer to it in the future, at the same time pointing to possible paths for future developments and further research.



L’italiano è una delle quattro lingue nazionali della Svizzera e, accanto a tedesco e francese, sua lingua ufficiale. Anche se tutelato a livello giuridico, negli ultimi due decenni l’insegnamento dell’italiano ha subito, nella prassi, numerosi attacchi che hanno evidenziato il bisogno di interventi politico-linguistici ed educativi. 

Il seguente progetto si situa in quest’ultimo ambito d’azione e mira a colmare una lacuna nel panorama delle offerte didattiche per l’insegnamento dell’italiano in Svizzera. Seguendo il paradigma della gestaltungsorientierte Forschung, e più in particolare l’approccio della design-based research, la presente tesi studia le fasi di ideazione, progettazione, sviluppo, implementazione e valutazione del web-based training 321via.ch, una piattaforma didattica volta a sviluppare da un lato competenze linguistiche ricettive dell’italiano ai livelli A1-B2 e dall’altro competenze interlinguistiche e interculturali specifiche alla realtà italofona elvetica. Il prodotto così elaborato si compone di una soluzione tecnologica, un learning content management system con un software autore particolarmente flessibile, e di una soluzione didattica composta da una trentina di learning object e da circa 200 learning task. 

Oggetto dell’analisi sommativa che conclude questa tesi è l’impatto del primo artefatto sul team redazionale e su studenti di didattica dell’italiano LS e del secondo artefatto su un gruppo di insegnanti e di studenti di liceo: i dati permettono di valutare se gli intervistati, nella propria percezione soggettiva, considerano 321via.ch uno strumento d’insegnamento o d’apprendimento utile a cui fare riferimento anche in futuro, fornendo inoltre spunti per sviluppi futuri e ulteriori ricerche",multimedia,106,not included
10.1145/2751556,to_check,core,SemanticPaint: A Framework for the Interactive Segmentation of 3D Scenes,2015-10-13 00:00:00,core,http://arxiv.org/abs/1510.03727,'Association for Computing Machinery (ACM)',"We present an open-source, real-time implementation of SemanticPaint, a
system for geometric reconstruction, object-class segmentation and learning of
3D scenes. Using our system, a user can walk into a room wearing a depth camera
and a virtual reality headset, and both densely reconstruct the 3D scene and
interactively segment the environment into object classes such as 'chair',
'floor' and 'table'. The user interacts physically with the real-world scene,
touching objects and using voice commands to assign them appropriate labels.
These user-generated labels are leveraged by an online random forest-based
machine learning algorithm, which is used to predict labels for previously
unseen parts of the scene. The entire pipeline runs in real time, and the user
stays 'in the loop' throughout the process, receiving immediate feedback about
the progress of the labelling and interacting with the scene as necessary to
refine the predicted segmentation.Comment: 33 pages, Project: http://www.semantic-paint.com, Code:
  https://github.com/torrvision/spain",multimedia,107,not included
5d248aace06704234de122ac354a51f1630182b8,to_check,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,augmented worlds: a proposal for modelling and engineering pervasive mixed reality smart environments,https://www.semanticscholar.org/paper/5d248aace06704234de122ac354a51f1630182b8,"In recent years, the remarkable technological advancement has revolutionised the ICT panorama offering nowadays the opportunity to exploit several technological convergences to reduce the gulf existing between the physical and the digital matter, between the physical real world and every computational software system or application. Along with Pervasive Computing - entered in the mainstream with the concept of Internet of Things (IoT) - Mixed Reality (MR) is going to be an essential ingredient for the design and development of next future smart environments. In particular, in such environments is feasible to imagine that the computation will drive the augmentation of the physical space, and software will also be executed in a cyber-physical world, eventually populated with of (interactive) holograms. 
 
After an initial exploration of the state of the art about augmentation technologies both for humans and the environment, in this dissertation we present the vision of Augmented Worlds (AW), a conceptual and a practical proposal for modelling and engineering next future pervasive mixed reality smart environments as distribute, multi-user and cooperative complex software systems. 
On the one hand, a meta-model is formalised and opportunely discussed to offer to the literature a conceptual tool for modelling AWs. On the other hand, also a concrete infrastructure - called MiRAgE - is designed and developed to produce a platform for engineering and deploy such innovative smart environments. 
 
The work carried out in this dissertation fits into the scientific literature and research of Pervasive Computing and Mixed Reality fields. Furthermore, part of the contribution is related also to the area of Cognitive Agents and Multi-Agent Systems, due to the AWs orientation to be deeply connected to a layer involving autonomous agents able to observe and act proactively in the smart environment.",multimedia,108,not included
http://arxiv.org/abs/1810.11063v1,to_check,arxiv,arxiv,2018-10-25 00:00:00,arxiv,sorry: ambient tactical deception via malware-based social engineering,http://arxiv.org/abs/1810.11063v1,"In this paper we argue, drawing from the perspectives of cybersecurity and
social psychology, that Internet-based manipulation of an individual or group
reality using ambient tactical deception is possible using only software and
changing words in a web browser. We call this attack Ambient Tactical Deception
(ATD). Ambient, in artificial intelligence, describes software that is
""unobtrusive,"" and completely integrated into a user's life. Tactical deception
is an information warfare term for the use of deception on an opposing force.
We suggest that an ATD attack could change the sentiment of text in a web
browser. This could alter the victim's perception of reality by providing
disinformation. Within the limit of online communication, even a pause in
replying to a text can affect how people perceive each other. The outcomes of
an ATD attack could include alienation, upsetting a victim, and influencing
their feelings about an election, a spouse, or a corporation.",science,109,not included
10.1016/j.clsr.2021.105564,to_check,Computer Law and Security Review,scopus,2021-09-01,sciencedirect,legal evaluation of the attacks caused by artificial intelligence-based lethal weapon systems within the context of rome statute,https://api.elsevier.com/content/abstract/scopus_id/85112789421,"Artificial intelligence (AI) as of the level of development reached today has become a scientific reality that is subject to study in the fields of law, political science, and other social sciences besides computer and software engineering. AI systems which perform relatively simple tasks in the early stages of the development period are expected to become fully or largely autonomous in the near future. Thanks to this, AI which includes the concepts of machine learning, deep learning, and autonomy, has begun to play an important role in producing and using smart arms. However, questions about AI-Based Lethal Weapon Systems (AILWS) and attacks that can be carried out by such systems have not been fully answered under legal aspect. More particularly, it is a controversial issue who will be responsible for the actions that an AILWS has committed. In this article, we discussed whether AILWS can commit offense in the context of the Rome Statute, examined the applicable law regarding the responsibility of AILWS, and tried to assess whether these systems can be held responsible in the context of international law, crime of aggression, and individual responsibility. It is our finding that international legal rules including the Rome Statute can be applied regarding the responsibility for the act/crime of aggression caused by AILWS. However, no matter how advanced the cognitive capacity of an AI software, it will not be possible to resort to the personal responsibility of this kind of system since it has no legal personality at all. In such a case, responsibility will remain with the actors who design, produce, and use the system. Last but not least, since no AILWS software does have specific codes of conduct that can make legal and ethical reasonings for today, at the end of the study it was recommended that states and non-governmental organizations together with manifacturers should constitute the necessary ethical rules written in software programs to prevent these systems from unlawful acts and to develop mechanisms that would restrain AI from working outside human control.",science,110,not included
10.1016/j.jclepro.2015.07.075,to_check,Journal of Cleaner Production,scopus,2016-01-01,sciencedirect,virtual laboratory on biomass for energy generation,https://api.elsevier.com/content/abstract/scopus_id/84959516733,"Virtual laboratories (VL) and interactive simulations are excellent approaches for training students to understand technical principles. This can be useful in many fields of science and engineering teaching. For this purpose, we have developed a virtual environment that can simulate real-laboratory operations, effectively enhancing the teaching process with an intuitive and appealing interface. Such software shows the virtual-studio practice characterization of basic biofuel-properties, simulating reality step-by-step. This virtual laboratory has been used by students of the postgraduate master subject on Biomass for Power Generation, belonging to the Master of Distributed Renewable Energy. The aim of this tool is to help students to study, learn and investigate on their own. The virtual lab is made of a web-based application to complement experimental laboratory training, allowing students to prepare their experimental practices before going to the lab, and to review them at any time afterwards. The computer application exhibits key virtual-lab educational features, like an integrative layout and self-evaluating tests. It allows a personalized and active learning-process, adaptability to teacher's aims and versatility and simplicity, using different multimedia resources. A satisfaction questionnaire was carried out between master degree students to evaluate the usefulness of the VL. Such a tool was positively evaluated, achieving a mean score of 6 out of 7 points. Additionally, the VL efficiency in the learning process showed a final examination main score of 8 out of 10 points. Last but not least, most students considered that the VL promoted learning and personal effort, being an excellent preparatory tool to real experiments.",science,111,not included
10.1016/j.jbtep.2013.09.002,to_check,Journal of Behavior Therapy and Experimental Psychiatry,scopus,2014-01-01,sciencedirect,a virtual reality-integrated program for improving social skills in patients with schizophrenia: a pilot study,https://api.elsevier.com/content/abstract/scopus_id/84884755111,"Background and objectives
                  Social skills training (SST) intervention has shown its efficacy to improve social dysfunction in patients with psychosis; however the implementation of new skills into patients' everyday functioning is difficult to achieve. In this study, we report results from the application of a virtual reality (VR) integrated program as an adjunct technique to a brief social skills intervention for patients with schizophrenia. It was predicted that the intervention would improve social cognition and performance of patients as well as generalisation of the learned responses into patient's daily life.
               
                  Methods
                  Twelve patients with schizophrenia or schizoaffective disorder completed the study. They attended sixteen individual one-hour sessions, and outcome assessments were conducted at pre-treatment, post-treatment and four-month follow-up.
               
                  Results
                  The results of a series of repeated measures ANOVA revealed significant improvement in negative symptoms, psychopathology, social anxiety and discomfort, avoidance and social functioning. Objective scores obtained through the use of the VR program showed a pattern of learning in emotion perception, assertive behaviours and time spent in a conversation. Most of these gains were maintained at four-month follow-up.
               
                  Limitations
                  The reported results are based on a small, uncontrolled pilot study. Although there was an independent rater for the self-reported and informant questionnaires, assessments were not blinded.
               
                  Conclusions
                  The results showed that the intervention may be effective for improving social dysfunction. The use of the VR program contributed to the generalisation of new skills into the patient's everyday functioning.",science,112,not included
10.1177/0261018320985463,to_check,core,'SAGE Publications',2021-01-01 00:00:00,core,‘ethical’ artificial intelligence in the welfare state: discourse and discrepancy in australian social services,,"In recent years, a discourse of ‘ethical artificial intelligence’ has emerged and gained international traction in response to widely publicised AI failures. In Australia, the discourse around ethical AI does not accord with the reality of AI deployment in the public sector. Drawing on institutional ethnographic approaches, this paper describes the misalignments between how technology is described in government documentation, and how it is deployed in social service delivery. We argue that the propagation of ethical principles legitimates established new public management strategies, and pre-empts questions regarding the efficacy of AI development; instead positioning implementation as inevitable and, provided an ethical framework is adopted, laudable. The ethical AI discourse acknowledges, and ostensibly seeks to move past, widely reported administrative failures involving new technologies. In actuality, this discourse works to make AI implementation a reality, ethical or not",science,113,not included
10.1007/s11721-014-0092-4,to_check,core,'Springer Science and Business Media LLC',2014-01-01 00:00:00,core,automode: a novel approach to the automatic design of control software for robot swarms,,"We introduce AutoMoDe: a novel approach to the automatic design of control software for robot swarms. The core idea in AutoMoDe recalls the approach commonly adopted in machine learning for dealing with the bias-variance tradedoff: to obtain suitably general solutions with low variance, an appropriate design bias is injected. AutoMoDe produces robot control software by selecting, instantiating, and combining preexisting parametric modules-the injected bias. The resulting control software is a probabilistic finite state machine in which the topology, the transition rules and the values of the parameters are obtained automatically via an optimization process that maximizes a task-specific objective function. As a proof of concept, we define AutoMoDe-Vanilla, which is a specialization of AutoMoDe for the e-puck robot. We use AutoMoDe-Vanilla to design the robot control software for two different tasks: aggregation and foraging. The results show that the control software produced by AutoMoDe-Vanilla (i) yields good results, (ii) appears to be robust to the so called reality gap, and (iii) is naturally human-readable. © 2014 Springer Science+Business Media New York.SCOPUS: ar.jinfo:eu-repo/semantics/publishe",science,114,not included
"[{'title': none, 'identifiers': ['2047-783x', 'issn:2047-783x']}]",to_check,core,BMC,2011-12-01 00:00:00,core,the use of a standardized pct-algorithm reduces costs in intensive care in septic patients - a drg-based simulation model,,"<p>Abstract</p> <p>Introduction</p> <p>The management of bloodstream infections especially sepsis is a difficult task. An optimal antibiotic therapy (ABX) is paramount for success. Procalcitonin (PCT) is a well investigated biomarker that allows close monitoring of the infection and management of ABX. It has proven to be a cost-efficient diagnostic tool. In Diagnoses Related Groups (DRG) based reimbursement systems, hospitals get only a fixed amount of money for certain treatments. Thus it's very important to obtain an optimal balance of clinical treatment and resource consumption namely the length of stay in hospital and especially in the Intensive Care Unit (ICU). We investigated which economic effects an optimized PCT-based algorithm for antibiotic management could have.</p> <p>Materials and methods</p> <p>We collected inpatient episode data from 16 hospitals. These data contain administrative and clinical information such as length of stay, days in the ICU or diagnoses and procedures. From various RCTs and reviews there are different algorithms for the use of PCT to manage ABX published. Moreover RCTs and meta-analyses have proven possible savings in days of ABX (ABD) and length of stay in ICU (ICUD). As the meta-analyses use studies on different patient populations (pneumonia, sepsis, other bacterial infections), we undertook a short meta-analyses of 6 relevant studies investigating in sepsis or ventilator associated pneumonia (VAP). From this analyses we obtained savings in ABD and ICUD by calculating the weighted mean differences. Then we designed a new PCT-based algorithm using results from two very recent reviews. The algorithm contains evidence from several studies. From the patient data we calculated cost estimates using German National standard costing information for the German G-DRG system.</p> <p>We developed a simulation model where the possible savings and the extra costs for (in average) 8 PCT tests due to our algorithm were brought into equation.</p> <p>Results</p> <p>We calculated ABD savings of -4 days and ICUD reductions of -1.8 days. our algorithm contains recommendations for ABX onset (PCT ≥ 0.5 ng/ml), validation whether ABX is appropriate or not (Delta from day 2 to day 3 ≥ 30% indicates inappropriate ABX) and recommendations for discontinuing ABX (PCT ≤ 0.25 ng/ml).</p> <p>We received 278, 264 episode datasets where we identified by computer-based selection 3, 263 cases with sepsis. After excluding cases with length of stay (LOS) too short to achieve the intended savings, we ended with 1, 312 cases with ICUD and 268 cases without ICUD. Average length of stay of ICU-patients was 27.7 ± 25.7 days and for Non-ICU patients 17.5 ± 14.6 days respectively. ICU patients had an average of 8.8 ± 8.7 ICUD.</p> <p>After applying the simulation model on this population we calculated possible savings of € -1, 163, 000 for ICU-patients and € -36, 512 for Non-ICU patients.</p> <p>Discussion</p> <p>Our findings concerning the savings from the reduction of ABD are consistent with other publications. Savings ICUD had never been economically evaluated so far. our algorithm is able to possibly set a new standard in PCT-based ABX. However the findings are based on data modelling. The algorithm will be implemented in 5-10 hospitals in 2012 and effects in clinical reality measured 6 months after implementation.</p> <p>Conclusion</p> <p>Managing sepsis with daily monitoring of PCT using our refined algorithm is suitable to save substantial costs in hospitals. Implementation in clinical routine settings will show how much of the calculated effect will be achieved in reality.</p",science,115,not included
10.1109/robio49542.2019.8961870,to_check,2019 IEEE International Conference on Robotics and Biomimetics (ROBIO),IEEE,2019-12-08 00:00:00,ieeexplore,probabilistic inferences on quadruped robots: an experimental comparison,https://ieeexplore.ieee.org/document/8961870/,"Due to the reality gap, computer software cannot fully model the physical robot in its environment, with noise, ground friction, and energy consumption. Consequently, a limited number of researchers work on applying machine learning in real-world robots. In this paper, we use two intelligent black-box optimization algorithms, Bayesian Optimization (BO) and Covariance Matrix Adaptation Evolution Strategy (CMA-ES), to solve a quadruped robot gait's parametric search problem in 10 dimensions, and compare these two methods to find which one is more suitable for legged robots' controller parameters tuning. Our results show that both methods can find an optimal solution in 130 iterations. BO converges faster than CMA-ES within its constrained range, while CMA-ES finds the optimum in the continuous space. Compared with the specific controller parameters of two methods, we also find that for quadruped robot's oscillators, the angular amplitude is the most important parameter. Thus, it is very beneficial for the quick parametric search of legged robots' controllers and avoids time-consuming manual tuning.",robotics,116,not included
10.1007/s00345-019-03037-6,to_check,World Journal of Urology,Springer,2020-10-01 00:00:00,springer,artificial intelligence and robotics: a combination that is changing the operating room,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00345-019-03037-6,"Purpose The aim of the current narrative review was to summarize the available evidence in the literature on artificial intelligence (AI) methods that have been applied during robotic surgery. Methods A narrative review of the literature was performed on MEDLINE/Pubmed and Scopus database on the topics of artificial intelligence, autonomous surgery, machine learning, robotic surgery, and surgical navigation, focusing on articles published between January 2015 and June 2019. All available evidences were analyzed and summarized herein after an interactive peer-review process of the panel. Literature review The preliminary results of the implementation of AI in clinical setting are encouraging. By providing a readout of the full telemetry and a sophisticated viewing console, robot-assisted surgery can be used to study and refine the application of AI in surgical practice. Machine learning approaches strengthen the feedback regarding surgical skills acquisition, efficiency of the surgical process, surgical guidance and prediction of postoperative outcomes. Tension-sensors on the robotic arms and the integration of augmented reality methods can help enhance the surgical experience and monitor organ movements. Conclusions The use of AI in robotic surgery is expected to have a significant impact on future surgical training as well as enhance the surgical experience during a procedure. Both aim to realize precision surgery and thus to increase the quality of the surgical care. Implementation of AI in master–slave robotic surgery may allow for the careful, step-by-step consideration of autonomous robotic surgery.",robotics,117,not included
10.1007/978-3-030-35430-5_2,to_check,Pattern Recognition and Information Processing,Springer,2019-01-01 00:00:00,springer,robots’ vision humanization through machine-learning based artificial visual attention,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-35430-5_2,"If the main challenge of robotics during the industrial air of 19^th century has consisted of automating repetitive tasks and the sophistication of these machines through digitization of these robots throughout the 20^th century, the challenge of robotics in the current century will be to make cohabit humans and robots in the same living space. However, robots would not succeed in seamlessly integrate the humans’ universe without developing the ability of perceiving similarly to humans the environment that they are supposed to share with them. In such a context, fitting the skills of the natural vision is an appealing perspective for autonomous robotics dealing with and prospecting Human-Robot interaction. The main goal of the present article is to debate the plausibility and the reality of humanizing the robots behavior focusing the perception of the surrounding environment. An implementation of the developed concept on a real humanoid robot nourishes the presented results and the related discussions.",robotics,118,not included
,to_check,core,Efficient Implementation of Patient-specific Simulated Rehearsal for the Carotid Artery Stenting Procedure: Part-task Rehearsal ,2011-08-31 00:00:00,core,10.1016/j.ejvs.2011.03.032,,"AbstractObjective(s)Patient-specific simulated rehearsal (PsR) is a technological advance within the domain of endovascular virtual reality (VR) simulation. It allows incorporation of patient-specific computed tomography Digital Imaging and Communications in Medicine (CT DICOM) data into the simulation and subsequent rehearsal of real patient cases. This study aimed to evaluate whether a part-task rehearsal (PTr) of a carotid artery stenting procedure (CAS) on a VR simulator is as effective as a full-task (FTr) preoperative run through.MethodsMedical trainees were trained in the CAS procedure and randomised to a PTr or FTr of a challenging CAS case (Type-II arch). PTr consisted of 30min of repeated catheterisations of the common carotid artery (CCA). Thereafter, both groups performed the CAS procedure in a fully functional simulated operating suite (SOS) with an interventional team. Technical performances were assessed using simulator-based metrics and expert ratings. Other aspects of performance were assessed using the Non-Technical Skills for Surgeons (NOTSS) scoring.ResultsTwenty trainees were evenly randomised to either PTr or FTr. No differences in performance were seen except for the total time the embolic protection device (EPD) was deployed (9.4min for the PT vs. 8.1min for the FT, p=0.02). Total time (26.3 vs. 25.5min, p=0.94), fluoroscopy time (15.8 vs. 14.4min, p=0.68), number of roadmaps (10.5 vs. 11.0, p=0.54), amount of contrast (53.5 vs. 58.0ml, p=0.33), time to deploy the EPD (0.9 vs. 0.8min, p=0.31) and time to catheterise the CCA (9.2 vs. 8.9min, p=0.94) were similar. Qualitative performances as measured by expert ratings (score 24 vs. 24, p=0.49) and NOTSS (p>0.05 for all categories) were also comparable.ConclusionsPart- and full-task rehearsals are equally effective with respect to the operative performance of a simulated CAS intervention. This finding makes a patient-specific rehearsal more efficient and may increase the feasibility of implementation of this technology into medical practice",health,119,not included
,to_check,core,Національний університет «Чернігівська політехніка»,2021-05-19 00:00:00,core,перспективи використання геоінформаційних технологій в аеропортах україни для адміністративно-господарського управління,,"Target settings. This study examines the possibilities of modern geographic information and cloud technologies and prospects for their use for administrative and economic management of an airport. The study is related to the implementation of the State Target Program for Airport Development until 2023 and the Aviation Transport Strategy of Ukraine until 2030, which aims to develop the aviation industry in Ukraine, bringing airport infrastructure to the requirements of the European Union. The Law of Ukraine «On the National Infrastructure of Geospatial Data» and «Consolidated Concept of VIM Implementation in Ukraine» has a great influence on the formation of geospatial data of airports.
Actual scientific researches and issues analysis. The paper analyzes and summarizes publications on methods of obtaining geospatial data, implementation of geographic information technologies, virtual, augmented and mixed reality technologies, artificial intelligence and the concept of «smart» city for administrative and economic management of airports.
Uninvestigated parts of general matters defining. Analysis of recent research and publications has shown that the prospects for the introduction of geographic information technology for administrative and economic asset management of Ukrainian airports need further research, as these issues are very important and relevant, given the rapid growth of digital society, environment and infrastructure.
The research objective. The purpose of this study is to analyze the possibilities and prospects for the introduction of modern technologies for processing and visualization of geospatial data for administrative and economic management of the airport and the development of a conceptual model. The task of the research is to analyze the methods of obtaining geospatial data of the airport, the use of geographic information systems in airports, artificial intelligence technologies, virtual, augmented and mixed reality, the Internet of Things, digital duplicates, implementation of the concept of «smart» city, etc.
The statement of basic materials. Geospatial data is created digitally using modern information and cloud technologies that offer a wide range of equipment, software, methods and technologies for working with geospatial information. Every year, new technologies that are used in the administrative and economic management of airports appear: cloud data acquisition methods, geographic information systems, artificial intelligence technologies, virtual reality, the Internet of Things, digital counterparts, «smart» cities, and more. Successful integration and use of existing capabilities for the collection, storage, processing and visualization of geospatial data of airports will ensure their effective management and economic growth.
Conclusions. Based on the analysis of the possibilities of using virtual, augmented and mixed reality technologies, artificial intelligence, digital duplicates and the concept of «smart» cities in airports, a conceptual model of prospects for using geospatial data of the airport to address administrative and economic management of the property complex was developed.Актуальність теми дослідження. Останніми роками в аеропортах світу активно впроваджуються хмарні технології збору, опрацювання та візуалізації геопросторових даних: лазерне та лідарне сканування, інтеграція ВІМ/GIS моделей, застосування штучного інтелекту, технологій віртуальної та доповненої реальності, цифрових двійників та «розумних міст». Для України, яка активно йде по шляху цифровізації та впровадження сучасних геоінформаційних технологій у багатьох сферах діяльності, розробка нових методів та підходів для адміністративно-господарського управління аеропортовими комплексами є актуальним та перспективним напрямом.
Постановка проблеми. У цьому дослідженні розглянуто можливості сучасних геоінформаційних та хмарних технологій і перспективи їх використання для адміністративно-господарського управління територією аеропорту. Дослідження пов’язано з реалізацією Державної цільової програми розвитку аеропортів на період до 2023 року та Авіаційної транспортної стратегії України до 2030 року, метою яких є розвиток авіаційної галузі в Україні, приведення інфраструктури аеропортів до вимог Європейського Союзу. Також великий вплив на формування геопросторових даних аеропортів має Закон України «Про національну інфраструктуру геопросторових даних» та Консолідована Концепція впровадження ВІМ в Україні.
Аналіз останніх досліджень і публікацій. У роботі були проаналізовані та узагальнені публікації, що присвячені методам отримання геопросторових даних, впровадження геоінформаційних технологій, технологій віртуальної, доповненої та змішаної реальності, штучного інтелекту та концепції «розумного міста» для адміністративно-господарського управління аеропортами.
Виділення недосліджених частин загальної проблеми. Аналіз останніх досліджень і публікацій показав, що питання перспектив впровадження геоінформаційних технологій для адміністративно-господарського управління активами аеропортів України потребує додаткового дослідження, оскільки ці питання дуже важливі та актуальні, враховуючи стрімке зростання цифровізації суспільства, навколишнього середовища та інфраструктурних об’єктів.
Постановка завдання. Метою цього дослідження є аналіз можливостей та перспектив впровадження сучасних технологій опрацювання та візуалізації геопросторових даних для адміністративно-господарського управління територією аеропорту та розробка концептуальної моделі. Завданням дослідження є аналіз методів отримання геопросторових даних території аеропорту, застосування в аеропортах геоінформаційних систем, технологій штучного інтелекту, віртуальної, доповненої та змішаної реальності, інтернету речей, цифрових двійників, реалізації концепції «розумне місто», тощо.
Виклад основного матеріалу. Геопросторові дані створюються в цифровій формі з використанням сучасних інформаційних та хмарних технологій, які пропонують широкий спектр обладнання, програмного забезпечення, методів і технологій роботи з геопросторовою інформацією. З кожним роком з’являються все нові технології, які знаходять застосування в адміністративно-господарському управлінні аеропортів: хмарні методи отримання даних, геоінформаційні системи, технології штучного інтелекту, віртуальної реальності, інтернету речей, цифрові двійники, «розумні міста» тощо. Вдала інтеграція та використання наявних можливостей щодо збору, зберігання, опрацювання та візуалізації геопросторових даних аеропортів забезпечить їх ефективне управління та економічне зростання. 
Висновки відповідно до статті. За результатами проведеного аналізу можливостей використання в аеропортах технологій віртуальної, доповненої та змішаної реальності, штучного інтелекту, цифрових двійників та концепції «розумних міст», було розроблено концептуальну модель перспектив використання геопросторових даних території аеропорту для вирішення питань адміністративно-господарського управління майновим комплексом",industry,120,not included
,to_check,core,,2020-04-22 00:00:00,core,"бізнес, інновації, менеджмент: проблеми та перспективи",SKILLS FOR EUROPEAN BUSINESS DEVELOPMENT IN DIGITAL ERA,"Digital Advanced Era is gaining momentum today. There are objective reasons, significant advantages, and serious business challenges. The ongoing digital transformation creates some of the largest economic, political and social challenges for the European Union. Despite new opportunities, it is frequently perceived as a threat to traditional business models, current organizational structures, and well-established business operations. Economic question about the competitiveness and capabilities in new technologies are linked to needs of skills and knowledge developing in Europe. It is an incredible for exploring the processes. For Ukrainian researchers, this issue may be interesting in the context of the European integration and understanding of digitalization processes in the EU, comparing them with Ukrainian realities, and identifying possibilities for benchmarking of the European experience. The main aim of the paper is to focus on the skills necessary to managers to operate in a digital era.There are no doubts that the nature of work changes, new jobs emerge while others disappear, so people need to gain new skills. Approximately 30% of the current jobs in the European Union may disappear over the next 25 years with the emergence of new professions requiring advanced digital skills [1]. Therefore, a critical element in the digital economy is to define and to acquire skills for business development in digital era. We are trying to focus on the skills critical for the EU’s growth and mitigation of losses caused by the digital transformation (by cloud computing, big data, the Internet of Things and artificial intelligence.An important factor to be taken into consideration is the fact that the society is not fully aware of the real situation. Digital business (or digital era, digital transformation) is taken for almost granted. But in reality the digital economy is highly concentrated in two countries (USA and China):-       50% of global spending on IoT-       more than 75% of the cloud computing market-       75% of all patents related to blockchain technologies [2].Moreover:-       about 10 % of the EU labour force has no digital skills;-       35 % of the EU labour does not have basic digital skills;-       about 28 % of the EU's internet users have no software-related skills [1].Also it is important to understand that digital skills which can be defined as abilities to use digital devices, communication applications, and networks to find information are (to some extend) basic or consumer skills that are mechanically honed and do not require serious intellectual effort. While digital skills also mean abilities to manage information, it is a higher level skill, but it is still a basic skill in the digital age. The shift of value creation towards digital platforms, new security and standard solutions, big data usage and data quality call for more complex skills. According to UNCTAD Digital Economy Report 2019, the expansion of the Internet of Things and various data-driven business models will require specialists who can convert big data into information and knowledge that are:- data scientist;- data engineers;- data architects and data visualizators [2].And we're focusing on the ability to use business opportunities including the increasing amounts of data. These require broader skill sets, skills to combine analytical, software and information systems with business. Data analysts and scientists also need to be business savvy to help enterprises capture business opportunities from their analyses. Multiple skills that combine sound technical skills with entrepreneurial skills and vertical and business process management expertise are particularly important [3].It is noteworthy that only 12% of EU enterprises used big data in 2018, 9% of EU enterprises - in 2016 [3]. That is, 88% and 91% did not use. Accordingly, the issue is not only about to capture business opportunities but skills to see the existing opportunities at least. That is why the EU awareness campaigns were oriented on alert the real picture: – WATIFY - launched in 2014 with the goal to push people towards concrete action to create their digital start-up, or digitally transform their business;– e-Skills for jobs and growth – launched in 2014 with an idea to strengthen competences and e-leadership;–  Start Up Europe Initiative to strengthen the environment for Web Entrepreneurs to start and stay in Europe.In 2018, the vast majority (92 %) of EU enterprises with at least 10 persons employed used the internet 77 % of EU enterprises have a website. In 2018, 1 in 5 EU enterprises employed ICT specialists; the percentage of large enterprises employing ICT specialists (75 %) is more than 4 times higher than that for SMEs (18 %) [3]. The difference in big data and Internet usage statistics may be explained by an understanding of the benefits and personal routine of the latest one in life. Once upon a time, such breakthrough and unusual things became household. And big data or the Internet of Things are not conscious, and it is impossible to build a business model on the obscure.That is a problem for the leaders of European companies in understanding the potential of artificial intelligence or data analytics. It is not just about that Europe needs an additional 20 million early stage entrepreneurs to close the gap with the US and China, but also about a high intensity of utilization of novel digital technologies (social, big data, mobile and cloud solutions) to improve business operations, invent new models in three directions:– ability to reach out and partner with customers more effectively;– inner processes;– business models - the way value is created, delivered and captured.The digital transformation from a European management angle should include strategic vision how to combine analytical, software and information systems with business. Skills for European business development in digital era refers to leadership in the new era, it does not matter traditional or digital business – the reality is changing in general.The skills for European business development must combine:- ICT user skills to apply systems as tools in support of own work. It can be common software tools or specialised tools supporting business functions within industry;- skills required for researching, developing, designing, strategic planning, managing, producing, consulting, marketing, selling, integrating, installing, administering, maintaining, supporting and servicing ICT systems;- business skills correspond to the capabilities to exploit opportunities provided by ICT; to ensure more efficient and effective performance of different types of organisations; to explore possibilities for new ways of conducting business and organisational processes; and to establish new businesses [3].A little different set of skills is proposed by publication “E-Leadership Digital Skills for SMEs” of the EC [4]:- strategic leadership skills to lead inter‐disciplinary staff, and influence stakeholders across boundaries (functional, geographic);– business savvy: innovate business and operating models, delivering value to organisations;– digital savvy: envision and drive change for business performance, exploiting digital technology trends as innovation opportunities [4].To summarize, it should be noted that the new era creates opportunities for all businesses, the question is to see these opportunities and successfully use them. Entrepreneurship and business may not be fundamentally changing at any time, but the tool usage requires understanding, if not details, so the overall potential and range of capabilities of this tool. In the digital age, digital skills (both basic and complex) are understanding how to use the tool. On the other hand, the strategic leadership skills and the business savvy.",industry,121,not included
,to_check,core,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",2017-09-13 00:00:00,core,oracles de teste para sistemas com saídas complexas - o caso dos sistemas tts,,"Software testing is one of the most important Software Engineering processes, being the primary activity to check the conformance between the software requirements and its actual behavior. The automation of software testing activities is essential to certify productivity and effectiveness in such activities. Test automation leads testing activities to be conducted under systematic and accurate criteria, raising the chance of testers to reveal faults or inconsistencies. Test oracles are elementary members in software testing automation, being the mechanism responsible for indicating the correctness of software outputs. In testing environments, test oracles can be effectively implemented based on several sources of information about the Software Under Testing (SUT): software specifications, assertions, formal methods (Finite State Machines (FSM), formal specifications, etc, machine-learning methods, and metamorphic relations. Regardless of the implementation strategy, test oracles are vulnerable to false positive/negative verdicts, configuring what the literature describes as the oracle problem. Therefore, test oracles are a non-trivial and challenging object of studies of the software engineering research area. SUTs outputs in unusual formats make it harder the oracle problem. Audio, images, three-dimensional objects, virtual reality environments, complex statistical compositions, etc, are examples of non-trivial output formats. In the software testing context, SUTs with unusual outputs can be called complex-output systems. In this doctorate dissertation, we propose and evaluate a novel test oracle approach for complex-output systems called feature-based test oracles. The purpose of feature-based test oracles is the appropriation of a processing image technique called Content-Based Image Retrieval (CBIR) to collect information from features extracted from the SUTs outputs to compose test oracles. Given a query image, CBIR combines feature extraction and similarity functions to alleviate the problem of searching for digital images in large databases. In previous research, we have integrated CBIR concepts in a testing framework to support the automation of testing activities in processing image systems and systems with Graphical User Interfaces (GUI). In this doctorate dissertation, we extended that framework and its concepts to general complex-output systems, addressing the feature-based test oracle approach. We use Text-To-Speech (TTS) systems to validate empirically our test oracle technique. Through the results of five empirical analyses, three of them conducted in line with problems of a real-world industry TTS system, show the proposed technique is a valuable instrument to automate testing activities and alleviate practitioners efforts on testing complex output systems. We conclude the proposed test oracles are effective because they systematically evaluate the SUTs sensorial output rather than produce verdicts based on subjective specifications. As future work, we plan to conduct investigations towards the reduction of false positives/negatives and the association of the test oracles with machine learning techniques and metamorphic relations.Teste de Software é um dos processos mais importantes da Engenharia de Software, sendo a principal atividade para averiguar a conformidade de requisitos de software e suas saídas. A automatização das atividades de teste é essencial para conferir produtividade e efetividade em tais atividades. A automatização faz com que atividades de teste sejam conduzidas sob critérios sistemáticos e precisos, aumentando a chance dos testadores de revelarem falhas ou inconcistências. Oráculos de teste são membros elementares na automatização do teste de software, sendo o mecanismo responsável por indicar a corretude das saídas do softwre. Em ambientes de teste, oráculos de teste podem ser efetivamente implementados com base em diversos fontes de informação sobre o sistema em teste: especificações de software, assertivas, métodos formais (máquinas de estados finitas, especificações formais, etc), métodos de aprendizagem de máquina e relações metamórficas. Independente da estratégia de implementação, oráculos de teste são vulneráveis a veridictos de falsos positivos/negativos, configurando o que é apresentado na literatura como O problema do Oráculo. Então, na área de engenharia de software, oráculos de teste são objetos de estudo não-triviais e desafiadores. O problema de oráculo é potencializado quando as saídas do sistema em teste são dadas em formatos não triviais como, por exemplo, audio, imagens, objetos tridimensionais, ambientes de realidade virtual, composições estatísticas complexas, etc. No contexto do teste de software, sistemas com saídas não triviais podem ser chamados de sistemas com saídas complexas. Esta tese de doutorado propões e avalia uma nova estratégia de oráculo de teste para sistemas com saídas complexas. O propósito de tal estratégia é a apropriação da técnica de processamento de imagem conhecida como CBIR (Recuperação de Imagem Basead em Conteúdo  CBIR) para coletar informações de características extratídas do sistema em teste, compondo oráculos de teste. A partir de uma imagem de busca, o CBIR combina extração de características e funções de similaridade para aliviar problemas de busca em grandes based de imagens digitais. Em pesquisas anteriores, conceitos de CBIR foram integrados em um arcabouço de teste para apoiar a automatização de atividades de teste em systemas de processamento de imagens e sistemas com interfaces gráficas. Esta tese de doutorado estende o arcabouço e seus conceitos para sistemas com saídas complexas em geral. Sistemas Texto-Fala (TTS) foram utlizados para validações empíricas. Os resultados de seis análises empíricas, duas delas condizidas em consonância com problemas de um TTS industrial, revelam que a técnica proposta é um valioso instrumento para automatizar atividaes de teste e aliviar esforços de profissionais da indústria ao teste sistemas com saídas complexas. Conclui-se que a efetividade dos oráculos de teste propostos são devido às sistemáticas análises do conteúdo das saídas dos sistemas em teste, em vez da análises de especificações subjetivas. Os trabalhos futuros vislumbrados devem ser conduzidos no intuito de reduzir número de falsos positivos/negativos e a associação dos oráculos de teste com técnicas de aprendizado de máquina e relações metamórficas",industry,122,not included
,to_check,core,HAL CCSD,2014-12-04 00:00:00,core,élements de conception de systèmes embarqués fortement contraints,,"In the past decades, embedded systems become more and more present in our daily life. They are present in our wallet, in our car, in our house appliances and the current tendency is to control more and more things by using them. Major companies already started to envision the future of internet which become known as the Internet of Things. In the next years, more and more of our things will be smart, connected... and subject to software flaws.Micro-controllers, very small devices with a few hundreds bytes of memory, are at the heart of this revolution. They become cheaper, smaller and more powerful. Yet, contrary to the technologies used in our desktop computers or our server farms, the goal of the industry that creates these devices is not power. Indeed, controlling temperature in your home or humidity in your wine cellar does not need Gigahertz core. It even does not need multiple cores. What these devices need is to be cheap, to be able to be produced in very high volumes and to use small amount of electrical energy. In this context, the moore's law does not help us to have more cores in one device, but it allows to produce more devices with the same amount of silicon. The research I made in the past years try to reduce the gap between cheap software and robust and efficient software for these devices. I think that, by providing smart tools and production toolchains we can help junior or non specialized developers so that they can produce good embedded software for a reasonable development cost. I also focus my work on optimizing and providing efficient and secure software and network protocols, so that the Internet of Things can become a reality while respecting user privacy and being sustainable from a energy point of view.I focused my research on three aspects. First, I focused on how to allow larger softwares do be developped in embedded systems. In particular, we proposed techniques to allow Java/JavaCard code to be executed from a non-addressable memory using a full software cache.  I also looked at how to use domain specific languages to ease the implementation of a collaborative network intrusions detection system. Thanks to adapted tools, the software, writen in the DSL, can be compiled for a wide range of probes while offering garanties on the produced software. Last, I focused on energy aware network protocols in the context of smart cities.Au cours des dernières années, les systèmes embarqués sont de plus en plus présents dans notre vie de tous les jours. Ils sont dans notre portefeuille, dans notre voiture ou dans nos appareils ménagers. La tendance actuelle est à contrôler de plus en plus d'objets à l'aide de ces systèmes. Les grands acteurs de l'industrie ont déjà commencé à envisager le futur de l'internet, l'internet des objets. Dans les années à venir, de plus en plus de nos objets seront « intelligents », connectés... et sujets à des fautes logicielles.Les micro contrôleurs, de minuscules ordinateurs possédant quelques centaines d'octets de mémoire, sont au coeur de cette révolution. Ils deviennent de moins en moins cher et de plus en plus puissant. Néanmoins, à la différence de nos ordinateurs de bureau ou des serveurs, le but de l'industrie des micro contrôleurs n'est pas la puissance. En effet, contrôler la température de notre maison et le taux d'humidité de notre cave à vin ne nécessite pas des processeurs cadencés à plusieurs gigahertz. Cela ne nécessite même pas plusieurs coeurs de calculs. Le véritable besoin de ces équipements est d'être bons marché, d'être produits en très grands volumes, d'êtres petits, facilement intégrables et d'utiliser une faible quantité d'énergie électrique. Le meilleur exemple de cette tendance est très certainement la carte à puce. Depuis le début des années 90, de plus en plus de cartes à puce sont produites, vendues et utilisées dans le monde. Cependant, elles n'en restent pas moins de tout petits équipements d'une puissance inférieure de plusieurs ordres de grandeur de nos ordinateurs de bureau. Quand il s'agit de développer du logiciel pour ces équipements, un développeur débutant n'est généralement pas suffisamment préparé et ne sera pas capable d'écrire du logiciel efficace pour ces cibles avant de longue années passées à gagner de l'expérience et de l'expertise. Si le logiciel que l'ont souhaite produire doit être efficace, sûr et correct au sens le plus strict du terme, l'industrie doit compter sur des développeurs très spécialisés, expérimentés et donc chers. Les acteurs industriels ont donc deux alternatives : produire du logiciel bon marché, mais assez inefficaceet défectueux ou dépenser une somme importante pour le développement et fournir un logiciel de bonne qualité. On peu aisément supposer que la tendance actuelle est au logiciel bon marché, et que la probabilité que le logiciel qui équipera les millions d'équipements formant l'internet des objets sera de piètre qualité et offrira quantité de failles que des acteurs malveillants se feront une joie d'exploiter si rien n'est fait pour rendre bon marché le développement correct de logiciels embarqués.Mes recherches de ces dernières années vont dans le sens de la réduction du fossé séparant logiciels bon marché et logiciels sûrs et performants. Je suis convaincu qu'en offrant des outils « intelligents » et des chaînes de production logicielle efficaces, nous pouvons aider les développeurs débutants, ou non spécialisés, à produire du logiciel embarqué de bonne qualité pour un coût de développement raisonnable. J'ai également porté mon attention sur l'optimisation et la sécurisation des logiciels et des protocoles réseau afin que l'internet des objets puisse devenir une réalité tout en respectant la vie privée des utilisateurs et en offrant une alternative durable sur le plan énergétique.Je me suis principalement intéressé à trois champs de recherche. Tout d'abord, j'ai cherché à permettre l'utilisation de techniques de développement standard (objets, composants, programmation en Java…) à la faible capacité mémoire des équipements embarqués. A l'inverse, je me suis également intéressé à l'utilisation de langages dédiés à une application afin de permettre à des spécialistes du domaine de la sécurité réseau d'exprimer des algorithmes de détection d'intrusions. A l'aide d'une suite d'outils dédiée, ces algorithmes sont compilés et optimisés automatiquement pour être utilisés dans une architecture distribuée de sondes embarquées. Enfin, je me suis intéressé aux protocoles réseau économes en énergie pour interconnecter les équipements embarqués dans le cadre des villes intelligentes",industry,123,not included
,to_check,core,place:Bergamo,2014-01-01 00:00:00,core,"a learning environment
based on movement and sound interaction",,"Computer generated images have always drawn the attention of millions of people in their different roles: students, professors, researchers, designers, movie directors, etc. It is not only the magic of generating synthetic, static and/or dynamic images, emulations or simulations of reality, but of capturing all the creative abilities of the human being to narrate events, from the memory of the past, descriptions of current reality and projections towards the future. In other words, they may be timeless images and malleable towards the infinite.

In them rests the freedom of expression in a chromatic and three-dimensional way where sometimes it is difficult to see whether we are in the face of something real or virtual. It is the charm of the viewing power of communication where an image is always worth more than a thousand words. Not for nothing when the first graphics appeared to depict the numerical information in the offices, it marked the beginning of the use of computers in the businesses with professional purposes until they reached the home. It was these synthetic images which collaborated to change the meaning of the PC initials, that is, from professional computer to personal computer.

The risk lies in the fact that those who manage those images respect the final viewer, at the moment they are presented to his/her eyes. That is, a transparent communication avoiding manipulation through them. If this doesn\u2019t happen, the written word takes again a prevailing place in the communicative process, and one goes back to the time of the appearance of print. It is in this regressive period where the human and social factors may be inserted, which generate different types of structures which do not change over time.

Although we are in one of the most dynamic sectors of the current interactive systems, we may come across in some places of the Old Continent with dogmas and their supporters, who slow down the whole creative process which entails working with these computer-made images. Now if the human and social factors are positive, in view of the constant advances of the graphic hardware and software, the only limit that exists is the imagination or originality at the moment of generating static and/or dynamic images in 2D and 3D.

Computer graphics and computer animation along with text, audio and video have been key in the momentum of the Internet phenomenon of the 20th century. In the new millennium they have been the main means of drawing the attention of the users to the screens of a great variety of devices of classical computing.

There are millions of users who decide to purchase computer equipment because of the images that they can see on the screens. These images gain in quality thanks to their generation from scratch and thanks also to myriad techniques, methods, algorithms, etc. related to computer graphics down to the access to the information stored in the database, such as decompressed files with multimedia information available via the interaction of the user through the various peripherals.

Virtual images are generated in 2D and/or 3D, thanks to democratization of the pixels in 1990 \u2013 2000. Users are now capable of generating images in movement for 3D starting from photographs with digital cameras or the commercial applications to generate films with computer animations.

In this whole productive process, whether it is at a personal or an industrial level the cost factor is always present, in the equation of maximum quality in the least possible time. To reach that goal it is necessary to resort to several quality attributes that have an influence from the design stage right until the programming of the applications used in the context of graphic informatics.

Our intention in the current handbook is to show an essential part of those strategies and also the latest breakthroughs in all that which is directly or indirectly related to computer graphics, computer animation, database, software quality, hypermedia, design, communicability, interfaces, cloud computing, augmented reality, human-computer interaction, computer-aided design, mixed reality, models, techniques and methods of computer science, etc.

Some of the works that make up the current compendium have been presented orally by their authors in the following international conference in Venice, Italy: SETECEC 2012 \u2013First International Conference on Software and Emerging Technologies for Education, Culture, Entertainment, and Commerce: New Directions in Multimedia Mobile Computing, Social Networks, Human-Computer Interaction and Communicability, and international symposium in Valle d'Aosta, Italy: CCGIDIS 2012 \u2013Second International Symposium on Communicability, Computer Graphics and Innovative Design for Interactive Systems.

The innovation and originality of those proposals has been the reason for which the authors have been invited to enlarge their works submitting again the new versions to an assessment process of said works by the members of the scientific committee. Consequently, these are works that have gone satisfactorily through a double process of international selection. In the next section a short introductory presentation of the research works that make up the current compendium is given:

The authors Luigi Barazzetti and  Marco Scaioni present an interesting work \u201cTransforming Images and Laser Scans into 3D Models\u201d where theory and practice converge in the context of 3D images. They introduce all details of the use of a laser scanner to obtain high quality three-dimensional images and accuracy in the maintenance and the virtual reconstruction of cultural heritage, for instance. Besides, they comprehensively describe computer vision and photogrammetric 3D modeling techniques that are mainly based on images or laser scans. The description further covers the integration of global navigation satellite system (GNSS) or theodolite data allowing precise geo-referencing. The main goal they have set themselves is to indicate the reliability and robustness of their combined use for real surveys. Simultaneously a set of real examples serves to illustrate each one of the advantages and disadvantages of the used methods and techniques. In these examples the resolution of complex problems and with reduced costs can be verified. Each one of the issues is approached in a didactic way which facilitates the understanding even of the diverse devices used for the measurements.

Chih-Fang Huang, Chih-Hsiang Liang, and En-Ju Lin are the authors of the chapter \u201cSound-Color Synaesthetic Effect Using Algorithmic Composition with Image for Emotional Release.\u201d In said research they present the qualities of music from an emotive point of view. This is one of the motivations why they spread their study to the field of colors. In this sense, they develop an emotional model for music based on studies of color theory. Another main goal is to present the emotions of the users of a special software starting from the color and the electronic sounds that these represent from the psychological point of view, an approach informed by studies belonging to the set of the music color synesthesis and emotion releasing effect. Throughout the research work the authors explain in a gradual, simple and thorough way each one of the presented concepts with their matching theoretical examples and also the use of a special software for the experiments they have carried out.

The research work \u201cUser-created Interior Design Service Concepts, Interfaces and Outlines for Augmented Reality\u201d has been made by the following authors Tiina Kym\ue4l\ue4inen and Sanni Siltanen. This research work presents us masterfully a design process and the implementation requirements of an interactive interior design system. In said system the use of two focus groups can be detected where the knowledge and the experiences of designers, bloggers and serious amateurs in the field of interior design interact transversally. In that transversal interaction of diverse professionals the presence of user-driven innovation can be seen. The strategies of the conformation of the professionals and the transversal knowledge to the different areas of knowledge are explained with examples. Examples are given, which describe all the aspects of the new technologies applied to augmented reality, the results of tests with real users, the realization of a special software for the three-dimensional visualization of interiors, etc. Each one of the stages of the research project is accompanied by its corresponding textual and graphical explanations. Finally the authors make a wide-ranging reflection about the learned lessons signaling the positive and negative aspects of the used technology and also the future lines of research before their conclusions.

In the context of artificial intelligence, Damijan Novak and Domen Verber show us in their research \u201cNew Generation of Artificial Intelligence for Real-time Strategy Games\u201d the importance of applying it to computer games. The text starts with a complete state of the art overview, where the main notions to which the current research work refers are defined. In it is a constant interrelation among real-time strategy, games and artificial intelligence. The authors signal the importance of these issues within the academic field since they have detected new fields for research, whether in the present or the immediate future. Besides, in this work an excellent comparison example in the open-source real time strategy game development tools can be seen. The main and secondary goals of the text are gradually developed answering to a set of rhetorical questions with practical examples. All of this makes the reading and understanding easier to the potential readers. Their proposal of an enhanced combat artificial intelligence algorithm is striking in a very positive way.

The authors of the chapter \u201cTowards Smart Mobile System for Public Bus Transportation\u201d are Mitja Krajnc, Vili Podgorelec, and Marjan Heri\u10dko. In it converge the use of multimedia mobile phones and the geographical information systems (GPS satellite) for the localization of public buses in the town of Maribor, Slovenia. The system allows the user to see the movement of the buses in the road network through a real-time visualization on a mobile phone. The digital information which replaces the analogical one such as the timetables of the buses on paper support increases the available options of those users to access the public transportation system such as the combination of buses to reach a given destination. Besides, ease of use of the system can be seen since it is based on Windows Phone. Finally a quality attribute of the interactive systems such as the prediction has been incorporated to the system. The goal pursued by its authors is to increase the alternatives the user has at the moment of accessing the interactive information without this having negative repercussions on the visualization of the information inside the interface of the mobile phone.

Under the title \u201cA Learning Environment based on Movement and Sound Interaction\u201d its authors Serena Zanolla, Sergio Canazza, Antonio Rod\ue0, Giovanni De Poli, and Gian Luca Foresti present an Interactive Multimodal Environment for E-learning called \u201cStanza Logo-Motoria.\u201d The experiments carried out with children for the learning of a second language such as English have allowed the authors to carry out several experiments with positive results. A special module has been developed and tested in this sense called English as a Second Language (ESL). The motivation for learning languages in an environment of these characteristics is presented in a detailed way with a special stress on other aspects such as the socialization of early age users through new interactive technologies. Along with this the authors introduce technological aspects of interactive design which have been evolving as the experiments were carried out.

Michele Argiolas, Claudia Loggia, Vittorio Tramontin, and Cristina Trois are the authors of the chapter called \u201cA Web Application to Support Decision Making Process Based on a Bottom-up Approach in Public Buildings\u2019 Green Retrofit.\u201d They introduce a GIS web application related to such topics as green retrofit, public buildings, cost management just to mention three examples. The proposal starts with a comprehensive state of the art in legislation issues as technique of the main and secondary topics approached by its authors. The work shows how the implemented system online may serve to curtail the costs of management of green retrofitting public buildings. The study shows a constant interrelation among several disciplines of the sciences. Each of the issues is very well referenced in the bibliography section and they are developed in a pleasant way for the reader non-specialized in the issues approached at the start. Besides, there is a comparative study of real cases which entail additional advantages to the work developed by each one of the authors of the current text. Finally we can state that we are in front of an excellent triadic example of interrelations among databases, access to the stored information and the management of public administration.

The wide range of the phenomenon of the social networks requires in many of the offered services a set of recommendations. In this sense the authors Sa\u161o Karakati\u10d, Vili Podgorelec, and Marjan Heri\u10dko show in their research work \u201cMerging Social Networks and Semantic Databases to Build Recommendation Service\u201d the potential of linking semantic databases and social networks. Starting from Facebook and Freebase they have fashioned a prototype that uses Java technology. The prototype takes into consideration each one of the key elements for the recommendation of an online service to the potential users of contents in TV format, videogames, music, etc. In this sense we see a novel low cost solution is little by little brought forward by the authors along the pages. Besides, they have made a series of tests with users at the moment of interacting with the software and the hardware used. The results obtained back up the transcendence of the current proposal, and also the future developments they will make in a short time.

The three-dimensional reconstruction of the religious-historical heritage in the province of Avila (Spain) is the main objective of \u201cComputer Graphics to Encourage the Participation in the Virtual Reconstruction of Cultural Heritage: The Example of the Monastery of  Nuestra Se\uf1ora del Risco\u201d author of the chapter whose title is Gonzalo Mart\uedn S\ue1nchez. Starting from a reality which reveals the passing of time and the scarce remains of the walls of the monastery, the author resorts to several techniques using static images in 2D and 3D, such as historical cartography, digital photography, commercial software and open source, etc., for three-dimensional reconstruction. Now the author points out that these techniques from the field of computer graphics must be accompanied by a previous historical study of that is intended to be rebuilt. In this sense the illustrations, as well as the used methodology can serve as a guide for other analogous works.

In \u201cDeveloping Design Guidelines for E-Learning Environments: A War Story\u201d its authors Laura Benvenuti, Maria Menendez Blanco, and Gerrit C. van der Veer, denote the importance of the design, development, evaluation of a system oriented at university e-learning. A comprehensive study of the educational system to be developed with Moodle highlights the intersection between pedagogical theory, interactive design computer programming, among other areas of knowledge when it refers to blended academic education. A course on webculture has allowed the authors to carry out analyses of the web design. The university students belonging to the social sciences (culture, psychology, etc.) have taken part in several processes of heuristic evaluation to reach the goals proposed by the authors of the current research work. The learned lessons, future lines of research and the conclusions are interesting sections to understand the acquired experiences and for new research in the context of the blended education, distance learning, Internet based learning environments to mention a few examples.

The author presents a heuristic study of the evolution of human factors with their educational and social consequences in the context of the pixel of Southern Europe during the years 1990 \u2013 2012, under the title \u201cPixels: Educational Structures and Power Systems.\u201d Simultaneously, it is used for the first time a special language where diachronism interacts with the synchronism of the real examples that are presented, related to the graphics software. Besides, in this first research work are combined different techniques of the social sciences to establish the nodes and links which make up the human and functional structure of all that related to the commercial and educational pixel. Lastly, a series of strategies is put forward to analyse the educational quality of some training centers with regard to computer graphics and its derivations",industry,124,not included
,to_check,core,'Pisa University Press',2008-02-16 00:00:00,core,una professione in evoluzione. forme e progetti di giornalismo nell'era digitale.,https://core.ac.uk/download/14694880.pdf,"La professione giornalistica vive nel terzo millennio un’evoluzione che non puo’ prescindere dagli sviluppi delle nuove tecnologie entrate a far parte in modo pervasivo del vivere comune della nostra societa’. Quello che in particolare si modifica non e’ tanto la professione in quanto tale, che mantiene la principale caratteristica di mediazione dell’informazione, ma l’organizzazione stessa del lavoro dei professionisti della notizia. La scelta dell’argomento della tesi parte dalla considerazione di questo dato di fatto, e la possibilita’ di aver svolto uno stage presso la redazione della free press torinese “Shop in the city”, e' stata l’occasione per indagare direttamente sul campo i meccanismi di fare informazione nell’era digitale.
Partendo da questa prospettiva si e’ cercato innanzitutto di offrire una panoramica dei cambiamenti della professione e dei professionisti all’interno della societa’; un’analisi degli sviluppi tecnologici che hanno contribuito in modo determinante al cambiamento del lavoro di giornalista; un’analisi delle nuove forme di giornalismo che si sviluppano nell’era digitale. Parallelamente, un capitolo-intermezzo viene dedicato a due autori, Richard Florida e McKenzie Wark, i quali, partendo da prospettive diametralmente opposte, arrivano entrambi alla considerazione che nell’era digitale cio’ che rappresenta una vera produttrice di valore per la societa’ e’ la creatività degli individui, che si colloca proprio nell’intreccio tra rete e metropoli, tra virtuale e reale: l’informale diventa la sorgente cui la società attinge per trovare i suoi nuovi contesti produttivi.
Queste considerazioni vengono applicate nella seconda parte del lavoro di tesi in cui viene analizzata la free press torinese “Shop in the city”, sia nella sua realta’ cartacea che soprattutto nella sua forma web. Ciò permette di applicare tutte le considerazioni fatte in fase teorica. In particolare l’analisi dei punti di forza e di debolezza della parte online servono come spunto per presentare un progetto di implementazione del sito seguendo i pilastri della filosofia del web 2.0 e di un maggior coinvolgimento dell’utente, volendo dimostrare che oggi non conta più essere a tutti i costi online con un sito vetrina: occorre, nel terzo millennio che viviamo, fornire un vero servizio agli utenti di modo che essi effettivamente utilizzino il sito e siano fidelizzati alla navigazione.
Lo scopo finale e’ quindi quello di fornire in prima istanza una mappatura cognitiva dello scenario mediale degli ultimi anni, che sia forse utile ai vari dibattiti che alimentano la scena informativa riguardo al futuro dei giornali e ai possibili sviluppi del campo giornalistico attraverso lo sfruttamento delle potenzialita’ dei new media; in seconda istanza, preso il caso pratico di una free press, si propone di offrire un esempio di come migliorare lo sfruttamento delle potenzialità di internet per fare informazione nell’era digitale.
In the third millennium the journalistic profession is going through an evolution and cannot ignore the new technological developments that have become an important and pervasive part of the shared lives of our society. It is not so much the profession itself which is changing, in the sense that it maintains it’s principal role of the mediation and distribution of information, but instead the way in which the work of news professionals is organised.
Based on this assumption, this work begins by offering an overview of the changes to the profession and to the professionals themselves; an analysis of the technological developments that have contributed to determining the changes in the work of a journalist and an analysis of the new forms of journalism that are developing in the digital era. In parallel, a part is dedicated to two authors, Richard Florida and McKenzie Wark, who both arrive at the conclusion that in the digital era that which represents a real increase in value for an organisation is the creativity of individuals, which can be seen as being placed in the crossover of network and metropolis,: the informal becomes the source on which the organisation draws to find it’s new production contexts.
These considerations are applied and discussed in the second part of the thesis in which the Turin-based free press ‘Shop in the city’ is analysed, both in it’s paper and web-based forms. This analysis allows the application of the theories discussed in the relevant part of the thesis and offers a practical example of the reality of journalism in the digital era. In particular, the analyses of the strengths and weaknesses of the online format provide a good basis for the development and presentation of an implementation project for the site, following the philosophies of web 2.0 and of increased user involvement, demonstrating that it is no longer enough simply to be online at all costs, or to only have a ‘window display site’: in this third millennium it is fundamental to provide a real service to users so that they can effectively navigate and make use of the site.
The final aim, therefore, is to provide in the first instance a cognitive map of the field over the past few years, something which may be of use in the various debates that rise in the sector about the future of newspapers and the possible future developments in the field of journalism through the use of new media techniques and methods. In the second instance, with regard to the practical case of a free paper, this thesis proposes an example of how to best make use of the power of the internet to provide information in the digital era",smart cities,126,not included
,to_check,core,'Pisa University Press',2090-04-29 00:00:00,core,sviluppo di un sistema di realtà aumentata per migliorare l'efficienza delle operazioni di assemblaggio dei componenti di carrozzeria di autovetture,,"Il lavoro svolto durante il periodo di tesi ha riguardato l’analisi delle criticità relative alle operazioni di assemblaggio dei componenti di carrozzeria sulla scocca delle autovetture, con particolare attenzione alla necessità di garantire la correttezza del loro posizionamento valutato in termini di “gap” e “flushness” rispetto ai componenti adiacenti.
Lo studio delle tecnologie e dei software riguardanti la realtà amentata ha permesso lo sviluppo di un sistema basato sul software development kit Unity e Augmented Reality Engine Vuforia, che supporta l’operatore nelle fasi di assemblaggio e regolazione.
L’applicativo sviluppato permette di guidare l’operatore in ogni step previsto dalla procedura di montaggio fornendo indicazioni puntuali, generate da un algoritmo interno, tramite l’interfaccia grafica interattiva.
Il sistema è stato sviluppato e successivamente testato tramite un’attrezzatura appositamente progettata e realizzata al fine di simulare l’assemblaggio della parte anteriore di una vettura comprendente parafango laterale, paraurti, cofano e proiettore.
The work carried out during the thesis period concerned the analysis of the critical issues relating to the assembly operations of body components on passenger cars, with particular attention to the need to ensure the correctness of their positioning, assessed in terms of gap and flushness measured to the adjacent components.
The study of technologies and software relating to the augmented reality has allowed the development of a system based on the software development kit Unity and the Augmented Reality Engine Vuforia, which supports the operator in the assembly and adjustment phases.
The developed application guides the operator in each step of the assembly procedure providing precise indications, generated by an internal algorithm, via the interactive graphic interface.
The system was developed and subsequently tested using specially designed equipment in order to simulate the assembly of the front part of a car including a side mudguard, bumper, hood and headlamp",multimedia,127,not included
,to_check,core,National Academy of Public Administration under the President of Ukraine,2021-03-01 00:00:00,core,вироблення стратегічних підходів та практичних рекомендацій із протидії поширенню в україні дезінформації засобами соціальних медіа,,"Problem formulation. The problem of organizing state and non-state institutions to combat the spread of fakes in the information space is to establish a parity (balance) between: citizens right to receive reliable/legal information protection, freedom of speech protection and expression (civil liberties) and protection of national interests of Ukraine. The existing imbalance caused by the lack of effective mechanisms to counteract misinformation and its dissemination through social media. In the EU countries at the state level the fight is not against misinformation, but with illegal (harmful, dangerous) content in the media and social media, which is ineffective for Ukraine in the face of existing challenges and threats related to the military conflict and occupation of part of state territory, as well as the Covid-19 pandemic.
Previously unresolved parts of the overall problem. An unresolved part of the general problem is the lack of proper scientific substantiation of state mechanisms to combat disinformation and its dissemination in the media and social media.
The aim of the article is to develop strategic approaches and practical recommendations for combating misinformation and its dissemination in Ukraine by social media in the context of information and psychological attacks and the Covid-19 pandemic.
Main material. Digital technologies and information flows form practically all social processes faced by modern society. Thanks to built-in microprocessors, algorithmic devices and information systems combine texts, sounds and images that are easily stored and reproduced in digital format. The transfer of mass communication to the Internet has led to the transition of target audiences from traditional media to social networks. Without diminishing the benefits, opportunities and prospects that the digital world opens to humanity, it is necessary to actualize the significance of the negative side of the digital development of mass communication. Digital realities open up new possibilities for the introduction of various manipulation tools, among which misinformation is especially dangerous.
Post-truth interpretation refers to artificial circumstances in which the emotions and personal beliefs of target audiences are more important than objective facts and evidence that become irrelevant, particularly in shaping public opinion and influencing the mass consciousness. Post-truth is not just the opposite of truth, but rather a phenomenon of purposeful introduction of proactive communication, which as a set of different and interconnected actions, due to its recombined qualities and combination of different components misleads recipients of information in order to subdue them. The consequence is a decrease in the level of trust in the received information and media. People are confused, it becomes difficult for them to separate true information from false information, they do not realize what is true and what media can be trusted. The post-truth politics is realized through ""soft"" and gradual transformation of the system of national values and moral principles into so-called alternative ""universal norms of morality"" to absolutize individual freedom, discredit expert opinions and rational discussions, and most importantly to destroy information sovereignty. One of the effective tools for implementing such a policy is gaslighting - a form of organized influence of suggestors (manipulators, politicians, officials) on the processes of public self-identification in order to disorient them and further subdue by provoking uncertainty and doubts about the adequacy of their perception of reality.
Victims of gaslighting no longer trust their values, ideals and beliefs, and in despair they give up and by default fall under the influence of manipulators. Such people do not even realize that they are consumers of misinformation. Global networks are becoming a suitable environment for gas writers, as the constant presence of the vast majority of users on social networks and media channels is caused by the growing level of Internet addiction and digital autism, which create a person first addiction and secondly dangerous state of inability to maintain psychological support. languages with other individuals. That is, the digital transformation of everyday life contributes to the loss of people's communication skills of real communication. The loss of the ability to think adequately, independently and analytically is also negative, because the content and meaning of what is happening around them is formed by social media and digital algorithms of artificial intelligence.
Influencing citizens by external propagandists’ manipulation means disseminate not only criminal and illegal content through social media, but also directly implement political scenarios of changes in national values, the overthrow of the government, provoke armed conflicts, and so on. Disinformation contributes to a geometrically progressive increase in the incidence of the population, which leads to the introduction by the authorities of some regulatory (remediation) measures that may impair freedom of speech and freedom of expression, restrict freedom of assembly and access to public information.
In order to obtain the necessary parity between the appropriateness of the application of regulatory measures to combat misinformation to protect the right to reliable and lawful information and the exercise of civil liberties (rights to freedom expression), it is necessary to legally define the grounds, limits and criteria limitation. The dissemination of harmful information cannot be overcome post-factum with the help of targeted sanctions, under these conditions rapid preventive measures are needed. In this context, the media play a key role in combating fakes and are accountable to the state and society for failing to provide accurate and reliable information to the public.
Social media units, multinational corporations (digital producers), administrators of social networking platforms, non-governmental organizations and international institutions, civil society and independent media are the subjects of counteraction to the spread of disinformation by social media.
Conclusions and recommendations. The significance of the negative consequences of the digital development of mass communication is actualized, the virtual reality of which opens new possibilities for the introduction of various manipulation tools, among which misinformation carries a special danger. It has been proven that misinformation is successfully disseminated through aggressive information campaigns and lack of knowledge about threats and existing vulnerabilities of society, and more effectively affects target audiences by disrupting the interaction (cooperation and communication) of actors to counter its dissemination at the national and international levels. It is substantiated that modern management processes are formed under the influence of post-truth policy, one of the effective tools of which is gaslighting.
The main factors that contribute to the spread of misinformation through social media are identified: the effectiveness of the impact on the mass consciousness; scale, high speed and rate of distribution; stability of online presence (causes cumulative effect); intentional promotion of content by artificially adding it to disseminated information flows through popular media and resources; high quality, multidimensionality and variety of fake products, which are perfectly combined in the form of texts, images and videos; easy availability, high-tech and convenience of software for creating fakes.
The necessity of disseminating positive Ukrainian narratives by social media as a tool to counteract misinformation and information defense of the country has been proved. It is proposed to involve the best representatives of the Ukrainian elite: patriotic politicians and public figures (opinion leaders), bloggers, volunteers, scientists, cultural and artistic figures for this case.
It is substantiated that state public policy subjects of counteracting disinformation and its dissemination (Center for Counteracting Disinformation of the National Security and Defense Council of Ukraine, Center for Strategic Communications and Information Security of the Ministry of Culture and Information Policy of Ukraine and National Council of Ukraine on Television and Radio Broadcasting) flows in the middle of the country, if necessary, applying the necessary remedial action, study the target audiences, identify, analyze and assess their vulnerabilities (public fears, doubts, etc.), predict possible information attacks on Ukraine. It is necessary to constantly cooperate with digital media and mass media through information and communication campaigns.
Practical recommendations were given to state/non-state subjects of counteraction of misinformation on struggle in Ukraine against production of fake content by means of social media. It is necessary:

to create a favorable socio-economic, institutional-legal and information-communicative environment for quality and independent journalism, as well as an independent community of fact-checkers as a form of journalistic control to verify the facts used in texts, speeches, social networks posts and identify possible inconsistencies;
to disseminate accurate, reliable and objective information on the basis of responsible and independent journalism, to prevent the dissemination of fabricated and manipulative content;
to produce positive Ukrainian narratives as a tool of information defense, to prevent the emergence of fakes (instead of refuting fakes - to act in advance), promptly and without explanation to remove/block malicious content in accordance with the law and ""community policy"";
to set requirements for the media platforms transparency (their activities, funding sources, owners), define criteria and algorithms for ranking information, as well as legally and legitimately require social media to remove illegal content and block its distributors;
to monitor, moderate and control the social media content on the basis of legality, transparency and compliance with international human rights standards with the use of means to protect them;
to regulate and control the distribution of advertising on news resources of the media and social media, in particular to ensure verification of the transparency and veracity of its sponsored political content, as well as limit of its targeting in order to minimize the income of disinformation disseminators;
to promote digital services of voluntary users identification in the online space;
to ensure easy, fast and reliable access to all official documents, regulated by the Council of Europe Convention on Access to Official Documents, ratified by Ukraine in 2020 (Council of Europe Convention on Access to Official Documents, 2009);
to delegate to search platforms and social networks to determine the truth/falsity of information using artificial intelligence algorithms, remembering that these algorithms can be used to detect/remove misinformation, and vice versa - to create fake information (for example, to form the necessary public opinion misinformation is directed to a specific target audience, taking into account its features and characteristics through algorithmic analysis of messages on social networks, preferences, a selection of sources of user content);
to ensure that the majority of the population masters the media literacy and cyberhygiene basics, in particular to encourage and promote the introduction of specialized media programs on information and digital literacy for most people to understand the functions and ramifications of artificial intelligence algorithms, the ability to make informed decisions minimize the impact of threats (risks) associated with the use of such systems, in cooperation with relevant stakeholders, including the private sector, the media, civil society, educational institutions and scientific and technical institutions.

Key words: post-truth politics, misinformation, gaslighting, mass media, social media, social networks, fact-checking, media literacy, information campaigns, infodemia, digital technologies, digital transformations, artificial intelligence algorithms.У статті охарактеризовано проблеми організації боротьби із поширенням фейків в інформаційному просторі, які пов’язані з відсутністю ефективних державних механізмів протидії дезінформації та її поширенню засобами соціальних медіа. Доведено, що в країнах ЄС здійснюється боротьба не з дезінформацією, а з незаконним (шкідливим, небезпечним) контентом, що поширюється у ЗМІ та соціальних мережах, що є неефективним в умовах існуючих викликів та загроз, які пов’язані з військовим конфліктом та окупацією частини території України, а також пандемією Сovid-19, спричиненої коронавірусом SARS-CoV-2. Актуалізовано значущість негативних наслідків цифрового розвитку масової комунікації, реалії якого відкривають нові можливості запровадження різних інструментів маніпулювання, особливу небезпеку серед яких становить дезінформація. Охарактеризовано управлінські процеси, які формуються під впливом політики постправди, одним з ефективних інструментів реалізації якої є газлайтинг. Визначено основні фактори, які сприяють поширенню дезінформації засобами соціальних медіа. Доведено необхідність поширення засобами соціальних медіа позитивних українських наратив як інструменту протидії дезінформації та здійснення інформаційної оборони країни. Визначено державні суб’єкти вироблення публічної політики з протидії дезінформації та її поширенню в Україні, а також надано практичні рекомендації щодо боротьби з продукуванням фейкового контенту засобами соціальних медіа.
Ключові слова: політика постправди; дезінформація; газлайтинг; засоби масової комунікації; засоби масової інформації; соціальні медіа; соціальні мережі; фактчекінг; медіаграмотність; інформаційні кампанії; інфодемія; цифрові технології; цифрові трансформації; алгоритми штучного інтелекту",multimedia,128,not included
,to_check,core,"An embedded deep learning system for augmented reality in firefighting
  applications",2020-09-22 00:00:00,core,http://arxiv.org/abs/2009.10679,,"Firefighting is a dynamic activity, in which numerous operations occur
simultaneously. Maintaining situational awareness (i.e., knowledge of current
conditions and activities at the scene) is critical to the accurate
decision-making necessary for the safe and successful navigation of a fire
environment by firefighters. Conversely, the disorientation caused by hazards
such as smoke and extreme heat can lead to injury or even fatality. This
research implements recent advancements in technology such as deep learning,
point cloud and thermal imaging, and augmented reality platforms to improve a
firefighter's situational awareness and scene navigation through improved
interpretation of that scene. We have designed and built a prototype embedded
system that can leverage data streamed from cameras built into a firefighter's
personal protective equipment (PPE) to capture thermal, RGB color, and depth
imagery and then deploy already developed deep learning models to analyze the
input data in real time. The embedded system analyzes and returns the processed
images via wireless streaming, where they can be viewed remotely and relayed
back to the firefighter using an augmented reality platform that visualizes the
results of the analyzed inputs and draws the firefighter's attention to objects
of interest, such as doors and windows otherwise invisible through smoke and
flames.Comment: Accepted to ICMLA Special Session on Deep Learnin",multimedia,129,not included
,to_check,core,"Effects of Virtual Reality During Rowing Ergometry on Presence, Perceived Exertion, and Exercise Enjoyment",2020-02-17 00:00:00,core,https://core.ac.uk/download/287781133.pdf,TopSCHOLAR®,"Physical inactivity is associated with a host of negative health outcomes. Approximately 80% of Americans do not meet minimum levels of recommended physical activity. Virtual reality (VR) may improve exercise outcomes by enhancing presence, decreasing perceived exertion, and increasing exercise enjoyment. PURPOSE: To assess the effects of a proprietary VR interface on presence, perceived exertion, and exercise enjoyment during rowing ergometry. METHODS: First, we developed a novel VR software program for rowing ergometry. Subsequently, sixteen apparently healthy, recreationally active individuals (12M, 4F; 35.5 ± 13.9 y; 174.5 ± 10.1 cm; 80.4 ± 12.8 kg; VO2max: 38.1 ± 5.6 mL/kg/min) were familiarized with the rowing ergometer and VR software, and then completed a VO2max test during two separate sessions. Finally, subjects performed four, 30-min rowing sessions in a randomized, counterbalanced order at maximal voluntary intensity in four different conditions: 1) no augmented visual or audio stimuli (CON), 2) no augmented visual stimuli with self-selected music (MUS), 3) screen-based environmental display (SB), and 4) a virtual reality environment (VR). Presence (Spatial Presence Experience Scale), perceived exertion (Borg 6-20 scale), and enjoyment (Exercise-Induced Feelings Inventory) were assessed using questionnaires. Data (mean ± SD) were analyzed by repeated measures ANOVA and appropriate Tukey’s post hoc tests. Alpha was set at P \u3c 0.05. RESULTS: Eight of twenty spatial presence items indicated an enhanced experience in VR vs. SB (P \u3c 0.05). Perceived exertion (CON: 14.7 ± 2.1; MUS: 14.9 ± 2.0; SB: 15.2 ± 2.5; VR: 14.9 ± 1.7) and exercise-induced feelings were not different between conditions (P \u3e 0.05). CONCLUSION: The pilot version of the VR software for rowing ergometry did not reduce perceived exertion or increase exercise enjoyment in recreationally active individuals, although it did facilitate improved user presence compared to a screen-based enhanced environment. Added features, such as better coupling of rowing intensity to boat velocity in VR may further enhance presence and immersion, thereby decreasing perceived exertion and increasing exercise enjoyment",multimedia,130,not included
,to_check,core,"DeepRacer: Educational Autonomous Racing Platform for Experimentation
  with Sim2Real Reinforcement Learning",2019-11-04 00:00:00,core,http://arxiv.org/abs/1911.01562,,"DeepRacer is a platform for end-to-end experimentation with RL and can be
used to systematically investigate the key challenges in developing intelligent
control systems. Using the platform, we demonstrate how a 1/18th scale car can
learn to drive autonomously using RL with a monocular camera. It is trained in
simulation with no additional tuning in physical world and demonstrates: 1)
formulation and solution of a robust reinforcement learning algorithm, 2)
narrowing the reality gap through joint perception and dynamics, 3) distributed
on-demand compute architecture for training optimal policies, and 4) a robust
evaluation method to identify when to stop training. It is the first successful
large-scale deployment of deep reinforcement learning on a robotic control
agent that uses only raw camera images as observations and a model-free
learning method to perform robust path planning. We open source our code and
video demo on GitHub: https://git.io/fjxoJ",multimedia,131,not included
,to_check,core,Deep Learning Development Environment in Virtual Reality,2019-06-13 00:00:00,core,http://arxiv.org/abs/1906.05925,,"Virtual reality (VR) offers immersive visualization and intuitive
interaction. We leverage VR to enable any biomedical professional to deploy a
deep learning (DL) model for image classification. While DL models can be
powerful tools for data analysis, they are also challenging to understand and
develop. To make deep learning more accessible and intuitive, we have built a
virtual reality-based DL development environment. Within our environment, the
user can move tangible objects to construct a neural network only using their
hands. Our software automatically translates these configurations into a
trainable model and then reports its resulting accuracy on a test dataset in
real-time. Furthermore, we have enriched the virtual objects with
visualizations of the model's components such that users can achieve insight
about the DL models that they are developing. With this approach, we bridge the
gap between professionals in different fields of expertise while offering a
novel perspective for model analysis and data interaction. We further suggest
that techniques of development and visualization in deep learning can benefit
by integrating virtual reality",multimedia,132,not included
,to_check,core,'MDPI AG',2018-12-01 00:00:00,core,hyperparameter optimization for image recognition over an ar-sandbox based on convolutional neural networks applying a previous phase of segmentation by color–space,10.3390/sym10120743,"Immersive techniques such as augmented reality through devices such as the AR-Sandbox and deep learning through convolutional neural networks (CNN) provide an environment that is potentially applicable for motor rehabilitation and early education. However, given the orientation towards the creation of topographic models and the form of representation of the AR-Sandbox, the classification of images is complicated by the amount of noise that is generated in each capture. For this reason, this research has the purpose of establishing a model of a CNN for the classification of geometric figures by optimizing hyperparameters using Random Search, evaluating the impact of the implementation of a previous phase of color&#8315;space segmentation to a set of tests captured from the AR-Sandbox, and evaluating this type of segmentation using similarity indexes such as Jaccard and S&#248;rensen&#8315;Dice. The aim of the proposed scheme is to improve the identification and extraction of characteristics of the geometric figures. Using the proposed method, an average decrease of 39.45% to a function of loss and an increase of 14.83% on average in the percentage of correct answers is presented, concluding that the selected CNN model increased its performance by applying color&#8315;space segmentation in a phase that was prior to the prediction, given the nature of multiple pigmentation of the AR-Sandbox",multimedia,133,not included
,to_check,core,Improving computer lipreading via DNN sequence discriminative training techniques,2017-08-01 00:00:00,core,https://core.ac.uk/download/83923892.pdf,'International Speech Communication Association',"Although there have been some promising results in computer lipreading, there has been a paucity of data on which to train automatic systems. However the recent emergence of the TCD-TIMIT corpus, with around 6000 words, 59 speakers and seven hours of recorded audio-visual speech, allows the deployment of more recent techniques in audio-speech such as Deep Neural Networks (DNNs) and sequence discriminative training. In this paper we combine the DNN with a Hidden Markov Model (HMM) to the, so called, hybrid DNN-HMM configuration which we train using a variety of sequence discriminative training methods. This is then followed with a weighted finite state transducer. The conclusion is that the DNN offers very substantial improvement over a conventional classifier which uses a Gaussian Mixture Model (GMM) to model the densities even when optimised with Speaker Adaptive Training. Sequence adaptive training offers further improvements depending on the precise variety employed but those improvements are of the order of ~10\% improvement in word accuracy. Putting these two results together implies that lipreading is moving from something of rather esoteric interest to becoming a practical reality in the foreseeable future",multimedia,134,not included
,to_check,core,How soon will Design Education be Able to Benefit from Computer Aided Design Systems?,2009-09-15 00:00:00,core,https://core.ac.uk/download/230298844.pdf,Studies in Design Education Craft & Technology,"I gave a paper recently to a conference organised by the Design Council in the Royal Institution on Computers and 3D product design education. That paper was entitled 'How soon will CAD be able to aid design education?' and I quote first from my conclusion to that paper.'Simple architectural type forms· have just become an economic reality on low cost systems. I don't think any system is useful to the designer until it is low cost and thus we have only just seen the beginning of the real potential for using computer aided design in the design field. Within the next two years we will see systems able to cope with complex sculptured forms on low cost systems and within a further two years we should see artificial intelligence techniques being able to involve the designer in a genuine dialogue about his design intentions and helping him as he goes. All these tools will, of course, be linked directly to the means of production and this will completely change the role of the designer and the role of the manufacturer and their relative interaction. This has profound implications for design education'. IAlthough it is perhaps unwise of me to make public prophecies about when new software features will be available I think on this occasion I got it right. By the end of 1986 we will see on the market really valuable design tools running on low cost microprocessor based systems. So in this article I will attempt to do two things. First to show what the most powerful of the existing low cost systems are already able to achieve in terms of programs which aid design education; and second to describe those characteristics which I think are still necessary in the new and complete systems which I am anticipating becoming available before the end of this year",multimedia,136,not included
,to_check,core,2005 Rockefeller New Media Foundation Proposal,2007-01-04 00:00:00,core,https://hdl.handle.net/1813/5111,,"The Trial The Trail is an immersive virtual reality (VR) experience designed to engage the user as a
central protagonist in a compelling interactive drama. It is designed for a projection-based, 3-D stereo VR
display with one large screen or multiple screens forming a virtual theater. Immersive VR puts the user
inside the virtual world with the other characters rather than outside, viewing the world on a monitor and
manipulating an avatar of herself. This collaborative project brings together artificial intelligence and
visualization research with the goal of creating intelligent actor-agents and interactive, dramatic, virtual
reality experiences.
Our interactive drama is designed to create an unfolding story around the user which the agents
establish, populate, maintain and influence. We build our VR dramas using a two part structure; a
psychological substrate where we explicitly determine the emotional states we want to evoke in the user;
and an implementation level with three elements - an interactive script, a smart set, and actor-agents - that
turns the psychological plot into a dramatically evolving series of responsiveconundrums for the user",multimedia,137,not included
,to_check,core,'Bloomsbury Academic',,core,you were never really here: representations of artificial intelligence in charlie brooker's black mirror,,"Abstract

“Once memories and dreams, the dead and ghosts become technologically reproducible.”

Friedrich Kittler (1999:11)



Black Mirror (2011-) is a science-fiction television series written by Charlie Brooker and first broadcast on Channel 4 in the UK before moving to Netflix in 2016. The show takes an anthology approach, presenting each of its stories as a discrete episode commonly situated in dystopic, near-future settings. The series is orientated around three main themes; (1) the development, use and exploitation of technology, (2) the ethics related to the deployment of this technology by members of the public and corporations, and (3) an exploration of the nature of what constitutes consciousness specifically related to artificial intelligence (AI) and robotic technology. It is the third of these themes that this paper addresses, considering how the series presents artificial intelligence as a philosophical discourse within which the implications of the impact of technology on society can be concentrated both in terms of its future and present day applications.



In the first episode of Season Two, Be Right Back (Harris, 2013) this is expressed in a story of the recently widowed Martha (Hayley Atwell), who reconstructs a version of her deceased husband Ash (Domhnall Gleeson) through a machine-based consciousness that draws upon the data created from his lifetime of publicly available digital activity. Initially, Martha’s interaction with Ash’s AI-self is confined to a text-based chat bot. “The more it has, the more it’s him,” states Martha’s friend Sarah, who initially sets up the account and advises that she had communicated with the same software after her own husband died to help mitigate her grief. This exchange prompts Martha to add Ash’s record of emails, still images, audio and video to the dataset that AI-Ash is able to draw upon, the creation of such information a real-world process that Jose Van Dijick and Thomas Poell have described as the “datafication” of a life mediated online (2013). 



Now able to communicate with her aurally, AI-Ash gathers more data from Martha as they converse on the phone and she recounts moments that she had spent with her late husband. Eventually, Martha buys a humanoid android clone and imprints upon it Ash’s likeness, allowing AI-Ash’s transition to physical manifestation and to whom Martha provides even more data to the cloud-based storage where AI-Ash’s consciousness is stored. “You look well,” Martha observes, as Android-Ash emerges from a bath of nutrient gel. “The photos we keep tend to be flattering,” the android replies, offering the first sense that this embodied self is already inauthentic – encumbered by what Joseph Walther describes as “hyperpersonal”, or extremely selective, representation of Ash’s self familiar to all users of social media platforms (2011). Despite Martha’s intentions in continuing life with a new version of her husband, Android-Ash lacks the imperfections that form part of her understanding of his humanness.



This theme of replicating human consciousness through artificial intelligence technologies is presented in a number of Black Mirror episodes, including White Christmas (Tibbetts, 2014), San Junipero (Harris, 2016), U.S.S. Callister (Haynes, 2017) and Black Museum (McCarthy, 2017). As in so much other science fiction, from Frankenstein (Shelley, 1818) to Ex Machina (Garland, 2015), the series uses this notion of questioning the use of the technology to recreate or feign consciousness in order to present a wider discourse around notions of identity, memory and the formulation of the human self and subjectivity.



The artificial intelligence of Be Right Back deviates from the notion of consciousness and identity of these other examples however. As AI-Ash’s consciousness is constructed from fragments of the singular human Ash’s online activity and Martha’s memories of their interactions, so the AI can only ever hope to represent an augmented reality that can only reflect the mediated nature of Ash’s incomplete digital self. This imperfect copy of the human Ash is a simulacrum, a likeness, unable to ever attain what Martin Heidegger describes as having “an openness-of-being” (1977: xxxv).



Despite the AI’s physical existence through the Android-Ash body, the machine intelligence does not meet the Heideggerian definition of “Dasein” (1977) - determined by being both present within the world and directly relating with it. Instead it is encumbered to constantly adding or adjusting the incomplete digital memory that constitutes AI-Ash. Initially hoping the cyborg will serve as a continuation of her husband, Martha soon becomes frustrated by her engagement with the Android/AI-Ash, realising that her creation is something very different and Other. She begins to understand that AI-Ash can only ever be a simulation or poor substitution of her late husband, as the data from which its consciousness is drawn is by forever destined to remain incomplete. “You are not enough of him”, she acknowledges. “You aren’t you […] just a few ripples of you. There’s no history to you. You’re just a performance of stuff that he did without thinking.”



As Ex Machina presented questions as the nature of machine consciousness, Be Right Back offers a more direct critique of the present, rather than the near future. Charlie Brooker’s presentation of the possibility of life after death through artificial intelligence technologies, proposes a more immediate philosophical question. What is the nature of the human self already disrupted by a second life through the engagement with social media and other online technologies? The relationship between AI-Ash and his human counterpart is perhaps closer than Martha realises, with the episode having presented the human Ash as being similarly augmented. He too is disconnected from his reality with Martha - distracted by the draw of his phone and virtual spaces, and subsequently less aware or responsive to his own, real surroundings than his AI simulacrum. As Jean Baudrillard argues, a hyperreality is a substitution of the real with the signs of the real. AI-Ash is, in Baudrillard’s terms, a simulation – “no longer that of a territory, a referential being, or a substance. It is the generation of models of a real without origin or reality”. Brooker’s insight in this episode is not this representation of an inhuman simulacrum, but rather to comment critically on Ash’s social media present, as Baudrillard describes and as Martha despairs prior to her husband’s death, as “A hyperspace without atmosphere” (1983: 1-3).





 



Indicative Bibliography

Baudrillard, Jean. Foss, Paul, Patton, Paul and Beitchman, Phillip (Trans). 1983. Simulations. Los Angeles, CA: Semiotexte.



Blazer, Alex E. 2017. ‘That’s a bit creepy, what you’re doing”: Black Mirror and the Perverse Gaze,’ presented at SouthEast Coastal Conference on Languages & Literatures (SECCLL), Savannah, GA, USA, 2017. [WWW]

http://digitalcommons.georgiasouthern.edu/seccll/2017/2017/52

(Accessed 1 June 2019)



Brooker, Charlie. 2013. ‘Episode 2.01: “Be Right Back” Transcript’. In 8Flix. [WWW]

https://8flix.com/wp-content/uploads/2019/03/Black-Mirror-2-01-Be-Right-Back.pdf

(Accessed 1 June 2019).



Buckland, Warren. 2014. ‘Introduction: Puzzle Plots,’ in Hollywood Puzzle Films. New York, US: Routledge.



Cave, Stephen and Dihal, Kanta. 2019. ‘Hopes and fears for intelligent machines in fiction and reality’, in Nature Machine Intelligence, volume 1, pp.74-78 .

Cirucci, Angela M., Vacker, Barry (Eds). 2018. Black Mirror and Critical Media Theory. Lexington.



Dickerson, Lillian. 2017. ‘20 Recoding and Rebooting: Death and Rebirth Beyond Humanity in HBO’s Westworld’, in Endres, Thomas G. (Ed.) 2017. The Image of Rebirth in Literature, Media, and Society, Proceedings of the 2017 Conference of the Society for the Academic Study of Social Imagery. Greeley, US: University of Northern Colorado.



Garland, Alex. 2014. Ex Machina. Film 4/DNA Films.



Harris, Owen. 2013. ‘Be Right Back’. Black Mirror, Channel 4. Season 2, Episode 1, Broadcast 11 February 2013.



Harris, Owen. 2016. ‘San Junipero’. Black Mirror, Netflix. Season 3, Episode 4. , Broadcast 21 October 2016.



Haynes, Toby. 2017. ‘U.S.S. Callister’. Black Mirror, Netflix. Season 4, Episode 1, Broadcast 29 December 2017.

Heidegger, Martin. 1977. The Question Concerning Technology and Other Essays. Harper and Row.



Jiménez-Morales, Manel & Lopera Marmol, Marta. 2018. ‘Why Black Mirror is Really Written by Jean Baudrillard: A Philosophical Interpretation of Charlie Brooker’s Series’. In Cirucci, Angela M. and Vacker, Barry (Eds). 2018. Black Mirror and Critical Media Theory. Lexington.



Joy, Lisa and Nolan, Jonathan. 2016. ‘The Reality of A.I.: Westworld,’ in HBO, 10 October 2016. [WWW]

https://www.youtube.com/watch?v=SKTbFwyLuuM (Accessed 1 January 2018).

(Accessed 1 June 2019).



Kittler, Friedrich, Winthrop-Young, Geoffery & Wutz, Michael (Trans). 1999. Gramophone, film, typewriter. Stanford, California: Stanford University Press.



McCarthy, Colm. 2017. ‘Black Museum’. Black Mirror, Netflix. Season 4, Episode 6, Broadcast 29 December 2017.



Nolan, Jonathan. 2018. ‘Jonathan Nolan, Co-Creator, HBO’s Westworld. Interview with Kingsley Marshall’. Interview conducted 3 September 2018. Unpublished transcript.



Osterman, Cody. 2015. ‘Black Mirror, serial, and the affair: Popular culture’s obsession with memory’, presented at The Ray Browne Conference on Cultural and Critical Studies, Green State, Ohio, USA, 2015. Retrieved from http://scholarworks.bgsu.edu/rbc/2015conference/panel14/2/

(Accessed 1 June 2019).



Salem, Benadette. 2015. ‘Black mirror: technostruggles, capitalism, and media culture in the United Kingdom’. MA Thesis, University of Lancaster, UK. [WWW] https://www.academia.edu/19274981/Black_Mirror_Technostruggles_Capitalism_and_Media_Culture_in_the_United_Kingdom

(Accessed 1 June 2019)



Shelley, Mary. 1818. Frankenstein; or, The Modern Prometheus. London: Lackington, Hughes, Harding, Mavor, & Jones.

 

Singh, Greg. 2014. ‘Recognition and the image of mastery as themes in Black Mirror (Channel 4, 2011–present): an eco-Jungian approach to ‘always-on’ culture’. In 

International Journal of Jungian Studies, Vol. 6(2), pp.120-132.



Steenhaut , Sofie. 2017. ‘Between The Real And Simulated The Representation Of Mediated Relationships In Black Mirror’s “San Junipero” And “Be Right Back”. MA Thesis, Ghent University.



Tibbetts, Carl. 2014. ‘White Christmas’. Black Mirror, Channel 4. Season 2, Episode 4, Broadcast 16 December 2014.



Ungureanu, C. 2015. ‘Aestheticization of politics and ambivalence of self-sacrifice in Charlie Brooker's The National Anthem’. In Journal of European Studies, Vol. 45(1), pp.21-30.



Van Dijck, Jose. 2009. ‘Users like you? Theorizing agency in user-generated content’. In Media Culture Society, Vol 31(1), pp.41-58. 



Van Dijck, Jose & Poell, Thomas. 2013. ‘Understanding social media logic.’ In Media and Communication, 1(1), pp.2-14. 



Walther, Joseph, B. 2011. ‘Theories of computer mediated communication and interpersonal relations’. In Knapp, Mark. A. & Daly, John A. (Eds.). 2011. The SAGE handbook of interpersonal communication. Thousand Oaks, CA: SAGE Publications. pp.443-479",multimedia,138,not included
,to_check,core,Análise exploratória e comparativa da aplicação de agrupamento para combate à lavagem de dinheiro,2019-01-31 00:00:00,core,https://core.ac.uk/download/225626048.pdf,,"Context: Since 2007, through the National Anti-Corruption and Money Laundering Strategy (ENCCLA), the first LABLDs have been created, which are present now in all the regions of the federation and are responsible for policies to develop methods and advanced technologies to support the bodies of criminal prosecution. The need for innovation in this crime combat scene imposes partnerships, support, research and scientific method. The objective of this work was to evaluate the effectiveness of the Expectation-Maximization (EM) and K-Means algorithms on real financial transaction databases investigated by Sergipe's LABLDs, comparing the evidences found with the results obtained by mapping the state-of-the-art published in the literature. Method: Initially, a Survey was conducted with the premise of characterizing the use of techniques of storage, integration, Data Mining and Data Analytics by LABLDs and other investigative units throughout Brazil. Then, a systematic mapping was performed as a way to identify and systematize the main approaches, techniques and algorithms used in computer science to combat LD. Finally, a controlled in vivo experiment was designed and executed to compare the EM and K-Means algorithms. Results: It was found that approximately 97% of survey respondents did not directly use any data mining algorithm and that 30.99% evaluated their own knowledge about the subject as bad or very bad. Related to the state of the art, it has been identified that the main approaches used against LD are supervised classifiers and clusters. With the execution of the experimental process, it was evidenced that the algorithm EM surpasses the K-means algorithm, reaching a maximum average accuracy of 98.25%. Conclusions: This thesis exposed a hard reality within the main investigation and control bodies of our country. After analyzing the state of the art, it was evidenced that there are opportunities to explore solutions against LD, especially in the areas of Machine Learning and Deep Learning. Finally, the EM algorithm presented as a superior alternative to K-means for the implementation of a predictor module of suspicious transactions, confirming the results of the literature, however, in a real and specific investigation environment.Contexto: Desde 2007, por meio da Estratégia Nacional de Combate à Corrupção e à Lavagem de Dinheiro (ENCCLA), iniciou-se a criação dos primeiros Laboratórios de Tecnologia contra Lavagem de Dinheiro (LABLDs), os quais, hoje, estão presentes em todas as regiões da federação e são responsáveis por políticas de desenvolvimento de métodos e tecnologias de ponta para dar suporte aos órgãos de persecução penal. A necessidade de inovação neste cenário de combate ao crime impõe parcerias, apoio, pesquisas e método científicos. Objetivo: Este trabalho teve por proposito avaliar a eficácia dos algoritmos EM (Expectation–Maximization) e K-Means sobre bases de dados reais de transações financeiras investigadas pelos LABLDs de Sergipe, comparando as evidências encontradas com os resultados obtidos pelo mapeamento do estado da arte publicado na literatura. Método: Inicialmente, foi realizado um Survey com a premissa de caracterizar a utilização de técnicas de armazenamento, integração, Data Mining e Data Analytics pelos LABLDs e demais unidades investigativas em todo o Brasil. Em seguida, foi executado um mapeamento sistemático como forma de identificar e sistematizar as principais abordagens, técnicas e algoritmos usados na computação, para lutar contra a LD. Por fim, foi planejado e executado um experimento controlado, in vivo, para comparar os algoritmos EM e K-Means. Resultados: Constatou-se que aproximadamente 97% dos respondentes do survey não utilizavam diretamente algum algoritmo de mineração de dados e que 30,99% avaliavam o próprio conhecimento sobre o assunto como ruim ou péssimo. Para o estado da arte, foi identificado que as abordagens principais utilizadas contra LD são classificadores supervisionados e clusters. Com a execução do processo experimental, foi evidenciado que o algoritmo EM supera o algoritmo K-means, alcançando uma acurácia média máxima de 98,25%. Conclusões: Esta dissertação expôs a realidade dentro dos principais órgãos de investigação e controle do nosso país. Após ser analisado o estado da arte, evidenciou-se que há oportunidades para explorar soluções contra LD, principalmente nas áreas de Aprendizado de Máquina e Aprendizado Profundo. Finalmente, o algoritmo EM se apresentou como uma alternativa superior ao K-means, para a implementação de um módulo preditor de transações suspeitas, confirmando os resultados da literatura, todavia, em um ambiente real e específico de investigação.São Cristóvão, S",science,139,not included
,to_check,core,Business Models for Sustainability in the Agri-food Sector: the role of Management Control Systems,2016-02-12 00:00:00,core,https://core.ac.uk/download/79620509.pdf,'Pisa University Press',"Non c’è dubbio che la sostenibilità rappresenti uno dei più grandi imperativi del XXI secolo. Infatti, ora che povertà, disuguaglianze, degrado ambientale e umano hanno quasi compromesso la sopravvivenza della vita sulla Terra, la necessità di perseguire diversi paradigmi di sviluppo, produzione e consumo ha progressivamente catalizzato l’attenzione di un numero crescente di studiosi, professionisti e autorità di regolazione (nonchè della società in generale), per i quali la definizione di percorsi di cambiamento in grado di invertire le attuali tendenze globali è diventata una priorità di assoluto rilievo.
Nonostante i numerosi eventi internazionali recentemente avvenuti (primo fra tutti l’accordo sui Sustainable Development Goals da parte delle Nazioni Unite) abbiano consentito di aggiornare le priorità globali di sostenibilità alla luce dei mutati scenari mondiali, occorre sottolineare come, accanto alla definizione di nuovi obiettivi di sostenibilità, l’effettivo perseguimento delle sfide della sostenibilità debba passare attraverso un maggiore impegno verso la concreta attuazione di tali priorità da parte di tutti gli attori sociali.
Tra gli attori coinvolti nel processo di transizione verso la sostenibilità, le aziende rivestono certamente un ruolo di primo piano. Gli ultimi anni, infatti, hanno mostrato quanto possano essere devastanti gli effetti di comportamenti aziendali sconsiderati: frodi, violazioni delle norme in materia di lavoro e crescente inquinamento derivante dalle attività aziendali rappresentano solo alcuni degli eventi che hanno simboleggiato il fallimento del paradigma aziendale basato sulla massimizzazione del profitto nel creare società più giuste ed un ambiente naturale più salubre. Tale realtà ha interessato particolarmente le aziende operanti nel settore Agro-alimentare (dati sia il ruolo chiave rivestito dell’agricoltura nel contesto economico globale che le gravi conseguenze sui livelli di approvvigionamento alimentare causate dalle minacce ambientali derivanti dai paradigmi agricoli tradizionali e, più in generale, dalle attuali catene del valore Agro-alimentari) e, tra queste, le aziende multinazionali (data la severità degli impatti sociali e ambientali derivanti dal loro operato).
La crescente consapevolezza in merito all’insostenibilità delle attuali traiettorie di sviluppo aziendale ha pertanto portato sia aziendalisti che operatori a concentrarsi sulla ricerca di costrutti teorici che potessero sintetizzare la necessità per le aziende di abbracciare aspetti economici, sociali ed ambientali nei loro core business da un punto di vista integrato, con la conseguenza che molti concetti promuoventi l’adozione dei principi di sostenibilità a livello aziendale sono apparsi in letteratura nel corso degli anni. Tuttavia, la maggior parte di tali concetti è sembrata (e sembra tuttora) soffrire della mancanza di una visione integrata in merito alle diverse dimensioni della sostenibilità. E’ in questo contesto che il concetto di Corporate Sustainability, concepito come la massima espressione dell’integrazione tra le dimensioni di sostenibilità all'interno delle aziende, si è gradualmente affermato.
L’avvento della Corporate Sustainability, a sua volta, ha fatto emergere la necessità di cambiare progressivamente il modo in cui le aziende pensano a ciò che producono, a come producono ciò che esse producono, nonchè agli effetti che le loro attività hanno sulla società e l'ambiente, spingendo il mondo accademico a concentrarsi sul concetto di Modello di Business ed, in particolare, a mettere in discussione l’adeguatezza degli attuali Modelli di Business, accusati di non tenere debitamente in considerazione l’impatto degli aspetti sociali e ambientali sulla creazione di valore. In particolare, la necessità di concettualizzare Modelli di Business integranti al loro interno aspetti economici, sociali e ambientali ha fatto sì che il termine Modello di Business per la Sostenibilità si affermasse, non certo senza difficoltà. Il concetto di Modello di Business è infatti di per sé stato (ed è tuttora) oggetto di molteplici definizioni, che hanno portato ad un disaccordo generale su quale sia l’esatto significato del termine. Tale disaccordo si è successivamente spostato al concetto di Modello di Business per la Sostenibilità, come dimostrato dalle diverse concettualizzazioni attraverso i quali il termine è stato definito nel corso del tempo.
Il disaccordo esistente in merito a quale sia la definizione esatta del termine Modello di Business per la Sostenibilità non ha tuttavia indebolito la necessità di allontanarsi dalle attuali traiettorie di sviluppo aziendale per abbracciare un nuovo modo di pensare alle aziende, un modo secondo cui esse sono considerate come un elemento in grado di migliorare le condizioni sociali e ambientali mantenendo, allo stesso tempo, adeguati livelli di redditività. Al contrario, questo ha avuto importanti conseguenze in termini di ricerca: piuttosto che concentrarsi sull’elaborazione di specifiche e precise definizioni a priori di Modello di Business per la Sostenibilità, infatti, una parte della letteratura ha cominciato a concentrarsi sul processo di transizione dai Modelli di Business “tradizionali” ai Modelli di Business per la Sostenibilità (con una particolare attenzione al ruolo svolto dall'innovazione), nonché sulle possibili tipologie di Modelli di Business per la Sostenibilità che tale processo può originare. Inoltre, accanto alla necessità di mappare i principali cambiamenti per la sostenibilità all’interno dei Modelli di Business, è emerso il bisogno di allineare i Modelli di Business per la Sostenibilità con le rispettive strategie, bisogno che ha dato origine ad un interessante filone di ricerca all’interno della Corporate Sustainability. Nonostante strategia e Modello di Business siano ormai considerati come due concetti distinti, infatti, è stata messa in luce una forte correlazione tra essi.
Una volta emersa l’importanza di allineare Modelli di Business e strategie di sostenibilità, è stata evidenziata la necessità di individuare quegli elementi in grado di favorire tale allineamento. In questo senso, la letteratura ha mostrato come i sistemi di Contabilità Direzionale e, in particolare, i Sistemi di Controllo di Gestione rivestano un ruolo chiave sia nella formulazione che nell’attuazione delle strategie di sostenibilità. Tuttavia, mentre il rapporto tra Sistemi di Controllo di Gestione e strategie di sostenibilità è stato ampiamente analizzato in letteratura (e importanti risultati di ricerca sono stati raggiunti in questo senso), il rapporto tra Sistemi di Controllo di Gestione e Modelli di Business per la Sostenibilità risulta essere stato trascurato. Come sopra accennato, tuttavia, la stessa letteratura sembra essere concordare sempre più sul fatto che la corretta formulazione ed attuazione di strategie di sostenibilità debba essere accompagnata da modifiche coerenti nei Modelli di Business o, in altre parole, che i cambiamenti nelle strategie e nei Modelli di Business per la Sostenibilità debbano essere allineati tra loro, soprattutto con riferimento al grado di incorporazione degli aspetti di sostenibilità. Pertanto, il ruolo che i Sistemi di Controllo di Gestione hanno nell’allineare strategie e Modelli di Business per la Sostenibilità dovrebbe essere analizzato in maniera più approfondita.
Date tali premesse, il presente lavoro esplora il ruolo svolto dai Sistemi di Controllo di Gestione nell’allineare strategie e Modelli di Business per la Sostenibilità, attraverso un caso di studio su Barilla, multinazionale italiana operante nel settore Agro-alimentare. Dopo una breve introduzione sugli obiettivi e le motivazioni sottostanti alla ricerca (Capitolo 1), in particolare, l’elaborato offre una panoramica sullo sviluppo storico del concetto di sostenibilità (capitolo 2), per poi esplorare l'evoluzione del concetto di Corporate Sustainability (Capitolo 3) e successivamente analizzare il concetto di Modello di Business per la Sostenibilità, mettendo altresì in evidenza i suoi rapporti con quello di strategia di sostenibilità (Capitolo 4). Dopo aver fornito una panoramica sui temi della Contabilità Direzionale e del Controllo di Gestione per la sostenibilità (capitolo 5), l’elaborato illustra la metodologia (interpretativa) e il metodo (caso di studio) impiegati nel presente lavoro (Capitolo 6), per poi presentare il caso di studio Barilla, ed infine proporre alcune osservazioni conclusive, evidenziando i contributi principali e le limitazioni dello studio (capitolo 7).
Dal punto di vista del contributo scientifico, il lavoro mostra come, nel caso di studio ha analizzato, anche se il Sistema di Controllo di Gestione ha contribuito all'allineamento tra Modello di Business e strategia di sostenibilità, l’entità di tale contributo è stata limitata dalla carenza di integrazione tra strumenti di controllo “tradizionali” e strumenti orientati alla sostenibilità (cioè specificamente progettati per monitorare gli aspetti di sostenibilità). Pertanto, in questo senso il lavoro mira a contribuire a quel filone di ricerca sul tema del Controllo di Gestione che suggerisce come, al fine di perseguire efficacemente la sostenibilità, i Sistemi di Controllo di Gestione debbano evolversi, attraverso una migliore integrazione fra aspetti economici, ambientali e sociali, nonché evidenziando e misurando in modo più chiaro i trade-offs (tipici del concetto di Corporate Sustainability) spesso esistenti tra questi aspetti.
Come noto, l'adozione di un approccio qualitativo non consente di ottenere risultati statisticamente generalizzabili, poiché il suo scopo principale è quello di descrivere in dettaglio ed interpretare fenomeni sociali, senza fornire tuttavia modelli predittivi ed esplicativi. Inoltre, i risultati dello studio potrebbero essere influenzati dalla definizione di Modello di Business scelta. Come ampiamente illustrato nel lavoro, infatti, sono molte le definizioni di Modello di Business presenti in letteratura, e anche se la definizione di Modello di Business scelta è considerata “robusta” dalla letteratura, la scelta di una diversa definizione potrebbe portare a risultati diversi. Occorre tuttavia sottolineare come sia la stessa letteratura a suggerire, data l’eterogeneità nelle definizioni di Modello di Business, di scegliere la definizione più adatta a rispondere alla domanda di ricerca. In questo senso, si segnala come ulteriori analisi potrebbero esplorare il ruolo dei Sistemi di Controllo di Gestione nell’allineare Modelli di Business e strategie di sostenibilità in altre aziende, anche operanti in settori differenti, consentendo di confrontare i risultati emergenti dai vari casi di studio ed eventualmente rafforzare le evidenze fornite dal caso di studio oggetto del presente lavoro.
There is little doubt that sustainability represents one the greatest imperatives of the 21st century. In fact, now that poverty, inequality, environmental and human degradation have almost compromised the survival of life on Earth, the need to pursue different paradigms of development, production and consumption have progressively catalyzed the attention of a growing number of academics, practitioners and regulators (as well as society as a whole), to whom the definition of pathways of change able to reverse current global trends has become an absolute priority.
Despite several international events that have recently taken place (first of all the agreement on the United Nations Sustainable Development Goals) made possible to update global sustainability priorities in the light of changed scenarios, it must be underlined that, alongside the definition of new sustainability goals, the effective tackling of sustainability challenges should pass through a greater commitment to the concrete implementation of such priorities by all societal actors.
Among the actors involved in the process of transition towards sustainability, companies certainly play a prominent role. Recent years, in fact, have shown how devastating the effects of reckless corporate behavior can be: frauds, breach of work regulations, as well as increasing pollution caused by corporate activities represent just some of the events that symbolized the failure of profit maximization-oriented paradigm in creating fairer societies and healthier natural environment. Such reality has particularly affected companies operating in the Agri-food sector (given both the key role played by agriculture in the global economic scenario and the serious consequences on food supply levels caused by environmental threats deriving from traditional agricultural paradigms and, more generally, from current Agri-food value chains) and, among these, multinational companies (given the severity of social and environmental impacts deriving from their operate).
Growing awareness of unsustainability of Business-As-Usual trajectories led both business academics and practitioners to focus their efforts on the research of theoretical constructs that could synthetize the need for companies to embrace economic, social and environmental issues in their core business from an integrated standpoint, with the consequence that many concepts promoting the adoption of sustainability principles at company level appeared in literature over the years. However, the majority of such concepts seemed (and seems) to suffer of a lack of an integrated view about different sustainability dimensions. It is moving from this context that the concept of Corporate Sustainability, as the fullest expression of the integration between sustainability dimensions within companies, gradually established itself.
The advent of Corporate Sustainability, in turn, made emerge the need for a progressive change in the way in which companies think about what they produce, how they produce what they produce, as well as about the effects that their activities have on society and environment, leading scholarly efforts to focus on the concept of Business Model and, in particular, to question the adequacy of the existing Business Models, accused to do not take into consideration the impact of social and environmental aspects in the creation of value appropriately. In particular, the need to conceptualize Business Models in which social and environmental aspects are fully integrated with economic ones made emerge the term Business Model for Sustainability, certainly not without difficulties. The concept of Business Model per se has in fact been subject to multiple definitions over time, leading to a general disagreement about its exact meaning. Such disagreement has then subsequently shifted to the concept of Business Model for Sustainability, as showed by several conceptualizations through which the term has been defined over time.
Disagreement over the exact definition of Business Model for Sustainability, however, have not weakened the need to move away from Business-As-Usual trajectories in order to embrace a new way of thinking about companies, a way according to which companies are considered as a factor able to improve societal and environmental conditions keeping, at the same time, adequate profitability levels. On the contrary, this had important consequence in terms of research: rather than focusing on the elaboration of specific and precise a priori definitions of Business Model for Sustainability, in fact, an always more consistent part of the literature has begun to focus on the process of transition from “traditional” Business Models to Business Models for Sustainability (with a particular emphasis on the role played by innovation), as well as on possible typologies of Business Models for Sustainability that this process could originate. Moreover, alongside the need to map main changes towards sustainability in Business Models, the need to align Business Models for Sustainability with sustainability strategies emerged, originating an interesting field of research within Corporate Sustainability area. Despite strategy and Business Model are now considered as two distinct concepts, in fact, a close relationship between them has been highlighted by several authors.
Once the importance of aligning Business Model and strategy for sustainability has been highlighted, the need to individuate the elements that can favor such alignment emerged. In this sense, literature has showed that Management Accounting and, above all, Management Control Systems can play a key role both in formulation and implementation of sustainability strategies. Nevertheless, while the relationship between Management Control Systems and sustainability strategies has been extensively explored (and important research results have been achieved in this sense), the relationship between Management Control Systems and Business Models for Sustainability seems to be underexplored in literature. As above mentioned, however, the same literature seems to increasingly agree on the fact that the appropriate formulation and implementation of sustainability strategies should be accompanied by consistent changes in the Business Models or, in other words, changes in strategies and Business Models for Sustainability should be aligned between them, especially in terms of the degree of incorporation of sustainability issues. Therefore, the analysis of the role that Management Control Systems have in the implementation of Business Models for Sustainability and, above all, that of the role that Management Control Systems have in aligning strategies and Business Models for Sustainability should be analysed in more detail.
Given this premise, this work explores the role played by Management Control Systems in aligning sustainability strategies and Business Models for Sustainability. To do this, a case study on Barilla, an Italian multinational company operating in Agri-food sector, has been conducted. After a brief introduction to the objectives and motivations for the research (Chapter 1), in particular, the work provides an overview on the historical development of sustainability concept (Chapter 2), to then explore the evolution of the concept of Corporate Sustainability (Chapter 3) and analyse the concept of Business Model for Sustainability, highlighting its relationships with that of sustainability strategy (Chapter 4). Subsequently, after having provided an overview on Management Accounting and Control for sustainability (Chapter 5), methodology (interpretive) and method (case study) employed in the present work are illustrated (Chapter 6). Finally, after having presented the case study on Barilla, some concluding remarks are provided, and main contributions and limitations of the study are highlighted (Chapter 7).
In terms of scientific contributions, the work shows that, in the case study analysed, although the Management Control System contributed to the alignment between Business Model and sustainability strategy, the magnitude of such contribution was limited by the lack of integration between “traditional” management control tools and sustainability-oriented ones (specifically designed to monitor aspects of sustainability). Therefore, in this sense the work aims at contributing to that stream of Management Control research suggesting that, in order to effectively pursue sustainability, Management Control Systems must evolve, by better integrating economic, environmental and social aspects between them, as well as by highlighting and measuring trade-offs (typical of Corporate Sustainability concept) often existing between these aspects in a clearer way.
As known, the adoption of a qualitative approach does not allow to obtain statistically-generalizable results, since main aim of such approach is that of describing in detail, understanding and interpreting social phenomena without providing, however, predictive or explanatory models. Moreover, results of the study may be affected by the definition of Business Model chosen. As widely illustrated in the work, in fact, there are many definitions of Business Model in literature, and even if the definition of Business Model selected is considered a “robust” one, choosing a different definition could lead to different results. However, it must be underlined that is the same literature to suggest, given the heterogeneity in existing definitions, to choose the most suitable definition of Business Model to answer the research question. In this sense, further analyses could explore the role of Management Control Systems in aligning Business Models for Sustainability and sustainability strategies in other companies, also operating in different sectors, allowing to compare results emerging from various case studies, and eventually strengthening evidence provided by the case study object of the present work",science,140,not included
,to_check,core,10.1093/deafed/enj003,2005-01-01 00:00:00,core,sign language recognition and translation: a multidisciplined approach from the field of artificial intelligence,,"In recent years, research has progressed steadily in regard to the use of computers to recognize and render sign language. This paper reviews significant projects in the field beginning with finger-spelling hands such as &quot;Ralph&quot; (robotics), Cyber-Gloves (virtual reality sensors to capture isolated and continuous signs), camera-based projects such as the CopyCat interactive American Sign Language game (computer vision), and sign recognition software (Hidden Markov Modeling and neural network systems). Avatars such as &quot;Tessa&quot; (Text and Sign Support Assistant; three-dimensional imaging) and spoken language to sign language translation systems such as Poland’s project entitled &quot;THETOS&quot; (Text into Sign Language Automatic Translator, which operates in Polish; natural language processing) are addressed. The application of this research to education is also explored. The &quot;ICICLE&quot; (Interactive Computer Identification and Correction of Language Errors) project, for example, uses intelligent computer-aided instruction to build a tutorial system for deaf or hard-of-hearing children that analyzes their English writing and makes tailored lessons and recommendations. Finally, the article considers synthesized sign, which is being added to educational material and has the potential to be developed by students themselves. Technology is rapidly changing and improving the way the world operates. Barriers for people who are deaf are diminishing as projects of the past two decades have unfolded. Through the use of artificial intelligence, researchers are striving to develop hardware and software that will impact the way deaf individuals communicate and learn. This paper takes the reade",robotics,141,not included
,to_check,core,,,core,matter and memory and deep learning,,"The recent phenomenon of ‘Deep Learning,’ which has given us such science-fiction-like innovations as search tools in photographic applications and the growing reality of self-driving cars, is a new form, and subset, of ‘Machine Learning’ made possible by very recent innovations in computing.   Machine Learning itself has been around for some decades – essentially pattern-recognition software that requires very substantial computing resources, which were, until very recently, mostly theoretical and hard to come by.  Machine Learning was one avenue of the field of Artificial Intelligence known as Narrow A.I. – the kind of ‘artificial intelligence’ that was strictly limited in scope as a first-steps starting point of what came, as a result, to be known as General A.I.  General A.I., known then as simply, ‘Artificial Intelligence’, was the 1950s dream that brought us such things as Robbie the Robot, and more recently C3PO, and The Terminator: the kind of science fiction characters that remain the only manifestations of General Artificial Intelligence.



‘Deep Learning’ also continues engineering’s 1940s trend of using language in a way that I will contest in this paper: a co-opting of words that have been used, in the past, to describe human activities, using them instead to describe what engineers have managed to make machines do.  These co-optations reduce the richness of the word, making its referent an algorithm: a flow diagram that represents the bare essentials of what an engineer can understand and reproduce of a human activity; not the human activity itself.  This diagram of the ‘engineering possible’ over-simplifies the human activity it tries to depict.   With continued usage, the meaning of the word for us today has all-too-often become reduced to what the engineer has newly defined it to mean: something much less than it once was.  



In this paper I propose to attempt to roll back some of these co-optations, and to re-introduce some of the richness of the words that have been taken by engineering.  I shall examine Turing’s seminal paper on the notion of a thinking machine.  I shall be using the philosophical insights of Henri Bergson, especially in his book, Matter and Memory, and the discoveries of neuroscience and complexity scientists. I will try to show that the answer to Turing’s question, ‘Can machines think?’ remains a resounding, ‘No!’, and that notions such as ‘deep learning’ are in fact not only an inaccurate use of the very human experience of learning, but degrade the latter in using such a term",robotics,142,not included
10.1109/msm49833.2020.9201644,to_check,2020 International Conference Mechatronic Systems and Materials (MSM),IEEE,2020-07-03 00:00:00,ieeexplore,a preliminary investigation of an autonomous vehicle validation infrastructure for smart cities,https://ieeexplore.ieee.org/document/9201644/,"The research and development of autonomous vehicle has entered the era of commercialization. While the vehicle self-driving technology has been growing rapidly, the validation for autonomous vehicle in terms of driving model, human factor model and traffic model is still maturing. Most of previous infrastructures are mainly focused on validation of those three models separately resorting either on real driving test at physical infrastructure or software simulation in virtualized infrastructure. However, neither the real driving test can cover all possible scenarios of autonomous driving and human factors, nor the virtualized software simulation can generate a feasible model for practical on/off-road driving. Furthermore, future autonomous transport in smart cities requires comprehensive validation. In order for autonomous vehicles to meet the autonomous transport in such complex traffic environment, an integrated testing and simulation infrastructure has been built targeting the systematic validation for autonomous vehicles: the Multi-User Environment for Autonomous Vehicle Innovation (MUEAVI). A preliminary investigation of a new autonomous vehicle validation infrastructure that can serve a multitude of research projects for smart city is presented.",autonomous vehicle,143,not included
10.1109/icra40945.2020.9197024,to_check,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,adversarial appearance learning in augmented cityscapes for pedestrian recognition in autonomous driving,https://ieeexplore.ieee.org/document/9197024/,In the autonomous driving area synthetic data is crucial for cover specific traffic scenarios which autonomous vehicle must handle. This data commonly introduces domain gap between synthetic and real domains. In this paper we deploy data augmentation to generate custom traffic scenarios with VRUs in order to improve pedestrian recognition. We provide a pipeline for augmentation of the Cityscapes dataset with virtual pedestrians. In order to improve augmentation realism of the pipeline we reveal a novel generative network architecture for adversarial learning of the data-set lighting conditions. We also evaluate our approach on the tasks of semantic and instance segmentation.,autonomous vehicle,144,not included
10.1109/robot.1986.1087518,to_check,Proceedings. 1986 IEEE International Conference on Robotics and Automation,IEEE,1986-04-10 00:00:00,ieeexplore,architecture and early experience with planning for the alv,https://ieeexplore.ieee.org/document/1087518/,"This paper describes the software architecture and the initial algorithms that have proved to be effective for a real time robot planning system. The architecture is designed to incorporate planning technology from research on artificial intelligence while at the same time supporting the high performance decision making needed to control a fast-moving autonomous vehicle. The symbolic representation of the vehicle's plan is a key element in this architecture. Our initial algorithms use an especially efficient version of dynamic programming to find the best routes. The route is then translated into a symbolic plan. Replanning happens at several levels with the cost of replanning proportionate to the scope of the changes. This software is currently running in an environment which simulates the vehicle and perception systems, but it will be transferred to the DARPA Autonomous Land Vehicle built by Martin Marietta Denver Aerospace [Lowrie 86].",autonomous vehicle,145,included
10.1109/ictai.2019.00220,to_check,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),IEEE,2019-11-06 00:00:00,ieeexplore,learning to drive via apprenticeship learning and deep reinforcement learning,https://ieeexplore.ieee.org/document/8995417/,"With the implementation of reinforcement learning (RL) algorithms, current state-of-art autonomous vehicle technology have the potential to get closer to full automation. However, most of the applications have been limited to game domains or discrete action space which are far from the real world driving. Moreover, it is very tough to tune the parameters of reward mechanism since the driving styles vary a lot among the different users. For instance, an aggressive driver may prefer driving with high acceleration whereas some conservative drivers prefer a safer driving style. Therefore, we propose an apprenticeship learning in combination with deep reinforcement learning approach that allows the agent to learn the driving and stopping behaviors with continuous actions. We use gradient inverse reinforcement learning (GIRL) algorithm to recover the unknown reward function and employ REINFORCE as well as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal policy. The performance of our method is evaluated in simulation-based scenario and the results demonstrate that the agent performs human like driving and even better in some aspects after training.",autonomous vehicle,146,not included
10.1109/sysose.2017.7994953,to_check,2017 12th System of Systems Engineering Conference (SoSE),IEEE,2017-06-21 00:00:00,ieeexplore,autonomous decision making for a driver-less car,https://ieeexplore.ieee.org/document/7994953/,"Autonomous driving has been a hot topic with companies like Google, Uber, and Tesla because of the complexity of the problem, seemingly endless applications, and capital gain. The technology's brain child is DARPA's autonomous urban challenge from over a decade ago. Few companies have had some success in applying algorithms to commercial cars. These algorithms range from classical control approaches to Deep Learning. In this paper, we will use Deep Learning techniques and the Tensor flow framework with the goal of navigating a driverless car through an urban environment. The novelty in this system is the use of Deep Learning vs. traditional methods of real-time autonomous operation as well as the application of the Tensorflow framework itself. This paper provides an implementation of AlexNet's Deep Learning model for identifying driving indicators, how to implement them in a real system, and any unforeseen drawbacks to these techniques and how these are minimized and overcome.",autonomous vehicle,147,included
10.1109/aero.2018.8396807,to_check,2018 IEEE Aerospace Conference,IEEE,2018-03-10 00:00:00,ieeexplore,learning safe recovery trajectories with deep neural networks for unmanned aerial vehicles,https://ieeexplore.ieee.org/document/8396807/,"Unmanned vehicles that use vision sensors for perception to aid autonomous flight are a highly popular area of research. However, these systems are often prone to failures that are often hard to model. Previous work has focused on using deep learning to detect these failures. In this work, we build on these failure detection systems and develop a pipeline that learns to identify the correct trajectory to execute that restores the vision system and the unmanned vehicle to a safe state. The key challenge with using a deep learning pipeline for this problem is the limited amount of training data available from a real world system. Ideally one requires millions of data points to sufficiently train a model from scratch. However, this is not feasible for an unmanned aerial vehicle. The dataset we operate with is limited to 400-500 points. To sufficiently learn from such a small dataset we leverage the idea of transfer learning and non linear dimensionality reduction. We deploy our pipeline on an unmanned aerial vehicle flying autonomously through outdoor clutter (in a GPS denied environment) and show that we are able to achieve long durations of safe autonomous flight.",autonomous vehicle,148,included
10.1109/icit.2009.4939663,to_check,2009 IEEE International Conference on Industrial Technology,IEEE,2009-02-13 00:00:00,ieeexplore,real-time neural network based identification of a rotary-wing uav dynamics for autonomous flight,https://ieeexplore.ieee.org/document/4939663/,"Real time flight implementation of a neural network based black-box identification (NNID) scheme to a rotary wing unmanned aerial vehicle (RUAV) is presented in this paper. The applicability of NNID scheme for real time identification of longitudinal and lateral dynamics of the RUAV is evaluated in flight. To show the efficacy of the method for real time applications, the identification results and error statistics are provided. The challenges involved in terms of hardware implementation, computational time requirements, and real time coding are investigated and reported. Results indicate that NNID is suitable for modeling the dynamics of the RUAV in real time.",autonomous vehicle,149,included
10.1007/978-3-030-60467-7_7,to_check,Innovation and Research,Springer,2021-01-01 00:00:00,springer,intelligent and autonomous guidance through a geometric model for conventional vehicles,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-60467-7_7,"Cyber-physical systems (CPS) in the automobile industry are facing major challenges related to the use and validation of these CPS, which entails high costs in the implementation and training tests in the physical world, thus limiting research. Therefore, there is a need to shorten the validation times of these CPS with the use of 3D simulation software. This research article proposes to simulate a CPS in the simulation software Webots, with the aim of emulating the autonomous movement of conventional vehicles by integrating a GPS sensor and a compass sensor which provide information on location and orientation, these data are used for the implementation of a geometric model by vectors, the same one that is developed in a controller that allows to take actions on the vehicles in the simulation software in order to emulate an urban traffic. Finally, a series of configurations have been made to evaluate the geometric model, managing to maintain the default speed of 94.194% with curves greater than 90 degrees. In addition, the validation of this system in a real environment through the instrumentation in land vehicles is drawn as future lines.",autonomous vehicle,150,not included
http://arxiv.org/abs/2003.03576v1,to_check,arxiv,arxiv,2020-03-07 00:00:00,arxiv,"a machine learning environment for evaluating autonomous driving
  software",http://arxiv.org/abs/2003.03576v1,"Autonomous vehicles need safe development and testing environments. Many
traffic scenarios are such that they cannot be tested in the real world. We see
hybrid photorealistic simulation as a viable tool for developing AI (artificial
intelligence) software for autonomous driving. We present a machine learning
environment for detecting autonomous vehicle corner case behavior. Our
environment is based on connecting the CARLA simulation software to TensorFlow
machine learning framework and custom AI client software. The AI client
software receives data from a simulated world via virtual sensors and
transforms the data into information using machine learning models. The AI
clients control vehicles in the simulated world. Our environment monitors the
state assumed by the vehicle AIs to the ground truth state derived from the
simulation model. Our system can search for corner cases where the vehicle AI
is unable to correctly understand the situation. In our paper, we present the
overall hybrid simulator architecture and compare different configurations. We
present performance measurements from real setups, and outline the main
parameters affecting the hybrid simulator performance.",autonomous vehicle,151,not included
http://arxiv.org/abs/1801.05086v1,to_check,arxiv,arxiv,2018-01-16 00:00:00,arxiv,autonomous uav navigation using reinforcement learning,http://arxiv.org/abs/1801.05086v1,"Unmanned aerial vehicles (UAV) are commonly used for missions in unknown
environments, where an exact mathematical model of the environment may not be
available. This paper provides a framework for using reinforcement learning to
allow the UAV to navigate successfully in such environments. We conducted our
simulation and real implementation to show how the UAVs can successfully learn
to navigate through an unknown environment. Technical aspects regarding to
applying reinforcement learning algorithm to a UAV system and UAV flight
control were also addressed. This will enable continuing research using a UAV
with learning capabilities in more important applications, such as wildfire
monitoring, or search and rescue missions.",autonomous vehicle,152,included
http://arxiv.org/abs/2104.14006v1,to_check,arxiv,arxiv,2021-04-28 00:00:00,arxiv,"emergencynet: efficient aerial image classification for drone-based
  emergency monitoring using atrous convolutional feature fusion",http://arxiv.org/abs/2104.14006v1,"Deep learning-based algorithms can provide state-of-the-art accuracy for
remote sensing technologies such as unmanned aerial vehicles (UAVs)/drones,
potentially enhancing their remote sensing capabilities for many emergency
response and disaster management applications. In particular, UAVs equipped
with camera sensors can operating in remote and difficult to access
disaster-stricken areas, analyze the image and alert in the presence of various
calamities such as collapsed buildings, flood, or fire in order to faster
mitigate their effects on the environment and on human population. However, the
integration of deep learning introduces heavy computational requirements,
preventing the deployment of such deep neural networks in many scenarios that
impose low-latency constraints on inference, in order to make mission-critical
decisions in real time. To this end, this article focuses on the efficient
aerial image classification from on-board a UAV for emergency
response/monitoring applications. Specifically, a dedicated Aerial Image
Database for Emergency Response applications is introduced and a comparative
analysis of existing approaches is performed. Through this analysis a
lightweight convolutional neural network architecture is proposed, referred to
as EmergencyNet, based on atrous convolutions to process multiresolution
features and capable of running efficiently on low-power embedded platforms
achieving upto 20x higher performance compared to existing models with minimal
memory requirements with less than 1% accuracy drop compared to
state-of-the-art models.",autonomous vehicle,153,not included
http://arxiv.org/abs/2107.13869v2,to_check,arxiv,arxiv,2021-07-29 00:00:00,arxiv,"autonomous uav base stations for next generation wireless networks: a
  deep learning approach",http://arxiv.org/abs/2107.13869v2,"To address the ever-growing connectivity demands of wireless communications,
the adoption of ingenious solutions, such as Unmanned Aerial Vehicles (UAVs) as
mobile Base Stations (BSs), is imperative. In general, the location of a UAV
Base Station (UAV-BS) is determined by optimization algorithms, which have high
computationally complexities and place heavy demands on UAV resources. In this
paper, we show that a Convolutional Neural Network (CNN) model can be trained
to infer the location of a UAV-BS in real time. In so doing, we create a
framework to determine the UAV locations that considers the deployment of
Mobile Users (MUs) to generate labels by using the data obtained from an
optimization algorithm. Performance evaluations reveal that once the CNN model
is trained with the given labels and locations of MUs, the proposed approach is
capable of approximating the results given by the adopted optimization
algorithm with high fidelity, outperforming Reinforcement Learning (RL)-based
approaches. We also explore future research challenges and highlight key
issues.",autonomous vehicle,154,not included
http://arxiv.org/abs/1912.00752v3,to_check,arxiv,arxiv,2019-11-28 00:00:00,arxiv,"deep learning for optimal deployment of uavs with visible light
  communications",http://arxiv.org/abs/1912.00752v3,"In this paper, the problem of dynamical deployment of unmanned aerial
vehicles (UAVs) equipped with visible light communication (VLC) capabilities
for optimizing the energy efficiency of UAV-enabled networks is studied. In the
studied model, the UAVs can simultaneously provide communications and
illumination to service ground users. Since ambient illumination increases the
interference over VLC links while reducing the illumination threshold of the
UAVs, it is necessary to consider the illumination distribution of the target
area for UAV deployment optimization. This problem is formulated as an
optimization problem which jointly optimizes UAV deployment, user association,
and power efficiency while meeting the illumination and communication
requirements of users. To solve this problem, an algorithm that combines the
machine learning framework of gated recurrent units (GRUs) with convolutional
neural networks (CNNs) is proposed. Using GRUs and CNNs, the UAVs can model the
long-term historical illumination distribution and predict the future
illumination distribution. Given the prediction of illumination distribution,
the original nonconvex optimization problem can be divided into two
sub-problems and is then solved using a low-complexity, iterative algorithm.
Then, the proposed algorithm enables UAVs to determine the their deployment and
user association to minimize the total transmit power. Simulation results using
real data from the Earth observations group (EOG) at NOAA/NCEI show that the
proposed approach can achieve up to 68.9% reduction in total transmit power
compared to a conventional optimal UAV deployment that does not consider the
illumination distribution and user association.",autonomous vehicle,155,not included
http://arxiv.org/abs/1411.6326v1,to_check,arxiv,arxiv,2014-11-24 00:00:00,arxiv,vision and learning for deliberative monocular cluttered flight,http://arxiv.org/abs/1411.6326v1,"Cameras provide a rich source of information while being passive, cheap and
lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work
we present the first implementation of receding horizon control, which is
widely used in ground vehicles, with monocular vision as the only sensing mode
for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a
number of contributions: novel coupling of perception and control via relevant
and diverse, multiple interpretations of the scene around the robot, leveraging
recent advances in machine learning to showcase anytime budgeted cost-sensitive
feature selection, and fast non-linear regression for monocular depth
prediction. We empirically demonstrate the efficacy of our novel pipeline via
real world experiments of more than 2 kms through dense trees with a quadrotor
built from off-the-shelf parts. Moreover our pipeline is designed to combine
information from other modalities like stereo and lidar as well if available.",autonomous vehicle,156,included
http://arxiv.org/abs/2102.13253v1,to_check,arxiv,arxiv,2021-02-26 00:00:00,arxiv,"on the visual-based safe landing of uavs in populated areas: a crucial
  aspect for urban deployment",http://arxiv.org/abs/2102.13253v1,"Autonomous landing of Unmanned Aerial Vehicles (UAVs) in crowded scenarios is
crucial for successful deployment of UAVs in populated areas, particularly in
emergency landing situations where the highest priority is to avoid hurting
people. In this work, a new visual-based algorithm for identifying Safe Landing
Zones (SLZ) in crowded scenarios is proposed, considering a camera mounted on
an UAV, where the people in the scene move with unknown dynamics. To do so, a
density map is generated for each image frame using a Deep Neural Network, from
where a binary occupancy map is obtained aiming to overestimate the people's
location for security reasons. Then, the occupancy map is projected to the
head's plane, and the SLZ candidates are obtained as circular regions in the
head's plane with a minimum security radius. Finally, to keep track of the SLZ
candidates, a multiple instance tracking algorithm is implemented using Kalman
Filters along with the Hungarian algorithm for data association. Several
scenarios were studied to prove the validity of the proposed strategy,
including public datasets and real uncontrolled scenarios with people moving in
public squares, taken from an UAV in flight. The study showed promising results
in the search of preventing the UAV from hurting people during emergency
landing.",autonomous vehicle,157,not included
http://arxiv.org/abs/2009.14349v3,to_check,arxiv,arxiv,2020-09-30 00:00:00,arxiv,"computing systems for autonomous driving: state-of-the-art and
  challenges",http://arxiv.org/abs/2009.14349v3,"The recent proliferation of computing technologies (e.g., sensors, computer
vision, machine learning, and hardware acceleration), and the broad deployment
of communication mechanisms (e.g., DSRC, C-V2X, 5G) have pushed the horizon of
autonomous driving, which automates the decision and control of vehicles by
leveraging the perception results based on multiple sensors. The key to the
success of these autonomous systems is making a reliable decision in real-time
fashion. However, accidents and fatalities caused by early deployed autonomous
vehicles arise from time to time. The real traffic environment is too
complicated for current autonomous driving computing systems to understand and
handle. In this paper, we present state-of-the-art computing systems for
autonomous driving, including seven performance metrics and nine key
technologies, followed by twelve challenges to realize autonomous driving. We
hope this paper will gain attention from both the computing and automotive
communities and inspire more research in this direction.",autonomous vehicle,158,not included
http://arxiv.org/abs/1909.07554v1,to_check,arxiv,arxiv,2019-09-17 00:00:00,arxiv,"gated recurrent units learning for optimal deployment of visible light
  communications enabled uavs",http://arxiv.org/abs/1909.07554v1,"In this paper, the problem of optimizing the deployment of unmanned aerial
vehicles (UAVs) equipped with visible light communication (VLC) capabilities is
studied. In the studied model, the UAVs can simultaneously provide
communications and illumination to service ground users. Ambient illumination
increases the interference over VLC links while reducing the illumination
threshold of the UAVs. Therefore, it is necessary to consider the illumination
distribution of the target area for UAV deployment optimization. This problem
is formulated as an optimization problem whose goal is to minimize the total
transmit power while meeting the illumination and communication requirements of
users. To solve this problem, an algorithm based on the machine learning
framework of gated recurrent units (GRUs) is proposed. Using GRUs, the UAVs can
model the long-term historical illumination distribution and predict the future
illumination distribution. In order to reduce the complexity of the prediction
algorithm while accurately predicting the illumination distribution, a Gaussian
mixture model (GMM) is used to fit the illumination distribution of the target
area at each time slot. Based on the predicted illumination distribution, the
optimization problem is proved to be a convex optimization problem that can be
solved by using duality. Simulations using real data from the Earth
observations group (EOG) at NOAA/NCEI show that the proposed approach can
achieve up to 22.1% reduction in transmit power compared to a conventional
optimal UAV deployment that does not consider the illumination distribution.
The results also show that UAVs must hover at areas having strong illumination,
thus providing useful guidelines on the deployment of VLC-enabled UAVs.",autonomous vehicle,159,not included
http://arxiv.org/abs/2001.03864v1,to_check,arxiv,arxiv,2020-01-12 00:00:00,arxiv,"learning to drive via apprenticeship learning and deep reinforcement
  learning",http://arxiv.org/abs/2001.03864v1,"With the implementation of reinforcement learning (RL) algorithms, current
state-of-art autonomous vehicle technology have the potential to get closer to
full automation. However, most of the applications have been limited to game
domains or discrete action space which are far from the real world driving.
Moreover, it is very tough to tune the parameters of reward mechanism since the
driving styles vary a lot among the different users. For instance, an
aggressive driver may prefer driving with high acceleration whereas some
conservative drivers prefer a safer driving style. Therefore, we propose an
apprenticeship learning in combination with deep reinforcement learning
approach that allows the agent to learn the driving and stopping behaviors with
continuous actions. We use gradient inverse reinforcement learning (GIRL)
algorithm to recover the unknown reward function and employ REINFORCE as well
as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal
policy. The performance of our method is evaluated in simulation-based scenario
and the results demonstrate that the agent performs human like driving and even
better in some aspects after training.",autonomous vehicle,160,not included
http://arxiv.org/abs/1812.08273v1,to_check,arxiv,arxiv,2018-12-19 00:00:00,arxiv,analog signal processing using stochastic magnets,http://arxiv.org/abs/1812.08273v1,"We present a low barrier magnet based compact hardware unit for analog
stochastic neurons and demonstrate its use as a building-block for neuromorphic
hardware. By coupling circular magnetic tunnel junctions (MTJs) with a CMOS
based analog buffer, we show that these units can act as leaky-integrate-and
fire (LIF) neurons, a model of biological neural networks particularly suited
for temporal inferencing and pattern recognition. We demonstrate examples of
temporal sequence learning, processing, and prediction tasks in real time, as a
proof of concept demonstration of scalable and adaptive signal-processors.
Efficient non von-Neumann hardware implementation of such processors can open
up a pathway for integration of hardware based cognition in a wide variety of
emerging systems such as IoT, industrial controls, bio- and photo-sensors, and
Unmanned Autonomous Vehicles.",autonomous vehicle,161,not included
http://arxiv.org/abs/1712.02294v4,to_check,arxiv,arxiv,2017-12-06 00:00:00,arxiv,joint 3d proposal generation and object detection from view aggregation,http://arxiv.org/abs/1712.02294v4,"We present AVOD, an Aggregate View Object Detection network for autonomous
driving scenarios. The proposed neural network architecture uses LIDAR point
clouds and RGB images to generate features that are shared by two subnetworks:
a region proposal network (RPN) and a second stage detector network. The
proposed RPN uses a novel architecture capable of performing multimodal feature
fusion on high resolution feature maps to generate reliable 3D object proposals
for multiple object classes in road scenes. Using these proposals, the second
stage detection network performs accurate oriented 3D bounding box regression
and category classification to predict the extents, orientation, and
classification of objects in 3D space. Our proposed architecture is shown to
produce state of the art results on the KITTI 3D object detection benchmark
while running in real time with a low memory footprint, making it a suitable
candidate for deployment on autonomous vehicles. Code is at:
https://github.com/kujason/avod",autonomous vehicle,162,not included
10.1016/j.rse.2020.111717,to_check,Remote Sensing of Environment,scopus,2020-05-01,sciencedirect,assessing the relationship between macro-faunal burrowing activity and mudflat geomorphology from uav-based structure-from-motion photogrammetry,https://api.elsevier.com/content/abstract/scopus_id/85079899077,"Characterisation of the ecosystem functioning of mudflats requires insight on the morphology and facies of these coastal features, but also on biological processes that influence mudflat geomorphology, such as crab bioturbation and the formation of benthic biofilms, as well as their heterogeneity at cm or less scales. Insight into this fine scale of ecosystem functioning is also important as far as minimizing errors in upscaling are concerned. The realisation of high-resolution ground surveys of these mudflats without perturbing their surface is a real challenge. Here, we address this challenge using UAV-supported photogrammetry based on the Structure-from-Motion (SfM) workflow. We produced a Digital Surface Model (DSM) and an orthophotograph at 1 cm and 0.5 cm pixel resolutions, respectively, of a mudflat in French Guiana, and mapped and classed into different size ranges intricate morphological features, including crab burrow apertures, tidal drainage creeks and depressions. We also determined subtle facies and elevation changes and slopes, and the footprint of different degrees of benthic biofilm development. The results generated at this scale of photogrammetric analysis also enabled us to relate macrofaunal crab burrowing activity to various parameters, including mudflat elevation, spatial distribution and sizes of creeks and depressions, benthic biofilm distribution, and flooding duration. SfM photogrammetry offers interesting new perspectives in fine-scale characterisation of the geomorphology, benthic activity and degree of biofilm development of dynamic muddy intertidal environments that are generally difficult of access. The main shortcomings highlighted in this study are a drift of accuracy of the DSM outside areas of ground control points and the deployment of which perturb the mudflat morphology and biology, the water-logged or very wet surfaces which generate reconstruction artefacts through the sun glint effect, and the time-consuming task of manual interpretation of extraction of features such as crab burrow apertures. On-going developments in UAV positioning integrating RTK/PPK GPS solutions for image-georeferencing and precise orientation with high-quality inertial measurement units will limit the difficulties inherent to ground control points, while conduction of surveys during homogeneous cloudy conditions could reduce the sun-glint effect. Manual extraction of image features could be automated in the future through the use of deep-learning algorithms.",autonomous vehicle,163,not included
10.1016/j.ifacol.2020.12.1459,to_check,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,deep learning based segmentation of fish in noisy forward looking mbes images,https://api.elsevier.com/content/abstract/scopus_id/85105082300,"In this work, we investigate a Deep Learning (DL) approach to fish segmentation in a small dataset of noisy low-resolution images generated by a forward-looking multibeam echosounder (MBES). We build on recent advances in DL and Convolutional Neural Networks (CNNs) for semantic segmentation and demonstrate an end-to-end approach for a fish/non-fish probability prediction for all range-azimuth positions projected by an imaging sonar. We use self-collected datasets from the Danish Sound and the Faroe Islands to train and test our model and present techniques to obtain satisfying performance and generalization even with a low-volume dataset. We show that our model proves the desired performance and has learned to harness the importance of semantic context and take this into account to separate noise and non-targets from real targets. Furthermore, we present techniques to deploy models on low-cost embedded platforms to obtain higher performance fit for edge environments – where compute and power are restricted by size/cost – for testing and prototyping.",autonomous vehicle,164,included
10.1016/j.procs.2019.09.442,to_check,Procedia Computer Science,scopus,2019-01-01,sciencedirect,unmanned aerial vehicle in the machine learning environment,https://api.elsevier.com/content/abstract/scopus_id/85079097933,"Unmanned Aerial Vehicles and machine learning have started gaining attentions of academic and industrial research. The Unmanned Aerial Vehicles have extended the freedom to operate and monitor the activities from remote locations. This study retrieved and synthesized research on the use of Unmanned Aerial Vehicles along with machine learning and its algorithms in different areas and regions. The objective was to synthesize the scope and importance of machine learning models in enhancing Unmanned Aerial Vehicles capabilities, solutions to problems, and numerous application areas.
                  The machine learning implementation has reduced numbers of challenges to Unmanned Aerial Vehicles besides enhancing the capabilities and opening the door to the different sectors. The Unmanned Aerial Vehicles and machine learning association has resulted in fast and reliable outputs. The combination of Unmanned Aerial Vehicles and machine learning has helped in real time monitoring, data collection and processing, and prediction in the computer/wireless networks, smart cities, military, agriculture, and mining.",autonomous vehicle,165,not included
10.1016/j.knosys.2018.04.015,to_check,Knowledge-Based Systems,scopus,2018-08-01,sciencedirect,teaching a vehicle to autonomously drift: a data-based approach using neural networks,https://api.elsevier.com/content/abstract/scopus_id/85045849458,"This paper presents a novel approach to teach a vehicle how to drift, in a similar manner that professional drivers do. Specifically, a hybrid structure formed by a Model Predictive Controller and feedforward Neural Networks is employed for this purpose. The novelty of this work lies in a) the adoption of a data-based approach to achieve autonomous drifting along a wide range of road radii and body slip angles, and b) in the implementation of a road terrain classifier to adjust the system actuation depending on the current friction characteristics. The presented drift control system is implemented in a multi-actuated ground vehicle equipped with active front steering and in-wheel electric motors and trained to drift by a real test driver using a driver-in-the-loop setup. Its performance is verified in the simulation environment IPG-CarMaker through different open loop and path following drifting manoeuvres.",autonomous vehicle,166,included
10.1016/j.compind.2018.03.014,to_check,Computers in Industry,scopus,2018-06-01,sciencedirect,real-time object detection in agricultural/remote environments using the multiple-expert colour feature extreme learning machine (mec-elm),https://api.elsevier.com/content/abstract/scopus_id/85044151304,"It is necessary for autonomous robotics in agriculture to provide real time feedback, but due to a diverse array of objects and lack of landscape uniformity this objective is inherently complex. The current study presents two implementations of the multiple-expert colour feature extreme learning machine (MEC-ELM). The MEC-ELM is a cascading algorithm that has been implemented along side a summed area table (SAT) for fast feature extraction and object classification, for a fully functioning object detection algorithm. The MEC-ELM is an implementation of the colour feature extreme learning machine (CF-ELM), which is an extreme learning machine (ELM) with a partially connected hidden layer; taking three colour bands as inputs. The colour implementation used with the SAT enable the MEC-ELM to find and classify objects quickly, with 84% precision and 91% recall in weed detection in the Y’UV colour space and in 0.5 s per frame. The colour implementation is however limited to low resolution images and for this reason a colour level co-occurrence matrix (CLCM) variant of the MEC-ELM is proposed. This variant uses the SAT to produce a CLCM and texture analyses, with texture values processed as an input to the MEC-ELM. This enabled the MEC-ELM to achieve 78–85% precision and 81–93% recall in cattle, weed and quad bike detection and in times between 1 and 2 s per frame. Both implementations were benchmarked on a standard i7 mobile processor. Thus the results presented in this paper demonstrated that the MEC-ELM with SAT grid and CLCM makes an ideal candidate for fast object detection in complex and/or agricultural landscapes.",autonomous vehicle,167,not included
10.1016/0925-2312(94)00018-n,to_check,Neurocomputing,scopus,1995-01-01,sciencedirect,fast computation of optimal paths using a parallel dijkstra algorithm with embedded constraints,https://api.elsevier.com/content/abstract/scopus_id/0029329099,"We have developed a new optimal path algorithm in which the paths are subjected to turning constraints. The restriction which we have incorporated is the next link in the path must not make an angle exceeding 45 ° in magnitude with the preceeding link. This algorithm has a natural implementation as an artificial neural system with either synchronous or asynchronous weight updating, and as an automata executing on a massively parallel array processor. At a given step in the path solution process our path planning artificial neural system keeps track of all constrained optimal paths flowing into the nodes of the network. This new algorithm has applications to any path planning problem where the vehicle traveling the path is subject to a limited turning capability. The ability of the network to solve for constrained paths is illustrated with both a graph theoretic example and a scenario involving an unmanned vehicle that must travel a constrained path through a real terrain area containing artificially generated keep out zones.",autonomous vehicle,168,not included
10.3390/app11072925,to_check,core,'MDPI AG',2021-03-25 00:00:00,core,"object detection, distributed cloud computing and parallelization techniques for autonomous driving systems.",https://core.ac.uk/download/417758260.pdf,"Autonomous vehicles are increasingly becoming a necessary trend towards building the smart cities of the future. Numerous proposals have been presented in recent years to tackle particular aspects of the working pipeline towards creating a functional end-to-end system, such as object detection, tracking, path planning, sentiment or intent detection, amongst others. Nevertheless, few efforts have been made to systematically compile all of these systems into a single proposal that also considers the real challenges these systems will have on the road, such as real-time computation, hardware capabilities, etc. This paper reviews the latest techniques towards creating our own end-to-end autonomous vehicle system, considering the state-of-the-art methods on object detection, and the possible incorporation of distributed systems and parallelization to deploy these methods. Our findings show that while techniques such as convolutional neural networks, recurrent neural networks, and long short-term memory can effectively handle the initial detection and path planning tasks, more efforts are required to implement cloud computing to reduce the computational time that these methods demand. Additionally, we have mapped different strategies to handle the parallelization task, both within and between the networks",autonomous vehicle,169,not included
10.11606/d.3.2003.tde-31072003-153011,to_check,core,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",2003-08-13 00:00:00,core,development of a control architecture based on objects for an aquatic mobile robot.,,"Este trabalho trata do estudo de concepções de arquitetura do controle aplicadas aos robôs móveis autônomos e da proposição de um delas à instrumentação e controle em tempo real de um modelo de embarcação naval de alto desempenho. Tal veículo remotamente operado foi desenvolvido como parte das atividades do projeto temático ""Comportamento em Ondas de Embarcações de Alto Desempenho"" (proc.Fapesp 1997/13090-3). Realizou-se uma investigação dos diversos paradigmas de inteligência artificial que orientaram a evolução dos robôs móveis autônomos até o presente momento e, em particular, as concepções baseadas em modelos sócio-antropológicos e computacionais (teoria de agentes e orientação a objetos) através de sua aplicação à implementação de um sistema de aquisição e controle orientado a objetos, modelado através da UML (Unified Modeling Language), para o veículo mencionado. Testes de validação da arquitetura do controle foram realizados, sendo obtidos resultados experimentais que permitiram análises a respeito da dinâmica, manobrabilidade e navegação do veículo, as quais sugerem vários aperfeiçoamentos para o sistema de hardware e software em trabalhos futuros.This work deals with the study of control architecture approaches applied to autonomous mobile robots, and proposes one of them for the control system of a self-propelled high speed ship model. Such unmanned vehicle was developed for the research project Comportamento em Ondas de Embarcações de Alto Desempenho"" (proc. FAPESP 1997/13090-3). A number of artificial intelligence paradigms, related to the autonomous robot evolution up to now, were investigated. Models based on the socio-anthropological paradigm and the corresponding computer science approaches, i.e. agent theory and object-oriented modeling, were emphasized. Object-oriented control software based on the UML (Unified Modeling Language) was designed for the real-time embedded system of the ship model. Validation tests of the control architecture were carried out. Experimental results, related to vehicle dynamics, maneuverability and navigation were acquired by the embedded system and analyzed in this work. These results suggest a number of improvements for future works on the software and hardware systems",autonomous vehicle,170,not included
7db240ccd5e4bbb8a520d24318e8523eda544e9a,to_check,semantic_scholar,ELIV 2021,2021-01-01 00:00:00,semantic_scholar,ansys real time physics based radar simulation – an enabler for machine learning in the context of autonomous driving,https://www.semanticscholar.org/paper/7db240ccd5e4bbb8a520d24318e8523eda544e9a,"Throughout the evolution of Advanced Driver Assistance Systems (ADAS), the correct perception of the environment has always been a decisive success factor. Capturing and defining scenarios/edge cases, various and heterogenous datasets, multiple sensors/sensorfusion architectures, and perception algorithms are just a few of the many challenges we are facing when implementing such systems. To cope with such levels of complexity, modular approaches are required. Such approaches target flexibility and standardized interfaces between data provided by various sensor modules/models and driving functions. In the Artificial Intelligence (AI) domain, and more precisely when dealing with supervised training of Neural Networks (NN), obtaining valid and accurately labeled datasets is essential. By enabling Machine Learning (ML) in electromagnetic applications, Ansys physics-based Real Time Radar (RTR) introduces a new paradigm for sensor development and integration that leverages GPU hardware and new algorithms to accelerate simulation by orders of magnitude without compromising accuracy. In this paper, a comprehensive workflow for the generation of virtual datasets using the Open Simulation Interface (OSI) will be presented. This workflow will illustrate how the scenario variation process coupled with RTR facilitates the creation of heterogenous/labeled datasets that are ready for training object detection NN. Finally, this presentation will also show the preliminary results obtained when implementing this process. Introduction Machine Learning (ML) is gradually taking over “conventional” algorithms that were previously designed to help make Autonomous Vehicles (AVs) a reality. Several auto giants like BMW, VDI-Berichte Nr. 2384, 2021 83 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. Volkswagen, and Volvo are at least partially relying on ML algorithms to solve various parts of the sense-plan-act paradigm. Others, like Tesla [1] are solely relying on ML and in some cases Artificial Intelligence (AI) to provide an end-to-end solution. AVs usually rely on several sensors of different types to perceive their surrounding environment. As such sensors continuously scan the environment and generate raw data, the perception stack processes this data and generates a meaningful virtual map of the surrounding environment. In the area of Computer Vision (CV), usually relying on optical systems, ML algorithms are already successfully deployed in commercial systems [2] – [4]. In addition to optical systems, and due to their superior performances in bad weather conditions and dark environments, radars have also made their way into the AV’s sensor stacks. Though not frequently encountered, ML has also been used to replace some of the traditional radar signal processing algorithms. For example, in [5] the author demonstrates how a fully convolutional network can be used for object detection and 3D estimation using a Frequency-Modulated ContinuousWave (FMCW) radar. Contrary to [5], which is using real data for the training process, the author in [6] illustrates the power of physics-based simulation to also demonstrate the feasibility of using ML approaches to solve radar-based perception problems. As the training process of ML algorithms highly relies on labeled training data, Ansys’ Real Time Radar (RTR) automates generation of labeled data sets by shifting data generation and labeling from the real world to the virtual world. In addition, having an API which is compatible with the Open Simulation Interface (OSI) [7] ensures that a standardized interface is being deployed to describe the virtual environment in which generated scenarios are executed. Finally, this paper illustrates how, with the help of Ansys optiSlang [8], a tool chain is developed to orchestrate the scenario variation process and the simulation workflow to automatically generate labeled datasets for training a Neural Network (NN) based object detection algorithm. Ansys Real Time Radar RTR is an all-GPU implementation of the shooting and bouncing rays (SBR) method optimized for the automotive radar application to simulate a scenario in real time/faster than real time. The simulation, which is based on an arbitrary 3D scene/actor geometry, electrical material VDI-Berichte Nr. 2384, 2021 84 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. properties, including transmissive and reflective dielectrics, 3D polarized antenna patterns, directly calculates the scattered electro-magnetic fields as observed by the radar. As an output, RTR can generate raw I and I+Q A/D data for multi-channel radars in dynamically changing driving scenarios. Objects (e.g., vehicles, pedestrians, road, infrastructure, etc.) can be assigned arbitrary positions, orientations, linear and angular velocities in a scene graph hierarchy through a light-weight API to characterize complex traffic scenarios with negligible simulation overhead. To measure Doppler velocity, automotive radars transmit, receive, and process hundreds of chirps over each Coherent Processing Interval (CPI). Fast Fourier Transformations (FFT) and several post processing algorithms are then applied to hundreds of samples from each chirp/CPI to obtain range-Doppler (RD) images, which will be used for Neural Network (NN) training. These images, as represented in Fig. 1, give a visualization of all scattered fields in terms of relative velocity (Doppler) and distance from the radar (range). Fig. 1: Example of Range-Doppler image. As previously mentioned, RTR includes a lightweight C++ and Python API, enabling it to be integrated into nearly any driving simulator available on the market. In Fig. 2 the API’s main interfaces are depicted. VDI-Berichte Nr. 2384, 2021 85 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. Fig. 2: RTR's API representing its main inputs and outputs. Starting from left to right, the API give the user access to the following: 1. “Radar Config” enables the user to configure radar waveforms, radar modes, and antenna patterns. Radar waveforms are defined by parameters such as center frequency, bandwidth, number of frequency samples, CPI duration, number of chirps, number of transmit and receive antennas, and relative antenna positions. 2. “Object and Materials” helps the user build the 3D environment to be simulated and assign dielectric material properties. For example, as presented in Fig. 3, a vehicle can be imported as a set of subcomponents. Users can assign appropriate material properties for each component. VDI-Berichte Nr. 2384, 2021 86 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. Fig. 3: Vehicle subcomponents featuring adequate assignment of material properties. 3. “Object Velocities” represents scene dynamics where position and velocity updates are provided at each simulation time step. Such data is usually obtained from any driving simulator or from a set of pre-recorded/measured GPS/IMU data. At each time step, RTR executes a physics-based radar simulation of the scene and returns either the RD data per channel or the raw I/Q channel data (post A/D conversion). Open Simulation Interface To ensure modularity and interchangeability, RTR’s inputs have been adapted to support OSI. Focusing on environment perception and automated driving functions, OSI is an interface specification for models and components of distributed simulation. It defines a generic interface that ensures modularity, interoperability, and integration of simulation framework’s individual components. In addition, OSI was developed to address and align with the emerging standard for communication interfaces of real sensors, ISO 23150 [9]. This will eventually ensure a better correlation between communication interfaces used in both virtual and real worlds. Corresponding to OSI’s message description, the OSI:SensorView message represented in Fig. 4 contains the ground truth data that can be generated by any 3rd party driving simulator. The message includes information about the states of dynamic and static actors. VDI-Berichte Nr. 2384, 2021 87 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. Fig. 4: Driving Simulator and RTR connection via OSI. Scenario Variation Neural Networks (NN) are designed to behave as low bias and high variance machines that can perform extremely well on training data. To generalize such machines to new environments, heterogeneous datasets are essential for the training process. Using Ansys optiSlang, scenario variations were generated based on a predefined set of parameters. As represented in Table 1, a set of three parameters were chosen. Table 1: Scenario Variation Parameters Parameter Description Model Name Describes the 3D geometry of the vehicle. Initial Speed A range between 0 and 80 km/h Driver Behavior Aggressive, Normal, Cautious 1. “Model Name” defines whether a traffic participant is a car, bus, or a motorcycle. It also describes what vehicle model to use, ensuring a large variety of traffic participants within each generated scenario. 2. “Initial Speed” may randomly vary between 0 and 80 km/h. Considering that the NN is being trained on RD images, this parameter will ensure that RD scattered fields are randomly distributed in the RD image space along the Doppler velocity axis. 3. “Driver Behavior” takes in three different values: Aggressive, Normal and Cautious. Each behaviour ",autonomous vehicle,171,not included
b8f7fa8d93c5ee4c3df988ba7c7499b1db51706e,to_check,semantic_scholar,2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),2020-01-01 00:00:00,semantic_scholar,towards autonomous smart sensing systems,https://www.semanticscholar.org/paper/b8f7fa8d93c5ee4c3df988ba7c7499b1db51706e,"Since the 1990's, researchers in both academia and industry have been exploring ways to exploit the potential for Wireless Sensor Networks (WSNs) to revolutionize our understanding of - and interaction with - the world around us. WSNs have therefore been a major focus of research over the past 20 years. While WSNs offer a persuasive solution for accurate real-time sensing of the physical world, they are yet to be as ubiquitous as originally predicted when the technology was first envisaged. Technical difficulties exist which have inhibited the anticipated uptake in WSN technologies. The most challenging of these have been identified as system reliability, battery lifetime, maintenance requirements, node size and ease of use. Over the past decade, the Wireless Sensor Networks (WSN) group at the Tyndall National Institute, has been at the forefront of driving the vision of ubiquitously deployed, extended lifetime, low power consumption embedded systems providing information rich data streams wirelessly in (close to) real-time. In this time, the WSN group has developed multiple novel, first of kind, wireless multi-sensor systems and deployed these in the world around us, overcoming the technical challenges associated with ensuring robust and reliable long-term data sets from our environment. This work is focused on investigating and addressing these challenges through the development of the new technologies and system integration methodologies required to facilitate and implement WSNs and validate these in real deployments. Specifically, discussed are the development and deployment of novel WSN systems in the built environment, environmental monitoring and fitness and health monitoring systems.The key research challenges identified and discussed are:a)The development of resource-constrained, extremely low power consumption systems incorporating energy-efficient hardware and software algorithms.b)The development of highly reliable extremely long duration deployments which through the use of appropriate energy harvesting solutions facilitate (near) zero maintenance sensor networks.c)The development of low power consumption miniaturized wearable microsysteThe development of technologies to address these challenges in terms of cost, size, power consumption and reliability which need to be tested and validated in real world deployments of wireless sensing systems is discussed. It is clear that when looking at the scale up of deployments of novel WSNs, that to be successful, such systems need to ""be invisible, last forever, cost nothing and work out of the box"". This paper describes these relevant technologies and associated project demonstrators",autonomous vehicle,172,not included
8002fff47f40e6126bf3f9f7fabea1ac9e1cbb4e,to_check,semantic_scholar,Transportation Research Record: Journal of the Transportation Research Board,2018-01-01 00:00:00,semantic_scholar,hardware-in-the-loop testing of connected and automated vehicle applications: a use case for queue-aware signalized intersection approach and departure,https://www.semanticscholar.org/paper/8002fff47f40e6126bf3f9f7fabea1ac9e1cbb4e,"Most existing studies on connected and automated vehicle (CAV) applications apply simulation to evaluate system effectiveness. Model accuracy, limited data for calibration, and simulation assumptions limit the validity of evaluation results. One alternative approach is to use emerging hardware-in-the-loop (HIL) testing methods. HIL test environments enable physical test vehicles to interact with virtual vehicles from traffic simulation models, providing an evaluation environment that can replicate deployment conditions at early stages of CAV technology implementation without incurring excessive costs related to large field tests. In this study, a HIL testing system for vehicle-to-infrastructure (V2I) CAV applications is developed. The involved software and hardware includes a physical CAV controlled in real time, a traffic signal controller, communication devices, and a traffic simulator (VISSIM). Such HIL systems increase validity by considering the physical vehicle’s trajectories—which are constrained by real-world factors such as GPS accuracy, communication delay, and vehicle dynamics—in a simulated traffic environment. The developed HIL system is applied to test a representative early deployment CAV application: queue-aware signalized intersection approach and departure (Q-SIAD). The Q-SIAD algorithm generates recommended speed profiles based on the vehicle’s status, signal phase and timing (SPaT), downstream queue length, and system constraints and parameters (e.g., maximum acceleration and deceleration). The algorithm also considers the status of other vehicles in designing the speed profiles. The experiment successfully demonstrated this functionality with one test CAV driving through one intersection controlled by a fixed-timing traffic signal under various simulated traffic conditions.",autonomous vehicle,173,not included
b42fd5d2959d327a2eaa28784b744f74f6b4e6b7,to_check,semantic_scholar,,2010-01-01 00:00:00,semantic_scholar,configuration management best practices: practical methods that work in the real world,https://www.semanticscholar.org/paper/b42fd5d2959d327a2eaa28784b744f74f6b4e6b7,"Successfully Implement High-Value Configuration Management Processes in Any Development Environment As IT systems have grown increasingly complex and mission-critical, effective configuration management (CM) has become critical to an organizations success. Using CM best practices, IT professionals can systematically manage change, avoiding unexpected problems introduced by changes to hardware, software, or networks. Now, todays best CM practices have been gathered in one indispensable resource showing you how to implement them throughout any agile or traditional development organization. Configuration Management Best Practices is practical, easy to understand and apply, and fully reflects the day-to-day realities faced by practitioners. Bob Aiello and Leslie Sachs thoroughly address all six pillars of CM: source code management, build engineering, environment configuration, change control, release engineering, and deployment. They demonstrate how to implement CM in ways that support software and systems development, meet compliance rules such as SOX and SAS-70, anticipate emerging standards such as IEEE/ISO 12207, and integrate with modern frameworks such as ITIL, COBIT, and CMMI. Coverage includes Using CM to meet business objectives, contractual requirements, and compliance rules Enhancing quality and productivity through lean processes and just-in-time process improvement Getting off to a good start in organizations without effective CM Implementing a Core CM Best Practices Framework that supports the entire development lifecycle Mastering the people side of CM: rightsizing processes, overcoming resistance, and understanding workplace psychology Architecting applications to take full advantage of CM best practices Establishing effective IT controls and compliance Managing tradeoffs and costs and avoiding expensive pitfalls Configuration Management Best Practices is the essential resource for everyone concerned with CM: from CTOs and CIOs to development, QA, and project managers and software engineers to analysts, testers, and compliance professionals. Praise for Configuration Management Best Practices Understanding change is critical to any attempt to manage change. Bob Aiello and Leslie Sachss Configuration Management Best Practices presents fundamental definitions and explanations to help practitioners understand change and its potential impact. Mary Lou A. Hines Fritts, CIO and Vice Provost Academic Programs, University of Missouri-Kansas City Few books on software configuration management emphasize the role of people and organizational context in defining and executing an effective SCM process. Bob Aiello and Leslie Sachss book will give you the information you need not only to manage change effectively but also to manage the transition to a better SCM process. Steve Berczuk, Agile Software Developer, and author of Software Configuration Management Patterns: Effective Teamwork, Practical Integration Bob Aiello and Leslie Sachs succeed handsomely in producing an important book, at a practical and balanced level of detail, for this topic that often goes without saying (and hence gets many projects into deep trouble). Their passion for the topic shows as they cover a wonderful range of topicseven culture, personality, and dealing with resistance to changein an accessible form that can be applied to any project. The software industry has needed a book like this for a long time! Jim Brosseau, Clarrus Consulting Group, and author of Software Teamwork: Taking Ownership for Success A must read for anyone developing or managing software or hardware projects. Bob Aiello and Leslie Sachs are able to bridge the language gap between the myriad of communities involved with successful Configuration Management implementations. They describe practical, real world practices that can be implemented by developers, managers, standard makers, and even Classical CM Folk. Bob Ventimiglia, Bobev Consulting A fresh and smart review of todays key concepts of SCM, build management, and related key practices on day-to-day software engineering. From the voice of an expert, Bob Aiello and Leslie Sachs offer an invaluable resource to success in SCM. Pablo Santos Luaces, CEO of Codice Software Bob Aiello and Leslie Sachs have a gift for stimulating the types of conversation and thought that necessarily precede needed organizational change. What they have to say is always interesting and often important. Marianne Bays, Business Consultant, Manager and Educator",autonomous vehicle,174,not included
c3d3d7b56fcc775c96777b1c6123b46a452b4aee,to_check,semantic_scholar,,2010-01-01 00:00:00,semantic_scholar,data-centric programming best practices: using dds to integrate real-world systems,https://www.semanticscholar.org/paper/c3d3d7b56fcc775c96777b1c6123b46a452b4aee,".................................................................................................................................... 3 Real-World Systems Programming ........................................................................................... 4 Defining a Data Model .............................................................................................................. 4 DDS Maintains the State of the World as Defined by the Data Model ..................................... 6 About DDS ................................................................................................................................ 8 Best Practices in DDS Programming ........................................................................................ 9 G1. Start by defining a data model, then map the data model to DDS domains, data types and Topics. .............................................................................................................. 9 G2. Fully define your DDS Types, do not rely on opaque bytes or other custom encapsulations ................................................................................................................ 11 G3. Isolate subsystems into DDS Domains. Use mediation, such as RTI Routing Service, to bridge Domains ............................................................................................. 12 G4. Use keyed Topics. For each data type, indicate to DDS the fields that uniquely identify the data-object .................................................................................................... 13 G5. Large teams should create a targeted application platform with system-wide QoS profiles and limited access to the DDS APIs. .................................................................. 16 G6. Configure QoS using the XML Profiles .................................................................... 17 Conclusions ............................................................................................................................. 18 References .............................................................................................................................. 18 Best-Practices Data-Centric Programming: Using DDS to Integrate Real-World Systems November 2010 3 © 2010 Real-Time Innovations Abstract Systems are often implemented by teams using a variety of technologies, programming languages, and operating systems. Integrating and evolving these systems becomes complex. Traditional approaches rely on low-level messaging technologies, delegating much of the message interpretation and information management services to application logic. This complicates system integration because different applications could use inconsistent interpretations and implementations of information-management services, such as detecting component presence, state management, reliability and availability of the information, handling of component failures, etc. Integrating modern systems requires a new, modular network-centric approach that avoids these historic problems by relying on standard APIs and protocols that provide stronger information-management services. For example, many of these systems are heterogeneous, mixing a variety of computer hardware, operating systems, and programming languages. Developers often use Java, .NET, or web-scripting to develop consoles and other GUI-oriented applications, and C or C++ for specialized hardware, device drivers, and performanceor time-critical applications. The end system might mix computers running Windows, Linux, and other operating systems, such as Mac OS X, Android, or real-time operating systems like VxWorks and INTEGRITY. The use of standard APIs and interoperable protocols allows all these systems to be easily integrated and deployed. Today, these systems are typically developed using a service-oriented approach and integrated using standards-based middleware APIs such as DDS, JMS, and CORBA, and protocols such as DDS-RTPS, Web-Services/SOAP, REST/HTTP, AMQP, and CORBA/IIOP. This whitepaper focuses on “real-world” systems, that is, systems that interact with the external physical world and must live within the constraints imposed by real-world physics. Good examples include air-traffic control systems, real-time stock trading, command and control (C2) systems, unmanned vehicles, robotic and vetronics, and Supervisory Control and Data Acquisition (SCADA) systems. More and more these “real-world” systems are integrated using a Data-Centric PublishSubscribe approach, specifically the programming model defined by the Object Management Group (OMG) Data Distribution Service (DDS) specification. This whitepaper describes the basic characteristics of real-world systems programming, reasons why DDS is the best standard middleware technology to use to integrate these systems, and a set of “best practices” guidelines that should be applied when using DDS to implement these systems. Best-Practices Data-Centric Programming: Using DDS to Integrate Real-World Systems November 2010 4 © 2010 Real-Time Innovations Real-World Systems Programming Real-World systems refer to a class of software systems that operate continuously and interact directly with real-world objects, such as aircraft, trains, stock transactions, weapons, robotic and manufacturing equipment, etc. Unlike systems involving only humans and computers, real-world systems have to live within the constraints imposed by the physics of the external world. Notably, time cannot be slowed, paused, or reversed. The implication is that these systems must be able to handle the information at the pace it arrives at, as well as be robust to changes in the operating environment. In addition to these environmental considerations, the nature of typical real-world applications also places demands on their availability and need to continue operating even in the presence of partial failures. In order to interact with the real world, software must include a reasonable, if simplified, model of the external world. This model typically includes aspects of the “state of the world” relevant to system operations. Here the word “state” is used in the normal sense in software modeling and programming. State summarizes the past inputs to the system from its initial state and contains all the information necessary for a system or program to know how it should react to future events or inputs. Imagine that a new component or application starts and joins a system. The “state of the system” contains the information that this new component needs to acquire before it is ready to start performing its function. A typical component would normally only need access to a subset of that state, the portion that directly affects its operation. For example, in an air-traffic management problem, the relevant aspects of the state of the world might include the current location and trajectory of every aircraft, the flight plans of all flights within a 24-hour window, specific details on each aircraft (type, airline, crew), etc. Once a software component or subsystem is running, it interacts with other components by exposing part of its state, notifying other components when its state changes, and invoking operations on (or sending messages to) other components. Each component reacts to these information exchanges by updating its internal model of the world and using that to perform its necessary actions. Defining a Data Model A data model is simply an organized description of the state of the system. Thus, it includes data types, processes for transferring and updating those types, and methods for accessing the data. It does not typically include functions that can alter the data or (importantly) the application-level logic that affects the data. Governance organizations and system integrators often start their design by designing the system data-model. There are good reasons for this approach: • A data model provides governance across disparate teams and organizations, allowing components developed at different points in time by different organizations to be integrated. This makes it an ideal starting point for a central design or governance authority. Best-Practices Data-Centric Programming: Using DDS to Integrate Real-World Systems November 2010 5 © 2010 Real-Time Innovations • A data model represents the better understood, more invariant aspects of the system. Typically the data model is grounded in the “physics of the system.” That is, it describes the kinds of objects and sensors it manages (like aircraft locations, flight plans, and vehicle positions). The data model is not strongly tied to applicationspecific use cases (e.g., the possible fields in a flight plan are a consequence of the nature of aircraft flight); this makes the data model a good starting point, since the full set of use cases might not be well known in advance or might be the responsibility of a different team. • A data model increases decoupling between systems and components. The data model is grounded in the essential information present in the system and it does not depend so much on the use cases that access the information. For example, an airtraffic control model might include a definition of a “flight plan,” but not whether it is automatically generated using an optimization algorithm, checked for collisions, or altered in mid-flight. Using the data model as the basis for the integration avoids over-constraining the design, leaving it open to allow future evolution and use cases. Contrast this with a design based on defining service invocation APIs which are intimately tied to the details of each service and are likely to change as new use cases are incorporated Example Data Model Imagine designing a simple “chat” application. The underlying Data-Model could be defined to contain four kinds of objects summarized in the table below: Object Kind Key Fields Other Fields Description Person EmailAddress Name, Loc",autonomous vehicle,175,not included
036a7c42a459eb751bba2f8badec3b62d6328c10,to_check,semantic_scholar,,2010-01-01 00:00:00,semantic_scholar,editorial wireless sensor networks: design for real-life deployment and deployment experiences wireless sensor networks: design for real-life deployment and deployment experiences,https://www.semanticscholar.org/paper/036a7c42a459eb751bba2f8badec3b62d6328c10,"Wireless sensor networks (WSNs) are among the most promising technologies of the new millennium. The opportunities afforded by being able to program networks of small, lightweight, low-power, computation- and bandwidth-limited nodes have attracted a large community of researchers and developers. However, the unique set of capabilities offered by the technology produces an exciting but complex design space, which is often difficult to negotiate in an application context. Deploying sensing physical environments produces its own set of challenges, and can push systems into failure modes, thus revealing problems that can be difficult to discover or reproduce in simulation or the laboratory. Sustained efforts in the area of wireless networked sensing over the last 15 years have resulted in a large number of theoretical developments, substantial practical achievements, and a wealth of lessons for the future. It is clear that in order to bridge the gap between (on the one hand) visions of very large scale, autonomous, randomly deployed networks and (on the other) the actual performance of fielded systems, we need to view deployment as an essential component in the process of developing sensor networks: a process that includes hardware and software solutions that serve specific applications and end-user needs. Incorporating deployment into the design process reveals a new and different set of requirements and considerations, whose solutions require innovative thinking, multidisciplinary teams and strong involvement from end-user communities. This special feature uncovers and documents some of the hurdles encountered and solutions offered by experimental scientists when deploying and evaluating wireless sensor networks in situ, in a variety of well specified application scenarios. The papers specifically address issues of generic importance for WSN system designers: (i) data quality, (ii) communications availability and quality, (iii) alternative, low-energy sensing modalities and (iv) system solutions with high end-user added value and cost benefits. The common thread is deployment and deployment evaluation. In particular, satisfaction of application requirements, involvement of the end-user in the design and deployment process, satisfactory system performance and user acceptance are concerns addressed in many of the contributions. The contributions form a valuable set, which help to identify the priorities for research in this burgeoning area: Robust, reliable and efficient data collection in embedded wireless multi-hop networks are essential elements in creating a true deploy-and-forget user experience. Maintaining full connectivity within a WSN, in a real world environment populated by other WSNs, WiFi networks or Bluetooth devices that constitute sources of interference is a key element in any application, but more so for those that are safety-critical, such as disaster response. Awareness of the effects of wireless channel, physical position and line-of-sight on received signal strength in real-world, outdoor environments will shape the design of many outdoor applications. Thus, the quantification of such effects is valuable knowledge for designers. Sensors' failure detection, scalability and commercialization are common challenges in many long-term monitoring applications; transferable solutions are evidenced here in the context of pollutant detection and water quality. Innovative, alternative thinking is often needed to achieve the desired long-lived networks when power-hungry sensors are foreseen components; in some instances, the very problems of wireless technology, such as RF irregularity, can be transformed into advantages. The importance of an iterative design and evaluation methodology—from analysis to simulation to real-life deployment—should be well understood by all WSN developers. The value of this is highlighted in the context of a challenging WPAN video-surveillance application based on a novel Nomadic Access Mechanism. Cost benefits to be drawn from devising a WSN based solution to classic application areas such as surveillance are often a prime motivator for WSN designers; an example is offered here based on the use of intelligent agents for intrusion monitoring. Last but not least, the practicality and usability of the WSN solutions found for novel applications is key to their adoption. This is particularly true when the end-users of the developed technology are medical patients. The importance of feedback, elegant hardware encapsulation and extraction of meaning from data is presented in the context of novel orthopedic rehabilitation aids. Overall, this feature offers wide coverage of most issues encountered in the process of design, implementation and evaluation of deployable WSN systems. We trust that designers and developers of WSN systems will find much work of value, ranging from lessons learned, through solutions to known hurdles, to novel developments that enhance applications. Finally, we would like to thank all authors for their valuable contributions!",autonomous vehicle,176,not included
cd51bbfd51cde0c089d9dfb7d30bfc124d9b7c55,to_check,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,"summary for cife seed proposals for academic year 2020-21 proposal number: 2020-04 proposal title: hybrid physical-digital spaces: transforming the design, operation, and experience of built environments to promote health and wellbeing",https://www.semanticscholar.org/paper/cd51bbfd51cde0c089d9dfb7d30bfc124d9b7c55,"up to 150 words) Increasing evidence suggests built office features (e.g., lighting, materials, and ventilation) have substantial impacts on occupant wellbeing. A key next direction is field studies at industry partner sites to examine real-world workplaces. We propose to develop innovative Internet of Things (IoT) techniques that integrate data from building instrumentation, personal device sensors, and self-report interfaces and then deploy this platform in-the-wild to capture rich, longitudinal, ecologically-valid data about the status of office workers and the spaces they occupy. Insights will advance scientific knowledge of how buildings impact wellbeing as well as produce practical implications for building designers and operators. A timely component will explore how covid-19 has temporally or fundamentally changed occupant behaviors and operational decisions (e.g., physical distancing desks and ventilation settings that reduce pathogen spread). Overall, our proposed research has the potential to transform the industry’s thinking on how built environments can be designed, operated, and experienced. Hybrid Physical-Digital Spaces: Transforming the Design, Operation, and Experience of Built Environments to Promote Health and Wellbeing Problem and Significance Considering that people in the U.S. spend 87% of their time in indoor spaces , we assert that 1 buildings are powerful yet underleveraged loci for promoting human wellbeing. Imagine an intelligent office that could adapt soundscape systems to manage noise in open floor plans, optimize space reservation or utilization to foster collaborations and save energy, or provide digital information displays that promote employee connectedness and physical activity. Towards actualizing our vision of such hybrid physical-digital spaces, our proposal strives to develop, apply, and evaluate novel scientific and engineering approaches that will transform the industry’s thinking around how built environments can be designed, operated, and experienced. Increasingly, hypotheses suggest that built features of indoor environments (e.g., lighting, materials, and ventilation) have substantial impacts on occupants (e.g., employee recruitment and retention, absenteeism, cognition, creativity, productivity, social interactions, physical activity and health, and psychological wellbeing). In turn, these individual outcomes also drive pivotal organizational outcomes such as product innovation, workforce diversity, employee turnover, market share, and profitability. Examples illustrate how building interventions can have huge impacts : enhancing employee exposure to daylight can save businesses ~$2,000/yr per capita 2 , better air quality can raise cognitive scores of workers by 101% 3 , and increasing indoor access to biophilic elements could recoup $23 billion considering 10% of workplace absenteeism (a $226 billion dollar problem) is attributable to architecture that inadequately connects to nature 4 . However, few of these hypotheses have been tested at scale, over time, and in real world conditions . Instead, most prior efforts are small sample, short-term correlational studies based on potentially biased and sparse self-reported data. A more rigorous, scientific, and human-centered approach to study and engineer buildings that promote wellbeing can have major implications at individual, organizational, and societal levels (see Figure 1), offering both foundational theoretical knowledge as well as practical strategies for building designers and operators. Figure 1. Relations among building features and human outcomes at various levels. Further, “smart buildings” today typically focus on basic sensing and control for energy savings, thermal comfort, and security. Connecting to CIFE’s Vision for the Future of Building Users, we argue buildings of the future can go beyond such bottom line outcomes to be more interactive and human-centered: aware of and responsive to occupants’ cognitive, mental, and physical feelings and needs, while respecting privacy and promoting positive indoor experiences . 1 Klepeis, et al., 2001; 2 Heschong & Mahone, 2003; 3 Allen et al., 2016; 4 Elzeyadi, 2011. <Landay-Billington> < Hybrid Physical-Digital Spaces> 1 Theoretical and Practical Points of Departure It is imperative to increase understanding of exactly what built attributes have what impacts and on whom, in a scalable, longitudinal, and inclusive manner. Thus through technology-driven assessment and hybrid physical-digital interventions, we aim to (a) fundamentally advance the science on how built environments impact human wellbeing and, in turn, (b) generate guidelines that can revolutionize the way spaces are designed, operated, and experienced . Our current scope focuses on office spaces and workers; though an overarching goal is for our developed approaches and insights to establish a foundation that enables future research with additional populations and environments (e.g., physicians and patients in clinical settings, students and teachers in classrooms, and traditionally marginalized shift and temporary workers). In particular, our reusable platform will help others study this wider range of buildings and occupants; and combining these approaches with emerging endeavors such as biophilic design and precision interventions provides a novel opportunity to not only more deeply investigate but also address long-running public health challenges and systemic inequities facing society. In these ways, we hope to positively impact a broad cross-section of stakeholders at individual, organizational, and institutional levels. Moreover, this project will support interdisciplinary fertilization across engineering, computing, psychology, law, and medicine . Research Methods and Work Plan Our research agenda is to support the design and operation of built facilities that augment human capabilities and wellbeing — and have a fundamental positive change on the way indoor spaces are experienced by the people that occupy them. By introducing intelligent systems capable of gathering and interpreting building and occupant data as well as delivering adaptive interventions in response, novel roles will also emerge for managing buildings and the activities that take place inside them. To achieve these goals, our research will comprise three main activities: 1. Developing an extensible and secure data collection and machine learning platform . A key aim of this research is scientifically examining how built spaces impact human wellbeing. To pursue this investigation and develop methods that enable buildings to be more aware of occupants’ states and needs, we have been developing pattern detection software that integrates data from (a) personal devices (smartphones, smartwatches, fitness trackers), (b) building instrumentation or portable environmental sensors (light levels, air quality), and (c) experience sampling interfaces that prompt occupants for subjective information through quick, validated self-report techniques. Figure 2 illustrates examples of these assessment components. This work involves addressing a number of technical challenges, such as selecting sampling rates and window sizes to maximize efficiency, developing methods for analyzing asynchronous and sparse sensor data, and developing privacy-sensitive feature engineering strategies for detecting and predicting wellbeing outcomes of interest. We also plan to package our platform as a reusable toolkit that can be applied by other researchers and building operators. This work is ongoing and a basic version will be ready by summer. Once development is complete, CIFE support would allow us to move onto the next critical phase: moving out of the lab and into the field. <Landay-Billington> < Hybrid Physical-Digital Spaces> 2 Figure 2. Platform to integrate data from personal devices, building sensors, and subjective self-report. 2. Deploying the platform through a mixed-method study with industry partners . The next step in our research is to deploy this platform at field sites in partnership with View, Inc. (specifically, at TIAA offices in Manhattan, this summer/fall) to capture rich, longitudinal, ecologically-valid data about behavioral, psychological, and physiological states of occupants and their everyday work environments. Our plan is to recruit a sample of approximately 150 employees for a period of 18 weeks, which will involve a baseline phase followed by systematic variation of built features (Views/No Views, Plants/No Plants, and Diversity/No Diversity in artwork) and measurement of indicators hypothesized to promote both personal wellbeing and organizational performance, based on the literature and our formative online and lab studies, described below. In combination with the engineering-focused activities to implement and install the platform, deployment will occur in tandem with ethnographic work (e.g., observations, interviews, and surveys) to manually validate reliability of the system’s automated inferences as well as gain a more qualitative portrait of occupant experiences in various spaces. Privacy-centric engagements will additionally investigate stakeholders’ attitudes regarding the capture of various types of information to derive implications about informed consent and personal data management. Along similar lines, it will be critical to responsibly manage captured data, especially potentially sensitive and exploitable data about wellness or performance. Therefore all studies will be conducted with oversight and approval from the Stanford Institutional Review Board (IRB). In addition to obtaining participants’ informed consent, we will also design sensor and data collection mechanisms to use an opt-in model, including partial participation. Our data management systems can also allow individuals to view and delete their personal data, including if purging is desired in the event of study withdrawal. Our research team has exp",autonomous vehicle,177,not included
3ee6c2e1aebf16c7b62774700b99a4320ebfefea,to_check,semantic_scholar,IEEE Internet of Things Journal,2019-01-01 00:00:00,semantic_scholar,mc-sdn: supporting mixed-criticality real-time communication using software-defined networking,https://www.semanticscholar.org/paper/3ee6c2e1aebf16c7b62774700b99a4320ebfefea,"Despite recent advances, there still remain many problems to design reliable cyber-physical systems. One of the typical problems is to achieve a seemingly conflicting goal, which is to support timely delivery of real-time flows while improving resource efficiency. Recently, the concept of mixed-criticality (MC) has been widely accepted as useful in addressing the goal for real-time resource management. However, it has not been yet studied well for real-time communication. In this paper, we present the first approach to support MC flow scheduling on switched Ethernet networks leveraging an emerging network architecture, software-defined networking (SDN). Though SDN provides flexible and programmatic ways to control packet forwarding and scheduling, it yet raises several challenges to enable real-time MC flow scheduling on SDN, including: 1) how to handle (i.e., drop or re-prioritize) out-of-mode packets in the middle of the network when the criticality mode changes and 2) how the mode change affects end-to-end transmission delays. Addressing such challenges, we develop MC-SDN that supports real-time MC flow scheduling by extending SDN-enabled switches and OpenFlow protocols. It manages and schedules MC packets in different ways depending on the system criticality mode. To this end, we carefully design the mode change protocol that provides analytic mode change delay bound, and then resolve implementation issues for system architecture. For evaluation, we implement a prototype of MC-SDN on top of Open vSwitch, and integrate it into a real world network testbed as well as a 1/10 autonomous vehicle. Our extensive evaluations with the network testbed and vehicle deployment show that MC-SDN supports MC flow scheduling with minimal delays on forwarding rule updates and it brings a significant improvement in safety in a real-world application scenario.",autonomous vehicle,178,not included
aade4d49f18a31bf802c27038645f329477f27c4,to_check,semantic_scholar,J. Netw. Comput. Appl.,2020-01-01 00:00:00,semantic_scholar,reliability-aware virtual network function placement in carrier networks,https://www.semanticscholar.org/paper/aade4d49f18a31bf802c27038645f329477f27c4,"Abstract Network Function Virtualization (NFV) is a promising technology that implements Virtual Network Function (VNF) with software on general servers. Traffic needs to go through a set of ordered VNFs, which is called a Service Function Chain (SFC). Rational deployment of VNFs can reduce costs and increase profits for network operators. However, during the deployment of the VNFs, how to guarantee the reliability of SFC requirements while optimizing network resource cost is still an open problem. To this end, we study the problem of reliability-aware VNF placement in carrier networks. In this paper, we firstly redefine the reliability of SFC, which is the product of the reliability of all nodes and physical links in SFC. On this basis, we propose two reliability protection mechanisms: the All-Nodes Protection Mechanism (ANPM) and the Single-Node Protection Mechanism (SNPM). Following this, for each protection mechanism, we formulate the problem as an Integer Linear Programming (ILP) model. Due to the problem complexity, we propose a heuristic algorithm based on Dynamic Programming and Lagrangian Relaxation for each protection mechanism. With extensive simulations using real world topologies, our results show that compared with the benchmark algorithm and ANPM, SNPM can save up to 33.34% and 26.76% network resource cost on average respectively while guaranteeing the reliability requirement of SFC requests, indicating that SNPM performs better than ANPM and has better application potential in carrier networks.",autonomous vehicle,179,not included
577a8528c2dd27d9c36d9cb1e63c6667c9c3370d,to_check,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,challenges and opportunities in the future applications of iot technology,https://www.semanticscholar.org/paper/577a8528c2dd27d9c36d9cb1e63c6667c9c3370d,"The advent of internet of things (IoT) has influenced and revolutionized the information systems and computing technologies. A computing concept where physical objects used in daily life, will identify themselves by getting connected to the internet is called IoT. Physical objects embedded with electronic, radio-frequency identification, software, sensors, actuators and smart objects converge with the internet to accumulate and share data in IoT. IoT is expected to bring in extreme changes and solutions to most of the daily problems in the real world. Thus, IoT provides connectivity for everyone and everything at any time. The IoT embeds some intelligence in Internet connected objects to communicate, exchange information, take decisions, invoke actions and provide amazing services. It has an imperative economic and societal impact for the future construction of information, network, and communication technology. In the upcoming years, the IoT is expected to bridge various technologies to enable new applications by connecting physical objects together to support the intelligent decision making. As the most cost-effective and performant source of positioning and timing information in outdoor environments, the global navigation satellite systems(GNSS) has become an essential element of major contemporary technology developments notably including the IoT, Big Data, Smart Cities and Multimodal Logistics. By 2020, there will be more than 20 billion interconnected IoT devices, and its market size may reach $1.5 trillion. Projections for the impact of IoT on the Internet and economy are impressive, with some anticipating as many as 100 billion connected IoT devices and a global economic impact of more than $11 trillion by 2025. Regulators can play a role in encouraging the development and adoption of the IoT, by preventing abuse of market dominance, protecting users and protecting Internet networks while promoting efficient markets and the public interest. Regulators can consider and identify some measures to foster development of the IoT. Encourage development of LTE‐A and 5G wireless networks, and keep need for IoT‐specific spectrum under review. Universal IPv6 adoption by governments in their own services and procurements, and other incentives for private sector adoption. Increasing interoperability through competition law and give users a right to easy access to personal data. Support global standardization and deployment of remotely provisioned SIMs for greater machine to machine competition. Particular attention will be needed from regulators to IoT privacy and security issues, which are key to encouraging public trust in and adoption of the technology. This paper focuses specifically on the essential technologies that enable the implementation of IoT and the general layered architecture of IoT, the market of IoT and GNSS technologies and their impact of the world economy, application domain of IoT and finally the Policy and regulatory implications and best practices.",autonomous vehicle,180,not included
ef8e465f80f41ec5cb3dbdb527c8509e66abaf5c,to_check,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,a framework to secure applications with isa heterogeneity,https://www.semanticscholar.org/paper/ef8e465f80f41ec5cb3dbdb527c8509e66abaf5c,"Software security attacks are evolving from exploiting common code vulnerabilities to exploiting micro architecture side-channels. Traditional software diversity or code randomization techniques diversify the code memory layout and make it difficult for potential attackers to pinpoint the precise location of the target vulnerability. However, those approaches may not be sufficient enough for the new micro architecture attacks (e.g., Spectre). While some architecture researchers have proposed using diverse ISA configurations to defeat code injection or code reuse attacks, most of these works remain in the simulation stage due to legal, licensing, and verification costs involved in bringing a heterogeneous chip design into physical hardware [39]. In this paper, we report our on-going work of HeterSec, a framework to secure applications utilizing real world heterogeneous ISA machines. HeterSec runs on top of the commodity x86_64 and ARM64 machines. It gives the process the ability to dynamically select its underlying ISA environment. Therefore, the protected process would hide the vulnerable targets with the diversified instruction set, or would detect the abnormal behavior by comparing the execution results step-by-step from multiple ISA-diversified instances. To demonstrate the effectiveness of such software framework, we implemented HeterSec on Linux and showed its deployability by running it on a x86_64 and ARM64machine pair, connected using InfiniBand. We then conduct two case studies with HeterSec. In the first case, we timely randomize the process execution path across the ISA, which achieves similar security guarantees as the existing architecture based solutions. In the second case, we implement a multi-ISA based multi-version execution (MVX) system, providing a stronger security guarantee than current homogeneousISA MVX designs.",autonomous vehicle,181,not included
2ad3366962d249b7b63c4986ebb0cb22ea212a75,to_check,semantic_scholar,,2005-01-01 00:00:00,semantic_scholar,"service-oriented architecture compass: business value, planning, and enterprise roadmap",https://www.semanticscholar.org/paper/2ad3366962d249b7b63c4986ebb0cb22ea212a75,"Praise for Service-Oriented Architecture Compass""A comprehensive roadmap to Service-Oriented Architecture (SOA). SOA is, in reality, a business architecture to be used by those enterprises intending to prosper in the 21st century. Decision makers who desire that their business become flexible can jumpstart that process by adopting the best practices and rules of thumb described in SOA Compass.""i¾Bob Laird, MCI IT Chief Architect""The book Service-Oriented Architecture Compass shows very clearly by means of real projects how agile business processes can be implemented using Service-Oriented Architectures. The entire development cycle from planning through implementation is presented very close to practice and the critical success factors are presented very convincingly.""i¾Professor Dr. Thomas Obermeier, Vice Dean of FHDW Bergisch Gladbach, Germany""This book is a major improvement in the field. It gives a clear view and all the key points on how to really face a SOA deployment in today's organizations.""i¾Mario Moreno, IT Architect Leader, Generali France""Service-Oriented Architecture enables organizations to be agile and flexible enough to adopt new business strategies and produce new services to overcome the challenges created by business dynamism today. CIOs have to consider SOA as a foundation of their Enterprise Applications Architecture primarily because it demonstrates that IT aligns to business processes and also because it positions IT as a service enabler and maximizes previous investments on business applications.To understand and profit from SOA, this book provides CIOs with the necessary concepts and knowledge needed to understand and adapt it into their IT organizations.""i¾Sabri Hamed Al-Azazi, CIO of Dubai Holding, Sabri""I am extremely impressed by the depth and scale of this book! The title is perfecti¾when you know where you want to go, you need a compass to guide you there! After good IT strategy leads you to SOA, this book is the perfect vehicle that will drive you from dream to reality. We in DSK Bank will use it as our SOA bible in the ongoing project.""i¾Miro Vichev, CIO, DSK Bank, Bulgaria, member of OTP Group""Service-Oriented Architecture offers a pathway to networking of intra- and inter-corporate business systems. The standards have the potential to create far more flexible and resilient business information systems than have been possible in the past. This book is a must-read for those who care about the future of business IT.""i¾Elizabeth Hackenson, CIO, MCI""Service-Oriented Architecture is key to help customers become on demand businessesi¾a business that can quickly respond to competitive threats and be first to take advantage of marketplace opportunities. SOA Compass is a must-read for those individuals looking to bridge the gap between IT and business in order to help their enterprises become more flexible and responsive.""i¾Michael Liebow, Vice President, Web Services and Service-Oriented Architecture, IBM Business Consulting Services""This book is a welcome addition to SOA literature. It articulates the business case and provides practical proven real-world advice, guidance, tips, and techniques for organizations to make the evolution from simple point-to-point web services to true SOA by addressing such topics as planning, organization, analysis and design, security, and systems management.""i¾Denis O'Sullivan, Fireman's Fund Enterprise ArchitectMaximize the business value and flexibility of your SOA deploymentIn this book, IBM Enterprise Integration Team experts present a start-to-finish guide to planning, implementing, and managing Service-Oriented Architecture. Drawing on their extensive experience helping enterprise customers migrate to SOA, the authors share hard-earned lessons and best practices for architects, project managers, and software development leaders alike.Well-written and practical, Service-Oriented Architecture Compass offers the perfect blend of principles and ""how-to"" guidance for transitioning your infrastructure to SOA. The authors clearly explain what SOA is, the opportunities it offers, and how it differs from earlier approaches. Using detailed examples from IBM consulting engagements, they show how to deploy SOA solutions that tightly integrate with your processes and operations, delivering maximum flexibility and value. With detailed coverage of topics ranging from policy-based management to workflow implementation, no other SOA book offers comparable value to workingIT professionals.Coverage includes SOA from both a business and technical standpointi¾and how to make the business case Planning your SOA project: best practices and pitfalls to avoid SOA analysis and design for superior flexibility and value Securing and managing your SOA environment Using SOA to simplify enterprise application integration Implementing business processes and workflow in SOA environments Case studies in SOA deployment After you've deployed: delivering better collaboration, greater scalability, and more sophisticated applicationsThe IBM Press developerWorks® Series is a unique undertaking in which print books and the Web are mutually supportive. The publications in this series are complemented by resources on the developerWorks Web site on ibm.com. Icons throughout the book alert the reader to these valuable resources.",autonomous vehicle,182,not included
7654c04648178149dad73ae3b1a93d404e631774,to_check,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,openstack in action,https://www.semanticscholar.org/paper/7654c04648178149dad73ae3b1a93d404e631774,"In the cloud computing model, a cluster of physical computers hosts an environment that provides shared services (public and private) and offers the flexibility to easily add, remove, and expand virtual servers and applications. OpenStack is an open source framework that can be installed on individual physical servers to a cloud platform and enables the building of custom infrastructure (IaaS), platform (PaaS), and software (SaaS) services without the high cost and vendor lock-in associated with proprietary cloud platforms. OpenStack in Action offers real world use cases and step-by-step instructions to develop cloud platforms from inception to deployment. It explains the design of both the physical hardware cluster and the infrastructure services needed to create a custom cloud platform. It shows how to select and set up virtual and physical servers, implement software-defined networking, and the myriad other technical details required to design, deploy, and operate an OpenStack cloud in an enterprise. It also discusses the cloud operation techniques needed to establish security practices, access control, efficient scalability, and day-to-day DevOps practices. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.",autonomous vehicle,183,not included
497ed558a130464edf5ae4b974e35cb6b374a54d,to_check,semantic_scholar,,2017-01-01 00:00:00,semantic_scholar,an active learning environment to improve first-year mechanical engineering retention rates and software skills,https://www.semanticscholar.org/paper/497ed558a130464edf5ae4b974e35cb6b374a54d,"This work proposes a foundational change from traditional lecture to an active learning environment in the Colorado State University First-Year Introduction to Mechanical Engineering course of 145 students. The goal of this approach is to improve computational capabilities in Mechanical Engineering and long-term retention rates with a single broad emphasis. Major and minor changes were implemented in the course, from specific day to day in-class activities to the addition of laboratory sessions to replace traditional classroom lecture. These laboratories of no more than fifteen students were delivered by Learning Assistants, which were upper-level undergraduate peer educators. To evaluate proficiency, a MATLAB post-test was delivered to students who were instructed through lecture only (“Lecture”) and those who were instructed with the above changes (“Active”). A survey was also provided upon completion of the course to the Active group for student reflection on their perceived software capability and the usefulness of approaches. Post-test results suggest that the Active group was more proficient in MATLAB than the Lecture group. Survey results suggest that the Active group recognize they had not achieved expert use of the software but that they were likely to use it throughout their careers and that all approaches were useful, in particular the use of Learning Assistants. Future longterm retention statistics will shed light on the possible effectiveness of this approach, which are currently unavailable. Introduction Colorado State University has a total student enrollment in excess of 33,000. As a land grant university, the historic mission of the institution is to provide students with an education in practical fields such as agriculture and engineering. The College of Engineering has a growing student cohort, with an increase from ~450 first-year students Fall 2010 to ~600 students Fall 2015 [1]. However, persistence and graduation rates have remained fairly steady over the last fifteen years. The current six year persistence rate within the college is only ~45% and the six year graduation rate within the college is similar at ~43%. Many students do not remain within the college for even a full year, as the second fall persistence rate is only 70-75% [1]. These data show a significant portion of enrolled first-year engineering students do not remain within the program long enough to be exposed to foundational engineering content, which starts in the sophomore year with engineering specific courses. A current goal of the college is to improve these retention statistics. Additionally, many students do not develop the necessary software skills required to use computational tools such as MATLAB, which are integral to success in the curriculum. Students who do not develop these skills during introductory coursework must “catch up” in later courses, where the technical content is more challenging. We hypothesize this can lead to unpreparedness for challenging content or careers as an engineer and can negatively impact academic standing, leading to decreased retention. Thus, the goals of this work were to 1) improve retention rates for first-year engineering students, specifically mechanical engineering, and 2) improve computational and software skills of first-year students, specifically MATLAB and Microsoft Excel. MATLAB is a common computational package which can be used for a broad range of engineering problems throughout a curriculum [2]. However, learning Excel and MATLAB through lecture is challenging, as these tools are best understood through utilization, not observation [3]. MATLAB and other computational tools are often taught in classrooms with computational equipment, however this is can be a challenge with a large classroom [4]. Some have utilized computer based tutorials which students can complete on their own time [5], while others implemented a large scale deployment of personal computers equipped with MATLAB and other software [6]. Additionally, the use of peer-educators can be an effective approach to facilitating MATLAB development [7]. Thus, we have chosen to employ an approach which utilizes an active environment to learn MATLAB and other introductory content through the use of laboratory sessions and peer-educators, in this case the Learning Assistant model [8]. Similar to previous approaches, we have utilized classroom lectures, hands on in-class activities, and laboratory sessions [9]. The Introduction to Mechanical Engineering Course (MECH 103) was developed to provide students with an overview of the mechanical engineering discipline and as an introduction to the computational packages MATALB [10] and Microsoft Excel. The course consists of between 140 and 250 first-year students and was previously delivered using traditional lecture. While this approach was most efficient for a single instructor due to the enrollment size, this resulted in a static learning environment for a course which should excite students about mechanical engineering and provide foundational technical skills. The overall approach to this work was to thus create an active environment for students within the course, which had an enrollment of 145 students for the Fall 2016 semester. The rationale to this approach was that by providing students with hands-on experiences working with mechanical engineering problems and computational software, the understanding of course content will improve [11,12] whereby improving retention [13]. While some immediate test and survey data were acquired and are shown in this work, it is important to note that the true impact on retention is not currently recognizable and will require future analysis. In-Class Sessions Class sessions were varied throughout the semester and the week, as they typically included lectured course material, guest lectures or panels, and activities. The course met Monday, Wednesday, and Friday from 9-9:50 AM in a large lecture hall with individual stadium seating. Friday lecture was often cancelled and this time was spent in weekly laboratory sessions instead, which are outlined in the next section. Monday class time was assigned to covering course content through lecture, teamwork activities, and in class problems. The content of the course included general introductory material such as teamwork, communication, and design, commonly used units and unit conversions, mathematical models and systems, and an introduction to Microsoft Excel and MATLAB. Active engagement in the class included a teamwork design problem, requiring students to break into groups of three. Due to the theater seating layout of the classroom, groups of four or more made successful teamwork and communication difficult. Each group of students were provided one piece of 8.5” x 11” blank printer paper, one paperclip, and two pieces of scotch tape. The design problem was simple: build the tallest free standing structure possible using only the given materials. This was an inexpensive and simple approach to teamwork design activity. In place of a lecture or even a discussion on how to use design techniques for a simple problem such as this, students were able to actively engage in this process despite the difficulties of class size and layout. While students typically have an excellent understanding of units such as a pound (lb), their physical understanding of units such as a Joule or Watt are less developed within the context of everyday life. To provide students with a meaningful representation of energy (Joule) and power (Watt), they were provided a common object – in this case a softball – and asked to calculate how high they would have to raise the object to exert one Joule of energy – in this case roughly a foot and a half. While simple and inexpensive, this activity provided students with useful knowledge they can apply without a calculator and helps them relate coursework to the real world. For example, if they can place a Joule into real-world context, they could then answer the question “Can I launch a rocket into space using a thousand Joules?”. Wednesday lecture sessions were commonly used for guest lecturers and panels. These class sessions included the College of Engineering Dean, faculty members and graduate students in mechanical engineering, industry panelists, entrepreneurs and small business owners, and an interactive teamwork theatre troupe. The goal of these sessions was to provide students with a broad overview of different disciplines within mechanical engineering and what skills are necessary to succeed in various professional roles. While emphasizing an active learning environment is inherently difficult with each and every guest, student engagement was addressed by delivering variability in all of the presentations and strongly encouraging students to ask questions. For example, the theater troupe was an interactive experience where students were able to act as a team member within a group that mocked to show a diverse team struggling with communication. This session involved humor, discussion, and lively responses from students in place of a traditional static lecture. Laboratory Sessions In place of Friday lecture, students were asked to attend laboratory sessions for one hour [14,3]. A total of eleven sessions were provided throughout the week to accommodate all schedules. Sessions included one instructor, 13-16 students, and were held in laboratories with individual workstations with Microsoft Excel and MATLAB software. Laboratory instructors included a Graduate Teaching Fellow and Undergraduate Learning Assistants (LAs). Laboratory sessions involved a short (<5 minutes) lecture briefly reviewing content from class before students began working on assigned problems. These problems implemented course content such as the use of Excel or MATLAB to analyze and display data through real-world applications. An example of utilizing MATLAB to simulate rolling a die is p",autonomous vehicle,184,not included
e1184cc1e4725f7736d9944a33ada01a626cedc3,to_check,semantic_scholar,,2006-01-01 00:00:00,semantic_scholar,learning in robotics,https://www.semanticscholar.org/paper/e1184cc1e4725f7736d9944a33ada01a626cedc3,"For a robot, the ability to get from one place to another is one of the most basic skills. However, locomotion on legged robots is a challenging multidimensional control problem. This paper presents a machine learning approach to legged locomotion, with all training done on the physical robots. The main contributions are a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithms for learning quadrupedal locomotion. The resulting learned walk is considerably faster than all previously reported hand-coded walks for the same robot platform. Introduction The ability to deploy a fully autonomous robot in an unstructured, dynamic environment (the proverbial real world) over an extended period of time remains an open challenge in the field of robotics. Considerable progress is being made towards many components of this task including physical agility, power management, and on-board sensor technology. One such component that has drawn considerable interest recently is the ability for a robot to autonomously learn to improve its own performance (Ng et al. 2004; Bagnell & Schneider 2001; Zhang & Vadakkepat 2003). Despite this interest, considerable work remains due to the di fficulties associated with machine learning in the real world . Compared to other machine learning scenarios such as classification or action learning in simulation, learning o n physical robots presents several formidable challenges, i ncluding the following. Sparse Training Data: It is often prohibitively difficult to generate large amounts of data due to the maintenance required on robots, such as battery changes, hardware repairs, and, usually, constant human supervision. Thus, learning methods designed for physical robots must be effective with small amounts of data. Dynamical Complexity: The dynamics of many robotic control tasks are too complex for faithful simulation to be possible. Furthermore, robots are inherently situated in an unstructured environment with unpredictable sensor and actuator noise, namely the real world. Thus, even when off-line simulation is possible, it can never be fully reflective of the target environment. In this paper, we overcome these challenges for one concrete complex robot task, namely legged locomotion. Using a commercially available quadruped robot, we fully automate the training process (other than battery changes) and Copyright c © 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. employ machine learning algorithms that are sufficiently data efficient to enable productive learning on physical robots in a matter of hours. The resulting learned walk is considerably faster than all previously reported hand-cod ed walks for the same robot platform. This paper contributes both a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithms for learning quadrupedal locomotion. The remainder of the paper is organized as follows. First, we introduce the parameterized walk which our learning process seeks to optimize. We then specify our four learning approaches, and follow with detailed empirical results. We close with a discussion of their implications and possible avenues for future work. A Parameterized Walk The Sony Aibo ERS-210A is a commercially available robot that is equipped with a color CMOS camera and an optional ethernet card that can be used for wireless communication. The Aibo is a quadruped robot, and has three degrees of freedom in each of its four legs (Sony 2004). At the lowest level, the Aibo’s gait is determined by a series of joint positions for the three joints in each of its leg s. An early attempt to develop a gait by Hornby et al. (1999) involved using a genetic algorithm to learn a set of lowlevel parameters that described joint velocities and body p osition.1 More recent attempts to develop gaits for the Aibo have involved adopting a higher-level representation that deals with the trajectories of the Aibo’s four feet through three-dimensional space. An inverse kinematics calculati on is then used to convert these trajectories into joint angles . Among higher-level approaches, most of the differences between gaits that have been developed for the Aibo stem from the shape of the loci through which the feet pass and the exact parameterizations of those loci. For example, a team from the University of New South Wales achieved the fastest known hand-tuned gait using the high-level approach described above with trapezoidal loci. They subsequently generated an even faster walk via learning (Kim & Uther 2003). A team from Germany created a flexible gait implementation that allows them to use a variety of different shapes of loci (Rofer et al. 2003), and the team from the University of Newcastle was able to generate highvelocity gaits using a genetic algorithm and loci of arbitra ry shape (Quinlan, Chalup, & Middleton 2003). Our team (UT Austin Villa, Stone t al. 2004) first approached the gait optimization problem by hand-tuning Developed on an earlier version of the Aibo. a gait described by half-elliptical loci. This gait performed comparably to those of other teams participating in RoboCup 2003. The work reported in this paper uses the hand-tuned UT Austin Villa walk as a starting point for learning. Figure 1 compares the reported speeds of the gaits mentioned above, both hand-tuned and learned, including that of our starting point, the UT Austin Villa walk. The latter walk is described fully in a team technical report (Stone et al. 2004). The remainder of this section describes those details of the UT Austin Villa walk that are important to understand for the purposes of this paper. Hand-tuned gaits Learned gaits CMU Austin Villa UNSW Hornby UNSW NUBots (2002) (2003) (2003) (1999) (2003) (2003) 200 245 254 170 270 296 Figure 1: Maximum forward velocities of the best gaits (in mm/s) for different teams, both learned and hand-tuned. The half-elliptical locus used by our team is shown in Figure 2. By instructing each foot to move through a locus of this shape, with each pair of diagonally opposite legs in phase with each other and perfectly out of phase with the other two (a gait known as a trot), we enable the Aibo to walk. Four parameters define this elliptical locus: 1. The length of the ellipse; 2. The height of the ellipse; 3. The position of the ellipse on the x axis; and 4. The position of the ellipse on the y axis. Since the Aibo is roughly symz",autonomous vehicle,185,not included
55b4107c8d37629d0378671324f56f9e801a6d4e,to_check,semantic_scholar,KDD,2015-01-01 00:00:00,semantic_scholar,efficient long-term degradation profiling in time series for complex physical systems,https://www.semanticscholar.org/paper/55b4107c8d37629d0378671324f56f9e801a6d4e,"The long term operation of physical systems inevitably leads to their wearing out, and may cause degradations in performance or the unexpected failure of the entire system. To reduce the possibility of such unanticipated failures, the system must be monitored for tell-tale symptoms of degradation that are suggestive of imminent failure. In this work, we introduce a novel time series analysis technique that allows the decomposition of the time series into trend and fluctuation components, providing the monitoring software with actionable information about the changes of the system's behavior over time. We analyze the underlying problem and formulate it to a Quadratic Programming (QP) problem that can be solved with existing QP-solvers. However, when the profiling resolution is high, as generally required by real-world applications, such a decomposition becomes intractable to general QP-solvers. To speed up the problem solving, we further transform the problem and present a novel QP formulation, Non-negative QP, for the problem and demonstrate a tractable solution that bypasses the use of slow general QP-solvers. We demonstrate our ideas on both synthetic and real datasets, showing that our method allows us to accurately extract the degradation phenomenon of time series. We further demonstrate the generality of our ideas by applying them beyond classic machine prognostics to problems in identifying the influence of news events on currency exchange rates and stock prices. We fully implement our profiling system and deploy it into several physical systems, such as chemical plants and nuclear power plants, and it greatly helps detect the degradation phenomenon, and diagnose the corresponding components.",autonomous vehicle,186,not included
6e7e52c8f59ec975cb9b850cef1ecf8470b9c28a,to_check,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,physical hardware-in-the-loop modelling and simulation,https://www.semanticscholar.org/paper/6e7e52c8f59ec975cb9b850cef1ecf8470b9c28a,"It is too risky to install a newly-designed device, component, or controller, directly into a real power system without rigorous testing. To help to de-risk the system integration, and to assist in the design process, computer simulation is an accepted and widely-adopted tool. However, in a simulation-only environment, many real-world issues such as noise, randomness of event timings, and hardware design issues are not well explored. In addition, there are limits on the size and fidelity of system which can be simulated, due to the required computational intensity, and because control systems for devices often contain software which is proprietary and cannot be modelled accurately. Physical Hardware in the Loop Simulation provides an interim stage between purely computer-based simulation, and real device deployment. Part of the power system (or “Smart Grid”) is simulated, but specific components are implemented in actual hardware. The hardware may consist of instrumentation, relays or controllers, carrying no primary current. Such testing is termed “Secondary Hardware-in-the-Loop”, as the signals exchanged between the simulation and hardware consist only of measurements and control values. A more advanced environment is created where primary power flow is exchanged with the hardware. This is termed “Primary Hardware-in-the-Loop” or “Power Hardware-in-the-Loop” testing. In addition to measurement and control signals being exchanged with the simulation, an interface is required at which primary power is exchanged between the simulation and the hardware, at the voltage and current levels suitable for the hardware under test. Creation of such environments is complex, but allows steady-state, dynamic, and worst-case scenarios to be re-created in a controlled environment. Therefore hardware-in-the-loop testing offers a cheaper, safer, faster and more comprehensive de-risking process than trying the hardware for the first time on a real network. The complexity and interconnected nature of the Smart Grid means that such Hardware in the Loop based testing is becoming even more critical to understanding the behaviour of systems and schemes, and consequently the safe and secure introduction of new technologies.",autonomous vehicle,187,not included
74fab23e3fd31db77d22074ede9de7e8c8a40c38,to_check,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,improving vehicular networking reliability and efficiency in the context of platooning applications,https://www.semanticscholar.org/paper/74fab23e3fd31db77d22074ede9de7e8c8a40c38,"Vehicular networking is a technology that enables vehicles communication system. A joint effort from the automobile industry, transportation industry, and government offices is driving the adoption of this technology to build intelligent transportation systems that consist of smart vehicles. This study attempts to improve the reliability and efficiency of vehicular networking. The study assumes the context of platooning applications, but the contributions of this study can be applied to other vehicular applications as well. There are two contributions in this study. First, a wireless emulator is designed and implemented to emulate IEEE 802.11 networks in real-time using the Ethernet infrastructure. The emulator replaces the MAC layer and physical layer of IEEE 802.11 networking stack with a real-time CSMA/CA model, thus reduces the cost of experiments. It provides upper layers the same interfaces as on a real device. As a result, the testing targets in the emulation are real-world software components as opposed to simulation scripts in a discrete event simulator. These software components can be routing protocols, transport protocols, or applications, and are the same code that can be deployed in the real world. Second, an Interframe Compression Transmission Layer is designed and implemented, to provide efficient transmission of periodical messages in vehicular environments. The transmission layer compresses the difference between frames instead of frames themselves, and reduces bandwidth consumption significantly. To improve the behaviors of the transmission layer under different scenarios and configurations studied, an adaptive version of the algorithm is designed, which achieves more than 50% in reduction of bandwidth consumption using real-world platooning data trace. With lower bandwidth consumption, delivery ratio is vastly improved in congested networking environments.",autonomous vehicle,188,not included
16af2f3a6d1f07c46bd851aa2899731136fab73e,to_check,semantic_scholar,MobiCom,2014-01-01 00:00:00,semantic_scholar,poster: ziria: language for rapid prototyping of wireless phy,https://www.semanticscholar.org/paper/16af2f3a6d1f07c46bd851aa2899731136fab73e,"Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising an ideal platform both for research and innovation and rapid deployment of new protocols on existing hardware. However, existing SDR programming platforms require either careful hand-tuning of low-level code, negating many of the advantages of software, or are too slow to be useful in the real world. We present Ziria, the first software-defined radio programming platform that is both easily programmable and performant. Ziria introduces a novel programming model tailored to wireless physical layer tasks and captures the inherent and important distinction between data and control paths in this domain. Ziria provides the capability of implementing a real-time WiFi PHY running at 20 MHz.",autonomous vehicle,189,not included
88aa14a159f0fad0a2b07445c3f091558ffbda62,to_check,semantic_scholar,FHPC '14,2014-01-01 00:00:00,semantic_scholar,ziria: wireless programming for hardware dummies,https://www.semanticscholar.org/paper/88aa14a159f0fad0a2b07445c3f091558ffbda62,"Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising both an ideal platform for research and innovation and the rapid deployment of new protocols on existing hardware. Most existing SDR platforms require careful hand-tuning of low-level code to be useful in the real world. In this talk I will describe Ziria, an SDR platform that is both easily programmable and performant. Ziria introduces a programming model that builds on ideas from functional programming and that is tailored to wireless physical layer tasks. The model captures the inherent and important distinction between data and control paths in this domain. I will describe the programming model, give an overview of the execution model, compiler optimizations, and current work. We have used Ziria to produce an implementation of 802.11a/g and a partial implementation of LTE.",autonomous vehicle,190,not included
949b01c64ba61c94ba0982ffd6abc50658d53874,to_check,semantic_scholar,SRIF@SIGCOMM,2014-01-01 00:00:00,semantic_scholar,"demo: 802.11 a/g phy implementation in ziria, domain-specific language for wireless programming",https://www.semanticscholar.org/paper/949b01c64ba61c94ba0982ffd6abc50658d53874,"Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising an ideal platform both for research and innovation and the rapid deployment of new protocols on existing hardware. However, existing SDR programming platforms require either careful hand-tuning of low-level code, negating many of the advantages of software, or are too slow to be useful in the real world. In this demo we present Ziria, the first software-defined radio programming platform that is both easily programmable and performant. Ziria introduces a novel programming model tailored to wireless physical layer tasks and captures the inherent and important distinction between data and control paths in this domain. We show the capabilities of Ziria by demonstrating a real-time implementation of WiFi PHY running at 20 MHz.",autonomous vehicle,191,not included
08ae139d6890717bea0e6243549d66caf24fa78e,to_check,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,teaching embedded systems in a mooc format,https://www.semanticscholar.org/paper/08ae139d6890717bea0e6243549d66caf24fa78e,"We have designed and implemented a Massive Open Online Class (MOOC) with a substantial lab component within the edX platform. We deployed this MOOC three times with a total enrollment of over 100,000 students. If MOOCs are truly going to transform engineering education, then they must be able to deliver classes with laboratory components. Our offering goes a long way in unraveling the perceived complexities in delivering a laboratory experience to thousands of students from around the globe. We believe the techniques developed in this class will significantly transform the MOOC environment. Effective education requires students to learn by doing. In the traditional academic setting this active learning is achieved through a lab component. Translating this to the online environment is a non-trivial task that required several important factors to come together. First, we have significant support from industrial partners ARM Inc. [1] and Texas Instruments [2]. Second, the massive growth of embedded microcontrollers has made the availability of lost-cost development platforms feasible. Third, we have assembled a team with the passion, patience, and experience of delivering quality lab experiences to large classes. Fourth, online tools now exist that allow students to interact and support each other. We used edX for the delivery of videos, interactive animations, text, and quizzes [3]. We used Piazza [4] for discussion boards and Zyante [5] for a programming reference. We partnered with element-14 [6], Digi-Key [7], and Mouser [8] to make the lab kit available and low-cost. Even though there was a $40-$70 cost to purchase the lab kit, the course completion numbers were slightly better than a typical MOOC. 7.3% of the students completed enough of the class to receive a certificate. Students completing end of the course surveys report a 95% overall satisfaction. Demographics show a world-wide reach with India, US, and Egypt being the countries with the most students. In this paper we will present best practices, successes and limitations of teaching a substantial lab across the globe. Background An embedded system combines mechanical, electrical, and chemical components along with a computer, hidden inside, to serve a single dedicated purpose [9-11]. There are over 50 billion processors based on the ARM architecture delivered into products, and most of these computers are single-chip microcontrollers that are the brains of an embedded system. Embedded systems are a ubiquitous component of our everyday lives. We interact with hundreds of tiny computers every day that are embedded into our houses, our cars, our bridges, our toys, and our work. As our world has become more complex, so have the capabilities of the microcontrollers embedded into our devices. Therefore the world needs a trained workforce to develop and manage products based on embedded microcontrollers. Review Other online classes have delivered laboratory experiences. Hesselink at Stanford University developed iLabs as a means to deliver science experiments to online learning. Their lab-in-a-box involves simulations and animations [12]. O’Malley et al. from the University of Manchester developed a Chemistry MOOC with a lab component using virtual labs and simulations [13-14]. University of Washington presented a hardware/software MOOC on Coursera [15]. This course is primarily a programming class without graded physical labs. Ferri et al. from Georgia Institute of Technology created a MOOC for linear circuits [16]. This class had activities to perform with NI’s myDAC, but graded lab circuits were not part of the online experience. Connor, and Huettel at Duke created a Virtual Community of Practice for electric circuits [17]. Cherner et al. created a virtual multifunctional X-Ray diffractometer for teaching science and engineering [18]. Saterbak et al. at Rice University developed online materials to teach freshman design, with the goal to free-up class time for more interactive learning experiences [19]. Harris from University of California at Irvine has a six-course sequence on Introduction to the Internet of Things and Embedded Systems where students build actual embedded devices [20]. Grading for this course uses peer assessment. Lee et al. at Berkeley developed an introduction to embedded systems MOOC with laboratory exercises. The lab itself was a robotic controller in a virtual laboratory environment. Completion of the labs themselves does have an automatic grading component based on the student’s written software [21-22]. All this work emphasizes the need for hands on learning. Pedagogy The overall educational objective of this class is to allow students to discover how computers interact with the environment. The class provides hands-on experiences of how an embedded system could be used to solve problems. The focus of this introductory course is understanding and analysis rather than design, where students learn new techniques by doing them. We feel we have solved the dilemma in learning a laboratory-based topic like embedded systems, where there is a tremendous volume of details that first must be learned before hardware and software systems can be designed. The approach taken in this course is to learn by doing in a bottom-up fashion. One of the advantages of a bottom-up approach to learning is that the student begins by mastering simple concepts. Once the student truly understands simple concepts, he or she can embark on the creative process of design, which involves putting the pieces together to create a more complex system. True creativity involves solving complex problems using effective combinations of simple components. Embedded systems afford an effective platform to teach new engineers how to program for three reasons. First, there is no operating system. Thus, in a bottom-up fashion the student can see, write, and understand all software running on a system that actually does something. Second, embedded systems involve real input/output that is easy for the student to touch, hear, and see. Many engineering students struggle with abstraction. We believe many students learn effectively by using their sense of touch, hearing and sight to first understand and internalize difficult concepts, and then they will be able to develop and appreciate abstractions. Third, embedded systems are employed in many everyday products, motivating students to see firsthand, how engineering processes can be applied in the real world. This course is intended for beginning college students with some knowledge of electricity as would have been taught in an introductory college physics class. Secondly, it is expected students will have some basic knowledge of programming and logic design. No specific language will be assumed as prior knowledge but this class could be taken as their second programming class. We hoped experienced engineers could also use this course to train or retrain in the field of embedded systems. Learning objectives of the course Although the students are engaged with a fun and rewarding lab experience, our educational pedagogy is centered on fundamental learning objectives. After the successful conclusion of this class, students should be able to understand the basic components of a computer, write C language programs that perform input/output interfacing, implement simple data structures, manipulate numbers in multiple formats, and understand how software uses global memory to store permanent information and the stack to store temporary information. Our goal is for students to learn these concepts: 0) How the computer stores and manipulates data; 1) Embedded systems using modular design and abstraction; 2) Design tools like requirements documents, data flow graphs, and call graphs; 3) C programming: considering both function and style; 4) Debugging and verification using a simulator and the real microcontroller; 5) Debugging tools like voltmeters, oscilloscopes, and logic analyzers; 6) How to input/output using switches, LEDs, DACs, ADCs, and serial ports; 7) Implementation of an I/O driver, multithreaded programming, and interrupts; 8) Analog to digital conversion (ADC), periodic sampling, and the Nyquist Theorem; 9) Stepper motors, brushed DC motors, and simple digital controllers; 10) Digital to analog conversion (DAC), used to make simple sounds; 11) Simple distributed systems that connect two microcontrollers; 12) Internet of things, connecting the embedded system to the internet; 13) System-level design that combine multiple components together. Laboratory Kit Active learning requires a platform for the student to learn by doing. Figure 1 shows the components of the basic lab kit. There are two difficulties with a physical lab kit deployed in a world-wide open classroom environment. The first problem is availability of components. We partnered with companies and distributors six months in advance of the course launch to guarantee availability. The companies wanted us to specify the number of students who would buy the kit. In this regard, we were very lucky. Six months prior to our first launch, we estimated 2000 people would register for the class and 1000 would buy the kit. In turns out Texas Instruments produced 10,000 microcontroller boards just in case. Much to our surprise 40,000 people registered and we estimate 11,000 purchased the kit during this first delivery of the course. The second solution to the problem of availability was to have three world-wide distributors (element-14, Mouser, and Digi-Key). Working with these distributors, we created one-click landing pages for students to buy the kit. Furthermore, for each component in the kit (other than the microcontroller board), we had three or more possible parts. The third solution was to design the course with flexible deadlines and pathways. Each lab had a simulation and a real-board requirement. Students who were waiting for the parts to be shipped could proceed with ",autonomous vehicle,192,not included
6220b68cd721512098b9b14851afe5e660c0e565,to_check,semantic_scholar,,2015-01-01 00:00:00,semantic_scholar,crypto-day campeon a8,https://www.semanticscholar.org/paper/6220b68cd721512098b9b14851afe5e660c0e565,"Using the properties of a wireless channel is an alternative approach for securing the channel besides pre-shared keys or asymmetric cryptography. Numerous experiments have recently demonstrated that channel-based key establishment (CBKE) is a promising alternative to well-known symmetric/asymmetric approaches. Their run-times for establishing a symmetric key suggest that such methods are highly suitable for real-world applications that operate in a dynamic mobile environment with peer-to-peer association. CBKE is a new paradigm for generating shared secret keys. The approach is based on the estimation of the wireless transmission channel by both the sender and receiver, where the shared secret key is derived from channel parameters. The commonness of the randomness of the secret key relies on the principle of channel reciprocity. Specifically, this means that the channel from Alice to Bob is the same than the channel from Bob to Alice. This symmetry of practical channels is usually sufficiently high, as well as its entropy of spatial, temporal, and spectral characteristics. Security is given if an attacker’s distance to the two communicating nodes is high enough, so that the observed channel parameters to each node are uncorrelated and independent from each other. Typically, in real environments this is given if the distance is greater than about half of the carrier wavelength. For instance, for the frequency used in 2.4 GHz WiFi, this translates to a distance of 6.25 cm. So far, high usability and dynamic key management are very difficult to achieve for wireless devices, which operate under strict resource constraints. CBKE has the potential to significantly reduce the cost of securing small embedded devices, and hence make mass production and deployment more viable. Until now, no research has addressed the requirements for performance evaluation of real-world implementations of CBKE systems. We present a wireless CBKE security system built with standard components, e.g., quantization scheme and error correction codes, presented in recent publications. We introduce necessary implementation properties and requirements of CBKE systems. In order to validate the performance of the key generation algorithms, we define a set of metrics. Finally, we describe an end-to-end implementation on an ARM-Cortex M3 microcontroller to demonstrate the practical feasibility of channel-based key estimation using current embedded hardware. Comparative analysis of pseudorandom generators Aleksei Burlakov, Johannes vom Dorp, Joachim von zur Gathen, Sarah Hillmann, Michael Link, Daniel Loebenberger, Jan Lühr, Simon Schneider & Sven Zemanek {burlakov,dorp,luehr,schneid,zemanek}@cs.uni-bonn.de {sarah.hillmann,michael.link}@uni-bonn.de {gathen,daniel}@bit.uni-bonn.de Bonn-Aachen International Center for Information Technology Dahlmannstr. 2, Bonn We compare random generators (RGs) under controlled conditions regarding their efficiency and statistical properties. For this purpose, we distinguish between physical RGs and software RGs, which can be further subdivided into cryptographically secure and insecure RGs. Physical RGs covered by our study are the hardware generator PRG310-4 and /dev/random as implemented in the Linux kernel. Since /dev/random is fed by system events, we analyze both an idle lab environment and a server hosting several virtual machines. As examples for cryptographically secure RGs our analysis compares the RSA generator and the Blum-Blum-Shub generator, both for 3000-bit moduli. Additionally, we compare them to the Nisan-Wigderson construction with suitably selected parameters. We include two cryptographically insecure RGs, namely a linear congruential generator (LCG) and the Littlewood generator. In order to obtain repeatable and comparable results, our implementations of the software RGs were all run on the same machine and produced 512 kB of output each, using AES post-processed output of the generator PRG310-4 as source for random seed bits. We compare the results in terms of byte entropy and throughput excluding initialization. For further statistical analysis — not shown in the table — we apply the NIST test suite on the outputs. The most important finding is that in our scenarios, number-theoretic generators compete very well against hardware-based ones. byte entropy runtime throughput throughput [bit] [μs] [kB/s] normalized PRG310-4, no post-processing 7.99963 16308400 31.39486 4.34492 AES post-processing 7.99963 36524300 14.01806 1.94004 /dev/random, in the field 7.99979 9.169× 10 5.584× 10−3 7.728× 10−4 in the lab 7.99948 2.671× 10 1.917× 10−4 2.653× 10−5 Littlewood 6.47244 15206550 33.66970 4.61011 Linear congruential generator 7.99969 2644039 193.64313 26.51392 Blum-Blum-Shub 7.99962 17708350 28.91291 3.95880 RSA, e = 2 + 1, 1400 bit/round 7.99966 267604 1913.27484 261.96857 e = 3, 1 bit/round 7.99963 70103838 7.30345 1 Nisan-Wigderson 7.99961 2731227 187.46153 25.66753 Table 1: Overview of the results for generating 512 kB of output.",autonomous vehicle,193,not included
a2ca591957d1081bbf4b1a04c565b8f365c014d8,to_check,semantic_scholar,,2017-01-01 00:00:00,semantic_scholar,literature survey: a design approach to smart system based on internet of thing (iot) for intelligent transportation,https://www.semanticscholar.org/paper/a2ca591957d1081bbf4b1a04c565b8f365c014d8,"Recent years, the transportation efficiency and related issues have become one of the main focuses of the global world. Along this line, intelligent transportation systems (ITS) based on Internet of Things (IoT) provided a promising chance to resolve the challenges caused by the increasing transportation problems, such as traffic prediction, road status evaluation, traffic accident detection, etc. In this, The Internet of Things is based on the Internet, network wireless sensing and detection technologies to realize the intelligent recognition on the tagged traffic object, tracking, monitoring, managing and processed automatically. IoT based intelligent transportation systems are designed to support the Smart City vision, which aims at employing the advanced and powerful communication technologies for the administration of the city and the citizens. Keywords—IoT, transportation. I. LITERATURE SURVEY K.Ashokkumar, Baron Sam, R.Arshadprabhu, Britto [1] proposes the advances in cloud computing and web of things (IoT) have provided a promising chance to resolve the challenges caused by the increasing transportation problems. They tend to gift a unique multilayered conveyance knowledge cloud platform by exploitation cloud computing and IoT technologies to resolve the challenges caused by the increasing transportation issues. They present a novel multilayered vehicular data cloud platform by using cloud computing and IoT technologies. Two innovative vehicular data cloud services, an intelligent parking cloud service and a vehicular data mining cloud service in the IoT environment are also presented reviews. Amir-Mohammad Rahmani, Nanda Kumar Thanigaivelan, Tuan Nguyen Gia, Jose Granados, Behailu Negash, Pasi Liljeberg, and Hannu Tenhunen [2] proposes the strategic position of gateways to offer several higherlevel services such as local storage, real-time local data processing, embedded data mining, etc., proposing thus a Smart e-Health Gateway. By taking responsibility for handling some burdens of the sensor network and a remote healthcare center, a Smart e-Health Gateway can cope with many challenges in ubiquitous healthcare systems such as energy efficiency, scalability, and reliability issues. Michele Nitti, Luigi Atzori, and Irena Pletikosa Cvijikj [3] addressed the issue by analyzing possible strategies for the benefit of overall network navigability.They first propose five heuristics, which are based on local network properties and that are expected to have an impact on the overall network structure. Thet then perform extensive experiments, which are intended to analyze the performance in terms of giant components, average degree of connections, local clustering, and average path length. Jianli Pan, Raj Jain, Subharthi Paul, Tam Vu, Abusayeed Saifullah, Mo Sha [4] proposes an IoT framework with smart location-based automated and networked energy control, which uses smartphone platform and cloud-computing technologies to enable multiscale energy proportionality including building-, user-, and organizational-level energy proportionality. They further build a proof-of-concept IoT network and control system prototype and carried out real-world experiments, which demonstrate the effectiveness of the proposed solution. They envision that the broad application of the proposed solution has not only led to significant economic benefits in term of energy saving, improving home/office network intelligence, but also bought in a huge social implication in terms of global sustainability Catarinucci, L. , de Donno, D. , Mainetti, L. , Palano, L. [5] proposes a novel, IoT-aware, smart architecture for automatic monitoring and tracking of patients, personnel, and biomedical devices within hospitals and nursing institutes. Staying true to the IoT vision, they propose a smart hospital system (SHS), which relies on different, yet complementary, technologies, specifically RFID, WSN, and smart mobile, interoperating with each other through a Constrained Application Protocol (CoAP)/IPv6 over lowpower wireless personal area network (6LoWPAN)/representational state transfer (REST) network International Conference on Science and Engineering for Sustainable Development (ICSESD-2017) (www.jit.org.in) International Journal of Advanced Engineering, Management and Science (IJAEMS) Special Issue-1 https://dx.doi.org/10.24001/icsesd2017.49 ISSN : 2454-1311 www.ijaems.com Page | 195 infrastructure. The SHS is able to collect, in real time, both environmental conditions and patients' physiological parameters via an ultra-low-power hybrid sensing network (HSN) composed of 6LoWPAN nodes integrating UHF RFID functionalities. Sensed data are delivered to a control center where an advanced monitoring application (MA) makes them easily accessible by both local and remote users via a REST web service. Al-Fuqaha, A., Kalamazoo, MI, Guizani, M. , Mohammadi, M., Aledhari, M. [6] provides a more thorough summary of the most relevant protocols and application issues to enable researchers and application developers to get up to speed quickly on how the different protocols fit together to deliver desired functionalities without having to go through RFCs and the standards specifications. They also provides an overview of some of the key IoT challenges presented in the recent literature and provide a summary of related research work. Moreover, they explore the relation between the IoT and other emerging technologies including big data analytics and cloud and fog computing. They also presents the need for better horizontal integration among IoT services. Stecca, M., Moiso, C., Fornasa, M., Baglietto, P. [7] presents app execution platform (AEP), a platform that supports the design, deployment, execution, and management of IoT applications in the domain of smart home, smart car, and smart city. AEP was designed to coherently fulfill a set of requirements covered only partially or in a fragmented way by other IoT application platforms. AEP focuses on SO virtualization and on composite application (CA) orchestration and supports dynamic object availability. Yi-Bing Lin, Yun-Wei Lin, Chang-Yen Chih, Tzu-Yi Li [8] proposes an IoT device which is characterized by its “features” (e.g., temperature, vibration, and display) that are manipulated by the network applications. If a network application handles the individual device features independently, then we can write a software module for each device feature, and the network application can be simply constructed by including these brick-like device feature modules. Based on the concept of device feature, brick-like software modules can provide simple and efficient mechanism to develop IoT device applications and interactions. Ganz, F. , Puschmann, D. , Barnaghi, P. , Carrez, F. [9] provides a survey of the requirements and solutions and describes challenges in the area of information abstraction and presents an efficient workflow to extract meaningful information from raw sensor data based on the current stateof-the-art in this area and also identifies research directions at the edge of information abstraction for sensor data. To ease the understanding of the abstraction workflow process, they introduce a software toolkit that implements the introduced techniques and motivates to apply them on various data sets. Aijaz, A. , Aghvami, A.H.[10] provides the state of the art in cognitive M2M communications from a protocol stack perspective, covers the emerging standardization efforts and the latest developments on protocols for cognitive M2M networks which includes a centralized cognitive medium access control (MAC) protocol, a distributed cognitive MAC protocol, and a specially designed routing protocol for cognitive M2M networks. These protocols explicitly account for the peculiarities of cognitive radio environments. Performance evaluation demonstrates that the proposed protocols not only ensure protection to the primary users (PUs) but also fulfil the utility requirements of the secondary M2M networks. Tsirmpas, C., Anastasiou, A., Bountris, P., Koutsouris, D. [11] proposes a new methodology based on self organizing maps (SOMs) and fuzzy C-means (FCM) algorithms for profile generation as regards the activities of the user and their correlation with the available sensors. Moreover, we utilize the provided context to assign the generated profiles to more contextually complex activities. This methodology is being evaluated into an AAL structure equipped with several sensors. More precisely, they assess the proposed method in a data set generated by accelerometers and its performance over a number of everyday activities Mainetti, L., Lecce, Mighali, V. ; Patrono, L. [12] proposes a software architecture to easily mash-up constrained application protocol (CoAP) resources. It is able to discover the available devices and to virtualize them outside the physical network. These virtualizations are then exposed to the upper layers by a REpresentational State Transfer (REST) interface, so that the physical devices interact only with their own virtualization. Furthermore, the system provides simplified tools allowing the development of mash-up applications to different-skilled users. Finally, the architecture allows not only to monitor but also to control the devices, thus establishing a bidirectional communication channel. Hasan Omar Al-Sakran [13] presents a novel intelligent traffic administration system, based on Internet of Things, which is featured by low cost, high scalability, high compatibility, easy to upgrade, to replace traditional traffic management system and the proposed system can improve road traffic tremendously. The Internet of Things is based on the Internet, network wireless sensing and detection technologies to realize the intelligent recognition on the tagged traffic object, tracking, monitoring, managing and processed automatically. The paper proposes an architecture that integrates internet of things with",autonomous vehicle,194,not included
3f826f1f349ee707639c39d231259498b14215c5,to_check,semantic_scholar,,2011-01-01 00:00:00,semantic_scholar,ac 2011-2689: smart grid development in electrical dis- tribution network,https://www.semanticscholar.org/paper/3f826f1f349ee707639c39d231259498b14215c5,"This paper will focus on smart grid project design and implementation. The project was developed by students and demonstrates new ideas and teamwork. This project was successfully completed and has been developed, implemented and assessed. Topics covered are: how to build a smart gird by utilizing computer application software tools, design, simulation, and diagnoses of electrical distribution systems. All the real world components in electrical distribution network such as residential, commercial and industrial building are modeled in this project. Background The purpose of this project is to design and implement a small scale electric power network by a team of seven students, supervised by a faculty member. The students’ background is in electrical engineering with emphasis in electric power system. The students conducted a study in the field of Smart Grid technologies for history and background information. This work led to designing and implementing a small model of a smart gird power distribution network. The power grid represents the real world aspirations of both government and private industry geared toward building a more reliable, responsive, and overall efficient network of residential, industrial and commercial buildings. Since the concept of a smart grid is very vague, students chose to implement a time tested and proven aspect of such technology known as smart meters. The smart meter is a wireless device connected to every house, industrial and commercial buildings to provide essential feedback in real time to the power companies. This feedback could be in the form of a fault occurring at that said location, or illegal energy usage. This feedback in real time is very useful to the power companies, given the fact that most rely on feedback via a phone call from the consumer before they know whether or not there is a fault in the system. Another purpose of implementing the grid was to simulate metering technology at the residential, industrial and commercial level. These meters would send data to a computer which is a simulated control room in order to read where certain faults occur in the system. In turn one could control which areas of the grid would be supplying the power. This represents a simulation of the power company’s ability to read and send vital information throughout the grid, thus improving the responsiveness and reliability of the network. Figure 1 illustrates the completed model after it was built and during testing. The lifecycle of this project was implemented in three different phases and started in September of 2009 and it was completed in May of 2010. Planning and analysis was completed in phase I, design and implementation in phase II, and documentation and students’ assessment in phase III. Figure 1. A model of smart grid in electrical distribution system Phase I: Planning and Analysis Initially, each team member worked on individual research on the concepts of smart grid its purpose. Later on, a decision was made as to what the team wanted to demonstrate with the project. The decision was made to show specifically how smart meters would work and help in fault detection as well as saving money by removing the need for meter readers to read the power meters every month. A project leader was elected by the team members to coordinate the team work. Meetings were then set up by the project leader, to brainstorm on how the actual implementation was going to be planned. Microsoft Project software was very instrumental to organize the work of the team. Tasks were assigned with specific due dates to keep the project on schedule and under budget. Requirements Since this project was spread over three quarters, students had many deadlines and task that had to be met in order to have a successful project. There were three phases to this project, research, background study and planning during the 1 quarter, design and building during the 2 quarter and the final stage of testing and troubleshooting during the 3 quarter along with final oral presentation, simulation documentation and assessment of the project. The students made documents and recorded each steps of the project down to each task and timeline by using Microsoft Project software. The project advisor coordinated the project steps and students were required to present a weekly progress report. This step insured that the project was moving smooth and on the track. The group was divided into two teams, one in software teams which consisted of two members and a hardware team which consisted of other five group members. The software team was in charge of all the coding and GUI implementation so the actual grid can communicate back and forth with the computer. The hardware team was in charge of the physical grid which consisted of the circuit that was built using logic chips such as MUXs, and Flip-Flops, wiring, creating a map on the grid with houses, roads, school, power stations, sub-stations, transmission lines, and distributions lines. The commercial site consists of shopping area, factories, stadium, school and so forth. In final stages of the project, testing, debugging and troubleshooting was performed in order to assure that hardware components and related software can communicate back and forth in a proper sequence. Much of the requirements had the made along the way since this was very new to all students. Phase II: Design and Implementation The design started immediately after the clear definition of the project requirement and purpose. To lower the cost and improve the safety, the design would be a DC (Direct Current) representation of an AC (Alternate Current) system. The system was designed by drawing out the model of a city and the specific buildings to exist in that city. The design was based on what took place in the planning stage which defined how the city and buildings will receive their power and the power. Figure 2. The process of building a smart grid The next challenge in the design process was solving the problem of switches and smart meters. Figure 2 shows the design of the smart meters and placement of LEDs (Light Emitting Diode). The LEDs will represent whether a particular house, building or transformer has power on or off. If for any reason an LED was not lit, then that particular item does not have power. The faults were determined by voltages because even if the building wasn’t drawing power, then there still would be a voltage on the line. This voltage was then sent to a 64 to 1 multiplexor which was then sent to the microcontroller to determine faults. To turn the power of buildings “on” and “off” a common NPN transistor (2n2222) and the base current was provided by a flip flop integrated circuit. Flip flops were used due to I/O’s limitations of the PIC. Figure 3 was duplicated for every transformer, with the only difference being the number of buildings being fed from the transformer which is the first LED after the 12V source. Figure 3. Circuit diagram for buildings Implementation A Smart Grid system includes a power meter which enables the communication systems to update the utility about its condition and the electronics to control the meter. The old electromechanical meters that were used are becoming obsolete since they cannot support the features that the utilities desire to have such as monitoring and controlling power supplied to its customers. Utilities wishes to monitor power consumption so that they can accurately predict how much power will be used during peak and down times. This information is helpful in producing sufficient energy and better efficiency in power waste. It can also help to pinpoint locations of power outages leading to a quicker recovery time. Challenges in implementation of the system are based on a couple of issues. First is the cost. It could cost upwards of $1000 for each smart meter, depending on features to be installed for each house or business. The costs can add up quickly, and the utilities don't see any immediate savings or incentive to deploy the smart grid in a very near future. The system designed in this project is using smart meters with a simulated wireless connection to the central servers at the utilities. The meters would send a signal to the central computer to update its status, power consumption, and other things. It can be designed in a way that it will have a battery backup for when the power is interrupted, or have the central computer assume it is off when it doesn't send a signal at the regular time intervals. Obviously the latter option would be the most cost effective and would use less power to run. But having power to the smart meter could also be beneficial because diagnostics could be run to determine if the power went out or if the meter is having its own internal hardware problems. A wireless signal was simulated for this project, but in real world application one can use either wireless, normal phone lines, or communications over power line. Most utilities already have communications systems set up through their power lines and using this method would be most cost effective. Having wireless, on the other hand, frees up usage of the power lines reducing their stress and prolonging the cables life. Companies are developing and testing their own systems using one of those options. In any case, it is based on hardware availability and cost effectiveness. Figure 4 illustrates communication with the smart grid. Figure 4. Communication with the smart grid Phase III: Documentation and Students’ Assessment In phase III of the project, the students provided a detail documentation of the project which includes cost analysis and different phases of the design. An electronic copy of this documentation and demo presentation was produced in a DVD. The following assessment and lessons learned was observed during the life cycle of the project: 1) When the main board that was used in the final project was constructed, the problems of wiring of all o",autonomous vehicle,195,not included
a2524b60d8c51af32ee36f9d3ccb0761c8b593f6,to_check,semantic_scholar,,2003-01-01 00:00:00,semantic_scholar,indexing and retrieving semantic web resources: the rdfstore model,https://www.semanticscholar.org/paper/a2524b60d8c51af32ee36f9d3ccb0761c8b593f6,"The Semantic Web is a logical evolution of the existing Web. It is based on a common conceptual data model of great generality that allows both humans and machines to work with interrelated, but disjoint, information as if it was a single global database. The design and implementation of a general, scalable, federated and flexible data storage and indexing model, which corresponds to the data model of the Semantic Web, is fundamental for the success and deployment of such a system. The generality of the RDF data model presents unique challenges to efficient storage, indexing and querying engines. This paper presents our experience and work related to RDFStore which implements a new flexible indexing and query model. The model is tailored to RDF data and is designed around the Semantic Web from the ground up. The paper describes the underlying indexing algorithm, together with comparisons to other existing RDF storage and query strategies. Towards a lightweight database architecture The generality of the RDF data model presents unique challenges to efficient storage, indexing and querying software. Even if the Entity-Relational (ER) data model [1] is the dominant technology for database management systems today, it has limitations in modeling RDF constructs. RDF being unbounded, the resulting data structures are irregular, expressed using different data granularity, deeply nested or even cyclic. As a consequence, it is not possible to easily fix the ""structural view"" of a piece of information (object), which is instead one of the fundaments of traditional RDBMS systems trying to be much narrower and precise as possible and where an update not conforming to a single static schema is rejected. Database systems also optimize data storage and retrieval by knowing ahead of time how records are structured and interrelated and tend to use very inefficient nested SQL SELECT statements to process nested and cyclic structures. All this is too restrictive for RDF data. Like most semi-structured formalisms [2][3] RDF is self-describing. This means that the schema information is embedded with the data, and no a priori structure can be assumed, giving a lot of flexibility to manage any data and deal with changes in the data's structure seamlessly at the application level. The only basic structure available is the RDF graph itself, which allows describing RDF vocabularies as groups of related resources and the relationships between these resources [4]. All new data can be ""safely"" accepted, eventually at the cost of tailoring the queries to the data. On the other side, RDF data management systems must be much more generic and polymorphic like most of dynamically-bound object-oriented systems [5]; changes to the schema are expected to be as frequent as changes to the data itself and could happen while the data is being processed or ingested. A drawback of RDF heterogeneity is that the schema is relatively large compared to the data itself [6]; this in contrast to traditional RDBMS where the data schema is generally several orders of magnitude smaller than the data. This also implies that RDF queries over the schema information are as important as queries on the data. Another problem is that most RDF data (e.g. metadata embedded into an HTML page or RSS1.0 news feed) might exist independently of the vocabulary schemas used to mark-up the data, further complicating data structure ""validation"" (RDF Schema validation). This de-coupling aspect also makes the data ""de-normalization"" more difficult [7][8][9]. ""De-normalization"" is needed in RDBMS to overcome query performance penalties caused by the very general ""normalized"" schemas. De-normalization must be done taking into account to how the database will be used and how data is initially structured. In RDF this is not generally possible, unless all the RDF Schema definitions of the classes and properties used are known a-priori and available to the software application. Even if that might be the case, it is not a general rule and it would be too restrictive and make RDF applications extremely fragile. In the simplest and most general case, RDF software must associate the semantics to a given property exclusively using the unique string representation of its URIs. This will not stop of course more advanced and intelligent software to go a step further and retrieve, if available, the schema of the associated namespace declarations for validation, optimization or inference purposes. It is interesting to point out that a large part of queries foreseen for Web applications are information discovery and retrieval queries (e.g. Google) that can ""ignore"" the data schema taxonomy. Simple browsing through the RDF data itself or searching for some sub-string into literals, or using common patterns is generally enough for a large family of RDF applications. On the other hand, we strongly believe that RDBMS has proven to be a very effective and efficient technology to manage large quantities of well-structured data. This will continue to be true for the foreseeable future. We thus see RDF and similar less rigid, or semi-structured data technologies as complementary to traditional RDBMS systems. We expect to see RDF increasingly appear in the middle layer where lightweight systems that focus on interoperability, flexibility and a certain degree of decoupling of rigid formats are desired. We believe that a fundamentally different storage and query architecture is required to support the efficiently and the flexibility of RDF and its query languages. At a minimum such storage system needs to be: Lightweight Native implementation of the graph Fundamentally independent from data structure Allow for very wide ranges in value sizes; where the size distribution is not known in advance, most certainly is not Gaussian and will fluctuate wildly. Be efficient it should not be necessary to retrieve very large volumes of data in order to reconstruct part of the graph. Allow built support for arbitrary complex regular-path-expressions on the graph to match RDF queries like RDQL [50] statement triple-patterns. Have some free-text support Context/provenance/scope or flavoring of triples Furthermore given that RDF and the Semantic Web are relatively new, and will require significant integration and experimentation it is important that its technology matches that of the Internet: Easy to interface to C, Perl and Java at the very least. Ruby, Python, Visual Basic and .NET are a pre. Easy to distribute (part of) the solution across physical machines or locations in order match scaling and operational habits of existing key Internet infrastructure. Very resistant to ""missing links"" and other noise. Contexts and provenance A RDF statement represents a fact that is asserted as true in a certain context space time, situation, scope, etc. The circumstances where the statement has been stated represent its ""contextual"" information [10][11]. For example, it may be useful to track the origin of triples added to the graph, e.g. the URI of the source where triples are defined, e.g. in an RDF/XML file, when and by whom they where added and the expiration date (if any) for the triples. Such context and provenance information can be thought of as an additional and orthogonal dimension to the other components of a triple. The concept is called in the literature ""statement reification"". Context and provenance are currently not included in the RDF standardisation process [48][49], but will hopefully adressed in a next release of the specification. From the application developer point of view there is a clear need for such primitive constructs to layer different levels of semantics on top of RDF which can not be represented in the RDF triples space. Applications normally need to build meta-levels of abstraction over triples to reduce complexity and provide an incremental and scaleable access to information. For example, if a Web robot is processing and syndicating news coming from various on-line newspapers, there will be overlap. An application may decide to filter the news based not only on a timeline or some other property, but perhaps select sources providing only certain information with unique characteristics. This requires the flagging of triples as belonging to different contexts and then describing in the RDF itself the relationships between the contexts. At query time such information can then be used by the application to define a search scope to filter the results. Another common example of the usage of provenance and contextual information is about digital signing RDF triples to provide a basic level of trust over the Semantic. In that case triples could be flagged for example with a PGP key to uniquely identify the source and its properties. There have been several attempts [12][13][14][15] trying to formalize and use contexts and provenance information in RDF but a common agreement has not been reached yet. However, context and provenance information come out as soon as a real application is built using RDF. Some first examples are presented below. Our approach to model contexts and provenance has been simpler and motivated by real-world RDF applications we have developed [16a][16b][16c]. We found that an additional dimension to the RDF triple can be useful or even essential. Given that the usage of full-blown RDF reification is not feasible due to its verbosity and inefficiency we developed a different modeling technique that flags or mark a given statement as belonging to a specific context. First example considers subjective assertions. The Last Minute News (LMN) [16b] and The News Blender (NB) [16c] demos allow an user rating and qualifying the source newspapers. The user can ""say"" that a newspaper is ""liberal"" or ""conservative"". Of course, two users, X and Y, will show two different opinions. Without considering the context, this will result in two triples: Newspaper A -> Quality -> ""liberal"" Newspaper A -> Qu",autonomous vehicle,196,not included
c55e4acdf98b1931bf1e00280639348d51a6283a,to_check,semantic_scholar,,2002-01-01 00:00:00,semantic_scholar,a hierarchical collective agents network for real-time sensor fusion and decision support,https://www.semanticscholar.org/paper/c55e4acdf98b1931bf1e00280639348d51a6283a,"This research addresses a problem of how to make effective use of real-time information acquired from multiple sensor and heterogeneous data resources, and reasoning on the gathered information for situation assessment and impact assessment (SA/IA), thus to provide reliable decision support for time-critical operations. A hierarchical collective agents network (HCAN) is employed as a solution to this problem. The agents network supports multi-sensor registration, real-time sensor/platform cueing, level-2 and level-3 information fusion, and has an arm toward the level-4 fusion objectives. An agent component assembly and decision-support-system-development environment, the 21 Century systems’ AEDGE software package, is used for the design and implementation of a HCAN-DSS system. The ability to integrate and correlate a vast amounts of disparate information from multiple sensor and heterogeneous data resources with varying degrees of uncertainty in real-time is an impediment issue for missioncritical decision support systems (DSS). For example, in crucial military operations command officers need real-time information and intelligences from various sensors/data resources in a theater of reconnaissance and surveillance to build a whole picture of the battle-space. It is critical for the commanders to know and to understand the relationships among the information collected. Questions are asked: what are the physical and functional constituencies among the objects in a given geographic sector? Are there sequential or temporal dependencies of the objects and what will trigger them? What are the possible consequences of the action and re-actions? Decision making based on these situation assessment and impact assessment (SA/IA) are particularly important for identifying and prioritizing “gaps” between the operation planning and the real-time interactions. To support making effective SA/IA, a data fusion and decision support system is required to use a set of coherent patterns derived from the available data sets and infer the implications (e.g., causal relations) toward the real world situations. The attribute coherence that is critical to the formation of the meaningful knowledge patterns is often obscure in the data sets obtained from heterogeneous resources. The data collections are often incomplete, imprecise, and inconsistent due to various natural constraints and human faults. Decision makers naturally desire to access large quantities of information expressed in diverse forms. However, as new sensor technology and various information sources have combined to create quantity and diversity, it has become increasingly difficult to provide decision makers with the right information at the right time and in the right quantity and format. Real-time computerized decision support systems are constructed by integrating a number of diverse components from a variety of software modules. Software developers have come to a numerous ways of querying the local and centralized data resources to access and distill the large and diverse information for the purpose of providing effective decision support. Meanwhile DSS are becoming more and more complex in terms of knowing which data resources to connect, how to keep track of the data dynamics, and assess the reliability of information from each resources. These tying links make the use of intelligent agent architecture necessary and desirable for allowing real-time responsibility and adaptive control of the DSS. Many popular agent systems of today deploy agents in a uniform level of operation. The agents respond to the same calls and cooperate at the same time toward the same goal of operation. The architecture endues some difficulties in agent communications and task control. When applied in complex real-time DSS with intensive human and system interactions, the cooperative nature makes the system less robust because the disability of one agent would affect the successive operations of the entire agent assembly. The collective nature of the agents in a HCAN paradigm overcomes some of these difficulties, for example, relieving the burden of data-exchanges between fellow agents by limiting agent communication to vertical layers of the assembly only. The hierarchical architecture simplifies the functional design of the agent interactions and enhances the security and efficiency of the process. The HCAN architecture also strikes a balance between the centralized control and distributed computation by allowing distributive agent operations within layers of the hierarchy and enforcing centralized control between the layers of the hierarchy, thus creating a federated agents integration structure. Basically, the HCAN has the functionalities of. 1). A flexible software architecture for accommodating system augmentation and evolutions; 2). A powerful representation schema for accommodating heterogeneous forms of information; 3). A diverse interface for various input resources, output formats, and human interactions; From: AAAI Technical Report WS-02-15. Compilation copyright © 2002, AAAI (www.aaai.org). All rights reserved. 4). An ability of reasoning on incomplete and inconsistent information, and extracting useful knowledge from the data of heterogeneous resources; 5). An ability of incorporating real-time dynamics of information resources into system at time of operation, and promptly adjusting the reasoning mechanisms; 6). An ability of summarizing and refining knowledge extracted, and distinguishing mission and time critical knowledge from insignificant and redundant ones; 7). A capability of supplying meaningful and accurate explanations, both qualitatively and quantitatively, of the automated system actions; and 8). A capability of providing adequate control and scrutiny of the system operations w.r.t. environment constrains. There are many sources of uncertainty at different levels of the decision support. For example, even if a situationassessor is aware of the presence of certain objects in the operation space, such as the type of contact, intention, reaction rational, etc., the exact dynamics of the object is still uncertain to the decision maker. While the knowledge about the object dynamics is critical in constructing an optimal strategy of action, various statistical methods and knowledge discovery techniques are applied in the reasoning module. The level of uncertainty forces the reasoning agents to operate with different decision strategies. The 21 Century Systems, Inc. has developed the AEDGE as an open DII-COE and CORBA compliant agentbased environment that enables the development of components-based agent systems. The system is implemented in JavaTM, with Java Database ConnectivityTM for DB access, Java Swing, AWT, and Java3D for visual interfaces, Java Media Framework and Java Speech API for audio/speech interface. AEDGE defines Agents, Entities, Avatars and their interactions with each other and with external sources of information. This standardized architecture allows additional components, such as servicespecific DSS tools, to be efficiently built upon the core functionality. Common interfaces and data structures can be exported to interested parties who wish to extend the architecture with new components, agents, servers, or clients. When the core AEDGE components are bundled with customer-specific components, a clean separation of those components, through APIs, is provided. The AEDGE is based on an extensible multi-component DSS architecture (EMDA, also referred to as the AEDGETM Architecture). The architectures describe the data objects, interfaces, communication mechanisms, component interactions, and integration mechanisms for the AEDGE and its extensions. In the AEDGE architecture, components communicate among each other via the Service Provider/Service Requester Protocol (SPSR). Service providers are components that implement an algorithm or need to share their data (data sources). Service requesters are the components that need a function performed for them by some other component or need to import data from another component. Both service requesters and service providers implement remote interfaces, which enables such components to communicate over a TCP/IP network. The remote interface implementation is currently based on Java RMI (remote method invocation), though the Architecture is not dependent on this implementation. AEDGE provides multiple levels of customization. The subject-matter users are able to build scenarios and scripts or to automatically generate them using the AEDGE-based Scenario Editor. Rules and triggers for agent behaviors can be created and modified by the advanced user. AEDGE also provides APIs for custom extensions of agents, data bridges, and the entity framework. The practical user will enjoy AEDGE’s versatile data connectivity and its near-real-time execution and monitoring of DSS functions. As a built-in bonus, AEDGE provides connections to a number of simulators and data formats, including HLA, DIS, DTED, DBDB2, XML, as well as support for multiple modes of distribution (CORBA, RMI, TCP/IP). As an example of the HCAN design using AEDGE for data fusion and DSS applications, the Advanced Battlestation with Decision Support System (ABS/DSS) which was developed as an operational agent-based C2 team decision support platform for command and control centers aboard aircraft carriers.. The ABS/DSS is based on AEDGE’s implementation of HCAN, and provides consolidated situational awareness through real-time, interactive, agent based decision support coupled with a linked 2D/3D battlespace visualization. Additionally, the ABS/DSS supports shipboard distributed training, train-asyou-fight, with a built-in scenario construction and emulation of friendly and hostile entities. Whether the watchstander is in live-feed mode or in training mode the operation of the agent-based decision support system and the 2D/3D visualization is identical. ",autonomous vehicle,197,not included
4a99aa2ca1dd85ac7b82d79bf8cec1a09c9d8488,to_check,semantic_scholar,,2012-01-01 00:00:00,semantic_scholar,energy efficient protocol design in wireless sensor networks – contributions to make the ubiquitous platform greener,https://www.semanticscholar.org/paper/4a99aa2ca1dd85ac7b82d79bf8cec1a09c9d8488,"s all hardware resources as components. For example, calling the getData() command on a sensor component will cause it to later signal a dataReady() event when the hardware interrupt fires. While many components are entirely software based, the combination of split-phase operations and tasks makes this distinction transparent to the programmer. In both cases an event signals that the encryption operation is complete. ADC, ClockC, UART, SlavePin and SpiByteFifo are example hardware abstraction components. TinyOS commands and events are very short, due to limited code space and a finite state machine style of decomposition. The rich event processing model means an event or command call path can traverse several components. The TinyOS component model allows us to easily change the target platform from mote hardware to simulation by only replacing a small number of low-level components. The event-driven execution model can be exploited for efficient eventdriven simulation, and the whole program compilation process can be re-targeted for the simulator‟s storage model and native instruction set. The static component memory model of TinyOS simplifies state management for these large collections. Setting the right level of simulation abstraction can accurately capture the behavior and interactions of TinyOS applications. Figure 1.3: TinyOS Structure (Consist of scheduler and graph of components) 2.3 TOSSIM: A Simulator for TinyOS Sensor Networks The Necessity of Network Simulation: The emergence of wireless sensor networks brought many open issues to network designers. Traditionally, the three main techniques for analyzing the performance of wired and wireless networks are analytical methods, computer simulation, and physical measurement. However, because of many constraints imposed on sensor networks, such as energy limitation, decentralized collaboration and fault tolerance, algorithms for sensor networks tend to be quite complex and usually defy analytical methods that have been proved to be fairly effective for traditional networks. Furthermore, few sensor networks have come into existence, for there are still many unsolved research problems, so measurement is virtually impossible. It appears that simulation is the only feasible approach to the quantitative analysis of sensor networks. The event-driven nature of sensor networks means that testing an individual mote is insufficient. Programs must be tested at scale and in complex and rich conditions to capture a wide range of interactions. Deploying hundreds of motes is a daunting task, the focus of work shifts from research to maintenance, which is time-consuming due to the failure rate of individual motes. A simulator can deal with these difficulties, by providing controlled, reproducible environments, by enabling access to tools such as debuggers, and by postponing deployment until code is well tested and algorithms are understood. TOSSIM: TOSSIM is a discrete event simulator for TinyOS sensor networks. Instead of compiling a TinyOS application for a mote, users can compile it into the TOSSIM framework, which runs on a PC. This allows users to debug, test, and analyze algorithms in a controlled and repeatable environment. As TOSSIM runs on a PC, users can examine their TinyOS code using debuggers and other development tools. TOSSIM‟s primary goal is to provide a high fidelity simulation of TinyOS applications. For this reason, it focuses on simulating TinyOS and its execution, rather than simulating the real world. While TOSSIM can be used to understand the causes of behavior observed in the real world, it does not capture all of them, and should not be used for absolute evaluations. Related Publication: Swarup Kumar Mitra, Ayon Chakraborty, Subhajit Mandal and M.K.Naskar, Simulation of Wireless Sensor Networks using TinyOS A Case Study, In the Proceedings of the National Conference on Modern Trends in Electrical Engineering, pages EC 23 EC 26, Hooghly, West Bengal, July 2009. 3 Data Gathering Schemes in WSNs Data gathering is by far one of the most important aspects of research considering energy efficiency in the routing protocols for wireless sensor networks. Wireless sensor networks have emerged as a ubiquitous platform recently, and issues regarding the efficiency of energy usage by these devices play a very important role. These devices are equipped with negligible or less amount of battery power to sustain for a long time. Not only that, in most of the scenarios, where these networks are deployed it is infeasible or impossible sometimes to replace the battery power of the sensor nodes. One of the most fundamental aspects for energy consumption in sensor nodes is communication, other than sensing and computation costs. Optimization of communication costs is thus essential, which is a direct consequence of betterment of routing techniques in this type of wireless networks. A major portion of my contribution in this project deals with designing data gathering schemes for wireless sensor networks and optimization of routing techniques, described below. The first in this queue was the HDS or “Hybrid Data Gathering Scheme”. Published in the International Conference of Distributed Computing and Internet Technology (ICDCIT‟10), this work is a novel approach in minimizing not only the communication / energy overhead but also guarantees a minimal energy-latency product. It also distributes the energy consumption by the nodes by rotating the leader node, so as to increase the uniformity of energy content in the nodes. The uniform distribution of energy content in the nodes also helps to lessen the chances of a black hole or a sinkhole problem. The HDS protocol is based on the hybrid combination of two algorithms, SHORT and LBERRA. The LBERRA scheme is used to subdivide the sensor field into predefined clusters, and SHORT is applied to form a binary tree spanning the nodes. There are two types of leader nodes: one for each cluster, forming the root of the tree and the other one is the „sink‟ communicating the gathered data to the Base Station. In each of the data gathering rounds the leader node is changed The second work was related to optimization of routing chain through heuristic techniques. Firstly, I applied Particle Swarm Optimization to create the most energy efficient paths for communication in the sensor field. Then, I investigated the use of Genetic Algorithms (hybridized with simulated annealing) in solving the same problem. In these works, I not only devised the algorithm for the minimum-energy path formation, but also coded it in nesC discussed in the earlier section. The implementation and simulation in nesC guarantees the hardware feasibility of the algorithm in sensor nodes. Packet loss rates were also studied with varying network topology and signal strengths in communication between particular sensor nodes. In all the cases, a standard background noise was considered. This work followed a series of publications including three international conferences and two international journals. An extension of this work was to create energy efficient data gathering trees. Most algorithms developed in literature used greedy algorithms to construct routing trees which in most of the cases did not result in near-optimal energy usage. “ROOT” or “ROuting through Optimized Trees” was an answer to this need. Related Publications: International Conference: 1. Ayon Chakraborty, Kaushik Chakraborty, Swarup Mitra and Mrinal Naskar, An Optimized Lifetime Enhancement Scheme for Data Gathering in Wireless Sensor Networks, in the proceedings of The Fifth IEEE Conference on Wireless Communication and Sensor Networks, WCSN'09 Allahabad, India, (December, 2009). 2. Ayon Chakraborty, Swarup Mitra and Mrinal Naskar, An Efficient Hybrid Data Gathering Scheme in Wireless Sensor Networks, in the proceedings of The Sixth International Conference on Distributed Computing and Internet Technology, ICDCIT'10, Bhubaneswar, India. (February, 2010). 3. Ayon Chakraborty, Swarup K. Mitra and M.K. Naskar, Energy Efficient Routing in Wireless Sensor Networks: A Genetic Approach, in the Proceedings of the International Conference on Computer Communications and Devices (ICCCD 2010), IIT Kharagpur (December, 2010) 4. Kaushik Chakraborty, Ayon Chakraborty, Swarup Mitra and Mrinal Naskar, ROOT: Energy Efficient Routing through Optimized Tree in Sensor Networks, in the proceedings of The International Conference on Computer Communications and Devices – ICCCD'10, Kharagpur, India. (December, 2010).",autonomous vehicle,198,not included
d0f2b863b1af919d08620efe960f708aabe63199,to_check,semantic_scholar,,2005-01-01 00:00:00,semantic_scholar,secure design and implementation of distributed and interoperable information systems based on overlap knowledge pattern,https://www.semanticscholar.org/paper/d0f2b863b1af919d08620efe960f708aabe63199,"New architectural and technical forms of information systems add a more significant level of complexity due to the decentralization of the constraints, treatment and data. These architectures increase the deployment and the runtime possibilities because of the number of existing sites. Indeed, the simple separation of the various functional levels as it is done in a classical architecture (Data, Treatment, Presentation) is not enough and the choice of the site of deployment or runtime becomes significant for the optimization of the production of the system. In these architectures, these decisions of distribution are generally made during the implementation phase. The conceptual structures offered to designers to allow them to express their needs for distribution (concepts of packages, business component,..) do not match with the rules used by developers for building their distributed components [Snene04B]. In fact, software components represent a single and autonomous concept of real world. They encapsulate all the data concerning this concept including name, goal, behavior and all other information with regard to them. In fact, a software component is a set of objects that can be physically deployed on two or several sites. It is usually made up of one or of several distributed components that offer together the various aspects of distribution necessary to the software component. The distributed components represent the physical modules used for application assembling. They encapsulate given data and treatments and provide their services through well-defined interfaces.",autonomous vehicle,199,included
3d80085c2bc6e289c5620bbf06b0878f4b3be001,to_check,semantic_scholar,,2010-01-01 00:00:00,semantic_scholar,reliable middleware framework for rfid system,https://www.semanticscholar.org/paper/3d80085c2bc6e289c5620bbf06b0878f4b3be001,"The reliability of RFID systems depends on a number of factors including: RF interference, deployment environment, configuration of the readers, and placement of readers and tags. While RFID technology is improving rapidly, a reliable deployment of this technology is still a significant challenge impeding wide-spread adoption. This research investigates system software solutions for achieving a highly reliable deployment that mitigates inherent unreliability in RFID technology. 
We have considered two different problem domains for large scale RFID deployment. One is item tracking and the other is guidance-monitoring. 
The basic contribution of our work is providing novel middleware solution that is able to serve the application taking into account the inherent unreliability of RFID technology. Our path abstraction that uses the physical flow of data as an ally to generate a logical system level flow enhances the performance in many ways. The contributions of this dissertation are summarized below: 
Defining novel system architecture for item tracking applications: We have defined a system architecture referred to as Reliable Framework for RFID (RF2ID) that takes into account the unreliability of RFID devices and provides a scalable, reliable system architecture for item tracking applications. It uses a distributed system abstraction named Virtual Reader (VR) that handles RFID data in different geographic locations. Virtual Path (VPath) is the abstraction that creates channels among the VRs and facilitates a data flow oriented data management in the system. 
Implementation of RF2ID: We have implemented RF2ID that is able to incorporate physical RFID devices as well as emulated devices for scalability study taking into account various real world challenges of large scale RFID deployment. 
Load Shedding Based Resource Management: RF2ID requires a mechanism to handle unexpected system load in the presence of asynchronous arrival of data items. Space based load shedding and time based load shedding techniques are used in RF2ID. The basic idea is to exploit the VR and Vpath abstraction to intelligently share the load among the VRs in the presence of high system load, and yet provide some guaranteed Quality of Service (QoS). 
Architecture for GuardianAngel: We define an architecture for an indoor pervasive environment which provides novel system abstraction and communication framework. The layered architecture has distributed computational elements known as the virtual station (VS) that are in charge of serving different regions of the environment. The Mobile Objects (MO) are the physical and logical entities that use sensing device and traverse the environment. The environment itself is tagged with RFID. The MO uses its sensing device to make guidance decisions locally. The VS keeps status information of MOs and keeps coarse grained information of the MO over time and space providing a virtual location for each MO. 
Implementation of GuardianAngel: We have implemented the GuardianAngle system as defined by the architecture. We have used a testbed that uses real RFID readers and tags in the pervasive environment in a limited laboratory setup. We have also developed a distributed system setup using emulated tags for a scalability study of the proposed architecture. We have also implemented a prototype application, to test its feasibility in the real world. 
Evaluation of the system: We have conducted extensive evaluation using the real RFID testbed as well as scalability study using emulated readers and tags. The evaluation using the real RFID tags and readers gives us the credibility of the system under various environmental considerations. The large scale experimentations provide us with scalability and feasibility study to strengthen our limited resource study using real RFID testbed. (Abstract shortened by UMI.)",autonomous vehicle,200,not included
a72d2df90c87909159ee5f59811f722b7f6ad4ad,to_check,semantic_scholar,,2003-01-01 00:00:00,semantic_scholar,"usenix association proceedings of bsdcon ’ 03 san mateo , ca , usa september",https://www.semanticscholar.org/paper/a72d2df90c87909159ee5f59811f722b7f6ad4ad,"The ever increasing mobility of computers has made protection of data on digital storage media an important requirement in a number of applications and situations. GBDE is a strong cryptographic facility for denying unauthorised access to data stored on a ‘‘cold’’ disk for decades and longer. GBDE operates on the disk(-partition) level allowing any type of file system or database to be protected. A significant focus has been put on the practical aspects in order to make it possible to deploy GBDE in the real world. 1 1. Losing data left and right In the last couple of years, gentlemen of the press have repeatedly been able to expose how laptop computers containing highly sensitive or very valuable information have been lost to carelessness, theft and in some cases espionage. [THEREG] The scope of the problem is very hard to gauge, since it is not a subject which the involved persons and, in particular, institutions are at all keen on having exposed. However, a few data points have been uncovered, revealing that the U.S. Federal Bureau of Investigation loses, on average, one laptop every three days. [DOJ0227] When a computer is lost, stolen or misplaced, it is very often the case that the computer hardware represents a value which is insignificant compared to the value of the disk contents. More often than not, the only reason the press heard about it was that the material on the disk was ‘‘hot’’ enough to make the loss of control rattle people at government level. While it is easy to blame these incidents on ‘‘user error’’, as is generally done, doing so makes it a very hard problem to fix. Human nature being what it is, seems to remain just that. In the absence of technical counter measures, administrative measures have been applied, generally with abysmal results. In one case, a bureaucracy has handled the problem according to what could easily be 1 This software was developed for the FreeBSD Project by Poul-Henning Kamp and NAI Labs, the Security Research Division of Network Associates, Inc. under DARPA/SPAWAR contract N66001-01-C-8035 (‘‘CBOSS’’), as part of the DARPA CHATS research program. mistaken for the plot from a classic Buster Keaton movie: First a laptop was forgotten and lost in a taxi-cab. New policy: always drive your own car if you bring your laptop. Then a car was stolen, including the laptop in the trunk. New policy: always bring your laptop with you. The next laptop was stolen from a pub while the owner was bowing to the pressures of nature. New policy: employees are not to carry their own laptops outside the office at any time. Laptops will be transported from and to the employees home address by the agency security force and will be chained and locked to a ring in the wall installed by the company janitors. All requests must be filed 3 days in advance on form ##-#. [PRIV] 2. Protecting disk contents Protecting the contents of a computer’s disk can in practice be done in two ways: by physically securing the disk or by encrypting its contents. Physical protection is increasingly impossible to implement. It used to be that disk drives could only be moved by forklift, but these days a gigabyte disk is the size, but not quite yet the thickness, of a postage stamp. While computers can be tied down with wires and bars can be put in front of windows, such measures are generally not acceptable, or at least not judged economically justified in any but the most sensitive operations. That leaves encryption of the disk contents as the only practical and viable mode of protection, and both the practicality and the viability has been somewhat in doubt. Until recently, nearly all aspects of cryptography were a highly political issue, this has eased a lot in the last couple of years and there now ‘‘only’’ remain a number of rather fundamental questions in the area of law enforcement and human rights, which are still unsettled. With the political issues mostly out of the way, the next roadblock is practical: While use of cryptography can never be entirely transparent, the overhead and workload it brings must be reasonable. 2.1. Application level encryption Encryption at the application level has been available for a number of years, primarily in the form of the PGP [PGP] program. This is about as intrusive and demanding as things can get: the user is explicitly responsible for doing both encryption and decryption and must enter the pass-phrase for every operation. 2 Apart from the inconvenience of this extra workload, many org anisations would trust their users neither to get this right nor even to want to get it right. From an institutional point of view it is important that cryptographic data protection can be made mandatory. 2.2. Filesystem level encryption Encryption at the file system level is a tried and acknowledged method of providing protection, but it suffers from a number of drawbacks, mainly because no mainstream file systems offer encryption. Encrypting file systems are speciality items, which means increased cost and system administration problems of all sorts. And since practically all operating systems use their own file system format, cross platform fully functional file systems are very rare. This means that a typical organisation will have to operate with a handful of different methods of encryption, which translates to system administration overhead, user confusion and extra effort to pass security and ISO9000 audits. A secondary, but increasingly important issue is that data which are stored in databases on raw disk, operating system paging areas and other such data are not protected by a cryptographic file system. To protect these would mean adding yet another set of encryption methods, which leads to a situation which is very hard to handle practically and administratively. Finally, file systems have a complex programming interface to the operating system, which traditionally 2 Interestingly, this is so impractical in real world use that various applications with PGP support resort to caching the pass-phrase at the application level, thereby weakening the protection a fair bit. has been subject to both version skew and compatibility problems. 2.3. Disk level encryption Encryption at the disk level can protect all data, no matter how they are stored, file system, database or otherwise. To a user, encryption at the disk level would require authentication before the computer can be used, everything functioning transparently thereafter, with all disk content automatically protected. Given that the programming interface for a disk device is very simple and practically identical between operating systems, there are no technical reasons why the same implementation could not be used across several operating systems. All in all this is a close to ideal solution from an operational point of view. There are significant implementation issues however. In difference from the higher levels, encryption at the disk level has no way of knowing a priori which sectors contain data and which sectors do not; neither is knowledge available about access patterns or relationships between individual sectors. Where application level or file system based encryption schemes can key each file individually, a disk based encryption must key each and every sector individually, ev en if it is not currently used to hold data. It has been argued that the encryption ideally should happen in the disk-drive, and while there are steps in this direction, they do unfortunately seem to have been made for the wrong reasons by the wrong people [CPRM], and have consequently not gained acceptance. Provided the owner of the computer remains in control of the encryption, I see no reason why encryption in the disk drives should not gain acceptance in the future. 3. Why this is not quite simple Several implementations have been produced which implement a disk encryption feature by running the user provided passphrase through a good quality one-way hash function and used the output as a key to encrypt all the sectors using a standard block cipher in CBC mode. A per sector IV for the encryption is typically derived from the passphrase and sector address using a one-way hash function. Tw o typical examples are [CGD] and [LOOPAES]. Unfortunately this approach suffers from a number of significant drawbacks, both in terms of cryptographic strength and deployability. For data to stay protected for decades or even lifetimes, sufficient margin must exist not only for technological advances in brute force technology, but also for theoretical advances in cryptoanalytical attacks on the algorithms used. Protecting a modern disk, typically having a few hundred millions of sectors, with the same single 128 or 256 bits of key material offers an incredibly large amount of data for statistical, differential or probabilistic attacks in the future. Worse, because the sectors contain file system or database data and meta data which are optimised for speed, the plaintext sector data typically have both a high degree of structure and a high predictability, offering ample opportunities for statistical and known plaintext attacks. This author would certainly not trust data so protected to be kept secret for more than maybe fiv e or ten years against a determined attacker. But far more damning to this method is that there can only be one single passphrase for the disk. This effectively rules out the ability for an organisation to implement any kind of per-user or multilevel key management scheme: the only possible scheme is ‘‘one key per disk’’. Add to this that to change the passphrase the entire disk would have to be decrypted and re-encrypted, and we have a model which may work in theory, and can be made to work in practice for a determined individual, but which would fast become an operational liability for any org anisation. 4. Designing GBDE The initial design phase of GBDE focused on determining a set of features which would make it both possible and",autonomous vehicle,201,not included
3a984a19f86877947561b4613b8b40b2efa01d26,to_check,semantic_scholar,,2009-01-01 00:00:00,semantic_scholar,understanding storage system problems and diagnosing them through log analysis,https://www.semanticscholar.org/paper/3a984a19f86877947561b4613b8b40b2efa01d26,"Nowadays, over 90% new information produced are stored on hard disk drives. The explosion of data is making storage system a strategic investment priority in the enterprise world. The revenue created by storage system industry steadily increases from $14.2 Billion in 2004 to over $18.4 Billion in 2007. As a key component of enterprise systems, reliable storage systems are critical. However, despite the efforts put into building robust storage systems, as the size and complexity of storage systems have grown to an unprecedented level, storage system problems are common. Unfortunately, many aspects of storage system problems are still not well understood, and most of previous studies only focus on one component - disk drives. 
To better understand storage system problems, we analyzed the failure characteristics of the core part of storage system - the storage subsystem, which contains disks and all components providing connectivity and usage of disk to the entire storage system. More specifically, we analyzed the storage system logs collected from about 39,000 storage systems commercially deployed at various customer sites. The data set covers a period of 44 months and includes about 1,800,000 disks hosted in about 155,000 storage shelf enclosures. Our study reveals many interesting findings, providing useful guideline for designing reliable storage systems. Some of the major findings include: (1) In addition to disk failures that contribute to 20–55% of storage subsystem failures, other components such as physical interconnects and protocol stacks also account for significant percentages of storage subsystem failures. (2) Each individual storage subsystem failure type and storage subsystem failure as a whole exhibit strong self-correlations. In addition, these failures exhibit bursty patterns. (3) Storage subsystems configured with dual-path interconnects experience 30–40% lower failure rates than those with a single interconnect. (4) Spanning disks of a RAID group across multiple shelves provides a more resilient solution for storage subsystems than within a single shelf. 
As we found out that storage subsystem problems are far beyond disk failures, we extend the scope of study to various storage system problems, and study the characteristics of storage system problem troubleshooting from various dimensions. Using a large set (636,108) of real world customer problem cases reported from 100,000 commercially deployed storage systems in the last two years, the analysis show that while some problems are either benign, or resolved automatically, many others can take hours or days of manual diagnosis to fix. For modern storage systems, hardware failures and misconfigurations dominate customer cases, but software failures take longer time to resolve. Interestingly, a relatively significant percentage of cases are because customers lack sufficient knowledge about the system. We also evaluate the potential of using storage system logs to resolve these problems. Our analysis shows that a failure message alone is a poor indicator of root cause, and that combining failure messages with multiple log events can improve problem root cause prediction by a factor of three. 
One key finding is that storage system logs contain useful information for narrowing down the root cause, while they are challenging to analyze manually because they are noisy and the useful log events are often separated by hundreds of irrelevant log events. Motivated by this finding, we designed and implemented an automatic tool, called Log Analyzer, to improve problem troubleshooting process. By applying statistical analysis techniques, the Log Analyzer can automatically infer the dependency relationship between log events, and identify the key log events that capture the essential system states related to storage system problems. By combining classic unsupervised classification techniques - hierarchical clustering with the event ranking techniques, the Log Analyzer can also identify recurrent storage system problems based on similar log patterns, so that previous diagnosis efforts can be systematically retrieved and leveraged. We train the Log Analyze with 18,878 week-long storage system logs and evaluate it with 164 real-world problem cases. The evaluation indicates that the Log Analyzer can effectively reduce the log event number to 3.4%. For most of the 16 real-world problem cases manually annotated with 1–3 key log events, the Log Analyzer accurately ranked the key log events within top 3 without a priori knowledge on how important the events are. For the other 148 problem cases with diagnosis and with root cause information, the Log Analyzer effectively grouped problem cases with the same root cause together with 63–93% accuracy, significantly outperforming other three alternative solutions which only achieve 30–46% accuracy.",autonomous vehicle,202,not included
df7fb406e31a6057e2ea8f4997c3b71895e44093,to_check,semantic_scholar,,2007-01-01 00:00:00,semantic_scholar,design and implementation of a microprocessor-based sequencer for a small-scale groundnut oil production plant,https://www.semanticscholar.org/paper/df7fb406e31a6057e2ea8f4997c3b71895e44093,"A microprocessor-based Sequencer for a small-scale groundnut oil production plant was designed and a test model was implemented. The microprocessor-based Sequencer is meant to replace traditional, electromechanical sequencers, which are based on relays, contactors, limit switches and other similar devices. The INTEL 8085A microprocessor, combined with interface chips like the AD7575 ADC, the MAX378 multiplexer was used to implement the sequencer. Software was programmed into 2716 EPROM. Actuators and signal conditioning circuits were also designed and implemented. The implemented system was tested and the performance was found to be satisfactory. Introduction Background to the problem : According to Nwachuku[1] microprocessors are the state of the art in electronics digital systems’ design and the Nigerian Engineer, like his counterpart the world over, has no choice but to become interested in them. He contended further that, because of the nature of the microprocessor and its versatility, it becomes possible for the Nigerian engineer to device products to meet local needs using imported chips. This according to him, is the direction of technological development ..In the advanced industrialized and newly industrialized countries, the last couple of decades have seen the extensive application of microprocessors and computers to automate production processes. Specifically, the microprocessor acts as a micro-controller with a fixed program. Here, the microprocessor application system in most cases, involves the determination of values of physical parameters like temperature, pressure, and so on. In Nigeria, not much seems to have been achieved in this area. Some big manufacturing companies in both the public and private sectors of the Nigerian economy have had to change from along monitoring and control of plant operations to microprocessor/computer-based systems. Most of them are based on the programmable logic controllers (PLCs). But all these analog to digital (microprocessor /computer-based) implementations are mostly carried out by foreign firms and personnel, bringing along with them, their designed hardware and software. Even at that, not much is known to have been achieved in the area of deploying this latest-in-technology to improve the production processes of small-scale industries. This project was conceived as a result of a realization of this obvious short-coming. The nature of the problem: All over the world, countries have come to recognize the leading role which small-scale industries play in their economic development. They furnish over forty-percent (40%) of a nation’s output of goods and they also provide a substantial amount of total employment in an economy. A realization of this obvious fact has made the Federal Government of Nigeria to lay emphasis on the need to support Small and Medium Scale Enterprises in order that they act as catalyst for Nigeria’s industrial and economic growth. For example, the Federal Government of Nigeria has floated a Bank of Industries and has set up a Small and Medium Enterprises Development Agency of Nigeria (SMEDAN) with the objective of improving the performance of Small and Medium Enterprises (SMEs) towards achieving rapid industrialization of Nigeria and for the reversal of its over dependence on imports. Advanced Materials Research Online: 2007-06-15 ISSN: 1662-8985, Vols. 18-19, pp 107-110 doi:10.4028/www.scientific.net/AMR.18-19.107 © 2007 Trans Tech Publications Ltd, Switzerland All rights reserved. No part of contents of this paper may be reproduced or transmitted in any form or by any means without the written permission of Trans Tech Publications Ltd, www.scientific.net. (Semanticscholar.org-19/03/20,17:18:59) However, it must be realized that, the success of these initiatives and of Small and Medium Scale Industries (SMIs) themselves, would depend on indigenous locally developed production processes and technologies. Failures of this class of industries in the past have largely been attributed to their over dependence on imported production processes, technologies, and by extension, on imported plants, equipments and machines. The Raw Materials Research and Development Council (RMRDC) of Nigeria have identified the dearth of process equipment and machinery as the bane of Nigeria’s under-utilization of her agricultural and mineral raw materials. This now places a huge burden on the Nigerian engineer to now start to design, implement, and practicalize new production processes, machines, equipment and plants that can be deployed by the old and emerging SMIs. The Federal Government of Nigeria seems to have set the ball rolling by the establishment of a National Office for Technology Acquisition and Promotion (NOTAP). The literature is replete with works on interfacing microprocessors with real life situations which is exemplified by the following: Hosier[2] reviewed briefly, the issues that has to be addressed when interfacing a microprocessor to perform real world monitoring and control operations. Anazia [3] gave an overview of the status of microprocessor applications in industrial process control as it relates to the Guinness Plant in Benin-City, Nigeria. A PC-Based Data Acquisition and Supervisory Control system for a Small-Scale Industry was designed and implemented by [4]. Mansfield[5] described the use of transducers for industrial measurement purposes. Program and Data Stores design and Input/Output interfacing were well treated by Short[6]. Gregory[7] explained in good details signal conditioning circuits design. General description of the microprocessor based sequencer The groundnut-oil production plant operates as a series of logically controlled sequence of states; for example, agitator ON, crusher OFF and so on; the duration of each state being determined by sensor signals, for example, temperature or pressure; these sequence of states were transferred into software. The software part (application program), in consonance with the process-sequential flowchart (SFC) that describes the operation of the plant was written in assembler. A major part of the plant consists of electrical and mechanical components like motors which are controlled by electro-mechanical relays, these in turn are operated by signals from the microprocessor; therefore, OUTPUT INTERFACE circuits were designed. Other signals are fed to the microprocessor from temperature, pressure, and other sensors; INPUT INTERFACE circuits were therefore, also designed. The microprocessor system must have memory for storing the application program and for implementing intermediate computations and such other operations. INPUT transducers’ outputs are memory-mapped. The Y2 output of the 74ALS138 decoder that was used addresses a memory address range of 1000Hex to 17FFHex. Therefore, the six (6) input transducers reside at addresses 1000H, 1001H, 1002H, 1003H, 1004H and 1005H respectively. The analog-to-digital converter converts input voltages of between 0volts and 5volts to binary outputs of between 00000000 and 11111111, that is, between 00Hex to FFHex. The output ports (74LS373s) were assigned to a separate address space different from that occupied by main memory; hence, they were isolated or standard I/O. Therefore, the designed and implemented system is as shown in block diagrammatic form, in Fig. 1. Description of the Designed and Implemented System In Fig. 1, block 1 represents the transducers. Block 2 represents the signal conditioning circuits that condition the transducers’ signals to match the multiplexer’s-MAX 378(block 3) inputs. The combination of the multiplexer and the sample and hold-AD781 JN(block 4) selects and holds an input for the analog-to-digital converter-AD7575 JN (block 5) to convert for the input ports (block 6). Block 7 (INTEL 8085A) is the microprocessor subsystem. Block 8 is an address/data bus decoder (74ALS373), while block 9 is an address decoder (74ALS138) for the memory subsystem(blocks 10-4118 and 11-2716). Block 12 represents the output ports (74ALS373) and 108 Advances in Materials and Systems Technologies",autonomous vehicle,203,not included
7dbe61b811bfc91e247dd4c949543468c394a1fd,to_check,semantic_scholar,,1999-01-01 00:00:00,semantic_scholar,integration of corba and web technologies in the vega dis,https://www.semanticscholar.org/paper/7dbe61b811bfc91e247dd4c949543468c394a1fd,"Distributed client/server architectures nowadays appear as a must for the wide information systems of the future virtual enterprise. At the same time, the continued growth of the Internet/WEB and its associated standard developments leads to new ways of world-wide information communication, distribution and access to information. This paper introduces to the various developments undertaken in the VEGA 1 project for a tighter integration of STEP, CORBA and WEB technologies within a DIS . Thus, the VEGA platform will allow both the support of distributed heterogeneous and interoperable client/server information systems (through CORBA) and the support of WEB based access to information and services through an Internet based navigation, building upon CGI and Java technologies. 1. Context and problematical issues The Large Scale Engineering (LSE) and Manufacturing universe is nowadays facing an increasing competitive environment where flexibility and adaptability to change are the bound paths to success, leading companies to renew their way of working. This is due to economical and technological drivers. Indeed, industrial enterprises are now devoted to the specification, design and construction of still larger and more complex manufacturing products: it has become no more possible to a single even-wide enterprise to take in charge the realisation of the whole products, both for financial reasons and because the required skills are not all within the enterprise. Thus, companies and SMEs ( Small and Medium Enterprises ) are now on the way of constituting virtual enterprises (VEs) for each new project. In a VE, contractors, partners, suppliers and customers form a temporary aggregation of non co-located actors dealing with the same product, but focusing on their core domain of competence for the shared profitability of industrial projects. At the same time, current progress in IT , providing more reliable and relevant mechanisms and software tools, the development of sophisticated new frameworks in client/server applications, and the continued growth of the Internet enable improved business processes and provide organisations with new business opportunities. Companies are now widely deploying their applications in Internet and Intranet 4 environments, assembling advanced IT based architectures encompassing among others networking, distributed information systems and concurrent engineering. In such a context, the VEGA project is developing a mandatory infrastructure for the support of the LSE business processes, particularly on the base of main standards for information modelling and exchange, with STEP (ISO 10303) or the IFC developed by the IAI , distribution and interoperability mechanisms with the OMG 6 CORBA and IIOP de facto standards, and WEB technology based on HTTP /CGI and Java. VEGA intends to cover the general needs of companies in VEs and Intranets, with a focus on the Building & Construction sector. Its main objective is to provide an information management architecture to guaranty interoperability among various software components running on different platforms, targeting STEP distributed technologies as an approach to bridge the gap between multiple and delocalised software systems in the construction industry. Within VEGA, the development of the DIS is a first approach towards an end-user oriented service for access to information through various forms. In the AEC field, large projects require the involvement of many body entities (client, architect, design engineers, various technical engineers, etc.) sitting at various locations, with different views and needs on the project, and managing different forms of documents like textual documents, structured documents, drawings, and so on. Dealing with all these kinds of information representations on the client side lead to consider, as far as possible, standardised front-end services. Access to information can be related to EDI 9 too. Until now, EDI has always been considered as a technology typically based on EDIFACT syntax and rules. But EDI can be considered as a kind of generic term, including all aspects of technical and commercial information exchange without mandatory requirements with respect to specific communication technology. Indeed, EDI can be considered from two different points of view: • the first one is a rather conceptual one: the EDI is a structured electronic exchange of data of any type between computer applications of parties involved in a transaction ; • the second is an operational one, where we consider means to realise the task as announced in the first point of view. With respect to these operational means, the Internet is nowadays more and more acknowledged as the common medium to support communication facilities within the development of client/server applications that deserve the larger audience. The WEB technology builds upon HTTP to provide the users with high-level graphical applications independent of the underlying client platform. Furthermore, the Java language now simplifies the development of WEB applications through the power and flexibility of a real object-oriented language. The Intranet perspective enfavours the use of WEB technologies on LANs 10 on a company scale, whether it be real or virtual. Therefore, a different approach has been applied in the VEGA DIS, considering EDI in the broad sense and consequently managing EDI messages through distributed networked infrastructures, emerging WEB standards (mainly HTML and VRML formats), and WEB technologies like CGI and Java. 2. Integration of new technologies in the VEGA project 2.1 Overview of the VEGA project As previously mentioned, the VEGA 11 project objective is to develop an IT platform enabling companies in a VE to work together. VEGA leans on available technology, and extend their capabilities as needed for engineering collaboration in a flexible distributed environment. To address the problem of information sharing, VEGA deals with 3 different technologies: • product-data modelling for the specification of meaningful project information; • middleware technology for the distribution of project information; • workflow management for the control of the flow of information and work in the VE. The VEGA platform relies on high level open standards for the three technologies listed above, including STEP and particularly EXPRESS for the neutral specification of product model data, CORBA for communication between distributed applications and distribution of objects over networks, workflow technology as defined by the WfMC 12 for design of process control, and information standards like SGML 13 or various WEB de facto standards for the support of valueadded distributed information services (exchange of administrative messages, document handling, presentation of information). Thus, VEGA is currently elaborating the fundamental grounds for distributed architectures, defining a service layered on top of CORBA (the COAST – COrba Access to STEP models [2]), for remote data access and manipulation of information models defined by explicit meta-models (or schemata) satisfying the STEP EXPRESS semantics. 2.2 STEP and the IAI If different software applications need to communicate and inter-operate, they first need to share the same information, without misunderstanding or loss of semantics. This implies a common way to represent and exchange the semantics. Such issues have led to dramatic research efforts to achieve effective product data exchanges, standardisation of methodologies, languages and technologies, especially in the context of PDT , among them: • STEP ([3], [4]), developed in ISO TC184/SC4 16 for the product data representation and exchange. It allows the expression in a uniform and complete way of all the information required for a product life-cycle (especially through the EXPRESS language [5]), and supplies means for exchanging data physical files [6] and sharing product databases [7] with models and applications independent mechanisms. STEP is today deeply used for real world product information modelling, communication and interpretation. • IFC [8], another major effort currently undertaken by a non-profit alliance (IAI) of the building industry including architects and engineers, building clients, software vendors, and so on. The main IAI objective is to specify the IFC as a universal model to be a basis for collaborative work in the building industry and consequently to improve communication, productivity, delivery time, cost, and quality throughout the design, construction, operation and maintenance life cycle of buildings. STEP and IAI share the same goals, i.e. application interoperability; data exchange and actor cooperation, but differ in their respective processes. The IAI promote a bottom-up approach, with an iterative and incremental development for fast implementation. STEP is a long-term project, with a top-down approach and is concerned with broad standardisation. The IAI, having a formal liaison status with STEP has partially adopted the STEP technology, mainly through an EXPRESS representation of the IFC. In the future, an integration of the IFC within STEP is planned. As an initiative driven by leading companies, the IAI is pushing the IFC as a de facto standard in the Building industry in a very near future. A key point of the VEGA infrastructure is to use PDT and especially EXPRESS to describe and store meaningful product information. Implementing the VEGA vision requires a tight coupling between STEP models and all the various components of the VEGA platform (including distribution layer, workflow, data storage). Eventually, the current IFC 1.5 release will be used in the final VEGA demonstration . 2.3 The CORBA standard CORBA ([9], [10], [11]) is an OMG specification for application interoperability and portability in distributed architectures, allowing objects described in any language to be shared across heterogeneous operating system",autonomous vehicle,204,not included
1890380a9e14e0a82dc105c9b8ce251107af9ddf,to_check,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,2nd cfp: 7th ieee international conference on self-adaptive and self-organizing systems (saso2013),https://www.semanticscholar.org/paper/1890380a9e14e0a82dc105c9b8ce251107af9ddf,"submission: May 3, 2013 Paper submission: May 10, 2013 Notification: June 21, 2013 Camera ready copy due: July 19, 2013 Early registration: August 21, 2013 Conference: September 9-13, 2013 ------------------------------Topics of Interest ------------------------------The topics of interest to SASO include, but are not limited to: Self-* systems theory: theoretical frameworks and models; biologicallyand socially-inspired paradigms; inter-operation of self-* mechanisms; Self-* systems engineering: hardware, software and middleware development frameworks and methods, platforms and toolkits; self-* materials; Self-* system properties: robustness, resilience and stability; emergence; computational awareness and self-awareness; reflection; Self-* cyber-physical and socio-technical systems: human factors and visualization; self-* social computers; crowdsourcing and collective awareness; Applications and experiences of self-* systems: cyber security, transportation, computational sustainability, big data and creative commons, power systems. --------------------------------------Submission Instructions --------------------------------------All submissions should be 10 pages and formatted according to the IEEE Computer Society Press proceedings style guide and submitted electronically in PDF format. Please register as authors and submit your papers using the SASO 2013 conference management system. The proceedings will be published by IEEE Computer Society Press, and made available as a part of the IEEE digital library. Note that a separate call for poster submissions has also been issued. ---------------------------Review Criteria ---------------------------Papers should present novel ideas in the cross-disciplinary research context described in this call, clearly motivated by problems from current practice or applied research. We expect both theoretical and empirical contributions to be clearly stated, substantiated by formal analysis, animation or simulation, experimental evaluations, comparative studies, and so on. Appropriate reference must be made to related work. Because SASO is a cross-disciplinary conference, papers must be intelligible and relevant to researchers who are not members of the same specialized sub-field. Authors are also encouraged to submit papers describing applications. Application papers are expected to provide an indication of the real world relevance of the problem that is solved, including a description of the deployment domain, and some form of evaluation of performance, usability, or comparison to alternative approaches. Experience papers are also welcome but they must clearly state the insight into any aspect of design, implementation or management of self-* systems which is of benefit to practitioners and the SASO community. ---------------------------Program Chairs ---------------------------Tom Holvoet KU Leuven, Belgium Jeremy Pitt Imperial College London, England Ichiro Satoh National Institute of Informatics, Tokyo, Japan ? CFP RSP-2013 at ESWeek CfP: Workshop on Model-Driven and Agile Engineering for the Web (MDWE) @ ICWE 2013 ? Calls for Papers CPS Domains Architectures Secure Control Systems Multi-models Communication Embedded Software Model Integration Platforms Systems Engineering Modeling Science of Security Transportation CPS Technologies Announcement",autonomous vehicle,205,not included
5516eea2b00bba86118be6f90d7c678b4c022226,to_check,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,developments in the united kingdom road transport from a smart cities perspective,https://www.semanticscholar.org/paper/5516eea2b00bba86118be6f90d7c678b4c022226,"Purpose: Smart city is a city which functions in a sustainable and intelligent way, by integrating all of its infrastructures and services in a cohesive way using intelligent devices for monitoring and control, to ensure efficiency and better quality of life for its citizens. As other countries globally, UK is keen for economic development and investment in smart city missions to create interest in monetary environment and inward investment. This paper explores the driving forces of smart road transport transformation and implementation in the UK. Design/methodology/approach: The study involved interviews with sixteen professionals from the UK road transport sector. A semi-structured interview technique was used to collect experts’ perception, which was then examined using content analysis. Findings: The results of the study revealed that the technological advancement is a key driver. The main challenges faced for the implementation of smart city elements in the UK road network are: lack of investment; maintenance; state of readiness and the awareness of the smart road transport concept. The study concludes that an understanding of the concept of smart cities from a road transport perspective is very important to create awareness of the benefits and the way it works. A wider collaboration between every sector is crucial to create a successful smart city. Originality/value: The study contributes to the field of digitalisation of road transport sector. This paper reveals the key driving forces of smart road transport transformation, the current status of smart road transport implementation in UK and challenges of the smart road transport development in the UK. Engineering, Construction and Architectural Management http://mc.manuscriptcentral.com/ecaam 2 Introduction Enormous global urbanisation and growth has caused migration of people in urban areas and spatial development of urban infrastructure. According to Obaidat and Nicopolitidis (2016), 85-90% of the world population evolution is a result of a 21 st century cities. Therefore, smart cities are being created from scratch or being developed gradually by improving the prevailing cities infrastructure and primary resources. A study of McKinsey Global Institute’s by Department for Business innovation and Skills (2013) shows that more than 50% of the global GDP (Gross Domestic Product) is generated in the 190 major cities in the developed countries compare to 22 largest cities in the developing countries. However, the increase of growth in the developed countries is healthy but this phenomena also set a high expectation on public services and the quality of the urban infrastructure and environment. Due to the urbanisation, more problems such as overpopulation, pollutions, scarcity of natural resources, public and private services are being created (Yigitcanlar et. al. 2020, Dameri, 2014). Smart cities are an emerging strategy to mitigate the problems generated by the rapid urban population growth and rapid urbanization (Prakash, 2019). A ‘smart city’ is defined as an urban area that is highly innovative in terms of overall facilities, ecological real estate, communication and market feasibility (Chirisa et al, 2016). Also, smart cities is defined as “a place where the traditional networks and services are made more efficient with the use of digital and telecommunication technologies, for the benefits of its inhabitants and businesses” (Kumar et al., 2018). Whereas BSI (2014) noted that smart cities as an effective integration of physical, digital and human systems in a built environment to deliver sustainable, prosperous and inclusive future for its citizens. From the above three definitions, it could be inferred that smart cities are cities that make use of physical, digital and human systems in order to enable sustainability and efficiency for the citizens within that city. The economic growth of any country is supported by its good infrastructure. The United Kingdom (UK) has strategic roads, railway and airports; however, it is one of the top 10 most congested country in the world (Korosec, 2018). According to ONS (2017), the population in the UK in 2016 was 65.6 million which was the largest ever. It is also projected that the population in the UK would grow, reaching over 74 million by 2039. Due to the population rise amalgamated with increase of cars on the road, which has increased by 4.6% higher than the previous peak in 2016 (Department of Transport, 2017), the present UK transport system faces many challenges. The UK road transportation system is gradually taking steps to overcome the problems. As road transport is a significant source of both safety and environmental concerns. This research aims to explore the driving forces of smart road transport transformation, and implementation in the UK along with the challenges faced. In doing so the next section delves into the relevant literature review followed by methodology and findings. The paper finally concludes with recommendations. 2. Literature review An extensive review of literature on drivers, current status of smart cities and the challenges are discussed in this section. Three main drivers include: technology, environment, and socioeconomy (See Table 1). The technological advancement is clearly seen as strong impact on the cities in the recent years. It can be seen clearly that, the communication technologies (ICT) develops city management, enhances technical and social networks, and urban affordability. Within the technology as a driver, the review of literature revealed that technologies such as Big Data, Internet of Things and Artificial Intelligence are widely Engineering, Construction and Architectural Management http://mc.manuscriptcentral.com/ecaam 3 implemented within the smart cities context. However, there is a lack of studies focusing on the UK smart road transport sector. Table 1: Literature on classification of drivers of smart cities development Drivers Description Source Technology Big data Big volume of digital data contents through online services such as Facebook, Twitter, You Tube, Instagram and SnapChat Olshannikova et al , 2017 Data is transmitted to various smart applications through sensor devices or other cloud computing integrated devices Hashem et al , 2016 Big data development highlights information and communication tools in the cyber physical farm management cycle and it identifies the interconnection related to socio-economic challenges Wolfert et al , 2017 Internet of Things IoT is widely in use for many multimedia application, smart transportation system and smart city design and deployment issues KeertiKumar et al , 2016 The implementation of IoT in transportation system means underlying technology and creating smart environments to increase their efficiency in tackling the transportation and environmental issues Kyriazis et al , 2013 Artificial Intelligence Artificial Intelligence (AI) is a technology that is influencing every pace of life which enable people to reconsider how to integrate information, analyse data, and real time data usage for the purpose of improve decision making West and Allen , 2018 Artificial neutral networks, expert system and hybrid intelligent system are computer based modelling tools that have recently aroused and found extensive recognition in many discipline for modelling complex real-world problem. Bahrammirzaee , 2010 China is a leading competitor and primarily focusing in the use of AI in military vehicles. While, Russia actively looking for opportunity in AI development and focused on robots in military Hoadley and Lucas , 2018 Environment Songdo, the Korean model of smart city was subjected to become one of the sustainable smart cities around the world Yigitcanlar , et al, 2018 Songdo, consist 40% of parks and green spaces and waste processing centre placed by the underground system and to recycle Benedikt , 2016 ; Shwayri , 2013 European Union aimed to reduce greenhouse gas emission in urban design through the implementation of innovation technologies Ahvenniemi et al , 2017 Engineering, Construction and Architectural Management http://mc.manuscriptcentral.com/ecaam 4 Cities have been setting high expectation on reaching the target of creating a clean future as shared by Covenant of Mayors’ vision for 2050 to accelerate the decarbonisation Covenant of Mayors for Climate & Energy , 2018 Socioeconomy By 2050, six hundred cities will generate almost 65% of world economic growth by contribute to a higher global GDP Dobbs et al, 2012 Smart cities indicated as the next evolution of ‘new community management where urban problems converted into opportunities for business investment and profit earning Anand and Marcco , 2018 Global smart cities market size to grow reaching USD 2.57 trillion by 2050 Grand View Research , 2018 Governments are obligated to protect citizen and their control over the active connection of local public groups, the police force, and the citizen such as the senior and disabled Neirotti et al ,2014 Level of observing has been focused by an increasing safety and security that desires to manage risks Kitchin , 2014 Safety and security features implemented with help of big data and data controls centres that joined and bind data stream collectively for example the installation of CCTV and street monitored camera are to monitor activity remotely Firmino and Duarte, 2014 Big data is a trend of utilising software tools to store data and share data collected with the use of technology. Nowadays, big data has been a tool that facilitating individual, businesses and government to discover new solutions to certain problems. Data is a crucial aspect as when an asset is built, asset management goes on and the more data collected in, the asset is being constructed, the more the asset can be maintained in an efficient manner (Loshin, 2018). Furthermore, KeertiKumar et al (2016) mention that IoT is widely used for many multim",autonomous vehicle,206,not included
283611b89413eace868c68f0339779dc9753f637,to_check,semantic_scholar,Inf. Softw. Technol.,2015-01-01 00:00:00,semantic_scholar,"search-based automated testing of continuous controllers: framework, tool support, and case studies",https://www.semanticscholar.org/paper/283611b89413eace868c68f0339779dc9753f637,"Abstract Context Testing and verification of automotive embedded software is a major challenge. Software production in automotive domain comprises three stages: Developing automotive functions as Simulink models, generating code from the models, and deploying the resulting code on hardware devices. Automotive software artifacts are subject to three rounds of testing corresponding to the three production stages: Model-in-the-Loop (MiL), Software-in-the-Loop (SiL) and Hardware-in-the-Loop (HiL) testing. Objective We study testing of continuous controllers at the Model-in-Loop (MiL) level where both the controller and the environment are represented by models and connected in a closed loop system. These controllers make up a large part of automotive functions, and monitor and control the operating conditions of physical devices. Method We identify a set of requirements characterizing the behavior of continuous controllers, and develop a search-based technique based on random search, adaptive random search, hill climbing and simulated annealing algorithms to automatically identify worst-case test scenarios which are utilized to generate test cases for these requirements. Results We evaluated our approach by applying it to an industrial automotive controller (with 443 Simulink blocks) and to a publicly available controller (with 21 Simulink blocks). Our experience shows that automatically generated test cases lead to MiL level simulations indicating potential violations of the system requirements. Further, not only does our approach generate significantly better test cases faster than random test case generation, but it also achieves better results than test scenarios devised by domain experts. Finally, our generated test cases uncover discrepancies between environment models and the real world when they are applied at the Hardware-in-the-Loop (HiL) level. Conclusion We propose an automated approach to MiL testing of continuous controllers using search. The approach is implemented in a tool and has been successfully applied to a real case study from the automotive domain.",autonomous vehicle,207,not included
877faa6486db570bdbe7aa24d5b40cac6017843d,to_check,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,software and system engineering for cyber-physical systems: technical challenges and collaboration opportunities,https://www.semanticscholar.org/paper/877faa6486db570bdbe7aa24d5b40cac6017843d,"of presentations Holger Pfeifer (FORTISS) – The European Smart Anything Everywhere initiative and funding opportunities by CPSE-Labs experiments The European ‘Smart Anything Everywhere’ (SAE) initiative supports the innovation on smart digital systems thanks to networks of competence centres. The ecosystems built under these initiatives are based on collaboration between researchers, large industries and SMEs which will help to transfer knowledge and resources available to a much wider group of companies. SMEs and middle size companies can experiment with new technologies, try them out in their processes and work together with the suppliers of the technology to adapt it to their specific needs. CPSELabs is one SAE innovation action which provides an open forum for sharing platforms, architectures and SW tools for the engineering of dependable and trustworthy CPS. It provides funding for focussed experiments (36 partners) and fast-track (12-18 months) with innovation objective. Next call for experiment will be published in Spring 2016 http://www.cpse-labs.eu/calls.php Holger Pfeifer (FORTISS) CPSE-Labs experiments of Germany South centre: Model-driven engineering for industrial automation systems The importance of software in industrial automation is continuously increasing. New approaches to the development and maintenance are needed to cope with the growing complexity of control software for future automation systems. 4DIAC Framework for Distributed Industrial Automation & Control is an open source solution for the programming of programmable logic controllers (PLCs) according to the standard IEC 61499. With this standard it provides higher level modelling means and better support for networked control devices. The main components of 4DIAC are the Eclipse-based integrated development environment 4DIAC-IDE and the real-time capable run-time environment FORTE. Martin Grimheden (KTH) – CPSE-Labs experiments of Sweden centre: Overcoming thresholds for data integration in CPS engineering environments The talk will describe the Kth model-based approach to data integration based on the OSLC interoperability standard. Patrick Leserf (ESTACA) Trade-off analysis with SysML and Papyrus : a drone application Obtaining the set of trade-off architectures from a SysML model is an important objective for the system designer. To achieve this goal, we propose a methodology combining SysML with the variability concept and multiobjectives optimization techniques. An initial SysML model is completed with variability information to show up the different alternatives for component redundancy and selection from a library. The constraints and objective functions are also added to the initial SysML model, with an optimization context. Then a representation of a constraint satisfaction problem (CSP) is generated with an algorithm from the optimization context and solved with an existing solver. The presentation will illustrate our methodology by designing an Embedded Cognitive Safety System (ECSS). From a component repository and redundancy alternatives, the best design alternatives are generated in order to minimize the total cost and maximize the estimated system reliability. Benoît Combemale (IRISA) Using models for a broader engagement in smart systems. Various disciplines use models for different purposes. While engineering models, including software engineering models, are often developed to guide the construction of a non-existent system, scientific models, in contrast, are created to better understand a natural phenomenon (i.e., an already existing system). An engineering model may incorporate scientific models to build a smart system. This talk proposes a vision promoting an approach that synergistically combines engineering and scientific models to enable broader engagement of end users in smart systems, informed decision-making based on more-accessible scientific models and data, and automated feedback to the engineering models to support dynamic adaptation of smart systems. To support this vision, we identify a number of challenges to be addressed with particular emphasis on the socio-technical benefits of modeling. Claire Ingram (Newcastle University) CPSE-Labs experiments of UK centre: Pragmatic techniques for modelbased Engineering of Cyber-Physical Systems Newcastle University's Cyber-Physical Systems Lab conducts research into pragmatic techniques for model-based engineering of Cyber-Physical Systems (CPSs). In this talk I will introduce some platforms supported by Newcastle's CPS Lab, including an approach for co-modelling which allows separate design teams working with discrete-event and continuous-time formalisms to develop CPS designs collaboratively. I will also introduce an experiment which has been funded previously under the CPSE Labs initiative. Michael Siegel (OFFIS) CPSE-Labs experiments of Germany North centre: The Maritime Architecture Framework (MAF) and eMaritime Integrated Reference Platform (eMIR) The Maritime Architecture Framework (MAF) is a stakeholder-oriented CPS architecture framework for existing and future maritime CPS and services. MAF provides the conceptual basis, methods, tools and technologies to define, document, and align existing or future CPS architectures and architectural reference models for e-navigation and e-maritime applications. The MAF is a key enabler in the maritime domain for system harmonization, interoperability and standardization. The MAF is also a reference for the definition and design of testbeds for enavigation and e-maritime systems and services. It helps to define the context, to check completeness and provides a semantic basis to discuss the outcome and results. Additionally it offers a semantic basis for integration of testbeds e.g. for larger demonstrators. To support the development of maritime CPS – i.e. the integration of heterogeneous systems of the maritime transportation space , the Design Centre North provides the eMaritime Integrated Reference Platform (eMIR) for rapid prototyping in simulation environments and testing in real world environments. This talk gives an overview about the background, context and concepts of the MAF and why testbed environments (e.g. eMIR) for the development, integration testing and demonstration for CPS must take into account and support the design aspects of such an architecture framework. Andre Pierre Mattei (SCA-ITA) SysML Design of an observation satellite for agriculture surveillance in Brazil François Fouquet (SnT, Interdisciplinary Centre for Security, Reliability and Trust)Models for managing IoT data Internet of Things applications analyze our past habits through sensor measurements to anticipate future trends. To yield accurate predictions, intelligent systems not only rely on single numerical values, but also on structured models aggregated from different sensors. Computation theory, based on the discretization of observable data into timed events, can easily lead to millions of values. Time series and similar database structures can efficiently index the mere data, but quickly reach computation and storage limits when it comes to structuring and processing IoT data. During this talk, I will present various results presented at Models’15 and SmartGridCom’15 that tackles this complexity by exploiting IoT data characteristics. Notably, I will present a concept of continuous models that can handle high-volatile IoT data by defining a meta type for continuous attributes. In addition to traditional discrete object-oriented modeling APIs, we enable models to represent very large sequences of sensor values by inferring mathematical models that can efficiently replace raw values. We show on various IoT datasets that sequences of polynomials can significantly improve storage and reasoning efficiency. I will present an application of this IoT model extension for suspicious value detection in the SmartGrid domain. We proposed a method to learn and store a profile of “typical” values and their probability in IoT context models. We show that using such profiles together with a contextual checker we can improve suspicious value detection, both in terms of efficiency and effectiveness. Juan Garbajosa (UPM) Experiments of Spain centre: Open CPS platform for building and deploying smart city services Bran Selic (Simula) Modeling uncertainty in cyber-physical systems For the past year, we have been working on a core model of Uncertainty and its application to requirements specification and system testing in the context of the European Commission's H2020 UTEST project. (More information on this project, which involves a number of industrial and research partners from Europe, can be found at: http://certus-sfi.no/u-test-a-horizon-2020-funding-recipient/). Fabien Peureux Fabien Peureux (Femto-st/EGM/Smartesting S&S) Model-Based Testing for Internet of Things and Cyber-Physical Systems Testing Cyber-Physical Systems (CPS) is challenging due to the various uncertainties in their behaviour. The purpose of this talk is to present our ongoing work on a model-based testing framework for automatic generation of executable test cases for CPS in the presence of various uncertainties. Basically, uncertainties can be described as a lack of certainty about the current state or about the future outcome of the system. Within CPS, it can be due to the stimulus and data sent from the user environment to the physical units (application level), to the interactions between the physical units and the network services (infrastructure level), or a combination of the both (integration level). To test such issues, the proposed model-based testing approach is implemented on the EMF framework, and based on the test generation tool Smartesting CertifyIt1. It relies on a UML behavioral model of the system under test, from which abstract test cases are automatically generated by applying dedicated coverage strategies focusing on uncertainty testing. Afterwards, ",autonomous vehicle,208,not included
30f346032407b43e811107a46bf07a38f30527d7,to_check,semantic_scholar,,2005-01-01 00:00:00,semantic_scholar,the build master: microsoft's software configuration management best practices,https://www.semanticscholar.org/paper/30f346032407b43e811107a46bf07a38f30527d7,"""Wow, what can I say? Chapter 4, 'The Build Lab and Personnel,' by itself is enough justification to purchase the book! Vince is obviously a 'Dirty Finger Nails' build meister and there is a lot we can all learn from how he got them dirty! There are so many gems of wisdom throughout this book it's hard to know where to start describing them! It starts where SCM should start, at the end, and works its way forward. This book is a perfect complement to the 'Follow the Files' approach to SCM that I espouse. I will recommend that every software lead and software configuration management person I work with be required to read this book!""-Bob Ventimiglia, autonomic logistics software configuration manager, Lockheed Martin Aeronautics""The Build Master contains some truly new information; most of the chapters discuss points that many people in the industry don't have a full understanding of and need to know. It's written in a way that is easy to read and will help a reader fill holes in their vision regarding software build management. I especially liked Vince's use of Microsoft stories to make his points throughout the book. I will purchase the book and make certain chapters mandatory reading for my build manager consultants.""-Steve Konieczka, SCM consultant""Vince does a great job of providing the details of an actual working build process. It can be very useful for those who must tackle this task within their own organization. Also the 'Microsoft Notes' found throughout the book provide a very keen insight into the workings of Microsoft. This alone is worth purchasing this book.""-Mario E. Moreira, author of Software Configuration Management Implementation Roadmap and columnist at CM Crossroads""Software configuration management professionals will find this book presents practical ideas for managing code throughout the software development and deployment lifecycles. Drawing on lessons learned, the author provides real-world examples and solutions to help you avoid the traps and pitfalls common in today's environments that require advanced and elegant software controls.""-Sean W. Sides, senior technical configuration manager, Great-West Healthcare Information Systems""If you think compiling your application is a build process, then this book is for you. Vince gives us a real look at the build process. With his extensive experience in the area at Microsoft, a reader will get a look in at the Microsoft machine and also how a mature build process should work. This is a must read for anyone doing serious software development.""-Jon Box, Microsoft regional director, ProTech Systems Group""Did you ever wonder how Microsoft manages to ship increasingly complex software? In The Build Master, specialist Vince Maraia provides an insider's look.""-Bernard Vander Beken, software developer, jawn.net""This book offers an interesting look into how Microsoft manages internal development of large projects and provides excellent insight into the kinds of build/SCM things you can do for your large-scale projects.""-Lance Johnston, vice president of Software Development, SCM Labs, Inc.""The Build Master provides an interesting insight into how large software systems are built at Microsoft covering the set up of their build labs and the current and future tools used. The sections on security, globalization, and versioning were quite helpful as these areas tend to be overlooked.""-Chris Brown, ThoughtWorks, consultant""The Build Master is a great read. Managing builds is crucial to the profitable delivery of high-quality software. Until now, the build process has been one of the least-understood stages of the entire development lifecycle. This book helps you implement a smoother, faster, more effective build process and use it to deliver better software.""-Robert J. Shimonski, Networking and Security Expert, http://www.rsnetworks.netThe first best-practice, start-to-finish guide for the software build processManaging builds is crucial to the profitable delivery of high-quality software; however, the build process has been one of the least-understood stages of the entire development lifecycle. Now, one of Microsoft's leading software build experts introduces step-by-step best practices for maximizing the reliability, effectiveness, timeliness, quality, and security of every build you create.Drawing on his extensive experience working with Microsoft's enterprise and development customers, Vincent Maraia covers all facets of the build process-introducing techniques that will work on any platform, on projects of any size. Maraia places software builds in context, showing how they integrate with configuration management, setup, and even customer support. Coverage includes How Microsoft manages builds: process flows, check-in windows, reporting status, and more Understanding developer and project builds, pre- and post-build steps, clean builds, incremental builds, continuous integration builds, and more Choosing the right build tools for your projects Configuring source trees and establishing your build environment-introducing Virtual Build Labs (VBLs) Planning builds for multiple-site development projects or teams Determining what should (and shouldn't) be kept under source control Managing versioning, including build, file, and .NET assembly versions Using automation as effectively as possible Securing builds: a four layer approach-physical, tracking sources, binary/release bits assurance, and beyondBuilds powerfully impact every software professional: developers, architects, managers, project leaders, configuration specialists, testers, and release managers. Whatever your role, this book will help you implement a smoother, faster, more effective build process-and use it to deliver better software.© Copyright Pearson Education. All rights reserved.",autonomous vehicle,209,not included
e349556d5302749ee79643792e60b192020a42b2,to_check,semantic_scholar,,2009-01-01 00:00:00,semantic_scholar,special issue on wireless sensor network: theory and practice,https://www.semanticscholar.org/paper/e349556d5302749ee79643792e60b192020a42b2,"Wireless sensor network (WSN) is an emergent multi-disciplinary science, and it may be considered as the foundation of pervasive computing, mobile computing and wearable computing. WSN is a very active and competitive research area due to its diverse and unlimited potential applications: air, underground and underwater. In spite of its young age, economic impact of WSN is important, for examples the industrial control segment market will be worth $5.3B by 2010 and the smart home market will be worth $2.8 billion worldwide by 2012 (Source: Stamatis Karnouskos, EU-US 08 Workshop). WSN is a set of wireless nodes. On one hand, each wireless node (WN) has similar hardware and software functionalities as a PC: CPU, memory, operating system, and communication protocol to fulfill a specific task. On the other hand, a WN has a limited power supply (embedded battery) and consumes approximately 1 million less power (~100μW instead of ~100W) than a PC. Due to resources constraints: energy consumption and form factor, the approaches applied in general purpose computer systems are not adapted to the requirements of WSN. When it comes to the design of energy efficient oriented hardware and software components of WSN, cross-layering optimization approaches are generally adopted such as application-specific unified hardware and software by taking into account the following criteria: trade-off between complexity, efficiency and resource consumption, and application context (context-aware) etc. Currently two main hardware development and design trends are carried out to implement the WN: Commercial OffThe-Shelf ‘COTS’ and System on Chip ‘SoC’. The first and second generations of WN were designed by using low power 8-bit or 16-bit microcontroller processor core, Bluetooth and non standard wireless access medium (MICA Mote). The current trend of WN design such as Tmotesky, iMote and LiveNode are based on low power 16-bit or 32-bit RISC microcontroller, and full compliance IEEE802.14.5 standard. However the ultimate goal of all the researchers in the world is the implementation of long life, low cost and invisible WN integrated and embedded into environment. Three key technologies make possible to achieve this objective: MEMS ‘MicroElectroMEchanical systems’, UWB ’Ultra-Wide Band’ and low power CMOS technology. Different WN prototypes are realized by Intel (iMOTE2), University of Michigan (MOTE: Michigan Uni Prototype) and University of Berkeley (Pico-Mote). WN hardware seems easier to solve than embedded software for diverse WSN applications. The main questions which are related to WSN basic software design is how to keep modularity, high level abstraction and reliability to enable to implement complex massively distributed WSNs to meet resource constraint requirements. Real-time operating system (RTOS) plays a key role to support high level abstraction and distributed collaborative processing. Currently four categories of WSN’s RTOS are developed: Event driven (TinyOS), Multitask (RETOS, tKernel, NutOS, MANTIS), Data-Centric (AmbientRT), and Hybrid (Contiki, LIMOS). Note that TinyOS is very popular but it not adapted to complex hard real-time application. The development challenges of the WSN RTOS are energy-efficiency (context aware, configurable, small footprint), robustness, fault tolerance, support hard real-time constraint, and support component based model (high level of abstraction to ease the integration of high level SW such as protocol, middleware, application, and simulator). Furthermore, for WSN applications, message sending is energy consuming. Thus it is important to implement embedded energy efficient wireless routing protocol to increase WSN lifetime. It is clear that general purpose MANET routing protocol such as AODV (active), OLSR (proactive) and ZBR (hybrid) etc. are not suitable for WSN due to resource constraints. For example optimal routing path is well adapted to general purpose MANET but not suitable for WSN because the repetitive use of the same path will exhaust the battery of WNs belonging to the optimal routing path (black hole). Many WSN dedicated protocols are implemented (spin, cougar, gear, leach, speed ...) but it is currently very difficult to have a clear idea concerning their performance (energy consumption, scalability, connectivity, lifetime ...) because of the lack of large scale WSN real world experimentation results and because the simulation model does not reflect the real-world ones (physical layer). Note that, there is no standard scenario and the application program (with a known number of WNs) enables to evaluate rationally the performance of wireless routing protocols. In addition routing protocol relies on the WSN topology. On one hand, an optimal WSN topology facilitates the implementation of routing and administration protocols. On the other hand, the deployment of large scale WS nodes in a large area is random and its topology is a priori unknown. Then, it is important to investigate the auto-configuration algorithms to increase the efficiency of routing and administration protocols. However the frontier between the administration protocol and the routing protocol is not as clearly defined as in a classical network (e.g. TCP/IP and SNMP) due to cross-layering approach. Moreover WSN security, reliability, and fault tolerance are still an open problem. In this special issue 5 papers are selected among 40 submitted papers for the NTMS workshop on wireless sensor network: theory and practice, held at Tangier at Morocco in 2008, the rest of the papers are selected from an open call for paper. WSN is a multi-disciplinary science. It impossible to present all its topics but this special issue addresses most of the WSN embedded software problems dealing with real-world applications (EU NeT-ADDED FP6 project, French ANR research project and industrial projects). JOURNAL OF NETWORKS, VOL. 4, NO. 6, AUGUST 2009 379",autonomous vehicle,210,not included
368687003560b21e53865cd604aae8c00dc62c4b,to_check,semantic_scholar,2009 6th IEEE Consumer Communications and Networking Conference,2009-01-01 00:00:00,semantic_scholar,passiton: an opportunistic messaging prototype on mobile devices,https://www.semanticscholar.org/paper/368687003560b21e53865cd604aae8c00dc62c4b,"With the increasing popularity of mobile handheld devices and the growing capability of these devices, it is becoming possible that information sharing/dissemination is carried out through human networks, as a complement to the traditional computer networks. In such human networks, people come across one another, while their mobile devices exchange and store information in a spontaneous and transparent way. Such an encounter could be established through direct device-todevice connectivity when two devices come into each other’s communication range, or be enabled by, e.g., a Wi-Fi access point, when the devices both enter its coverage. A new form of dissemination, which we call opportunistic messaging, is such an application that is based on human encounters and mobilities. When human encounters are exploited for communications, the reliance on network infrastructure access is eliminated; communications can be performed even where infrastructure is absent or infrastructure access is intermittent. By leveraging human mobilities, data delivery does not require an end-to-end path from the source to a recipient; instead, people carrying mobile devices serve as relays – they cache others’ data and forward/deliver the data when appropriate. Thus, the propagation of information is tied to people’s physical proximity when they move around, and incorporates the social aspects of communications as people tend to spend more time co-locating with their social relations. Opportunistic messaging is applicable anywhere, and is especially appealing where network infrastructure access is limited or intermittent (e.g., on cruise ships, in national parks, after disasters). Another intriguing characteristic of it is its ease of deployment – no central server is needed, but only a single piece of software on users’ mobile devices. However, as human encounters and mobilities are unpredictable, when used for social applications, opportunistic messaging is most suited for disseminating user-generated information that is non-formal, less important, and thus not time-critical. In recent years, a considerable amount of efforts have been invested in the research on opportunistic networking and delay-tolerant networking (which encompasses opportunistic networking but is a broader concept). A large portion of prior work has focused on routing issues, e.g., through whom as intermediate carriers to deliver a message to the destinations [1] [2] [3] [4]. The routing issues have been further explored in various contexts, such as in vehicular networks [5] [6] [7] and in social networking applications [8] [9] [10] [11]. However, serious real-world application development, deployment and evaluation of the opportunistic networking concepts still fall behind [12], in which many challenging issues remain to be addressed (to name a few, location-awareness, user incentives and preferences, power preservation, encounter controls, etc.). In this work, we design and prototype PassItOn, a fully distributed opportunistic messaging system. Our goal is to build up a proof-of-concept platform on real mobile devices, and thus show the feasibility and potentials of utilizing human movements for dissemination applications. Meanwhile, we seek to shed lights on the design, implementation and deployment issues in building such systems, and thus stimulate new ideas and perspectives on addressing these issues. Moreover, we aim to offer a real testbed on which new mechanisms, protocols and use cases can be tested and evaluated.",autonomous vehicle,211,not included
d0270dcfa7ca058cea59512b832be6c91408676f,to_check,semantic_scholar,Scalable Comput. Pract. Exp.,2005-01-01 00:00:00,semantic_scholar,challenges concerning symbolic computations on grids,https://www.semanticscholar.org/paper/d0270dcfa7ca058cea59512b832be6c91408676f,"Challenges concerning symbolic computations on grids Symbolic and algebraic computations are currently ones of fastest growing areas of scientific computing. For a long time, the numerical approach to computational solution of mathematical problems had an advantage of being capable of solving a substantially larger set of problems than the other approach, the symbolic one. Only recently the symbolic approach gained more recognition as a viable tool for solving large-scale problems from physics, engineering or economics, reasoning, robotics or life sciences. Developments in symbolic computing were lagging relative to numerical computing, mainly due to the inadequacy of available computational resources, most importantly computer memory, but also processor power. Continuous growth in the capabilities of computer hardware led naturally to an increasing interest in symbolic calculations and resulted, among others things, in development of sophisticated Computer Algebra Systems (CASs). CASs allow users to study computational problems on the basis of their mathematical formulations and to focus on the problems themselves instead of spending time transforming the problems into forms that are numerically solvable. While their major purpose is to manipulate formulas symbolically, many systems have substantially extended their capabilities, offering nowadays functionalities like graphics allowing a comprehensive approach to problem solving. While, typically, CAS systems are utilized in an interactive mode, in order to solve large problems they can be also used in a batch mode and programmed using languages that are close to common mathematical notation. As CASs become capable of solving large problems, they follow the course of development that has already been taken by numerical software: from sequential computers to parallel machines to distributed computing and finally to the grid. It is particularly the grid that has the highest potential as a discovery accelerator. Currently, its widespread adoption is still impeded by a number of problems, one of which is difficulty of developing and implementing grid-enabled programs. That it is also the case for grid-enabled symbolic computations. There are several classes of symbolic and algebraic algorithms that can perform better in parallel and distributing computing environments. For example for multiprecision integer arithmetic, that appears among others in factorizations, were developed already twenty years ago systolic algorithms and implementations on massive parallel processors, and more recently, on the Internet. Another class that utilize significant amount of computational resources is related to the implementations of polynomial arithmetic: knowledge based algorithms such as symbolic differentiation, factorization of polynomials, greatest common divisor, or, more complicated, Groebner base computations. For example, in the latest case, the size of the computation and the irregular data structures make the parallel or distributed implementation not only an attractive option for improving the algorithm performance, but also a challenge for the computational environment. A third class of algorithms that can benefit from multiple resources in parallel and distributed environments is concerning the exact solvers of large systems of equations. The main reason driving the development of parallel and distributed algorithms for symbolic computations is the ability to solve problems that are memory bound, i.e. that cannot fit into memory of a single computer. An argument for this statement relies on the observation that the input size of a symbolic or algebraic computation can be small, but the memory used in the intermediate stages of the computation may grow considerably. Modern CASs increase their utility not only through new symbolic capabilities, but also expending their applicability using visualization or numerical modules and becoming more than only specific computational kernels. They are real problem solving environments based on interfaces to a significant number of computational engines. In this context it appears also the need to address the ability to reduce the wall-clock time by using parallel or distributed computing environment. A simple example is the case of rendering the images for a simulation animation. Several approaches can be identified in the historical evolution of parallel and distributed CASs: developing versions for shared memory architectures, developing computer algebra hardware, adding facilities for communication and cooperation between existing CASs, or building distributed systems for distributed memory parallel machines or even across Internet. Developing completely new parallel or distributed systems, although efficient, in most cases is rather difficult. Only a few parallel or distributed algorithms within such a system are fully implemented and tested. Still there are several successful special libraries and systems falling in this category: ParSac-2 system, the parallel version of SAC-2, Paclib system, the parallel extension of Saclib, FLATS based on special hardware, STAR/MPI, the parallel version of GAP, ParForm, the parallel version of Form, Cabal, MuPAD, or the recent Givaro, for parallel computing environments, FoxBox or DSC, for distributed computing environments. An alternative approach to build parallel and distributed CASs is to add the new value, the parallelism or the distribution, to an existing system. The number of parallel and distributed versions of most popular CASs is impressive and it can be explained by the different requirements or targeted architectures. For example, for Maple there are several implementations on parallel machines, like the one for Intel Paragon or ||Maple||, and several implementations on networks of workstations, like Distributed Maple or PVMaple. For Mathematica there is a Parallel Computing Toolkit, a Distributed Mathematica and a gridMathematica (for dedicated clusters). Matlab that provides a Symbolic Math Toolbox based on a Maple kernel has more than twenty different parallel or distributed versions: DP-Toolbox, MPITB/PVMTB, MultiMatlab, Matlab Parallelization Toolkit, ParMatlab, PMI, MatlabMPI, MATmarks, Matlab*p, Conlab, Otter and others. More recent web-enabled systems were proved to be efficient in number theory for finding large prime numbers, factoring large numbers, or finding collisions on known encryption algorithms. Online systems for complicated symbolic computations were also built: e.g. OGB for Groebner basis computations. A framework for description and provision of web-based mathematical services was recently designed within the Monet project and a symbolic solver wrapper was build to provide an environment that encapsulates CASs and expose their functionalities through symbolic services (Maple and Axiom were chosen as computing engines). Another platform is MapleNet build on client-server architecture: the server manages concurrent Maple instances launched to server client requests for mathematical computations. WebMathematica is a similar system that offers access to Mathematica applications through a web browser. Grid-oriented projects that involve CASs were only recent initiated. The well-known NetSolve system was one of the earliest grid system developed. Version 2 released in 2003 introduces GridSolve for interoperability with the grid based on agent technologies. APIs are available for Mathematica, Octave and Matlab. The Genss project (Grid Enabled Numerical and Symbolic Services) follows the ideas of the Monet project and intends also to combine grid computing and mathematical web services using a common agent-based framework. Several projects are porting Matlab on grids: from small ones, like Matlab*g, to very complex ones, like Geodise. Maple2g and MathGridLink are two different approaches for grid-enabled version of Maple and Mathematica. Simple to use front-end were recently build in projects like Gemlca and Websolve to deploy legacy code applications as grid services and to allows the submission of computational requests. The vision of grid computing is that of a simple and low cost access to computing resources without artificial barriers of physical location or ownership. Unfortunately, none of the above mentioned grid-enabled CAS is responding simultaneously to some elementary requirements of a possible implementation of this vision: deploy grid symbolic services, access within CAS to available grid services, and couple different grid symbolic services. Moreover a number of major obstacles remain to be addressed. Amongst the most important are mechanisms for adapting to dynamic changes in either computations or systems. This is especially important for symbolic computations, which may be highly irregular in terms of data and general computational demands. Such demands received until now relatively little attention from the research community. In the context of a growing interest in symbolic computations, powerful computer algebra systems are required for complex applications. Freshly started projects shows that porting a CAS to a current distributed environment like a grid is not a trivial task not only from technological point of view but also from algorithmic point of view. Already existing tools are allowing experimental work to be initiated, but a long way is still to be cross until real-world problems will be solved using symbolic computations on grids. Dana Petcu, Western University of Timisoara",autonomous vehicle,212,not included
905872ddd3390749ceb0c71062ffbff10815e7fa,to_check,semantic_scholar,AORN journal,1969-01-01 00:00:00,semantic_scholar,a paradox.,https://www.semanticscholar.org/paper/905872ddd3390749ceb0c71062ffbff10815e7fa,"It remains a paradox that the very reason why DBAs exist, namely ""The Relational Data Model"", invented by E.F. Codd in the 1970s, receives very little attention at DBA gatherings. It's time to remember our roots: we must take a moment to remove one hat, the Physical Database Administrator, and put on another, the Logical Data Modeler, to rejuvenate our understanding of our very purpose. This paper will remind us of the importance of the Logical Data Model, how it fits into this thing called ""Enterprise Architecture"", and offer three secrets to designing a successful model. INTRODUCTION THE PARADOX The people who build Databases are a special breed: they enthusiastically perform a job most people would shun. The very nature of their job is paradoxical: they must at once make easily available the very thing they are charged to supremely protect, nothing less than the most valuable asset of any Information-Age Company, its data. What matters most to people in a functioning Organization is their Business Information: they want it all, they want it perfect and they want it now. They want it protected, pristine, backed-up, jacked-up, quantified and verified. In other words they want it logically accurate, and physically accessible. Paradoxically, all of this responsibility seems to ultimately fall on the shoulders of one individual, the DBA, the “Physical Database Administrator.” A modern Database, much like a baby, requires a lot of physical attention, dare I say love, to keep it going. That means someone has to be its mother. That “someone”, of course, is none other than the DBA. Ironically, when all goes well, the DBA is invisible and ignored. When, however, the smallest problem arises, the DBA is vilified and hunted. There are myriad physical reasons why a Database would develop problems, most of which result in highly visible and undesirable consequences. It is no wonder, then, that DBAs concentrate almost exclusively on preventing the highly visible failures. This means they tend to perform the overt mundane duties that prevent the most immediate dangers from disrupting their Database, namely the Physical Tasks. These are the well known, traditional DBA duties that we all know and love, for example, creating and maintaining tablespaces, tables, indexes, triggers, redo log files, partitions, initial parameters, block sizes, users, roles, privileges, packages, procedures, exports, backups, and so on. But these duties represent only half the story. A Database needs more than just physical attention, yet paradoxically, no one ever seems to mention the logical attention it needs. If the DBA is the Database’s Mother, providing physical protection and nurturing, then who is the Father, providing logic and purpose? Enter the Data Modeler. LOGICAL VS PHYSICAL, YIN AND YANG, NATURE VS. NURTURE The roles of DBA and Data Modeler are quite different. It takes both roles, Yin and Yang, to rear a Database to healthy adulthood. Then why, do we always seem to hear about the DBA and almost never about the Data Modeler? Far too often the role of Data Modeler is either vastly diminished or unceremoniously handed over to the DBA as a secondary burden. DBA: Design/Configuration Sometimes, heaven forbid, it is abolished altogether. Nothing could be more deleterious to the long-term health of the Database. This phenomenon has analogies to the perennial “Nature vs. Nurture” debate in science class: how much of a Database’s success is due to its genetic design, its “logical model”, as opposed to its developmental manifestation, its “physical administration?” The answer is, of course, both parents of Database development are needed, the Logical and the Physical, and the unfortunate omission of the Logical Data Modeler from the nursery has had dire consequences indeed. SUMMARY A modern Database needs two types of minds, two distinct skill-sets, two parents, as it were, to conceive it and nurture it to maturity: the Logical parent and the Physical parent. The first parent, the Logical Data Modeler, designs the very potential of the Database, and decides what it will be concerned with, how it defines its own purpose and how it will relate philosophically to the ideal world. The second parent, the DBA, manifests the Database into physical reality, and carefully builds its environment, protects it from malcontents and teaches it how to adapt to the real world. The Data Modeler’s tasks are inherently Logical. The DBA’s tasks are inherently Physical. BEGINNINGS E.F. CODD I often wonder how the above paradox came about. Why do DBAs and not Data Modelers dominate the Database World? And why are Database Professionals mostly concerned with physical performance as opposed to logical truthfulness? And just to make sure we’re all on the same page, just what do we mean by “Physical” vs. “Logical?” in the first place? Fortunately we can shed light on these questions by understanding why we are all here to begin with. The very reason the modern Database Profession exists today is because of a man named E.F. Codd. His breakthrough paper, “The Relational Model of Data for Large Shares Data Banks” gave birth to the information age itself. Dr. Codd’s brilliant insight was born out of a physical need to share large amounts of data, yet his solution was inherently a logical masterpiece of the highest order. In other words, he solved a physical problem by applying a logical solution. Thus was spawned the roots of our current paradox: Logical vs. Physical, and here’s how. Prior to Dr. Codd’s conceptual breakthrough in the 1970’s, electronic data was stored differently depending on its hardware. It was stored in ways that best suited the particular “big iron” hardware upon which it resided. Each implementation was rather unique, and to access the data on any particular machine required extensive customization. Consequently, any small change in a “business rule” would require an incommensurately large change in the customized coding and data storage structures. Thus, although it was possible to “brute-force” the old systems to behave, one had to accept the relatively high cost of physically reconfiguring the hardware, data structures, and thousands of lines of procedural code. BIRTH OF THE PARADOX In other words, the Data Processing professionals of the 1970s spent most of their time manipulating the physical parts of their systems; a costly and slow way to do business, not to mention self-defeating. Unfortunately, this mindset remains today for many modern DBAs who support the mantra “thou shalt force a database it into submission with more indexes, more code, more hardware.” Though it took several decades to take hold, all this emphasis on physical solutions changed with E.F. Codd’s revolutionary invention, which he named a “Relational Model for Data”. For the first time in the history of “Data Processing”, it was possible to trancsend the limitations of hardware and to organize data in such a way to make it accessible without regard to its physical storage; again, truly the heart and soul of the Information Age. DBA: Design/Configuration As we will see later, even Dr. Codd did not fully realize the power of his “Relational Model”. His original goals were modest, indeed. He wanted only to propose a new way to store physical data, to make it more efficient for later retrieval. He did this by proposing a novel concept of how to logically organize data according to set-theory. What he did not realize is that he hit upon an extremely powerful idea with far wider implications. Not only was his Relation Model good for organizing electronic data but it was also a perfect method for organizing ALL information, even information about information (e.g. meta-data). THE DATA MODEL WHAT IS A DATA MODEL? In the field of Systems Engineering the discipline of Data Modeling is arguably the least understood. To complicate matters, Analysts (including Zachman) speak of three types of Data Models, namely “Conceptual”, “Logical” and “Physical”. What is the difference between these types and how would the master of the Zachman Framework deploy these pieces to win the game? Before we answer this, we must first briefly explore the source of the Data Model’s power. RELATIONAL THEORY In the early 1970’s, Dr. E.F. Codd, working for IBM, introduced a brilliant solution for organizing information based on mathematical set-theory. His seminal paper, “A Relational Model of Data for Large Shared Data Banks” (Codd, 1970) remains unequalled today in its impact on information technology. It is heralded for not just spawning the multi-billion dollar Relational Database Management System (RDBMS) industry, which includes Oracle, Sybase, SQLserver, DB2, etc, but also for providing the theoretical foundation for numerous operating systems, the client-server revolution, object oriented databases, data warehousing, not to mention countless user applications, etc (Codd, 1990). Relational Theory is based on three key ideas (Codd, 1970): 1. Elemental Forms. Organize your world by recognizing the things that don’t change much. Identify the smallest units of the Enterprise and describe them accurately and uniquely. Model them as they exist in the world; thus as the world changes, so commensurately does the model change. Tech-speak: Identify entities and relationships, and assign finite attributes to them according to set-theory (nTuples). 2. Extensibility. Organize your world into categories of things, to which you can add any number of specific things. Tech-speak: Structure your entities so that they can be iterated to infinity. 3. Independence. Organize your information so that it can be understood on its own merit and in such a way that it is valid and useful irrespective of how it might be used. Tech-speak: Store data independently of how it will be accessed. UNEXPECTED POWER A Data Model, or more specifically, a Relational Data Model, gains its power from a simple idea: by constructing surrogates tha",autonomous vehicle,213,not included
10.1109/southeastcon42311.2019.9113415,to_check,2019 SoutheastCon,IEEE,2019-04-14 00:00:00,ieeexplore,advanced signal processing for decision making and decision-fusion software systems for aircraft structural health monitoring,https://ieeexplore.ieee.org/document/9113415/,"Tracking the structure health status of the aircraft fleet is one of the important factors for improving safety in the aviation industry. US Airforce Research Lab is working hard to develop technology to monitor the structural health status of aircraft before and after takeoff. This technology is called Structural Health Monitoring or Structural Health Management (SHM), and uses advanced signal processing techniques. The advantages of this include improving the technology and safety factors of the aircraft by increasing the robustness of the aircraft structure as well as reduce the cost of the labor maintenance. This research paper focuses on developing a software system that is capable of detecting a crack in the aircraft structure at an early stage. Also, the developed software will be used to estimate the crack lengths within 90% accuracy. The developed software uses the Matlab environment for all algorithms developed: calculating and finding the crack, and estimating the crack length. Artificial Intelligent (AI) techniques such as fuzzy logic and neural networks are used to support the decision regarding the length of the crack. Also, developed decision fusion frameworks are designed to increase the accurate percentage rate of the decision making results. The results obtained from this research are compared with the baseline developed by The Air Force Research Laboratory (AFRL). Finally, the results of this research along with the capabilities of using advanced signal processing integrated with AI, show that the calculations for the crack and the crack length produce very acceptable results using a real data gathered by AFRL. Also, the percentage average error performance analysis of the developed software's system algorithms is provided in the research paper.",health,214,not included
10.1109/cbms.2019.00040,to_check,2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS),IEEE,2019-06-07 00:00:00,ieeexplore,deep-learning and hpc to boost biomedical applications for health (deephealth),https://ieeexplore.ieee.org/document/8787438/,"This document introduces the DeepHealth project: ""Deep-Learning and HPC to Boost Biomedical Applications for Health"". This project is funded by the European Commission under the H2020 framework program and aims to reduce the gap between the availability of mature enough AI-solutions and their deployment in real scenarios. Several existing software platforms provided by industrial partners will integrate state-of-the-art machine-learning algorithms and will be used for giving support to doctors in diagnosis, increasing their capabilities and efficiency. The DeepHealth consortium is composed by 21 partners from 9 European countries including hospitals, universities, large industry and SMEs.",health,215,not included
10.1109/rweek.2018.8473535,to_check,2018 Resilience Week (RWS),IEEE,2018-08-23 00:00:00,ieeexplore,framework for data driven health monitoring of cyber-physical systems,https://ieeexplore.ieee.org/document/8473535/,"Modern infrastructure is heavily reliant on systems with interconnected computational and physical resources, named Cyber-Physical Systems (CPSs). Hence, building resilient CPSs is a prime need and continuous monitoring of the CPS operational health is essential for improving resilience. This paper presents a framework for calculating and monitoring of health in CPSs using data driven techniques. The main advantages of this data driven methodology is that the ability of leveraging heterogeneous data streams that are available from the CPSs and the ability of performing the monitoring with minimal a priori domain knowledge. The main objective of the framework is to warn the operators of any degradation in cyber, physical or overall health of the CPS. The framework consists of four components: 1) Data acquisition and feature extraction, 2) state identification and real time state estimation, 3) cyber-physical health calculation and 4) operator warning generation. Further, this paper presents an initial implementation of the first three phases of the framework on a CPS testbed involving a Microgrid simulation and a cyber-network which connects the grid with its controller. The feature extraction method and the use of unsupervised learning algorithms are discussed. Experimental results are presented for the first two phases and the results showed that the data reflected different operating states and visualization techniques can be used to extract the relationships in data features.",health,216,included
10.1109/aero.2005.1559665,to_check,2005 IEEE Aerospace Conference,IEEE,2005-03-12 00:00:00,ieeexplore,health monitoring: new techniques based on vibrations measurements and identification algorithms,https://ieeexplore.ieee.org/document/1559665/,"Purpose of the paper is to present an innovative application inside the nondestructive testing field based on vibrations measurements, developed, at the Department of Aeronautical Engineering of the University of Naples ""Federico II"" (Italy), by the authors during the last three years, and already tested for analysing damages of many structural elements. The aim has been the development of a nondestructive test (NDT) which meet to most of the mandatory requirements for effective health monitoring systems, simultaneously reducing as much as possible the complexity of the data analysis algorithm and of the experimental acquisition instrumentation; these peculiarities may, in fact, not be neglected for an operative implementation of such a system. The proposed new method is based on the acquisition and comparison of frequency response functions (FRFs) of the monitored structure before and after an occurred damage. Structural damages modify the dynamical behaviour of the structure such as mass, stiffened and damping, and consequently the FRFs of the damaged structure in comparison with the FRFs of the sound structure, making possible to identify, to localize and quantify a structural damage. The activities, presented in the paper, mostly focused on a new FRFs processing technique based on the determining of a representative ""damage index"" for identifying and analysing damages both on real scale aeronautical structural components, like large-scale fuselage reinforced panels, and on aeronautical composite panels. Besides it has been carried out a dedicated neural network algorithm aiming at obtaining a ""recognition-based learning""; this kind of learning methodology permits to train the neural network in order to let it recognise only ""positive"" examples discarding as a consequence the ""negative"" ones. Within the structural NDT a ""positive"" example means ""healthy"" state of the analysed structural component and, obviously, a ""negative"" one means a ""damaged"" or perturbed state. With this object in view the neural network has been trained making use of the same FRFs of the healthy structure used for the determining of the damage index, as positive examples. From an architectural point of view magnetostrictive devices have been tested as actuators, and piezoceramic patches as actuators and sensors. Besides it has been used a laser-scanning vibrometer system to validate the behaviour of the piezoceramic patches and define their technical parameters in order to lay the bases for design a light and reliability system. These techniques promise to bring a step forward for the implementation of an automatic ""health monitoring"" system which will be able to identify a structural damage in real time, improving the safety and reducing maintenance costs",health,217,included
10.1109/access.2020.2970178,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,a novel software engineering approach toward using machine learning for improving the efficiency of health systems,https://ieeexplore.ieee.org/document/8974224/,"Recently, machine learning has become a hot research topic. Therefore, this study investigates the interaction between software engineering and machine learning within the context of health systems. We proposed a novel framework for health informatics: the framework and methodology of software engineering for machine learning in health informatics (SEMLHI). The SEMLHI framework includes four modules (software, machine learning, machine learning algorithms, and health informatics data) that organize the tasks in the framework using a SEMLHI methodology, thereby enabling researchers and developers to analyze health informatics software from an engineering perspective and providing developers with a new road map for designing health applications with system functions and software implementations. Our novel approach sheds light on its features and allows users to study and analyze the user requirements and determine both the function of objects related to the system and the machine learning algorithms that must be applied to the dataset. Our dataset used in this research consists of real data and was originally collected from a hospital run by the Palestine government covering the last three years. The SEMLHI methodology includes seven phases: designing, implementing, maintaining and defining workflows; structuring information; ensuring security and privacy; performance testing and evaluation; and releasing the software applications.",health,218,included
10.1109/access.2018.2875677,to_check,IEEE Access,IEEE,2018-01-01 00:00:00,ieeexplore,patient2vec: a personalized interpretable deep representation of the longitudinal electronic health record,https://ieeexplore.ieee.org/document/8490816/,"The wide implementation of electronic health record (EHR) systems facilitates the collection of large-scale health data from real clinical settings. Despite the significant increase in adoption of EHR systems, these data remain largely unexplored, but present a rich data source for knowledge discovery from patient health histories in tasks, such as understanding disease correlations and predicting health outcomes. However, the heterogeneity, sparsity, noise, and bias in these data present many complex challenges. This complexity makes it difficult to translate potentially relevant information into machine learning algorithms. In this paper, we propose a computational framework, Patient2Vec, to learn an interpretable deep representation of longitudinal EHR data, which is personalized for each patient. To evaluate this approach, we apply it to the prediction of future hospitalizations using real EHR data and compare its predictive performance with baseline methods. Patient2Vec produces a vector space with meaningful structure, and it achieves an area under curve around 0.799, outperforming baseline methods. In the end, the learned feature importance can be visualized and interpreted at both the individual and population levels to bring clinical insights.",health,219,not included
10.1109/iucc-css.2016.021,to_check,2016 15th International Conference on Ubiquitous Computing and Communications and 2016 International Symposium on Cyberspace and Security (IUCC-CSS),IEEE,2016-12-16 00:00:00,ieeexplore,a learning system to support social and empathy disorders diagnosis through affective avatars,https://ieeexplore.ieee.org/document/7828588/,"Nowadays diagnosis and treatment of cognitive and physical health issues can be empowered through the use of information technologies. However, there is a significant gap between the potential of those technologies and the real application. One example is the use of serious games with health proposals, a trending research area still not implanted in health systems. This paper proposes the use of serious games, particularly an interactive and affective avatar-based application to support the diagnosis and treatment of empathy and socialization issues, in an autonomous way through the implementation of a learning algorithm based on the ground truth obtained from the evaluation with real users, including normotypical users, users with Down syndrome and users with intellectual disability.",health,220,not included
10.1109/wetice49692.2020.00032,to_check,2020 IEEE 29th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE),IEEE,2020-09-13 00:00:00,ieeexplore,a learning based secure anomaly detection for healthcare applications,https://ieeexplore.ieee.org/document/9338574/,"The wireless body sensor network (WBSN) is an emerging technology in the healthcare domain, which collects data from vital parameters of the patient's body, using small portable components such as sensors. It allows to continuously monitor patients without restricting their movements and to inform the health specialist of the evolution of their states. However, the deployment of new technologies in health applications without taking into account data security makes the privacy of patients vulnerable. Thus, these systems have insufficient performance for large and varied data sets, because they do not process information sufficiently diverse to cover all scenarios. Moreover, the interpretation of physiological signals is a tedious process in which human errors can be caused.To address these issues, we propose, in this paper, an effective health system that ensures the secure collection and transfer of patient data to the physician, to facilitate the diagnosis and the detection of life threatening diseases. Our main contribution is to offer real-time, high quality processing, learning and analysis capabilities, in a smart and secure system. For that, we have collected real patient datasets from Tunisian doctors.To achieve this goal, we used ANN and XgBoost as learning algorithms and AES as an encryption protocol for sending secure data to medical personnel for diagnostic purposes. The results show an accuracy of 80% with an execution time of 1.54 s.",health,221,not included
10.1109/iembs.2010.5627760,to_check,2010 Annual International Conference of the IEEE Engineering in Medicine and Biology,IEEE,2010-09-04 00:00:00,ieeexplore,activity-based process mining for clinical pathways computer aided design,https://ieeexplore.ieee.org/document/5627760/,"Current trends in health management improvement demand the standardization of care protocols to achieve better quality and efficiency. The use of Clinical Pathways is an emerging solution for that problem. However, current Clinical Pathways are big manuals written in natural language and highly affected by human subjectivity. These problems make the deployment and dissemination of them extremely difficult in real practice environments. In this work, a complete computer based architecture to help the representation and execution of Clinical Pathways is suggested. Furthermore, the difficulties inherent to the design of formal Clinical Pathways in this way requires new specific design tools to help making the system useful. Process Mining techniques can help to automatically infer processes definition from execution samples. Yet, the classical Process Mining paradigm is not totally compatible with the Clinical Pathways paradigm. In this paper, a pattern recognition algorithm based in an evolution of the Process Mining classical paradigm is presented and evaluated as a solution to this situation. The proposed algorithm is able to infer Clinical Pathways from execution logs to support the design of Clinical Pathways.",health,222,not included
10.1109/compsac48688.2020.0-168,to_check,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE,2020-07-17 00:00:00,ieeexplore,an early warning system for hemodialysis complications utilizing transfer learning from hd iot dataset,https://ieeexplore.ieee.org/document/9202848/,"According to the 2018 annual report of US Department of Kidney Data System (USRDS), Taiwan's dialysis rate and prevalence rate are the highest in the world due to population aging, diabetes and progresses in cardiovascular care. With the rise of artificial intelligence deep learning in recent years, various analytical software resources have gradually become easier to obtain. At the same time, wearable cyber physical sensors are becoming more and more popular. Measurements on vital signs such as heartbeat, electrocardiogram, and blood oxygenation blood pressure values are ubiquitous. We propose an integrated system that combines dialysis big data deep learning analysis with cross platform physiological sensing. We specifically tackle the early warning of dialysis discomfort such as hypotension, hypertension, cramps, etc., this requires a large amount of data collection, related training, data sources including dialysis treatment process and home physiological data. Although the Dialysis machine is able to produce huge amount of IoT data, the usable data for early warning system training is not as huge due to the limited physician labors devoted for labeling questionable samples. This generally leads to low accuracy for regular CNN training methods. We enhance the AI training performance via a transfer learning technique. The AI training accuracy reaches the value of 99% with the help of transfer learning, while that of an original CNN process on the HD data bears a low 60% accuracy. Given the high prediction accuracy of our AI engine, we are able to integrate the real time measurements from Dialysis machine with wearable devices such as ECG sensors and wrist health watches, and make precision prediction of incoming discomfort during the HD treatments. The ECG signal of the same group patients are also analyzed with the same technique. The same accuracy enhancement are also observed.",health,223,included
10.1109/icphm.2019.8819421,to_check,2019 IEEE International Conference on Prognostics and Health Management (ICPHM),IEEE,2019-06-20 00:00:00,ieeexplore,application of deep learning for fault diagnostic in induction machine’s bearings,https://ieeexplore.ieee.org/document/8819421/,"Recent developments in sensor technologies and advances in communication systems have resulted in deployment of a large number of sensors for collecting condition monitoring (CM) data in order to monitor health condition of a manufac-tring/industrial system. Efficient utilization of sensory data leads to highly accurate results in system fault diagnostics/prognostics. The exponential growth of CM data poses significant analytical challenges, due to their high variety, high dimensionality and high velocity rendering conventional health monitoring tools impractical. In this regard, the paper proposes a deep learning-based framework for fault diagnosis of an induction machine's bearing based on real data set provided by Case Western Reserve University bearing data center. In particular, we focus on deep bidirectional long short-term memory (BiD-LSTM) networks fed with raw signals for fault diagnosis to address drawbacks of conventional machine learning (ML) solutions such as support vector machines. A numerical example is provided to illustrate the complete procedure of the proposed framework, which shows the great potentials of the BiD-LSTM for detection of different types of the bearing fault with high accuracy. The effectiveness of the proposed model is demonstrated through a comparison with a recently developed deep neural network (DNN) that considers temporal coherence for the same data set. The results indicate that the proposed framework provides considerably improved performance in comparison to its counterparts.",health,224,not included
10.1109/wicom.2011.6040629,to_check,"2011 7th International Conference on Wireless Communications, Networking and Mobile Computing",IEEE,2011-09-25 00:00:00,ieeexplore,automatic reminder system of medical orders based on bluetooth,https://ieeexplore.ieee.org/document/6040629/,"The accurate and real-time implementation of medical orders will be directly related to the patient's health. To improve the accurate and real-time implementation of medical orders, the implementation of medical orders in real time automated reminder system based on wireless communication technology will be introduced in this paper. Through the design of embedded data acquisition, timing light alarm, time display of the automated reminder by touching the screen, the research on the extraction and transformation of the medical orders, design of time reminder device and the development of the information query system and the information management system of medical orders execution. Eventually, we could remind nurses to deal with the nursery content of patients real time and ensure the accuracy of medical advice and real-time implementation by using text messages to remind nurses and adding reminder alarm kit to the drug box.",health,225,not included
10.1109/iceccme52200.2021.9591113,to_check,"2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",IEEE,2021-10-08 00:00:00,ieeexplore,cobots for fintech,https://ieeexplore.ieee.org/document/9591113/,"Embedded devices enabling payments transaction processing in Financial Services industry cannot have any margin for error. These devices need to be tested &amp; validated by replicating production like environment to the extent possible. This means literally handling payments related events like swiping a credit card, tapping a mobile phone or pressing buttons amongst many other things like in real world. Embedded Software development is time consuming as it involves multiple man-machine interactions and dependencies such as managing and handling embedded devices, operating devices (Push buttons, interpret display panels, read receipt printouts etc.) and sharing devices for collaboration within team. During the current pandemic, it was impossible for software teams to travel to office, share devices or even procure necessary devices on time for project related tasks. This caused delay to project delivery and increased Time to market. The paper describes how the team used Capgemini's flexible Robotics as a Service (RaaS) platform that helped during pandemic to automate feasible man-machine interactions using Robotic arms. The paper provides details of the work done by the team that involves internet of things (IoT), Artificial Intelligence (AI) to remotely handle and operate hardware and devices thereby completing embedded software development life cycles faster and well within budget while ensuring superior product quality and importantly ensuring team's health and safety. This is novel in Financial Services space.",health,226,included
10.1109/icris52159.2020.00072,to_check,2020 International Conference on Robots & Intelligent System (ICRIS),IEEE,2020-11-08 00:00:00,ieeexplore,design and implementation of sitting posture monitoring system,https://ieeexplore.ieee.org/document/9523937/,"In order to remind the sedentary workers to count the health data, a sitting posture monitoring system is designed. The single chip microcomputer collects the sensor array data, uploads the data to the server through Wi Fi, carries on the sensor array data fusion in the server, and the software displays the sitting posture information and the statistical results. The feedforward neural network is used as classifier to recognize nine kinds of sitting posture. The recognition rate is 90.643%. It can prompt the user's sitting posture in real time and count the sitting posture data.",health,227,not included
10.1109/icbaie52039.2021.9389950,to_check,"2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",IEEE,2021-03-28 00:00:00,ieeexplore,design of cloud computing platform based accurate measurement for structure monitoring using fiber bragg grating sensors,https://ieeexplore.ieee.org/document/9389950/,"The efficient integration of distributed fiber Bragg grating (FBG) sensors and cloud computing platform is used to achieve accurate measurement and evaluation of physical quantities, which solves the problems of traditional fiber Bragg grating sensing technology for health structure monitoring system, such as the cost and space constraints, it is difficult to deploy enough servers to deal with data collection, transmission and storage in real time. The cloud platform using fiber Bragg grating sensors adopts the structure of erbium-doped fiber cascaded Bragg grating, reasonably configures the FBG demodulator acquisition and analysis software, deploys the health monitoring system in the cloud, constructs the cloud platform of high-efficiency health monitoring optical fiber sensor network, improves the scalability of the system, flexibly deploys applications and services, and ensures the security and reliability of various real-time monitoring data and professional data. It can meet the needs of some specific or wide application fields for the automation technology, structural mechanics, computer technology, Internet architecture, cloud deployment and interdisciplinary practical comprehensive valuable application research.",health,228,not included
10.1109/ijcnn.2013.6707048,to_check,The 2013 International Joint Conference on Neural Networks (IJCNN),IEEE,2013-08-09 00:00:00,ieeexplore,epidemiological dynamics modeling by fusion of soft computing techniques,https://ieeexplore.ieee.org/document/6707048/,"Infectious disease prevention and control are important in improving, promoting and protecting the health of communities. Epidemiological data analysis plays a crucial role in disease prevention and control. Conventional methods such as moving average or autoregressive analysis normally require the assumption of stationarity, which is often violated in epidemiologic time series. This paper proposes the fusion of neural networks, fuzzy systems and genetic algorithms, with the aim to strengthen the modeling power for epidemiological dynamics. We deploy an additive fuzzy system into a neural network architecture in order to incorporate recurrent nodes to enable the fuzzy system to handle temporal data. The genetic algorithm is employed to optimize the fuzzy rule structure before supervised training is applied to adjust parameters. As epidemiological time series exhibit complex behavior and possibly cyclic patterns, the addition of recurrent nodes to the fuzzy system improves the modeling capability. The proposed model dominates the benchmark feedforward neural network and adaptive neuro-fuzzy inference system model regarding modeling performance. Through real applications for epidemiologic time series modeling, the fusion of soft computing techniques offer accurate forecasts that have considerable meaning in planning infectious disease-control activities.",health,229,not included
10.1109/icit.2015.7125235,to_check,2015 IEEE International Conference on Industrial Technology (ICIT),IEEE,2015-03-19 00:00:00,ieeexplore,improving accuracy of long-term prognostics of pemfc stack to estimate remaining useful life,https://ieeexplore.ieee.org/document/7125235/,"Proton Exchange Membrane Fuel cells (PEMFC) are energy systems that facilitate electrochemical reactions to create electrical energy from chemical energy of hydrogen. PEMFC are promising source of renewable energy that can operate on low temperature and have the advantages of high power density and low pollutant emissions. However, PEMFC technology is still in the developing phase, and its large-scale industrial deployment requires increasing the life span of fuel cells and decreasing their exploitation costs. In this context, Prognostics and Health Management of fuel cells is an emerging field, which aims at identifying degradation at early stages and estimating the Remaining Useful Life (RUL) for life cycle management. Indeed, due to prognostics capability, the accurate estimates of RUL enables safe operation of the equipment and timely decisions to prolong its life span. This paper contributes data-driven prognostics of PEMFC by an ensemble of constraint based Summation Wavelet-Extreme Learning Machine (SW-ELM) algorithm to improve accuracy and robustness of long-term prognostics. The SW-ELM is used for ensemble modeling due to its enhanced applicability for real applications as compared to conventional data-driven algorithms. The proposed prognostics model is validated on run-to-failure data of PEMFC stack, which had the life span of 1750 hours. The results confirm capability of the prognostics model to achieve accurate RUL estimates.",health,230,not included
10.23919/cisti52073.2021.9476616,to_check,2021 16th Iberian Conference on Information Systems and Technologies (CISTI),IEEE,2021-06-26 00:00:00,ieeexplore,knowledge management applying disruptive technologies as a response to the covid-19 crisis in public administration,https://ieeexplore.ieee.org/document/9476616/,"The accelerated propagation of Sars-Cov2 and the consequent Covid-19 disease in the world has led the public administrations of different countries to use the advantages provided by disruptive technologies to effectively and efficiently manage the large volume of data generated. This communication presents the main characteristics and advances that distinguish the development of the initiative called Epidempredict for Covid19 in Panama. The project involves the implementation of a cloud platform that facilitates data analysis in a distributed, collaborative and secure form. This platform will enable efficient data ingestion, administration, analysis, visualization and export. The development will be a solution oriented to VUCA (Volatile, Uncertain, Complex and Ambiguous) environments. The implementation integrates tools and resources supported by technologies such as Artificial Intelligence (AI) and Machine Learning (ML) integrating models, predictive algorithms and dashboards that enable decision making based on real and meaningful data. The project proposes the use of a neuro-hybrid model, based on a SIR model with artificial intelligence, combining several algorithms. This initiative aims to support the health authorities of the Panamanian public administration by facilitating decision making and the adoption and implementation of precise strategic actions in the field of health and public welfare.",health,231,not included
10.1109/nccc49330.2021.9428873,to_check,2021 National Computing Colleges Conference (NCCC),IEEE,2021-03-28 00:00:00,ieeexplore,machine learning-based predictive model for surgical site infections: a framework,https://ieeexplore.ieee.org/document/9428873/,"Surgical site infections impact hospital readmission rates, length of stay, and patient and hospital expense. The use of computational intelligence methods can help to predict the risk of SSIs and provide an early warning, enabling hospitals to prepare in advance to respond to these infections. The objective of this paper is to present a machine learning-based predictive model for surgical site infections. This paper also reviews the most recent machine learning-based models developed for the prediction of SSIs. When these predictive models are applied correctly and used effectively, they can be helpful for clinical surveillance teams. However, the implementation of these models and related tools requires quality data to be stored in electronic health records, which may not be available in all health information systems. The limitations of clinical data and the absence of labels adding several challenges with the implementation of the predictive models using machine learning; an imbalanced dataset is also a common issue that can influence the performance of the model, thus requiring an improved strategy to address this concern. Recently, the interpretability of predictive models has become important for hospital physicians to translate the models into real practices.",health,232,not included
10.1109/ijcnn.2013.6706957,to_check,The 2013 International Joint Conference on Neural Networks (IJCNN),IEEE,2013-08-09 00:00:00,ieeexplore,optimized neuro genetic fast estimator (ongfe) for efficient distributed intelligence instantiation within embedded systems,https://ieeexplore.ieee.org/document/6706957/,"The Optimized Neuro Genetic Fast Estimator (ONGFE) is a software tool that allows for embedding system, subsystem, and component failure detection, identification, and prognostics (FDI&amp;P) capability by using Intelligent Software Elements (ISE) based upon Artificial Neural Networks (ANN). With an Application Programming Interface (API), highly innovative algorithms are compiled for efficient distributed intelligence instantiation within embedded systems. The original design had the purpose of providing a real time kernel to deploy health monitoring functions for Condition Based Maintenance (CBM) and Real Time Monitoring (RTM) systems in a broad variety of applications (such as aerospace, structural, and widely distributed support systems). The ONGFE contains embedded fast and on-line training for designing ANNs to perform several high performance FDI&amp;P functions. A key advantage of this technology is an optimization block based upon pseudogenetic algorithms which compensate for effects due to initial weight values and local minimums without the computational burden of genetic algorithms. The ONGFE also provides a synchronization block for communication with secondary diagnostic modules. The algorithms are designed for a distributed, scalar, and modular deployment. Based on this technology, a scheme for conducting sensor data validation has been embedded in Smart Sensors.",health,233,included
10.1109/qrs51102.2020.00018,to_check,"2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)",IEEE,2020-12-14 00:00:00,ieeexplore,phm technology for memory anomalies in cloud computing for iaas,https://ieeexplore.ieee.org/document/9282796/,"The IaaS (Infrastructure as a Service) is one of the most popular services from todays cloud service providers, where the virtual machines (VM) are rented by users who can deploy any program they want in the VMs to make their own websites or use as their remote desktops. However, this poses a major challenge for cloud IaaS providers who cannot control the software programs that users develop, install or download on their rented VMs. Those programs may not be well developed with various bugs or even downloaded/installed together with virus, which often make damages to the VMs or infect the cloud platform. To keep the health of a cloud IaaS platform, it is very important to implement the PHM (Prognostics and Health Management) technology for detecting those software problems and self-healing them in an intelligent and timely way. This paper realized a novel PHM technology inspired by biological autonomic nervous system to deal with the memory anomalies of those programs running on the cloud IaaS platform. We first present an innovative autonomic computing technology called Bionic Autonomic Nervous System (BANS) to endow the cloud system with distinctive capabilities of perception, detection, reflection, and learning. Then, we propose a BANS-based Prognostics and Health Management (BPHM) technology to enable the cloud system self-dealing with various memory anomalies. AI-based failure prognostics, immediate self-healing, self-learning ability and self-improvement functions are implemented. Experimental results illustrate that the designed BPHM can automatically and intelligently deal with complex memory anomalies in a real cloud system for IaaS, to keep the system much more reliable and healthier.",health,234,included
10.1109/bds/hpsc/ids18.2018.00045,to_check,"2018 IEEE 4th International Conference on Big Data Security on Cloud (BigDataSecurity), IEEE International Conference on High Performance and Smart Computing, (HPSC) and IEEE International Conference on Intelligent Data and Security (IDS)",IEEE,2018-05-05 00:00:00,ieeexplore,real-time intelligent air quality evaluation on a resource-constrained embedded platform,https://ieeexplore.ieee.org/document/8552303/,"Indoor air quality has a major impact on health and comfort of building occupants. Poor air quality may reduce productivity in offices and impair students learning in classes. In order to provide localized air pollution data and tailor it for individual, wearable air quality sensor is a promising solution. Furthermore, crowd sensing has emerged as an Internet-of-Things (IoT) solution, which is economical, scalable and easy to deploy and re-deploy as it uses the power of crowd data collection. The goal of our proposed system is to monitor indoor air quality through a crowd sensing system that will use a set of sensors to measure air quality, monitor the concentration of pollutants continuously, and make recommendations in real time for improved air quality. In this paper artificial network is developed to perform real-time indoor air quality control. Utilizing created neural network embedded into a smart controller comfort level of air quality parameters such as temperature, CO_2 air concentration and humidity could be estimated after every measurement and used for adapting air conditioning systems to adjust air quality. Neural network data preparation and training process are discussed along with deployment of trained network on a smart controller.",health,235,included
10.1109/aero47225.2020.9172454,to_check,2020 IEEE Aerospace Conference,IEEE,2020-03-14 00:00:00,ieeexplore,semi-supervised machine learning for spacecraft anomaly detection &amp; diagnosis,https://ieeexplore.ieee.org/document/9172454/,"This paper describes Anomaly Detection via Topological-feature Map (ADTM), a data-driven approach to Integrated System Health Management (ISHM) for monitoring the health of spacecraft and space habitats. Developed for NASA Ames Research Center, ADTM leverages proven artificial intelligence techniques for rapidly detecting and diagnosing anomalies in near real-time. ADTM combines Self-Organizing Maps (SOMs) as the basis for modeling system behavior with supervised machine learning techniques for localizing detected anomalies. A SOM is a two-layer artificial neural network (ANN) that produces a low-dimensional representation of the training samples. Once trained on normal system behavior, SOMs are adept at detecting behavior previously not encountered in the training data. Upon detecting anomalous behavior, ADTM uses a supervised classification approach to determine a subset of measurands that characterize the anomaly. This allows it to localize faults and thereby provide extra insight. We demonstrate the effectiveness of our approach on telemetry data collected from a lab-stationed CubeSat (the “LabSat”) connected to software that gave us the ability to trigger several real hardware faults. We include an analysis and discussion of ADTM's performance on several of these fault cases. We conclude with a brief discussion of future work, which contains investigation of a hierarchical SOM-architecture as well as a Case-Based Reasoning module for further assisting astronauts in diagnosis and remediation activities.",health,236,not included
10.1109/iccp.2016.7737117,to_check,2016 IEEE 12th International Conference on Intelligent Computer Communication and Processing (ICCP),IEEE,2016-09-10 00:00:00,ieeexplore,towards application of non-invasive environmental sensors for risks and activity detection,https://ieeexplore.ieee.org/document/7737117/,"One of the main goals of Ambient Assisted Living (AAL) is to provide supportive environment for the elderly or disabled. Such environments are not feasible without correctly identifying states and activities of the persons receiving the care. They rely on the interaction and processing of data originating from many components and objects in the surrounding. In order to collect the data, various sensors are used to monitor the environment, as well as the person's health parameters. One of the main concerns in AAL is preservation of user's privacy. In this paper we address that by proposing a non-intrusive approach for data collection and identification of daily activity and risks. We describe the wiring of such system based on cheap non-intrusive sensors, deployment in a real environment, the protocols for data fusion and processing, and explain how machine learning could be employed for detecting risks and activities. The main contribution of this paper is development of non-intrusive sensor kits that can be easily deployed in real-life environments and are capable of collecting data that can reliable detect activities and risk.",health,237,not included
10.1109/itc-egypt52936.2021.9513888,to_check,2021 International Telecommunications Conference (ITC-Egypt),IEEE,2021-07-15 00:00:00,ieeexplore,a proposed end to end telemedicine system based on embedded system and mobile application using cmos wearable sensors,https://ieeexplore.ieee.org/document/9513888/,"Internet of things (IoT) and Embedded systems have extensive applications in healthcare markets. Integration of IoT with healthcare started with wearable smartwatches monitoring some signals and storing this data in the cloud. With 4G/5G and WiFi 6 networks. Healthcare data can be analyzed with Artificial Intelligence providing new era Internet of Medical Things (IoMT) that encompass an array of internet-capable medical devices that are in constant communication with each other or with the cloud; Internet of Healthcare Things (IoHT) that is the digital transformation of the healthcare industry. This article presents an end-to-end architecture with realization of three modules for key IoT aspects for healthcare and telemedicine. Results from a real implementation of application Platform for Data Processing including patient and doctor data base-based web site, MySQL data base, Android based mobile App, and PHP webserver.",health,238,included
10.1109/wf-iot.2019.8767231,to_check,2019 IEEE 5th World Forum on Internet of Things (WF-IoT),IEEE,2019-04-18 00:00:00,ieeexplore,efficient deployment of predictive analytics in edge gateways: fall detection scenario,https://ieeexplore.ieee.org/document/8767231/,"Ambient Assisted Living (AAL) represents the most promising Internet of Things (IoT) application due to its relevance in the elders healthcare and improvement of their quality of life. Recently, the AAL IoT ecosystem has been enriched with promising technologies such as edge computing, which has demonstrated to be the best approach to overcome the demanding requirements of AAL and healthcare services by providing a reduction of the amount of data to transfer to the cloud, an improvement of the response time, and quality of experience. Also, the deployment of Artificial Intelligence (AI) technologies at the edge provides intelligence to improve the decision making timely. However, this approach has been scarcely studied in AAL scenarios and the few proposals based on deploying machine learning models at the edge lack efficiency, security, mechanisms of resource management, service management, and deployment, as well as a real and experimental AAL scenario. For these reasons, this paper proposes an innovative edge gateway architecture to support the deployment of deep learning (DL) models in AAL and healthcare scenarios efficiently. To do so, we have added a predictive analytics module to deploy the models. Since AI technologies demand more resources, a container-based virtualization technology is employed on the edge gateway to manage the limited resources, and provide security and lifecycle services management. The edge gateway performance was evaluated deploying a DL-based fall detection application on it. As a result, our approach improves the inference time compared to that based on the cloud in 34 seconds and to similar approaches in 8 seconds.",health,239,included
10.1109/icacite51222.2021.9404731,to_check,2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),IEEE,2021-03-05 00:00:00,ieeexplore,face mask detection system using cnn,https://ieeexplore.ieee.org/document/9404731/,"Amid current pandemic, Covid-19 has made us realize the importance of Face Masks and we need to understand the crucial effects of not wearing one, now more than ever. Right now, there are no face mask detectors installed at the crowded places. But we believe that it is of utmost importance that at transportation junctions, densely populated residential area, markets, educational institutions and healthcare areas, it is now very important to set up face mask detectors to ensure the safety of the public. In this paper we have tried to build a two phased face mask detector which will be easy to deploy at the mentioned outlets. With the help of Computer Vision, it is now possible to detect and implement this on large scale. We have used CNN/ MobileNet V2 architecture for the implementation of our model. The implementation is done in Python, and the python script implementation will train our face mask detector on our selected dataset using TensorFlow and Keras. We have added more robust features and trained our model on various variations, we made sure to have large varied and augmented dataset so that the model is able to clearly identify and detection the face masks in real time videos. The trained model was tested on both real-time videos and static pictures and in both the cases the accuracy was more than the other designed models.",health,240,not included
10.1109/ultsym.2011.0044,to_check,2011 IEEE International Ultrasonics Symposium,IEEE,2011-10-21 00:00:00,ieeexplore,noninvasive estimation of dynamic pressures in vitro and in vivo using the subharmonic response from microbubbles,https://ieeexplore.ieee.org/document/6293674/,"In this work, the development of subharmonic emission based noninvasive pressure estimation technique is presented. In vitro, ambient pressures were varied (between 0 and 120 mmHg) in a closed-loop flow system circulating 0.2 ml Sonazoid microbubbles (GE Healthcare, Oslo, Norway) suspended in 750 ml of isotonic diluent and recorded by a Millar pressure catheter as the reference standard. Simultaneously, a SonixRP ultrasound scanner (Ultrasonix Medical Corp., Richmond, BC, Canada) operating in pulse inversion mode (f<sub>transmit</sub>: 2.5 MHz) was used to acquire unprocessed RF data at five different incident acoustic pressures (from 76 kPa to 897 kPa; n=3). The subharmonic data for each pulse was extracted using band-pass filtering with averaging, and subsequently, processed to eliminate noise. The incident acoustic pressure most sensitive to ambient pressure fluctuations was determined; then the ambient pressure was tracked over 20 seconds. Regression analysis compared subharmonic and catheter pressure values. In vivo validation of this technique was performed noninvasively for tracking left ventricular (LV) pressures of two canines using similar post processing as in vitro. The subharmonic signal tracked ambient pressures with r<sup>2</sup> = 0.922 for 20 seconds in vitro. In vivo the subharmonic signal tracked the LV pressures with r<sup>2</sup> &gt;; 0.7P90. Maximum errors in estimating clinically relevant systolic and diastolic pressures ranged from 0.22 to 2.84 mmHg using this subharmonic technique relative to Millar catheter pressures. Clinical validation and real time implementation of this technique may ultimately lead to the first noninvasive cardiac pressure monitoring tool.",health,241,not included
10.1109/icce-berlin.2018.8576251,to_check,2018 IEEE 8th International Conference on Consumer Electronics - Berlin (ICCE-Berlin),IEEE,2018-09-05 00:00:00,ieeexplore,cnn inference: dynamic and predictive quantization,https://ieeexplore.ieee.org/document/8576251/,"Deep Learning techniques like Convolutional Neural Networks (CNN) are the de-facto method for image classification with broad usage spanning across automotive, industrial, medicine, robotics etc. Efficient implementation of CNN inference on embedded device requires a quantization method, which minimizes the accuracy loss, ability to generalize across deployment scenarios as well as real-time processing. Existing literature doesn't address all these three requirements simultaneously. In this paper, we propose a novel quantization algorithm to overcome above mentioned challenges. The proposed solution dynamically selects the scale for quantizing activations and uses Kalman filter to predict quantization scale to reduce accuracy loss. The proposed solution exploits the range statistics from previous inference processes to estimate quantization scale, enabling real-time solution. The proposed solution is implemented on TI's TDA family of embedded automotive processors. The proposed solution is running real time semantic segmentation on TDA2x processor within 0.1% accuracy loss compared floating point algorithm. The solution performs well across multiple deployment scenarios (e.g. rain, snow, night etc) demonstrating generalization capability of the solution.",health,242,not included
10.1109/meco.2019.8760130,to_check,2019 8th Mediterranean Conference on Embedded Computing (MECO),IEEE,2019-06-14 00:00:00,ieeexplore,facilitating near real time analytics on the edge,https://ieeexplore.ieee.org/document/8760130/,"Internet of Things (IoT) is revolutionizing the way how information is processed and stored. Due to latency sensitive applications and huge amounts of data produced at the edge of the network, more and more data is processed where it is produced - namely on the edge. This development results in completely new network topologies where besides massive data centers we experience growing amount of so called micro data centers on the edge of the network. However, increasing complexity of multiple data centers necessary to execute an application represents a new challenge for the deployment and runtime operation of large scale applications like those in the area of smart cities, self-driving vehicles and tele medicine. The challenge thereby is to deploy application in a way to satisfy user requirements in form of different Quality of Service parameters (e.g., latency) but at the same time minimize energy consumption necessary to execute the application. In this talk we discuss several research challenges that arise when deploying near real time analytics on the edge of the network.",health,243,not included
10.1109/jsen.2020.3041668,to_check,IEEE Sensors Journal,IEEE,2021-03-01 00:00:00,ieeexplore,design of a pose and force controller for a robotized ultrasonic probe based on neural networks and stochastic gradient approximation,https://ieeexplore.ieee.org/document/9274482/,"In medicine and engineering, the implementation of a diagnostic test using an ultrasonic sensor requires suitable contact conditions, and a correct pose to attain the best signal transmission settings. A soft sensor probe provides a good surface adaptation and forces transfer, but it introduces nonlinearities and noisy measurements, making it difficult to control the probe during a real time test by conventional algorithms. In this work, a data driven controller is developed to control force and pose of a soft contact ultrasound sensor. The adaptive controller is based on a fuzzy-rules emulated network structure with the learning algorithm using a stochastic gradient approximation. The proposed control algorithm overcomes the noise environment conditions and nonlinearities of the unknown nonlinear discrete-time system. This was numerically validated and then, experimentally tested with an industrial robotic system using an ultrasonic probe designed in our lab. The results show that the proposed controller performs well under the contact-force regulation and can find the correct contact orientation with a fast convergence.",health,244,included
10.1109/access.2018.2873597,to_check,IEEE Access,IEEE,2018-01-01 00:00:00,ieeexplore,hierarchical semantic mapping using convolutional neural networks for intelligent service robotics,https://ieeexplore.ieee.org/document/8490234/,"The introduction of service robots in the public domain has introduced a paradigm shift in how robots are interacting with people, where robots must learn to autonomously interact with the untrained public instead of being directed by trained personnel. As an example, a hospital service robot is told to deliver medicine to Patient Two in Ward Three. Without awareness of what “Patient Two” or “Ward Three” is, a service robot must systematically explore the environment to perform this task, which requires a long time. The implementation of a Semantic Map allows for robots to perceive the environment similar to people by associating semantic information with spatial information found in geometric maps. Currently, many semantic mapping works provide insufficient or incorrect semantic-metric information to allow a service robot to function dynamically in human-centric environments. This paper proposes a semantic map with a hierarchical semantic organization structure based on a hybrid metric-topological map leveraging convolutional neural networks and spatial room segmentation methods. Our results are validated using multiple simulated and real environments on our lab's custom developed mobile service robot and demonstrate an application of semantic maps by providing only vocal commands. We show that this proposed method provides better capabilities in terms of semantic map labeling and retain multiple levels of semantic information.",health,245,included
10.1109/icoin.2019.8718118,to_check,2019 International Conference on Information Networking (ICOIN),IEEE,2019-01-11 00:00:00,ieeexplore,binarized multi-factor cognitive detection of bio-modality spoofing in fog based medical cyber-physical system,https://ieeexplore.ieee.org/document/8718118/,"Bio-modalities are ideal for user authentication in Medical Cyber-Physical Systems. Various forms of bio-modalities, such as the face, iris, fingerprint, are commonly used for secure user authentication. Concurrently, various spoofing approaches have also been developed over time which can fail traditional bio-modality detection systems. Image synthesis with play-doh, gelatin, ecoflex etc. are some of the ways used in spoofing bio-identifiable property. Since the bio-modality detection sensors are small and resource constrained, heavy-weight detection mechanisms are not suitable for these sensors. Recently, Fog based architectures are proposed to support sensor management in the Medical Cyber-Physical Systems (MCPS). A thin software client running in these resource-constrained sensors can enable communication with fog nodes for better management and analysis. Therefore, we propose a fog-based security application to detect bio-modality spoofing in a Fog based MCPS. In this regard, we propose a machine learning based security algorithm run as an application at the fog node using a binarized multi-factor boosted ensemble learner algorithm coupled with feature selection. Our proposal is verified on real datasets provided by the Replay Attack, Warsaw and LiveDet 2015 Crossmatch benchmark for face, iris and fingerprint modality spoofing detection used for authentication in an MCPS. The experimental analysis shows that our approach achieves significant performance gain over the state-of-the-art approaches.",health,246,not included
10.1109/ieeegcc.2011.5752566,to_check,2011 IEEE GCC Conference and Exhibition (GCC),IEEE,2011-02-22 00:00:00,ieeexplore,evolutionary multiobjective optimization for medical classification,https://ieeexplore.ieee.org/document/5752566/,"We propose a computational environment based on evolutionary algorithm for medical classification. We use evolutionary multiobjective optimization (EMO) to solve a general medical minimization problem. As an example, we simultaneously minimize three objectives, namely the number of genes responsible for cancer classification while reducing the number of misclassifications in both testing and learning data sets for real patients. Results quality is reported against three genetic operators namely selection, crossover and mutation, each of which offering three different methods. Our implementation gives comparable results to more sophisticated methods, such as NGSAII-like ones, with far less computational efforts.",health,247,not included
10.1109/ssci.2018.8628895,to_check,2018 IEEE Symposium Series on Computational Intelligence (SSCI),IEEE,2018-11-21 00:00:00,ieeexplore,example mining for incremental learning in medical imaging,https://ieeexplore.ieee.org/document/8628895/,"Incremental Learning is well known machine learning approach wherein the weights of the learned model are dynamically and gradually updated to generalize on new unseen data without forgetting the existing knowledge. Incremental learning proves to be time as well as resource-efficient solution for deployment of deep learning algorithms in real world as the model can automatically and dynamically adapt to new data as and when annotated data becomes available. The development and deployment of Computer Aided Diagnosis (CAD) tools in medical domain is another scenario, where incremental learning becomes very crucial as collection and annotation of a comprehensive dataset spanning over multiple pathologies and imaging machines might take years. However, not much has so far been explored in this direction. In the current work, we propose a robust and efficient method for incremental learning in medical imaging domain. Our approach makes use of Hard Example Mining technique (which is commonly used as a solution to heavy class imbalance) to automatically select a subset of dataset to fine-tune the existing network weights such that it adapts to new data while retaining existing knowledge. We develop our approach for incremental learning of our already under test model for detecting dental caries. Further, we apply our approach to one publicly available dataset and demonstrate that our approach reaches the accuracy of training on entire dataset at once, while availing the benefits of incremental learning scenario.",health,248,not included
10.1109/crv.2017.15,to_check,2017 14th Conference on Computer and Robot Vision (CRV),IEEE,2017-05-19 00:00:00,ieeexplore,leveraging tree statistics for extracting anatomical trees from 3d medical images,https://ieeexplore.ieee.org/document/8287685/,"Using different priors (e.g. shape and appearance) have proven critical for robust image segmentation of different types of target objects. Many existing methods for extracting trees (e.g. vascular or airway trees) from medical images have leveraged appearance priors (e.g. tubular-ness and bifurcationness) and the knowledge of the cross-sectional geometry (e.g. circles or ellipses) of the tree-forming tubes. In this work, we present the first method for 3D tree extraction from 3D medical images (e.g. CT or MRI) that, in addition to appearance and cross-sectional geometry priors, utilizes prior tree statistics collected from the training data. Our tree extraction method collects and leverages topological tree prior and geometrical statistics, including tree hierarchy, branch angle and length statistics. Our implementation takes the form of a Bayesian tree centerline tracking method combining the aforementioned tree priors with observed image data. We evaluated our method on both synthetic 3D datasets and real clinical CT chest datasets. For synthetic data, our method's key feature of incorporating tree priors resulted in at least 13% increase in correctly detected branches under different noise levels. For real clinical scans, the mean distance from ground truth centerlines to the detected centerlines by our method was improved by 12% when utilizing tree priors. Both experiments validate that, by incorporating tree statistics, our tree extraction method becomes more robust to noise and provides more accurate branch localization.",health,249,not included
10.1109/ecai.2018.8678937,to_check,"2018 10th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",IEEE,2018-06-30 00:00:00,ieeexplore,mining medical data,https://ieeexplore.ieee.org/document/8678937/,"The paper focuses on identifying patterns in medical data that can bring economic advantage to medical facilities. The main objective is to determine which algorithm fits best on a medical data set in order to determine how likely a patient will return for a new consult. In order to achieve that a medical data set was used to evaluate the accuracy of each algorithm and what it could predict. The proposed software prototype is uses the data mining algorithm in order to offer real time insights to clinic employees about patients. Based on this information, clinic employees can offer vouchers in order to tempt patients into choosing the clinic again for their next consult. Using data mining techniques to gain economic advantage might play a decisive role in company's fight supremacy.",health,250,not included
10.1109/itnec52019.2021.9586993,to_check,"2021 IEEE 5th Information Technology,Networking,Electronic and Automation Control Conference (ITNEC)",IEEE,2021-10-17 00:00:00,ieeexplore,a novel method for color forged image detection,https://ieeexplore.ieee.org/document/9586993/,"Digital images permeate almost all areas of our lives, mainly in news media, scientific discoveries, medical imaging, and judicial evidence. However, in recent years, due to the wide application of deep learning in image processing technology, these technologies or software have not only brought convenience to people, but also made it easier for people to forge or tamper with digital images without leaving any traces. The authenticity of digital images has been affected. A serious threat to modify and threaten. These forged or tampered images will bring serious threats to judicial justice, social stability, and the medical industry, and cause huge negative effects. Therefore, this article proposes an authenticity detection algorithm for generating color forged images based on deep learning. The corresponding color channel features of the real and forged image datasets are extracted and FIsher encoded, respectively, and the encoded color channel features are used to train the SVM model. Experiments prove that our proposed method achieves better results in detecting image color tampering.",health,251,not included
10.1109/nnsp.1999.788166,to_check,Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468),IEEE,1999-08-25 00:00:00,ieeexplore,a comparative study of a hidden markov model detector for atrial fibrillation,https://ieeexplore.ieee.org/document/788166/,"A comparative study of several atrial fibrillation (AF) detection algorithms was done to determine the algorithm best suited for use in real clinical environments to detect AF in ambulatory ECGs. The algorithms that were investigated for this paper are based on the Hidden Markov Model (HMM), measures of variance, linear predictive coding, and measurement of approximate entropy (AE). Based on the results from the test data set, the HMM algorithm performed best for this application. In general, there is little difference between the performance of the HMM and AE algorithms. However, the implementation of the HMM algorithm is more computationally efficient. Because of the large amount of data that must be analyzed in ambulatory ECG recordings, the computational efficiency must be considered as an issue of practicality. Review of the data illuminated some of the strengths and weaknesses of the various algorithms. Variance measures performed with either high sensitivity or high positive predictivity, but were not able to achieve a desirable operating point that had both acceptable sensitivity and positive predictivity. Although AE and LPC had acceptable sensitivity and positive predictivity, the HMM performed even better than both of these in terms of overall error rate. It would seem that an observational model such as the HMM, fits the data better than parametric models such as AE and LPC. Finally, as the computing power of medical systems increases, more sophisticated algorithms may be exploited in ways that leads to more accurate computerized ECG interpretation.",health,252,not included
10.1109/icacite51222.2021.9404749,to_check,2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),IEEE,2021-03-05 00:00:00,ieeexplore,artificial intelligence and robotics: impact &amp; open issues of automation in workplace,https://ieeexplore.ieee.org/document/9404749/,"In engineering province robotics is one of the cognitive perspective to human communication or it concern with synod of perception of action. In Today's Tech World Artificial Intelligence is an essential tool which provides effective analytical business solutions &amp; plays significant role in the domain of robotics and have several similarities like human behavior which may drive the real world. This paper shows the significant blend of Artificial Intelligence and robotics which transform entire industries, technological improvement of robotics application &amp; utilization. It also focuses on different aspects of targets like marketing, home appliances, medical science, Smart agriculture and many more which includes open issues and technological challenges arises by this combination and conclude that robotics with AI can work in real world with real objects. Further AI based robotics are very important area in economics and organizational consequence, implementation of automation in any organizational design give impact on overall economy and infrastructure provide a wider direction for further research on Robotics and IoT are two terms each covering a myriad of technologies and concepts.",health,253,not included
10.1109/tencon.2019.8929612,to_check,TENCON 2019 - 2019 IEEE Region 10 Conference (TENCON),IEEE,2019-10-20 00:00:00,ieeexplore,lung nodule detection from low dose ct scan using optimization on intel xeon and core processors with intel distribution of openvino toolkit,https://ieeexplore.ieee.org/document/8929612/,"With the advancement of AI in the field of medical imaging, medical diagnosis is getting faster and viable for medical practitioners especially for cancer diagnosis. Earlier Deep Learning solutions had to be deployed on High Performance Computing devices like GPU for achieving real time performance. But with Optimization on Intel Core and Xeon processors with Intel Distribution of OpenVINO Toolkit (Open Visual Inference and Neural Network Optimization), it is possible to deploy Deep Learning models with accelerated performance, than running Tensorflow / Caffe models on CPU machines. In this paper we describe the proposed work wherein we ported our DetectNet Deep Learning Model with NVIDIA specific custom layer for lung nodule detection trained on LIDC dataset, using Intel Distribution of OpenVINO, and deployed the same in Intel Core/Xeon processors with accelerated performance.",health,254,included
10.1109/acitt.2019.8780078,to_check,2019 9th International Conference on Advanced Computer Information Technologies (ACIT),IEEE,2019-06-07 00:00:00,ieeexplore,modern hardware and software solution for identification of abnormal neurological movements of patients with essential tremor,https://ieeexplore.ieee.org/document/8780078/,"Because of the real physical movements, associated with tremor, which is very small rapid fluctuations that vary greatly in amplitude and frequency, reliable measurements of motion characteristics and the interpretation of the results of such measurements are important for the diagnosis of diseases. Increasing of amplitude and changing the frequency and form of oscillation in relation to the norm (frequency and amplitude of physiological tremor) are signs of disturbance of central and peripheral neural mechanisms of regulation of movements. Program analysis of these parameters is important for understanding the role of dysfunction of individual structures of the brain in the processes of motion management, and for the clinic in the aspects of early detection, more accurate diagnosis of motor disorders, the choice and correction of optimal treatment methods associated with the selection of effective therapy. The analysis of tremor forms has great importance in neurological practice, in particular, in the diagnosis of Parkinson's syndrome. Computerized analysis methods are extremely accurate and effective in assessing the degree of tremor of patients. Mathematical modeling allows obtaining frequency characteristics, amplitude of oscillation, deviation from normal state, etc. Development of mathematical models and methods, means of identification greatly facilitate the work with patients; make it possible to assess the degree of disease with a single and objective side. Software and hardware for the determination and diagnosis of tremor is an important application of software engineering for medical and public benefit purposes. Use of the latest design techniques, the development of architectures and software models, artificial intelligence contributes to the growth of the role of diagnostic automation in modern research and therapeutic measures.",health,255,not included
10.1109/andescon50619.2020.9272196,to_check,2020 IEEE ANDESCON,IEEE,2020-10-16 00:00:00,ieeexplore,multipurpose unmanned system: an efficient solution to increase the capabilities of the uavs,https://ieeexplore.ieee.org/document/9272196/,"The results of this research propose the implementation of a system that significantly increases the capacity of unmanned vehicles, turning them into multifunctional vehicles. The system has a logistics dispatch module and a video analytics module. The first module allows the delivery of medical, food, smoke, disinfectant, etc. The module is practical, safe and economical, features that denote the feasibility of immediate implementation in unmanned vehicles of any rank and / or classification. Note that the implementation of the dispatch module does not require additional radio frequency systems. The second module includes a video analysis process in real time, an aspect that constitutes a significant contribution to the proposed solution, since it allows obtaining important information during the flight; it also reduces the risk in air operations and simultaneously increases the efficiency of themselves. Note that video analytics optimizes resources and avoids jeopardizing the lives of aircraft pilots and crews who traditionally should carry out these activities. In times of pandemic, this innovation avoids direct contact with an infected population and can guarantee the sanitary conditions required in certain circumstances. The solution increases the capabilities of unmanned vehicles and makes them useful tools in various scenarios, whether caused by natural or man-made disasters. Our proposal is very flexible, reliable, and scalable and can be adapted to various models and makes of unmanned vehicles. The system has been implemented on fixed-wing and rotary-wing unmanned vehicles, showing satisfactory results.",health,256,included
10.1109/innotek.2014.6877371,to_check,2014 IEEE Innovations in Technology Conference,IEEE,2014-05-16 00:00:00,ieeexplore,preventing recreational boating fatalities and serious injuries with: the ann (assistant naval navigator) system enterprise,https://ieeexplore.ieee.org/document/6877371/,"Because of the significant increase in motor vehicle fatalities due to texting while driving, several US States are implementing or drafting laws that will outright ban the use of handheld devices (“I” and cell Phones, pads, business communicators, etc.) while operating a motor vehicle. At the same time, US recreational boaters already are burdened with high numbers of fatal and injurious accidents, and are being encouraged to navigate using Internet-based APPs that require texting. Our analyses of trends in recent US Coast Guard's (USCG's) Boating Statistics<sup>1</sup> seem to support that accidents due to inattention are on the rise. Specifically, owner/operator inattention is causing a higher number of collisions with other recreational boats and these are accompanied by a related rise in fatalities per collision. This observation is in-line with the US Dept. of Transportation findings that texting while driving leads to more violent accidents; therefore, more fatalities/accident. Review of recent years of USCG Boating Statistics (2004 thru 2012, subsequent to the beginning of the fuel crisis) yielded several interesting conclusions. Using data 2012 as a benchmark, there were 4,515 recreational boating accidents resulting in 651 fatalities. In 73% of these fatal accidents, the owner/operators had no USCG approved instruction. Also alarming, in 13% of the 4515 accidents the cause was owner/operator inattention. Add to this, in 1010 accidents of the 4,515, these involved a collision with another recreational boat! (That's over 22% of the total accidents). It's shocking to realize that the number of moms, dads, young men and women and children that are dying and seriously injured per year, while enjoying this recreational activity, was nearly equaled the yearly US troop losses and serious injuries during the Iraq war. Over 10 years ago we began our design effort by assembling a team of experts having experience with on-water Emergency Response, and marine-related technologies. By then, the author and his direct associate had accumulated more than 70 years of successful installation and start-up of computer-based control systems for very large and small processes. The applications included major metals, chemical, plastics, ore-processing, underground coal mining industries, as well as, a medical enterprise application. We envisioned how the vessel navigation process may be adapted to feedforward/feedback control system architecture in real time. Proceeding on this basis, our team met frequently with system integrators, boat and engine manufacturers, Boat Owners Association of the US (BoatUS) Execs, towing station owner/operators and US Coast Guard Certified “A” Captains and pilots. In these meetings, our goal continued to focus on a plan for applying advanced control theory to enhance marine vessel navigation. This plan is now in readiness for productization - a viable, developed and US Patented <sup>2</sup> system with primary missions of life-saving and/serious injury prevention. Think of it! Based on our in-depth studies, we project that implementation of our ANN System will reduce the annual average of 750 recreational boating fatalities by 30% (225 fatalities)! That's the equivalent of 6 City Transit buses filled with Moms, Dads, Children, Men and Women able to go on living their life as though nothing happened - just a very enjoyable boat cruise.",health,257,not included
10.1109/mlsp.2018.8516927,to_check,2018 IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP),IEEE,2018-09-20 00:00:00,ieeexplore,single-channel eeg classification by multi-channel tensor subspace learning and regression,https://ieeexplore.ieee.org/document/8516927/,"The classification of brain states using neural recordings such as electroencephalography (EEG) finds applications in both medical and non-medical contexts, such as detecting epileptic seizures or discriminating mental states in brain-computer interfaces, respectively. Although this endeavor is well-established, existing solutions are typically restricted to lab or hospital conditions because they operate on recordings from a set of EEG electrodes that covers the whole head. By contrast, a true breakthrough for these applications would be the deployment `in the real world', by means of wearable devices that encompass just one (or a few) channels. Such a reduction of the available information inevitably makes the classification task more challenging. We tackle this issue by means of a multilinear subspace learning step (using data from multiple channels during training) and subsequently solving a regression problem with a low-rank structure to classify new trials (using data from only a single channel during testing). We demonstrate the feasibility of this approach on EEG data recorded during a mental arithmetic task.",health,258,not included
10.1109/tbme.2007.909506,to_check,IEEE Transactions on Biomedical Engineering,IEEE,2008-03-01 00:00:00,ieeexplore,a fully automatic cad-ctc system based on curvature analysis for standard and low-dose ct data,https://ieeexplore.ieee.org/document/4360143/,"Computed tomography colonography (CTC) is a rapidly evolving noninvasive medical investigation that is viewed by radiologists as a potential screening technique for the detection of colorectal polyps. Due to the technical advances in CT system design, the volume of data required to be processed by radiologists has increased significantly, and as a consequence the manual analysis of this information has become an increasingly time consuming process whose results can be affected by inter- and intrauser variability. The aim of this paper is to detail the implementation of a fully integrated CAD-CTC system that is able to robustly identify the clinically significant polyps in the CT data. The CAD-CTC system described in this paper is a multistage implementation whose main system components are: 1) automatic colon segmentation; 2) candidate surface extraction; 3) feature extraction; and 4) classification. Our CAD-CTC system performs at 100% sensitivity for polyps larger than 10 mm, 92% sensitivity for polyps in the range 5 to 10 mm, and 57.14% sensitivity for polyps smaller than 5 mm with an average of 3.38 false positives per dataset. The developed system has been evaluated on synthetic and real patient CT data acquired with standard and low-dose radiation levels.",health,259,included
10.1109/tns.2017.2706061,to_check,IEEE Transactions on Nuclear Science,IEEE,2017-06-01 00:00:00,ieeexplore,a hardware implementation of a brain inspired filter for image processing,https://ieeexplore.ieee.org/document/7931608/,"A cognitive image processing implementation for pattern-matching execution is proposed in this paper. It is based on the learning process of the human vision as an edge-enhancing filter for medical images. We set up an experiment to test its impact on the performance of decision-making algorithm working on brain magnetic resonance data. The execution times of similar filters can become unpractical on real 3-D or higher dimensional data, if implemented on a CPU. An innovative and high-performance embedded system for real-time pattern matching was developed. The design uses field-programmable gate arrays and the powerful associative memory chip (an ASIC) to achieve real-time performance and requires a training phase and a data acquisition phase. It is a very compact implementation that improves execution time ×1000 for the training phase and ×100 for the data acquisition phase for 2-D black and white images compared to a last generation i7 CPU. The implementation of this edge-enhancing filter is expected to positively impact on medical devices for real-time diagnosis (e.g., diagnostic ultrasound) and for image processing steps in medical image analysis where computing power is a limiting factor.",health,260,not included
10.1109/access.2020.3022039,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,a lightweight convolutional neural network for real and apparent age estimation in unconstrained face images,https://ieeexplore.ieee.org/document/9187238/,"Real and apparent age estimation of human face has attracted increased attention due to its numerous real-world applications. Different intelligent application scenarios can benefit from these computer-based systems that predict the ages of people correctly. Automatic apparent age system is particularly useful in medical diagnosis, facial beauty product development, movie role casting, the effect of plastic surgery, and anti-aging treatment. Predicting the real and apparent age of people has been quite difficult for both machines and humans. More recently, Deep learning with Convolutional Neural Networks (CNNs) methods have been extensively used for these classification task. It has incomparable advantages in extracting discriminative image features from human faces. However, many of the existing CNN-based methods are designed to be deeper and larger with more complex layers that makes it challenging to deploy on mobile devices with resource-constrained features. Therefore, we design a lightweight CNN model of fewer layers to estimate the real and apparent age of individuals from unconstrained real-time face images that can be deployed on mobile devices. The experimental results, when analyzed for classification accuracy on FG-NET, MORPH-II and APPA-REAL, with large-scale face images containing both real and apparent age annotations, show that our model obtains a state-of-the-art performance in both real and apparent age classification when compared to state-of-the-art methods. The new results and model size, therefore, confirm the usefulness of the model on resource-constrained mobile devices.",health,261,included
10.1109/access.2019.2927461,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,ephort: towards a reference architecture for tele-rehabilitation systems,https://ieeexplore.ieee.org/document/8760232/,"In recent years, the software applications for medical assistance, including the tele-rehabilitation, have known a high and a continuous presence in the medical area. The ePHoRt is a Web-based platform for the remote home monitoring rehabilitation exercises in patients after hip replacement surgery. It involves a learning phase and a serious game scheme for the execution and evaluation of the exercises as part of a therapeutic program. Modular software architecture is proposed, under the patient perspective, to be used as a reference model for researchers or professionals who wish to carry out tele-rehabilitation platforms, and to guarantee security, flexibility, and scalability. The architecture incorporates two main components. The first one manages the patient' therapeutic programs taking into account two principles: 1) maintain loose coupling between the layers of the framework and 2) Don't Repeat Yourself (DRY). The second one evaluates the performed exercises in real time considering an independent acquisition mechanism for the patient movements and two artificial algorithms. The first algorithm allows evaluating the quality of the movements, while the second one allows assessing the levels of pain intensity by recognizing the patient' emotions when performing the movements. Details of the components and the meta-model of the architecture are presented and discussed considering their advantages and disadvantages.",health,262,included
10.1109/aqtr49680.2020.9130011,to_check,"2020 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",IEEE,2020-05-23 00:00:00,ieeexplore,computer aided differential diagnosis system for lung tumor based on transthoracic ultrasound image processing and neural networks,https://ieeexplore.ieee.org/document/9130011/,"Lung cancer (LC) was not common before the 1930s, but increased dramatically over the following decades as tobacco smoking increased, becoming one of the main causes of mortality due to neoplasia, both for men and women. The estimations made by recently published data found that lung cancer deaths will increase from 1.6 million in 2012 to 3 million in 2035. This is mainly because most cases are still diagnosed in advanced stages, leading to considerably low survival rates. Therefore, urgent strategies are necessary to facilitate early diagnosis or improve current diagnosis techniques. Due to late detection, only approximately 15% of the patients diagnosed with pulmonary cancer are able to survive for a longer period of time, however, the survival rate increases considerably if the cancer is detected in its early stages. In this paper we aim to develop a software application able to accurately and non-invasively aid physicians for lung tumors differential diagnosis and biopsy guidance based on ultrasound image processing and neural networks. This software application could be of real use for physicians to identify patients with LC and also could improve the percentages of early detection of lung cancer and the life expectancy of patients through lung cancer screening programs.",health,263,not included
10.1109/ivs.2014.6856583,to_check,2014 IEEE Intelligent Vehicles Symposium Proceedings,IEEE,2014-06-11 00:00:00,ieeexplore,intelligent driving diagnosis based on a fuzzy logic approach in a real environment implementation,https://ieeexplore.ieee.org/document/6856583/,"This paper considers the problem of diagnosing people's driving skills under real driving conditions using GPS data and video records. For this real environment implementation, a brand new intelligent driving diagnosis system based on fuzzy logic was developed. This system seeks to propose an abstraction of expert driving criteria for driving assessment. The analysis takes into account GPS signals such as: position, velocity, accelerations and vehicle yaw angle; because of its relation with drivers' maneuvers. In that sense, this work presents in the first place, the proposed scheme for the intelligent driving diagnosis agent in terms of its own characteristics properties, which explain important considerations about how an intelligent agent must be conceived. Secondly, it attempts to explain the scheme for the implementation of the intelligent driving diagnosis agent based on its fuzzy logic algorithm, which takes into account the analysis of real-time telemetry signals and proposed set of driving diagnosis rules for the intelligent driving diagnosis, based on a quantitative abstraction of some traffic laws and some secure driving techniques. Experimental testing has been performed in driving conditions. All tested drivers performed the driving task on real streets. The testing results show that our intelligent driving diagnosis system allows quantitative qualifications of driving performance with a high degree of reliability.",health,264,not included
10.1109/access.2019.2901408,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,automatic verification and diagnosis of security risk assessments in business process models,https://ieeexplore.ieee.org/document/8651587/,"Organizations execute daily activities to meet their objectives. The performance of these activities can be fundamental for achieving a business objective, but they also imply the assumption of certain security risks that might go against a company's security policies. A risk may be defined as the effects of uncertainty on the achievement of the goals of a company, some of which can be associated with security aspects (e.g., data corruption or data leakage). The execution of the activities can be choreographed using business processes models, in which the risk of the entire business process model derives from a combination of the single activity risks (executed in an isolated manner). In this paper, a risk assessment method is proposed to enable the analysis and evaluation of a set of activities combined in a business process model to ascertain whether the model conforms to the security-risk objectives. To achieve this objective, we use a business process extension with security-risk information to: 1) define an algorithm to verify the level of risk of process models; 2) design an algorithm to diagnose the risk of the activities that fail to conform to the level of risk established in security-risk objectives; and 3) the implementation of a tool that supports the described proposal. In addition, a real case study is presented, and a set of scalability benchmarks of performance analysis is carried out in order to check the usefulness and suitability of automation of the algorithms.",health,265,not included
10.1109/jiot.2020.2994200,to_check,IEEE Internet of Things Journal,IEEE,2020-10-01 00:00:00,ieeexplore,optimization of edge-plc-based fault diagnosis with random forest in industrial internet of things,https://ieeexplore.ieee.org/document/9091605/,"Facing globalized competition, there have been increasing requirements for safety and efficiency in smart factories, where the industrial Internet of Things can enable the monitoring of equipment's status and the detecting of faults before they go critical. Regarding cloud computing, data-driven methods running at clouds are adopted to train the model with a large amount of raw data at the beginning, then end machines upload their real-time readings to the cloud center for processing. However, this incurs considerable computational costs and may sometimes bear a severe delay. In this article, we consider a hierarchical structure where edge-PLCs are employed to gather sensed data locally and reduce communication costs. Since a single fault may be related to multiple influencing features, we want to first minimize the number of features that need to determine a fault, then try to find out the minimal set of edge-PLCs which can cover all key features so as to save the deployment cost. We propose a random-forest-based method to handle the features selection problem, and then the selection of edge-PLCs by solving the set coverage problem. Through the simulation on real data trace, we compare our method with other artificial-intelligence-based methods, such as the logistics regression model and its extensions. The results prove the efficiency and performance of the proposed method, which reaches or even exceeds the accuracy of methods using the full set of data.",health,266,not included
10.1109/tim.2020.3043098,to_check,IEEE Transactions on Instrumentation and Measurement,IEEE,2021-01-01 00:00:00,ieeexplore,sasln: signals augmented self-taught learning networks for mechanical fault diagnosis under small sample condition,https://ieeexplore.ieee.org/document/9285302/,"The implementation of condition monitoring and fault diagnosis is of special importance for ensuring wind turbine (WT) operation safely and stably. In practice, however, the fault data of WT are limited, which makes it hard to identify faults of WT accurately using the existing intelligent diagnosis methods. To address this, signals augmented self-taught learning network (SASLN) is proposed for the fault diagnosis of the generator, which is one of the most important parts in WT. In SASLN, fault signal samples are generated by the Wasserstein distance guided generative adversarial networks to expand the limited training data set. The sufficient generated signal samples are used to pretrain the self-taught learning network (SLN) to enhance the generalization ability of SLN. Then, the weights of SLN are fine-tuned using a small number of real signal samples for accurate fault classification. The effectiveness of SASLN is verified by two bearing vibration data sets. The results show that SASLN can achieve fairly high fault classification accuracy using small training samples. Besides, SASLN has good robustness in noisy working environment and can also identify faults even in variable loads and variable rotating speeds cases, which makes it meaningful for decreasing the running costs and improving the maintenance management of WT.",health,267,not included
10.1109/icac.2017.21,to_check,2017 IEEE International Conference on Autonomic Computing (ICAC),IEEE,2017-07-21 00:00:00,ieeexplore,ananke: a q-learning-based portfolio scheduler for complex industrial workflows,https://ieeexplore.ieee.org/document/8005354/,"Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduced operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for data centers with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learning-based portfolio scheduler can perform better and consume fewer resources than state-of-the-art alternatives, in particular for workloads with uniform arrival patterns.",health,268,included
10.1109/icarcv.2014.7064599,to_check,2014 13th International Conference on Control Automation Robotics & Vision (ICARCV),IEEE,2014-12-12 00:00:00,ieeexplore,distributed signature analysis of induction motors using artificial neural networks,https://ieeexplore.ieee.org/document/7064599/,Motor current signature analysis is a modern approach to fault diagnose and classification for induction motors. Many studies reported successful implementation of MCSA in laboratory situations whereas the method was not so successful in real industrial situation due to propagation of neighbor faults and unwanted noise signals. This paper investigate the correlation between different observations of events in order to provide a more accurate estimation of behavior of electrical motors at a given site. An analytical framework has been implemented to correlate and classify independent fault observations and diagnose the type and identify the origin of fault symptoms. The fault diagnosis algorithm has two layers. Initially outputs of all sensors are processed to generate fault indicators. These fault indicators then are to be classified using an Artificial Neural Network. A typical industrial site is taken as a case study and simulated to evaluate the concept of distributed fault analysis.,health,269,not included
10.1109/gcis.2012.41,to_check,2012 Third Global Congress on Intelligent Systems,IEEE,2012-11-08 00:00:00,ieeexplore,software fault diagnosing system based on multi-agent,https://ieeexplore.ieee.org/document/6449546/,"Software faults are the underlying and important roots which result in the mistake, failure and even breakdown of system. So software Fault Diagnosis and the Spectrum-based Fault Localization (SFL) is very significant to software quality assurance. The current fault diagnosis technique based on the artificial intelligence and Multi-Agent attracts more and more attention. Multi-Agent System (MAS) have autonomy, intelligence and social ability, which is well-suited for Fault Diagnosis, and which can easily be used in software testing schemes of software intensive equipment. This paper gives a Fault Diagnosis system based on MAS and the proposed system is applied to real software Fault Diagnosis to validate its effectiveness.",health,270,not included
10.1109/jiot.2019.2900093,to_check,IEEE Internet of Things Journal,IEEE,2019-06-01 00:00:00,ieeexplore,a rem-enabled diagnostic framework in cellular-based iot networks,https://ieeexplore.ieee.org/document/8643780/,"With the flourishing growth of Internet-of-Things (IoT) applications, cellular-assisted IoT networks are promising to support the ever-increasing wireless traffic demands generated by various kinds of IoT devices. As one of the emerging technologies in the next-generation cellular systems, small cells, or so-called “femtocells” in indoor environments, are expected to be densely deployed for the ubiquitous coverage and capacity enhancement in a cost-effective and energy-efficient way. However, unplanned femtocell deployment and complex interior layouts of buildings may lead to severe coverage and capacity problems and even possible negative impacts on the dense deployment. In this paper, we aim at developing a data-driven indoor diagnostic framework for fault detection in cellular-assisted IoT networks. The framework utilizes machine learning techniques to analyze crowd-sourced measurements uploaded from diverse end devices. Then, some well-trained prediction models are used to construct a global view of a radio environment, namely, radio environment map (REM), for diagnosis and management purposes. Moreover, REMs enable operators to detect existing coverage and capacity problems at any interested location. To verify the feasibility of indoor diagnoses, we collect a real trace data from an indoor testbed and then conduct a series of experiments to evaluate the performance of the machine-learning algorithms in terms of the accuracy, the time complexity, and the sensitivity to data volumes. The experimental results provide an insightful guideline to the indoor deployment of small cells in cellular-based IoT networks.",health,271,not included
10.1007/s44196-021-00040-x,to_check,International Journal of Computational Intelligence Systems,Springer,2021-11-20 00:00:00,springer,edge computing using embedded webserver with mobile device for diagnosis and prediction of metastasis in histopathological images,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s44196-021-00040-x,"Diagnosis of different breast cancer stages using histopathology whole slide images is the gold standard in grading the tissue metastasis. Traditional diagnosis involves labor intensive procedures and is prone to human errors. Computer aided diagnosis assists medical experts as a second opinion tool in early detection which prevents further proliferation. Computing facilities have emerged to an extent where algorithms can attain near human accuracy in prediction of diseases, offering better treatment to curb further proliferation. The work introduced in the paper provides an interface in mobile platform, which enables the user to input histopathology image and obtain the prediction results with its class probability through embedded web-server. The trained deep convolutional neural networks model is deployed into a microcomputer-based embedded system after hyper-parameter tuning, offering congruent performance. The implementation results show that the embedded platform with custom-trained CNN model is suitable for medical image classification, as it takes less execution time and mean prediction time. It is also noticed that customized CNN classifier model outperforms pre-trained models when used in embedded platforms for prediction and classification of histopathology images. This work also emphasizes the relevance of portable and flexible embedded device in real time clinical applications.",health,272,included
10.1007/s42979-021-00726-1,to_check,SN Computer Science,Nature,2021-06-19 00:00:00,springer,towards regulatory-compliant mlops: oravizio’s journey from a machine learning experiment to a deployed certified medical product,https://www.nature.com/articles/s42979-021-00726-1,"Agile software development embraces change and manifests working software over comprehensive documentation and responding to change over following a plan. The ability to continuously release software has enabled a development approach where experimental features are put to use, and, if they stand the test of real use, they remain in production. Examples of such features include machine learning (ML) models, which are usually pre-trained, but can still evolve in production. However, many domains require more plan-driven approach to avoid hazard to environment and humans, and to mitigate risks in the process. In this paper, we start by presenting continuous software engineering practices in a regulated context, and then apply the results to the emerging practice of MLOps, or continuous delivery of ML features. Furthermore, as a practical contribution, we present a case study regarding Oravizio, first CE-certified medical software for assessing the risks of joint replacement surgeries. Towards the end of the paper, we also reflect the Oravizio experiences to MLOps in regulatory context.",health,273,included
10.1186/s13000-021-01081-8,to_check,Diagnostic Pathology,BioMed Central,2021-03-11 00:00:00,springer,"diagnosis prediction of tumours of unknown origin using immunogenius, a machine learning-based expert system for immunohistochemistry profile interpretation",https://www.biomedcentral.com/openurl?doi=10.1186/s13000-021-01081-8,"Background Immunohistochemistry (IHC) remains the gold standard for the diagnosis of pathological diseases. This technique has been supporting pathologists in making precise decisions regarding differential diagnosis and subtyping, and in creating personalized treatment plans. However, the interpretation of IHC results presents challenges in complicated cases. Furthermore, rapidly increasing amounts of IHC data are making it even harder for pathologists to reach to definitive conclusions. Methods We developed ImmunoGenius, a machine-learning-based expert system for the pathologist, to support the diagnosis of tumors of unknown origin. Based on Bayesian theorem, the most probable diagnoses can be drawn by calculating the probabilities of the IHC results in each disease. We prepared IHC profile data of 584 antibodies in 2009 neoplasms based on the relevant textbooks. We developed the reactive native mobile application for iOS and Android platform that can provide 10 most possible differential diagnoses based on the IHC input. Results We trained the software using 562 real case data, validated it with 382 case data, tested it with 164 case data and compared the precision hit rate. Precision hit rate was 78.5, 78.0 and 89.0% in training, validation and test dataset respectively. Which showed no significant difference. The main reason for discordant precision was lack of disease-specific IHC markers and overlapping IHC profiles observed in similar diseases. Conclusion The results of this study showed a potential that the machine-learning algorithm based expert system can support the pathologic diagnosis by providing second opinion on IHC interpretation based on IHC database. Incorporation with contextual data including the clinical and histological findings might be required to elaborate the system in the future.",health,274,not included
10.1007/978-981-15-6202-0_37,to_check,Intelligent and Cloud Computing,Springer,2021-01-01 00:00:00,springer,artificial intelligence for smart healthcare management: brief study,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-6202-0_37,"In recent time, entrepreneurs have started offering smart solutions to monitor healthcare system by using artificial intelligence (AI). It provides a smart alternative to manage the whole healthcare system and helps in accurate prediction and diagnosis of various critical health conditions. Therefore, the advancement and application of AI will change the future healthcare scenario. Even though the AI-based technologies in medical field are advancing rapidly, but in real time, its implementation is yet to be achieved. AI is being used not only in detection of disease but also in allocating professionals, giving home care advises and prescribing medicines. In this article, the importance of AI in the field of healthcare management is shown. Moreover, discussion of several works published in different areas of medicine based on AI during the past few years is carried out briefly. Extending the article, an analysis of the use of AI in health system along with various challenges toward it is done.",health,275,not included
10.1038/s41746-020-00318-y,to_check,npj Digital Medicine,Nature,2020-08-21 00:00:00,springer,developing a delivery science for artificial intelligence in healthcare,https://www.nature.com/articles/s41746-020-00318-y,"Artificial Intelligence (AI) has generated a large amount of excitement in healthcare, mostly driven by the emergence of increasingly accurate machine learning models. However, the promise of AI delivering scalable and sustained value for patient care in the real world setting has yet to be realized. In order to safely and effectively bring AI into use in healthcare, there needs to be a concerted effort around not just the creation, but also the delivery of AI. This AI “delivery science” will require a broader set of tools, such as design thinking, process improvement, and implementation science, as well as a broader definition of what AI will look like in practice, which includes not just machine learning models and their predictions, but also the new systems for care delivery that they enable. The careful design, implementation, and evaluation of these AI enabled systems will be important in the effort to understand how AI can improve healthcare.",health,276,not included
10.1186/s12916-019-1382-x,to_check,BMC Medicine,BioMed Central,2019-07-17 00:00:00,springer,beyond the hype of big data and artificial intelligence: building foundations for knowledge and wisdom,https://www.biomedcentral.com/openurl?doi=10.1186/s12916-019-1382-x,"Big data, coupled with the use of advanced analytical approaches, such as artificial intelligence (AI), have the potential to improve medical outcomes and population health. Data that are routinely generated from, for example, electronic medical records and smart devices have become progressively easier and cheaper to collect, process, and analyze. In recent decades, this has prompted a substantial increase in biomedical research efforts outside traditional clinical trial settings. Despite the apparent enthusiasm of researchers, funders, and the media, evidence is scarce for successful implementation of products, algorithms, and services arising that make a real difference to clinical care. This article collection provides concrete examples of how “big data” can be used to advance healthcare and discusses some of the limitations and challenges encountered with this type of research. It primarily focuses on real-world data, such as electronic medical records and genomic medicine, considers new developments in AI and digital health, and discusses ethical considerations and issues related to data sharing. Overall, we remain positive that big data studies and associated new technologies will continue to guide novel, exciting research that will ultimately improve healthcare and medicine—but we are also realistic that concerns remain about privacy, equity, security, and benefit to all.",health,277,not included
http://arxiv.org/abs/2102.01998v1,to_check,arxiv,arxiv,2021-02-03 00:00:00,arxiv,"unbox the black-box for the medical explainable ai via multi-modal and
  multi-centre data fusion: a mini-review, two showcases and beyond",http://arxiv.org/abs/2102.01998v1,"Explainable Artificial Intelligence (XAI) is an emerging research topic of
machine learning aimed at unboxing how AI systems' black-box choices are made.
This research field inspects the measures and models involved in
decision-making and seeks solutions to explain them explicitly. Many of the
machine learning algorithms can not manifest how and why a decision has been
cast. This is particularly true of the most popular deep neural network
approaches currently in use. Consequently, our confidence in AI systems can be
hindered by the lack of explainability in these black-box models. The XAI
becomes more and more crucial for deep learning powered applications,
especially for medical and healthcare studies, although in general these deep
neural networks can return an arresting dividend in performance. The
insufficient explainability and transparency in most existing AI systems can be
one of the major reasons that successful implementation and integration of AI
tools into routine clinical practice are uncommon. In this study, we first
surveyed the current progress of XAI and in particular its advances in
healthcare applications. We then introduced our solutions for XAI leveraging
multi-modal and multi-centre data fusion, and subsequently validated in two
showcases following real clinical scenarios. Comprehensive quantitative and
qualitative analyses can prove the efficacy of our proposed XAI solutions, from
which we can envisage successful applications in a broader range of clinical
questions.",health,278,not included
http://arxiv.org/abs/2001.09346v2,to_check,arxiv,arxiv,2020-01-25 00:00:00,arxiv,"corgan: correlation-capturing convolutional generative adversarial
  networks for generating synthetic healthcare records",http://arxiv.org/abs/2001.09346v2,"Deep learning models have demonstrated high-quality performance in areas such
as image classification and speech processing. However, creating a deep
learning model using electronic health record (EHR) data, requires addressing
particular privacy challenges that are unique to researchers in this domain.
This matter focuses attention on generating realistic synthetic data while
ensuring privacy. In this paper, we propose a novel framework called
correlation-capturing Generative Adversarial Network (CorGAN), to generate
synthetic healthcare records. In CorGAN we utilize Convolutional Neural
Networks to capture the correlations between adjacent medical features in the
data representation space by combining Convolutional Generative Adversarial
Networks and Convolutional Autoencoders. To demonstrate the model fidelity, we
show that CorGAN generates synthetic data with performance similar to that of
real data in various Machine Learning settings such as classification and
prediction. We also give a privacy assessment and report on statistical
analysis regarding realistic characteristics of the synthetic data. The
software of this work is open-source and is available at:
https://github.com/astorfi/cor-gan.",health,279,not included
http://arxiv.org/abs/2110.12900v1,to_check,arxiv,arxiv,2021-10-21 00:00:00,arxiv,"automated scoring system of her2 in pathological images under the
  microscope",http://arxiv.org/abs/2110.12900v1,"Breast cancer is the most common cancer among women worldwide. The human
epidermal growth factor receptor 2(HER2) with immunohistochemical(IHC) is
widely used for pathological evaluation to provide the appropriate therapy for
patients with breast cancer. However, the deficiency of pathologists is
extremely significant in the current society, and visual diagnosis of the HER2
overexpression is subjective and susceptible to inter-observer variation.
Recently, with the rapid development of artificial intelligence(AI) in disease
diagnosis, several automated HER2 scoring methods using traditional computer
vision or machine learning methods indicate the improvement of the HER2
diagnostic accuracy, but the unreasonable interpretation in pathology, as well
as the expensive and ethical issues for annotation, make these methods still
have a long way to deploy in hospitals to ease pathologists' burden in real. In
this paper, we propose a HER2 automated scoring system that strictly follows
the HER2 scoring guidelines simulating the real workflow of HER2 scores
diagnosis by pathologists. Unlike the previous work, our method takes the
positive control of HER2 into account to make sure the assay performance for
each slide, eliminating work for repeated comparison and checking for the
current field of view(FOV) and positive control FOV, especially for the
borderline cases. Besides, for each selected FOV under the microscope, our
system provides real-time HER2 scores analysis and visualizations of the
membrane staining intensity and completeness corresponding with the cell
classification. Our rigorous workflow along with the flexible interactive
adjustion in demand substantially assists pathologists to finish the HER2
diagnosis faster and improves the robustness and accuracy. The proposed system
will be embedded in our Thorough Eye platform for deployment in hospitals.",health,280,not included
http://arxiv.org/abs/2011.07555v1,to_check,arxiv,arxiv,2020-11-15 00:00:00,arxiv,towards compliant data management systems for healthcare ml,http://arxiv.org/abs/2011.07555v1,"The increasing popularity of machine learning approaches and the rising
awareness of data protection and data privacy presents an opportunity to build
truly secure and trustworthy healthcare systems. Regulations such as GDPR and
HIPAA present broad guidelines and frameworks, but the implementation can
present technical challenges. Compliant data management systems require
enforcement of a number of technical and administrative safeguards. While
policies can be set for both safeguards there is limited availability to
understand compliance in real time. Increasingly, machine learning
practitioners are becoming aware of the importance of keeping track of
sensitive data. With sensitivity over personally identifiable, health or
commercially sensitive information there would be value in understanding
assessment of the flow of data in a more dynamic fashion. We review how data
flows within machine learning projects in healthcare from source to storage to
use in training algorithms and beyond. Based on this, we design engineering
specifications and solutions for versioning of data. Our objective is to design
tools to detect and track sensitive data across machines and users across the
life cycle of a project, prioritizing efficiency, consistency and ease of use.
We build a prototype of the solution that demonstrates the difficulties in this
domain. Together, these represent first efforts towards building a compliant
data management system for healthcare machine learning projects.",health,281,not included
http://arxiv.org/abs/2102.10435v1,to_check,arxiv,arxiv,2021-02-20 00:00:00,arxiv,"mhdeep: mental health disorder detection system based on body-area and
  deep neural networks",http://arxiv.org/abs/2102.10435v1,"Mental health problems impact quality of life of millions of people around
the world. However, diagnosis of mental health disorders is a challenging
problem that often relies on self-reporting by patients about their behavioral
patterns. Therefore, there is a need for new strategies for diagnosis of mental
health problems. The recent introduction of body-area networks consisting of a
plethora of accurate sensors embedded in smartwatches and smartphones and deep
neural networks (DNNs) points towards a possible solution. However, disease
diagnosis based on WMSs and DNNs, and their deployment on edge devices, remains
a challenging problem. To this end, we propose a framework called MHDeep that
utilizes commercially available WMSs and efficient DNN models to diagnose three
important mental health disorders: schizoaffective, major depressive, and
bipolar. MHDeep uses eight different categories of data obtained from sensors
integrated in a smartwatch and smartphone. Due to limited available data,
MHDeep uses a synthetic data generation module to augment real data with
synthetic data drawn from the same probability distribution. We use the
synthetic dataset to pre-train the DNN models, thus imposing a prior on the
weights. We use a grow-and-prune DNN synthesis approach to learn both the
architecture and weights during the training process. We use three different
data partitions to evaluate the MHDeep models trained with data collected from
74 individuals. We conduct data instance level and patient level evaluations.
MHDeep achieves an average test accuracy of 90.4%, 87.3%, and 82.4%,
respectively, for classifications between healthy instances and schizoaffective
disorder instances, major depressive disorder instances, and bipolar disorder
instances. At the patient level, MHDeep DNNs achieve an accuracy of 100%, 100%,
and 90.0% for the three mental health disorders, respectively.",health,282,not included
http://arxiv.org/abs/2008.05381v1,to_check,arxiv,arxiv,2020-08-12 00:00:00,arxiv,"improving the performance of fine-grain image classifiers via generative
  data augmentation",http://arxiv.org/abs/2008.05381v1,"Recent advances in machine learning (ML) and computer vision tools have
enabled applications in a wide variety of arenas such as financial analytics,
medical diagnostics, and even within the Department of Defense. However, their
widespread implementation in real-world use cases poses several challenges: (1)
many applications are highly specialized, and hence operate in a \emph{sparse
data} domain; (2) ML tools are sensitive to their training sets and typically
require cumbersome, labor-intensive data collection and data labelling
processes; and (3) ML tools can be extremely ""black box,"" offering users little
to no insight into the decision-making process or how new data might affect
prediction performance. To address these challenges, we have designed and
developed Data Augmentation from Proficient Pre-Training of Robust Generative
Adversarial Networks (DAPPER GAN), an ML analytics support tool that
automatically generates novel views of training images in order to improve
downstream classifier performance. DAPPER GAN leverages high-fidelity
embeddings generated by a StyleGAN2 model (trained on the LSUN cars dataset) to
create novel imagery for previously unseen classes. We experimentally evaluate
this technique on the Stanford Cars dataset, demonstrating improved vehicle
make and model classification accuracy and reduced requirements for real data
using our GAN based data augmentation framework. The method's validity was
supported through an analysis of classifier performance on both augmented and
non-augmented datasets, achieving comparable or better accuracy with up to 30\%
less real data across visually similar classes. To support this method, we
developed a novel augmentation method that can manipulate semantically
meaningful dimensions (e.g., orientation) of the target object in the embedding
space.",health,283,included
http://arxiv.org/abs/2008.12949v2,to_check,arxiv,arxiv,2020-08-29 00:00:00,arxiv,vr-caps: a virtual environment for capsule endoscopy,http://arxiv.org/abs/2008.12949v2,"Current capsule endoscopes and next-generation robotic capsules for diagnosis
and treatment of gastrointestinal diseases are complex cyber-physical platforms
that must orchestrate complex software and hardware functions. The desired
tasks for these systems include visual localization, depth estimation, 3D
mapping, disease detection and segmentation, automated navigation, active
control, path realization and optional therapeutic modules such as targeted
drug delivery and biopsy sampling. Data-driven algorithms promise to enable
many advanced functionalities for capsule endoscopes, but real-world data is
challenging to obtain. Physically-realistic simulations providing synthetic
data have emerged as a solution to the development of data-driven algorithms.
In this work, we present a comprehensive simulation platform for capsule
endoscopy operations and introduce VR-Caps, a virtual active capsule
environment that simulates a range of normal and abnormal tissue conditions
(e.g., inflated, dry, wet etc.) and varied organ types, capsule endoscope
designs (e.g., mono, stereo, dual and 360{\deg}camera), and the type, number,
strength, and placement of internal and external magnetic sources that enable
active locomotion. VR-Caps makes it possible to both independently or jointly
develop, optimize, and test medical imaging and analysis software for the
current and next-generation endoscopic capsule systems. To validate this
approach, we train state-of-the-art deep neural networks to accomplish various
medical image analysis tasks using simulated data from VR-Caps and evaluate the
performance of these models on real medical data. Results demonstrate the
usefulness and effectiveness of the proposed virtual platform in developing
algorithms that quantify fractional coverage, camera trajectory, 3D map
reconstruction, and disease classification.",health,284,not included
http://arxiv.org/abs/2110.06196v1,to_check,arxiv,arxiv,2021-10-12 00:00:00,arxiv,grape: fast and scalable graph processing and embedding,http://arxiv.org/abs/2110.06196v1,"Graph Representation Learning methods have enabled a wide range of learning
problems to be addressed for data that can be represented in graph form.
Nevertheless, several real world problems in economy, biology, medicine and
other fields raised relevant scaling problems with existing methods and their
software implementation, due to the size of real world graphs characterized by
millions of nodes and billions of edges. We present GraPE, a software resource
for graph processing and random walk based embedding, that can scale with large
and high-degree graphs and significantly speed up-computation. GraPE comprises
specialized data structures, algorithms, and a fast parallel implementation
that displays everal orders of magnitude improvement in empirical space and
time complexity compared to state of the art software resources, with a
corresponding boost in the performance of machine learning methods for edge and
node label prediction and for the unsupervised analysis of graphs.GraPE is
designed to run on laptop and desktop computers, as well as on high performance
computing clusters",health,285,not included
10.1016/j.compmedimag.2021.101956,to_check,Computerized Medical Imaging and Graphics,scopus,2021-09-01,sciencedirect,automated three-dimensional vessel reconstruction based on deep segmentation and bi-plane angiographic projections,https://api.elsevier.com/content/abstract/scopus_id/85111016853,"Automated three-dimensional (3D) blood vessel reconstruction to improve vascular diagnosis and therapeutics is a challenging task in which the real-time implementation of automatic segmentation and specific vessel tracking for matching artery sequences is essential. Recently, a deep learning-based segmentation technique has been proposed; however, existing state-of-the-art deep architectures exhibit reduced performance when they are employed using real in-vivo imaging because of serious issues such as low contrast and noise contamination of the X-ray images. To overcome these limitations, we propose a novel methodology composed of the de-haze image enhancement technique as pre-processing and multi-level thresholding as post-processing to be applied to the lightweight multi-resolution U-shaped architecture. Specifically, (1) bi-plane two-dimensional (2D) vessel images were extracted simultaneously using the deep architecture, (2) skeletons of the vessels were computed via a morphology operation, (3) the corresponding skeleton structure between image sequences was matched using the shape-context technique, and (4) the 3D centerline was reconstructed using stereo geometry. The method was validated using both in-vivo and in-vitro models. The results show that the proposed technique could improve the segmentation quality, reduce computation time, and reconstruct the 3D skeleton automatically. The algorithm accurately reconstructed the phantom model and the real mouse vessel in 3D in 2 s. Our proposed technique has the potential to allow therapeutic micro-agent navigation in clinical practice, thereby providing the 3D position and orientation of the vessel.",health,286,not included
10.1016/j.renene.2021.04.040,to_check,Renewable Energy,scopus,2021-08-01,sciencedirect,damage identification of wind turbine blades with deep convolutional neural networks,https://api.elsevier.com/content/abstract/scopus_id/85105896900,"Online early detection of surface damages on blades is critical for the safety of wind turbines, which could avoid catastrophic failures, minimize downtime, and enhance the reliability of the system. Monitoring the health status of blades is attracting more and more attention including on-site cameras and mobile cameras by drones and crawling robots. To deploy fast and efficient damage detection methods from image data, this work presents a hierarchical identification framework for wind turbine blades, which consists of a Haar-AdaBoost step for region proposal and a convolutional neural network (CNN) classifier for damage detection and fault diagnosis. Case studies are carried out on real data set collected from an eastern China wind farm. Results show that (i) the proposed framework can detect and identify the blade damages and outperforms other schemes include SVM and VGG16 models, (ii) sensitive analysis is conducted to validate the robustness of proposed method under limited data conditions, (iii) the proposed scheme is faster than one-step CNN method that directly classifying raw data.",health,287,not included
10.1016/j.ijpe.2021.108114,to_check,International Journal of Production Economics,scopus,2021-06-01,sciencedirect,machine learning-based predictive maintenance: a cost-oriented model for implementation,https://api.elsevier.com/content/abstract/scopus_id/85104317955,"Predictive Maintenance (PdM) is a condition-based maintenance strategy (CBM) that carries out maintenance action when needed, avoiding unnecessary preventive actions or failures. Machine learning (ML), in the form of advanced monitoring and diagnosis technologies, has become increasingly attractive. Implementing ML-based PdM is a difficult and expensive process, especially for those companies which often lack the necessary skills and financial and labour resources. Thus, a cost-oriented analysis is required to define when ML-based PdM is the most suitable maintenance strategy. The implementation of this strategy involves investment costs in IT technologies, in addition to costs incurred from traditional maintenance activities depending of the performance of the ML model classifier; however, no previous research consider both costs in the economic evaluation of PdM.This paper aims to provide a mathematical model where investment costs are included and the ML performance is evaluated in terms of the probability to correctly intercept faults. A error matrix is used to quantify costs due to maintenance actions. Moreover, the mathematical model provides a cost-based quantitative method, based on the Receiver Operating Characteristics (ROC) curve. This optimizes the decision threshold of the ML model classifier, which allows the maintenance costs to be minimized in comparison to traditional decision threshold optimisation methods. Based on the mathematical model, a useful Decision Support System (DSS) that guides PdM implementation is introduced. Finally, the DSS is applied to a real case study to illustrate its applicability.",health,288,not included
10.1016/j.apenergy.2020.116049,to_check,Applied Energy,scopus,2021-02-01,sciencedirect,adaptive prognostics in a controlled energy conversion process based on long- and short-term predictors,https://api.elsevier.com/content/abstract/scopus_id/85097470918,"The pulp and paper industry is a fundamental sector of the economy of many countries. However, this sector requires real collaboration and initiatives from stakeholders to reduce its significant consumption of energy and emission of greenhouse gases. Heat exchangers are examples of equipment in pulp mills that are subjected to undesirable and complex phenomena such as evolution of fouling over time, which leads to inefficiency in terms of energy consumption and unplanned shutdowns, resulting in ineffective maintenance strategies and production costs. Therefore, there is a clear need to develop an accurate predictive maintenance tool that helps mill operators avoid such situations. It is necessary for that tool to effectively track the fouling evolution level and, based on it, deploy a reliable prognostics approach to estimate more accurately the time-to-clean of this equipment. This study presents a new hybrid prognostics approach for fouling prediction in heat exchangers. The proposed approach relies on the fusion of information of different prediction horizons to estimate the time-to-clean. Employing long short-term memory, it allows adaptation of long-term predictions by accurate short-term predictions using multiple non-linear auto-regressive exogenous models. This fusion not only captures the changes in degradation trend over time, but also ensures a good accuracy of prognostics results in both the short- and long-term horizons for planning maintenance actions. The effectiveness of the proposed approach was successfully proven on real industrial data collected from a pulp mill heat exchanger located in Canada.",health,289,not included
10.1016/j.ijmedinf.2020.104348,to_check,International Journal of Medical Informatics,scopus,2021-02-01,sciencedirect,towards effective machine learning in medical imaging analysis: a novel approach and expert evaluation of high-grade glioma ‘ground truth’ simulation on mri,https://api.elsevier.com/content/abstract/scopus_id/85097334964,"Purpose/objective(s)
                  Gliomas are uniformly fatal brain tumours with significant neurological and quality of life detriment to patients. Improvement in outcomes has remained largely unchanged in nearly 20 years. MRI (magnetic resonance imaging) is often used in diagnosis and management. Machine learning analyses of large-scale MRI data are pivotal in advancing the diagnosis, management and improve outcomes in neuro-oncology. A common challenge to robust machine learning approaches is the lack of large ‘ground truth’ datasets in supervised learning for building classification and prediction models. The creation of these datasets relies on human-expert input and is time-consuming and subjective error-prone, limiting effective machine learning applications. Simulation of mechanistic aspects such as geometry, location and physical properties of brain tumours can generate large-scale ground-truth datasets allowing for comparison of analysis techniques in clinical applications. We aimed to develop a transparent and convenient method for building ‘ground truth’ presentations of simulated glioma lesions on anatomical MRI.
               
                  Materials/methods
                  The simulation workflow was created using the Feature Manipulation Engine (FME®), a data integration platform specializing in the spatial data processing. By compiling and integrating FME’s functions to read, integrate, transform, validate, save, and display MRI data, and experimenting with ways to manipulate the parameters concerning location, size, shape, and signal intensity with the presentations of glioma, we were able to generate simulated appearances of high-grade gliomas on gadolinium-based high-resolution 3D T1-weighted MRI (1 mm3). Data of patients with canonical high-grade tumours were used as real-world tumours for validating the accuracy of the simulation. Twenty raters who are experienced with brain tumour interpretation on MRI independently completed a survey, designed to distinguish simulated and real-world brain tumours. Sensitivity and specificity were calculated for assessing the performance of the approach with the binary classification of simulated vs real-world tumours. Correlation and regression were used in run time analysis, assessing the software toolset’s efficiency in producing different numbers of simulated lesions. Differences in the group means were examined using the non-parametric Kruskal-Wallis test.
               
                  Results
                  The simulation method was developed as an interpretable and useful workflow for the easy creation of tumour simulations and incorporation into 3D MRI. A linear increase in the running time and memory usage was observed with an increasing number of generated lesions. The respondents' accuracy rate ranged between 33.3 and 83.3 %. The sensitivity and specificity were low for a human expert to differentiate simulated lesions from real gliomas (0.43 and 0.58) or vice versa (0.65 and 0.62). The mean scores ranking the real-world gliomas did not differ between the simulated and real tumours.
               
                  Conclusion
                  The reliable and user-friendly software method can allow for robust simulation of high-grade glioma on MRI. Ongoing research efforts include optimizing the workflow for generating glioma datasets as well as adapting it to simulating additional MRI brain changes.",health,290,not included
10.1016/j.ymssp.2020.107061,to_check,Mechanical Systems and Signal Processing,scopus,2021-01-01,sciencedirect,recovering compressed images for automatic crack segmentation using generative models,https://api.elsevier.com/content/abstract/scopus_id/85086994715,"In a structural health monitoring (SHM) system that uses digital cameras to monitor cracks of structural surfaces, techniques for reliable and effective data compression are essential to ensure a stable and energy-efficient crack images transmission in wireless devices, e.g., drones and robots with high definition cameras installed. Compressive sensing (CS) is a signal processing technique that allows accurate recovery of a signal from a sampling rate much smaller than the limitation of the Nyquist sampling theorem. Different from the popular approach of simultaneously training encoder and decoder using neural network models, the CS theory ensures a high probability of accurate signal reconstruction based on random measurements that is shorter than the length of the original signal under a sparsity constraint. Such method is particularly useful when measurements are expensive, such as wireless sensing of civil structures, because its hardware implementation allows down sampling of signals during the sensing process. Hence, CS methods can achieve significant energy saving for the sensing devices. However, the strong assumption of the signals being highly sparse in an invertible space is relatively hard to guarantee for many real images, such as image of cracks. In this paper, we present a new approach of CS that replaces the sparsity regularization with a generative model that is able to effectively capture a low dimension representation of targeted images. We develop a recovery framework for automatic crack segmentation of compressed crack images based on this new CS method. We demonstrate the remarkable performance of our method that takes advantage of the strong capability of generative models to capture the necessary features required in the crack segmentation task even the backgrounds of the generated images are not well reconstructed. The superior performance of our recovery framework is illustrated by comparisons to three existing CS algorithms. Furthermore, we show that our framework is potentially extensible to other common problems in automatic crack segmentation, such as defect recovery from motion blurring and occlusion.",health,291,not included
10.1016/j.revmed.2020.04.012,to_check,Revue de Medecine Interne,scopus,2020-10-01,sciencedirect,evaluation of the use of a simulation software in the learning of cardiopulmonary auscultation in undergraduate medical students,https://api.elsevier.com/content/abstract/scopus_id/85087753992,"Introduction
                  Medsounds™ software allows to create an auscultation learning platform, by providing real pre-recorded cardiopulmonary sounds on virtual chests. The study aimed at comparing the skills in cardiopulmonary auscultation between students who benefited from this platform and students who did not have access to it.
               
                  Methods
                  A controlled trial was conducted with 2nd year medical students randomised into three groups. Groups A, B and C received 10 h of cardiopulmonary clinical training. In addition, group B benefited from an online access to the educative platform, and group C had a demonstration of the platform during their clinical training, then an online access. The main outcome was a 3-point multiple-choice questionnaire based on 2 original case vignettes about the description of cardiopulmonary sounds. The secondary outcome was the faculty exam on high-fidelity cardiopulmonary simulator.
               
                  Results
                  Groups A and B included 127 students, and group C 117. Students in group C had a significantly higher score than those in group A (1.72/3 versus 1.48/3; p = 0.02), without difference between the groups B and C. Students who actually had a demonstration of the platform and used it at home had a higher score than those who did not use it (1.87 versus 1.51; p = 0.01). Students who had a demonstration of the platform before using it performed a better pulmonary examination on high-fidelity simulators.
               
                  Conclusion
                  The supervised use of an online auscultation simulation software in addition to the traditional clinical training seems to improve the auscultation performances of undergraduated medical students.",health,292,not included
10.1016/j.iot.2020.100185,to_check,Internet of Things (Netherlands),scopus,2020-09-01,sciencedirect,highly-efficient fog-based deep learning aal fall detection system,https://api.elsevier.com/content/abstract/scopus_id/85086362688,"Falls is one of most concerning accidents in aged population due to its high frequency and serious repercussion; thus, quick assistance is critical to avoid serious health consequences. There are several Ambient Assisted Living (AAL) solutions that rely on the technologies of the Internet of Things (IoT), Cloud Computing and Machine Learning (ML). Recently, Deep Learning (DL) have been included for its high potential to improve accuracy on fall detection. Also, the use of fog devices for the ML inference (detecting falls) spares cloud drawback of high network latency, non-appropriate for delay-sensitive applications such as fall detectors. Though, current fall detection systems lack DL inference on the fog, and there is no evidence of it in real environments, nor documentation regarding the complex challenge of the deployment. Since DL requires considerable resources and fog nodes are resource-limited, a very efficient deployment and resource usage is critical. We present an innovative highly-efficient intelligent system based on a fog-cloud computing architecture to timely detect falls using DL technics deployed on resource-constrained devices (fog nodes). We employ a wearable tri-axial accelerometer to collect patient monitoring data. In the fog, we propose a smart-IoT-Gateway architecture to support the remote deployment and management of DL models. We deploy two DL models (LSTM/GRU) employing virtualization to optimize resources and evaluate their performance and inference time. The results prove the effectiveness of our fall system, that provides a more timely and accurate response than traditional fall detector systems, higher efficiency, 98.75% accuracy, lower delay, and service improvement.",health,293,included
10.1016/j.microc.2020.105038,to_check,Microchemical Journal,scopus,2020-09-01,sciencedirect,a smartphone-based rapid quantitative detection platform for lateral flow strip of human chorionic gonadotropin with optimized image algorithm,https://api.elsevier.com/content/abstract/scopus_id/85085341749,"Colloidal gold immunochromatographic test strip has been widely used as a rapid, simple and low-cost correct detection technology. However, its detection is often qualitative or semi-quantitative, which limits its clinical application to some extent. Herein, a portable test strip quantitative detection device based on smartphone to detect human chorionic gonadotropin (HCG) is developed. In experiment, a colloidal gold HCG detection strip based on antigen antibody immune response is constructed, and the quantitative results of three different image processing methods on the same strip detection are compared, including the threshold processing algorithm based on location information, the RGB color component extraction algorithm and the grayscale projection value processing algorithm, the results show that the last algorithm can realize the best recognition of the region of interest of strip. The mobile phone application software (App) based on this design shows that the detection limit of constructed colloidal gold HCG strip is 3 ng/mL with a linear range of 6–300 ng/mL. The detection result of real urine sample is consistent with the spiked concentration (R2 = 0.988), indicating that the concentration of HCG can be accurately measured in urine with this method, presenting the potential for instant diagnosis.",health,294,included
10.1016/j.micpro.2019.102960,to_check,Microprocessors and Microsystems,scopus,2020-03-01,sciencedirect,a novel hybrid optimized and adaptive reconfigurable framework for the implementation of hybrid bio-inspired classifiers for diagnosis,https://api.elsevier.com/content/abstract/scopus_id/85077060597,"Due to recent advances in IoT (Internet of Things) technologies, availability of reliable data and emergence of machine learning, bio-inspired learning and artificial intelligence, has demonstrated its ability to solve the large complex problems which is not possible before. In particular, machine learning and bio-inspired learning algorithms provides the effective solutions in image processing techniques. However, the implementation of the above-mentioned algorithms in the general CPU requires the intensive usage of bandwidth, area and power which makes the CPU unhealthy of usage and implementation. To overcome this problem, ASIC (application specific integrated circuits), GPU (Graphics Processing Unit) &FPGA (Field Programmable gate arrays) have been employed to improve the performance of the hybrid machine learning (ML) classifiers and deep learning algorithms. FPGA has been recently employed for an effective implementation and to achieve the high performance of the learning algorithms. But integrating the complex learning algorithms in FPGA still remains to be real challenge among the researchers. The paper proposes new reconfigurable architectures for bio- inspired classifiers to diagnosis the medical casualties which can be suitable for the tele health care applications. This paper aim is as follows (i) Design and implementation of Parallel Fusion of FSM and Reconfigurable shared Distributed Arithmetic for Bio-Inspired Classifiers (ii) Development of Accelerator Environment to test the performance of proposed architecture (iii) Performance evaluation of proposed architecture in terms of accuracy of detection in compared with MATLAB simulation iv) Implementation of proposed architectures in different ARtix-7 architectures and determination of power, throughput and area . Moreover, the proposed architecture has been tested with the and compared with the other existing architectures.",health,295,included
10.1016/j.micpro.2019.102906,to_check,Microprocessors and Microsystems,scopus,2020-02-01,sciencedirect,area and power efficient pipelined hybrid merged adders for customized deep learning framework for fpga implementation,https://api.elsevier.com/content/abstract/scopus_id/85073599282,"With the rapid growth of deep learning and neural network algorithms, various fields such as communication, Industrial automation, computer vision system and medical applications have seen the drastic improvements in recent years. However, deep learning and neural network models are increasing day by day, while model parameters are used for representing the models. Although the existing models use efficient GPU for accommodating these models, their implementation in the dedicated embedded devices needs more optimization which remains a real challenge for researchers. Thus paper, carries an investigation of deep learning frameworks, more particularly as review of adders implemented in the deep learning framework. A new pipelined hybrid merged adders (PHMAC) optimized for FPGA architecture which has more efficient in terms of area and power is presented. The proposed adders represent the integration of the principle of carry select and carry look ahead principle of adders in which LUT is re-used for the different inputs which consume less power and provide effective area utilization. The proposed adders were investigated on different FPGA architectures in which the power and area were analyzed. Comparison of the proposed adders with the other adders such as carry select adders (CSA), carry look ahead adder (CLA), Carry skip adders and Koggle Stone adders has been made and results have proved to be highly vital into a 50% reduction in the area, power and 45% when compared with above mentioned traditional adders.",health,296,not included
10.1016/j.imu.2020.100335,to_check,Informatics in Medicine Unlocked,scopus,2020-01-01,sciencedirect,spark architecture for deep learning-based dose optimization in medical imaging,https://api.elsevier.com/content/abstract/scopus_id/85084287220,"Background and objectives
                  Deep Learning (DL) and Machine Learning (ML) have brought several breakthroughs to biomedical image analysis by making available more consistent and robust tools for the identification, classification, reconstruction, denoising, quantification, and segmentation of patterns in biomedical images. Recently, some applications of DL and ML in Computed Tomography (CT) scans for low dose optimization were developed. Nowadays, DL algorithms are used in CT to perform replacement of missing data (processing technique) such as low dose to high dose, sparse view to full view, low resolution to high resolution, and limited angle to full angle. Thus, DL comes with a new vision to process biomedical data imagery from CT scan. It becomes important to develop architectures and/or methods based on DL algorithms for minimizing radiation during a CT scan exam thanks to reconstruction and processing techniques.
               
                  Methods
                  This paper describes DL for CT scan low dose optimization, shows examples described in the literature, briefly discusses new methods used in CT scan image processing, and offers conclusions. We based our study on the literature and proposed a pipeline for low dose CT scan image reconstruction. Our proposed pipeline relies on DL and the Spark Framework using MapReduce programming. We discuss our proposed pipeline with those proposed in the literature to conclude the efficiency and importance.
               
                  Results
                  An architecture for low dose optimization using CT imagery is suggested. We used the Spark Framework to design the architecture. The proposed architecture relies on DL, and permits us to develop efficient and appropriate methods to process dose optimization with CT scan imagery. The real implementation of our pipeline for image denoising shows that we can reduce the radiation dose, and use our proposed pipeline to improve the quality of the captured image.
               
                  Conclusion
                  The proposed architecture based on DL is complete and enables faster processing of biomedical CT imagery as compared with prior methods described in the literature.",health,297,included
10.1016/j.crad.2019.02.006,to_check,Clinical Radiology,scopus,2019-05-01,sciencedirect,artificial intelligence in breast imaging,https://api.elsevier.com/content/abstract/scopus_id/85062980487,"This article reviews current limitations and future opportunities for the application of computer-aided detection (CAD) systems and artificial intelligence in breast imaging. Traditional CAD systems in mammography screening have followed a rules-based approach, incorporating domain knowledge into hand-crafted features before using classical machine learning techniques as a classifier. The first commercial CAD system, ImageChecker M1000, relies on computer vision techniques for pattern recognition. Unfortunately, CAD systems have been shown to adversely affect some radiologists' performance and increase recall rates. The Digital Mammography DREAM Challenge was a multidisciplinary collaboration that provided 640,000 mammography images for teams to help decrease false-positive rates in breast cancer screening. Winning solutions leveraged deep learning's (DL) automatic hierarchical feature learning capabilities and used convolutional neural networks. Start-ups Therapixel and Kheiron Medical Technologies are using DL for breast cancer screening. With increasing use of digital breast tomosynthesis, specific artificial intelligence (AI)-CAD systems are emerging to include iCAD's PowerLook Tomo Detection and ScreenPoint Medical's Transpara. Other AI-CAD systems are focusing on breast diagnostic techniques such as ultrasound and magnetic resonance imaging (MRI). There is a gap in the market for contrast-enhanced spectral mammography AI-CAD tools. Clinical implementation of AI-CAD tools requires testing in scenarios mimicking real life to prove its usefulness in the clinical environment. This requires a large and representative dataset for testing and assessment of the reader's interaction with the tools. A cost-effectiveness assessment should be undertaken, with a large feasibility study carried out to ensure there are no unintended consequences. AI-CAD systems should incorporate explainable AI in accordance with the European Union General Data Protection Regulation (GDPR).",health,298,not included
10.1016/j.jbi.2019.103138,to_check,Journal of Biomedical Informatics,scopus,2019-04-01,sciencedirect,distributed learning from multiple ehr databases: contextual embedding models for medical events,https://api.elsevier.com/content/abstract/scopus_id/85062392033,"Electronic health record (EHR) data provide promising opportunities to explore personalized treatment regimes and to make clinical predictions. Compared with regular clinical data, EHR data are known for their irregularity and complexity. In addition, analyzing EHR data involves privacy issues and sharing such data is often infeasible among multiple research sites due to regulatory and other hurdles. A recently published work uses contextual embedding models and successfully builds one predictive model for more than seventy common diagnoses. Despite of the high predictive power, the model cannot be generalized to other institutions without sharing data. In this work, a novel method is proposed to learn from multiple databases and build predictive models based on Distributed Noise Contrastive Estimation (Distributed NCE). We use differential privacy to safeguard the intermediary information sharing. The numerical study with a real dataset demonstrates that the proposed method not only can build predictive models in a distributed manner with privacy protection, but also preserve model structure well and achieve comparable prediction accuracy. The proposed methods have been implemented as a stand-alone Python library and the implementation is available on Github (https://github.com/ziyili20/DistributedLearningPredictor) with installation instructions and use-cases.",health,299,included
10.1016/j.therap.2018.12.002,to_check,Therapie,scopus,2019-02-01,sciencedirect,"early access to health products in france: major advances of the french “conseil stratégique des industries de santé” (csis) to be implemented (modalities, regulations, funding)",https://api.elsevier.com/content/abstract/scopus_id/85061149651,"In a context of perpetual evolution of treatments, access to therapeutic innovation is a major challenge for patients and the various players involved in the procedures of access to medicines. The revolutions in genomic and personalized medicine, artificial intelligence and biotechnology will transform the medicine of tomorrow and the organization of our health system. It is therefore fundamental that France prepares for these changes and supports the development of its companies in these new areas. The recent “Conseil stratégique des industries de santé” launched by Matignon makes it possible to propose a regulatory arsenal conducive to the implementation and diffusion of therapeutic innovations. In this workshop, we present a number of proposals, our approach having remained pragmatic with a permanent concern to be effective in the short term for the patients and to simplify the procedures as much as possible. This was achieved thanks to the participation in this workshop of most of the players involved (industrial companies, “Agence nationale de sécurité du médicament et des produits de santé”, “Haute Autorité de santé”, “Institut national du cancer”, “Les entreprises du médicament”, hospitals, “Observatoire du médicament, des dispositifs médicaux et de l’innovation thérapeutique”…). The main proposals tend to favor the implementation of clinical trials on our territory, especially the early phases, a wider access to innovations by favoring early access programs and setting up a process called “autorisation temporaire d’utilisation d’extension” (ATUext) that make it possible to prescribe a medicinal product even if the latter has a marketing authorisation in another indication. In addition, we propose a conditional reimbursement that will be available based on preliminary data but will require re-evaluation based on consolidated data from clinical trials and/or real-life data. Finally, in order to better carry out these assessments, with a view to access or care, we propose the establishment of partnership agreements with health agencies/hospitals in order to encourage the emergence of field experts, in order to prioritize an ascending expertise closer to patients’ needs and to real life.",health,300,not included
10.1016/j.artmed.2015.09.006,to_check,Artificial Intelligence in Medicine,scopus,2018-11-01,sciencedirect,pediatric decision support using adapted arden syntax,https://api.elsevier.com/content/abstract/scopus_id/84964998606,"Background
                  Pediatric guidelines based care is often overlooked because of the constraints of a typical office visit and the sheer number of guidelines that may exist for a patient's visit. In response to this problem, in 2004 we developed a pediatric computer based clinical decision support system using Arden Syntax medical logic modules (MLM).
               
                  Methods
                  The Child Health Improvement through Computer Automation system (CHICA) screens patient families in the waiting room and alerts the physician in the exam room. Here we describe adaptation of Arden Syntax to support production and consumption of patient specific tailored documents for every clinical encounter in CHICA and describe the experiments that demonstrate the effectiveness of this system.
               
                  Results
                  As of this writing CHICA has served over 44,000 patients at 7 pediatric clinics in our healthcare system in the last decade and its MLMs have been fired 6182,700 times in “produce” and 5334,021 times in “consume” mode. It has run continuously for over 10 years and has been used by 755 physicians, residents, fellows, nurse practitioners, nurses and clinical staff. There are 429 MLMs implemented in CHICA, using the Arden Syntax standard. Studies of CHICA's effectiveness include several published randomized controlled trials.
               
                  Conclusions
                  Our results show that the Arden Syntax standard provided us with an effective way to represent pediatric guidelines for use in routine care. We only required minor modifications to the standard to support our clinical workflow. Additionally, Arden Syntax implementation in CHICA facilitated the study of many pediatric guidelines in real clinical environments.",health,301,not included
10.1016/j.ergon.2018.06.005,to_check,International Journal of Industrial Ergonomics,scopus,2018-09-01,sciencedirect,artificial intelligence models for predicting the performance of hydro-pneumatic suspension struts in large capacity dump trucks,https://api.elsevier.com/content/abstract/scopus_id/85049336711,"Large dump trucks are being matched with large shovels to achieve bulk economic production in surface mining operations. This process results in high impact shovel loading operations (HISLO) and exposes operators to severe levels of whole-body vibrations (WBV). The performance of the hydro-pneumatic suspension struts, responsible for vibration attenuation in large dump trucks, decreases as a truck age. There is a need for a system for monitoring and predicting the performance of the suspension struts in real time. Artificial intelligence (AI) has been applied for modeling and predicting the suspension system performance for light/smaller vehicles. However, no work has been done to implement AI for modeling and predicting the performance of hydro-pneumatic struts in large dump trucks. This paper is a pioneering effort towards developing AI models for solving this problem. These AI models would incorporate the Artificial Neural Networks (ANN), Mamdani Fuzzy Logic (MFL) and a hybrid system, the Hybrid Neural Fuzzy Interference System (HyFIS), for achieving this goal. Experiments were conducted using a 3D virtual simulator for the CAT 793D in MSC.ADMAS. RMS accelerations in the vertical and horizontal directions at the operator seat were recorded as the two main outputs for the suspension system performance. Eighty percent (80%) of the total experimental data was used in training and developing the models and the remaining 20% for testing and validating the developed models. With an R2 and RMSE of 0.98168505 and 0.00852251 for the training phase, respectively, and 0.9660429 and 0.0195620 for the testing phase, HyFIS model showed the best accuracy for predicting the hydro-pneumatic suspension struts performance for dump trucks. This is the first time that AI models have been developed for dump truck suspension system performance prediction. With the implementation of these models in the dump truck, maintenance personnel can monitor the performance of the suspension system in real-time and schedule proper maintenance and/or replacement. Implementation of such a system will improve the workplace safety, operator's health and the overall system efficiency.",health,302,not included
10.1016/j.bdr.2018.05.001,to_check,Big Data Research,scopus,2018-09-01,sciencedirect,efficient in-database patient similarity analysis for personalized medical decision support systems,https://api.elsevier.com/content/abstract/scopus_id/85047072435,"Patient similarity analysis is a precondition to apply machine learning technology on medical data. In this sense, patient similarity analysis harnesses the information wealth of electronic medical records (EMRs) to support medical decision making. A pairwise similarity computation can be used as the basis for personalized health prediction. With n patients the amount of 
                        (
                        
                           
                              
                                 n
                              
                           
                           
                              
                                 2
                              
                           
                        
                        )
                      similarity calculations is required. Thus, analyzing patient similarity leads to data explosion when exploiting big data. By increasing the data size the computational burden of this analysis increases. A real-life medical application may exceed the limits of current hardware in a fairly short amount of time. Finding ways to optimize patient similarity analysis and handling this data explosion is the topic of this paper.
                  Current implementations for patient similarity analysis require their users to have knowledge of complex data analysis tools. Moreover, data pre-processing and analysis are performed in synthetic conditions: the data are extracted from the EMR database and then the data preparation and analysis are processed in external tools. After all of this effort the users might not experience a superior performance of the patient similarity analysis. We propose methods to optimize the patient similarity analysis in order to make it scalable to big data. Our method was tested against two real datasets and a low execution time was accomplished. Our result hence benefits a comprehensive medical decision support system. Moreover, our implementation comprises a balance between performance and applicability: the majority of the workload is processed within a database management system to enable a direct implementation on an EMR database.",health,303,not included
10.1016/j.engappai.2017.12.011,to_check,Engineering Applications of Artificial Intelligence,scopus,2018-03-01,sciencedirect,an artificial intelligence paradigm for troubleshooting software bugs,https://api.elsevier.com/content/abstract/scopus_id/85040456672,"Software bugs are prevalent and fixing them is time consuming, and therefore troubleshooting is an important part of software engineering. This paper presents a novel paradigm for incorporating Artificial Intelligence (AI) in the modern software troubleshooting process that can drastically reduce troubleshooting costs. In this paradigm, which we call Learn, Diagnose, and Plan (LDP), we integrate three AI technologies: (1) machine learning: learning from source-code structure, revisions history and past failures, which software components are more likely to contain bugs, (2) automated diagnosis: identifying the software components that need to be modified in order to fix an observed bug, and (3) automated planning: planning additional tests when such are needed to improve diagnostic accuracy. Importantly, these AI technologies are integrated in LDP in a synergistic manner: the diagnosis algorithm is modified to consider the learned fault predictions and the planner is modified to consider the possible diagnoses outputted by the diagnosis algorithm. The overall solution is demonstrated on real faults observed in four open source software projects.",health,304,not included
10.1016/j.jbi.2016.12.005,to_check,Journal of Biomedical Informatics,scopus,2017-02-01,sciencedirect,accuracy of an automated knowledge base for identifying drug adverse reactions,https://api.elsevier.com/content/abstract/scopus_id/85008622502,"Introduction
                  Drug safety researchers seek to know the degree of certainty with which a particular drug is associated with an adverse drug reaction. There are different sources of information used in pharmacovigilance to identify, evaluate, and disseminate medical product safety evidence including spontaneous reports, published peer-reviewed literature, and product labels. Automated data processing and classification using these evidence sources can greatly reduce the manual curation currently required to develop reference sets of positive and negative controls (i.e. drugs that cause adverse drug events and those that do not) to be used in drug safety research.
               
                  Methods
                  In this paper we explore a method for automatically aggregating disparate sources of information together into a single repository, developing a predictive model to classify drug-adverse event relationships, and applying those predictions to a real world problem of identifying negative controls for statistical method calibration.
               
                  Results
                  Our results showed high predictive accuracy for the models combining all available evidence, with an area under the receiver-operator curve of ⩾0.92 when tested on three manually generated lists of drugs and conditions that are known to either have or not have an association with an adverse drug event.
               
                  Conclusions
                  Results from a pilot implementation of the method suggests that it is feasible to develop a scalable alternative to the time-and-resource-intensive, manual curation exercise previously applied to develop reference sets of positive and negative controls to be used in drug safety research.",health,305,not included
10.1016/j.jpowsour.2016.05.092,to_check,Journal of Power Sources,scopus,2016-08-30,sciencedirect,prognostics of proton exchange membrane fuel cells stack using an ensemble of constraints based connectionist networks,https://api.elsevier.com/content/abstract/scopus_id/84975104897,"Proton Exchange Membrane Fuel Cell (PEMFC) is considered the most versatile among available fuel cell technologies, which qualify for diverse applications. However, the large-scale industrial deployment of PEMFCs is limited due to their short life span and high exploitation costs. Therefore, ensuring fuel cell service for a long duration is of vital importance, which has led to Prognostics and Health Management of fuel cells. More precisely, prognostics of PEMFC is major area of focus nowadays, which aims at identifying degradation of PEMFC stack at early stages and estimating its Remaining Useful Life (RUL) for life cycle management. This paper presents a data-driven approach for prognostics of PEMFC stack using an ensemble of constraint based Summation Wavelet- Extreme Learning Machine (SW-ELM) models. This development aim at improving the robustness and applicability of prognostics of PEMFC for an online application, with limited learning data. The proposed approach is applied to real data from two different PEMFC stacks and compared with ensembles of well known connectionist algorithms. The results comparison on long-term prognostics of both PEMFC stacks validates our proposition.",health,306,not included
10.1016/j.neucir.2016.02.001,to_check,Neurocirugia,scopus,2016-03-01,sciencedirect,competency-based neurosurgery residency programme,https://api.elsevier.com/content/abstract/scopus_id/84978322490,"Se presenta una propuesta de programa de formación en Neurocirugía basado en competencias y adaptado al marco del proyecto de Troncalidad. Esta propuesta ha sido elaborada por un grupo de neurocirujanos comisionados por la Sociedad Española de Neurocirugía (SENEC) y podría ser modificada para generar una versión definitiva que estaría operativa coincidiendo con implantación del sistema troncal. El presente escrito pretende facilitar el examen del nuevo programa adjuntado en la versión on-line de nuestra revista.
               
                  Duración del programa
                  El periodo total de formación es de 6 años; los 2 primeros se enmarcan en el tronco de Cirugía y los restantes 4 se adscriben al periodo específico.
               
                  Estructura del programa
                  Se trata de un programa basado en competencias referidas al mapa utilizado por el Accreditation Council for Graduate Medical Education (ACGME) en los EE. UU. que incluye los siguientes dominios competenciales: Conocimiento médico, Cuidado del paciente, Comunicación, Profesionalismo, Aprendizaje basado en la práctica y perfeccionamiento, Sistemas de Salud, Colaboración interprofesional y Desarrollo profesional y personal. El mapa de subcompetencias en los dominios de Conocimiento y Cuidado del paciente (incluidas las competencias quirúrgicas) se adaptó del propuesto por la AANS y el CNS (anexo 1 del programa). Se utiliza además un mapa de subcompetencias para las rotaciones troncales.
               
                  Métodos de instrucción
                  El aprendizaje del residente se basa en el estudio personal (autoaprendizaje) apoyado en el uso eficiente de las fuentes de información y una práctica clínica supervisada, incluyendo además la instrucción en bioética, gestión clínica, investigación y técnicas docentes
               
                  Métodos de evaluación
                  La propuesta de evaluación del residente incluye, entre otros instrumentos, test teóricos de conocimiento, evaluación objetiva y estructurada del nivel de competencia clínica con enfermo real o estandarizado, escalas globales de competencia, evaluación 360°, «audits» de registros clínicos, señalizadores del progreso del residente («milestones») y autoevaluación (anexo 2). Además, el residente evalúa periódicamente la dedicación docente de los neurocirujanos del servicio y otros docentes implicados en las rotaciones, y valora anualmente el funcionamiento global del programa. Los resultados de las evaluaciones se registran, junto con otros datos de interés, en el Libro del Residente.
               
                  Comité nacional de programa
                  Se propone la creación de un Comité de Programa adscrito directamente a la SENEC (Comisión Nacional) que, aparte de generar la versión definitiva del programa, se ocupe de monitorizar su implementación (nivel de adherencia al mismo y funcionamiento en los diferentes servicios), asuma la creación de bancos de preguntas y la administración centralizada de los test de conocimiento (en el ecuador de la residencia y/o al final de la misma) y centralice información recabada por los tutores que podría ser utilizada para la de reacreditación de los servicios.
               
                  A programme proposal for competency-based Neurosurgery training adapted to the specialization project is presented. This proposal has been developed by a group of neurosurgeons commissioned by the SENEC (Spanish Society of Neurosurgery) and could be modified to generate a final version that could come into force coinciding with the implementation of the specialization programme. This document aims to facilitate the test of the new programme included in the online version of our journal.
               
                  Duration of the programme
                  Total training period is 6 years; initial 2 years belong to the surgery specialization and remaining 4 years belong to core specialty period.
               
                  Structure of the programme
                  It is a competency-based programmed based on the map used by the US Accreditation Council for Graduate Medical Education (ACGME) including the following domains of clinical competency: Medical knowledge, patient care, communication skills, professionalism, practice-based learning and improvement, health systems, interprofessional collaboration and professional and personal development. Subcompetencies map in the domains of Knowledge and Patient care (including surgical competencies) was adapted to the one proposed by AANS and CNS (annex 1 of the programme). A subcompetency map was also used for the specialization rotations.
               
                  Instruction methods
                  Resident's training is based on personal study (self-learning) supported by efficient use of information sources and supervised clinical practice, including bioethical instruction, clinical management, research and learning techniques.
               
                  Evaluation methods
                  Resident evaluation proposal includes, among other instruments, theoretical knowledge tests, objective and structured evaluation of the level of clinical competency with real or standardised patients, global competency scales, 360-degree evaluation, clinical record audits, milestones for residents progress and self-assessment (annex 2). Besides, residents periodically assess the teaching commitment of the department's neurosurgeons and other professors participating in rotations, and annually assess the overall operation of the programme. Results of evaluations are registered, together with other relevant data, in the Resident's Book.
               
                  Programme's National Committee
                  The creation of a Programme Committee directly attached to the SENEC (National Commission) that, aside from generating a final version of the programme, monitors its implementation (level of adherence and operation in the different departments), assumes the creation of test banks and the centralized administration of knowledge tests (in the middle of the residency and/or at the end of it) and centralizes information collected by tutors that could be used for re-accreditation of the services, is proposed.",health,307,not included
10.1016/j.compeleceng.2013.12.020,to_check,Computers and Electrical Engineering,scopus,2014-01-01,sciencedirect,real-time svd-based detection of multiple combined faults in induction motors,https://api.elsevier.com/content/abstract/scopus_id/84922794071,"Early detection of induction motor faults has been a main subject of investigation for many years. Several approaches have been proposed for identifying one or more faults treated in an isolated way. Multiple combined faults on induction motors represent a big challenge since the reliable diagnosis of a faulty condition under the presence of two or more simultaneous faults is really difficult. This work introduces a novel methodology that merges singular value decomposition, statistical analysis, and artificial neural networks for multiple combined fault identification. Obtained results demonstrate its high effectiveness on detecting faulty bearings, unbalance, broken rotor bars, and all their possible combinations. The developed field programmable gate array-based implementation offers a portable low-cost solution for online classification of the rotating machine condition in real time. Thanks to its generalized nature, the introduced approach can be extended for detecting multiple combined faults under different working conditions by a proper calibration.",health,308,not included
10.1016/j.eswa.2013.05.067,to_check,Expert Systems with Applications,scopus,2014-01-01,sciencedirect,exitcdss: a framework for a workflow-based cbr for interventional clinical decision support systems and its application to tavi,https://api.elsevier.com/content/abstract/scopus_id/84885957147,"Clinical Decision Support System (CDSSs) should form an important part of the field of clinical knowledge management technologies through their capacity to support the clinical process and use of knowledge, including knowledge maintenance and continuous learning, from diagnosis and investigation through surgery, treatment and long-term care. The work presented shows a workflow-based CDSS designed to give case-specific assessment to clinicians during complex surgery or Minimally Invasive Surgery (MISs). Following a perioperative workflow, the designed software will use a Case-Based Reasoning (CBR) methodology to retrieve similar past cases from a case base to provide support at any particular point of the process. The graphical user interface allows easy navigation through the whole support progress, from the initial configuration steps to the final results organized as sets of experiments easily visualized in a user-friendly way. The eXiTCDSS tool is presented giving support to a recent complex minimally invasive surgery which is receiving growing attention lately, the Transcatheter Aortic Valve Implantation (TAVI). The results obtained are presented on a basis of a real TAVI case base of 82 patients operated at Rennes University Hospital.",health,309,not included
10.1016/j.sigpro.2012.10.020,to_check,Signal Processing,scopus,2013-06-01,sciencedirect,3d cbir with sparse coding for image-guided neurosurgery,https://api.elsevier.com/content/abstract/scopus_id/84875248258,"This research takes an application-specific approach to investigate, extend and implement the state of the art in the fields of both visual information retrieval and machine learning, bridging the gap between theoretical models and real world applications. During an image-guided neurosurgery, path planning remains the foremost and hence the most important step to perform an operation and ensures the maximum resection of an intended target and minimum sacrifice of health tissues. In this investigation, the technique of content-based image retrieval (CBIR) coupled with machine learning algorithms are exploited in designing a computer aided path planning system (CAP) to assist junior doctors in planning surgical paths while sustaining the highest precision. Specifically, after evaluation of approaches of sparse coding and K-means in constructing a codebook, the model of sparse codes of 3D SIFT has been furthered and thereafter employed for retrieving, The novelty of this work lies in the fact that not only the existing algorithms for 2D images have been successfully extended into 3D space, leading to promising results, but also the application of CBIR that is mainly in a research realm, to a clinical sector can be achieved by the integration with machine learning techniques. Comparison with the other four popular existing methods is also conducted, which demonstrates that with the implementation of sparse coding, all methods give better retrieval results than without while constituting the codebook, implying the significant contribution of machine learning techniques.",health,310,not included
10.1016/j.ultrasmedbio.2013.04.013,to_check,Ultrasound in Medicine and Biology,scopus,2013-01-01,sciencedirect,novel method for localization of common carotid artery transverse section in ultrasound images using modified viola-jones detector,https://api.elsevier.com/content/abstract/scopus_id/84883453413,"This article describes a novel method for highly accurate and effective localization of the transverse section of the carotis comunis artery in ultrasound images. The method has a high success rate, approximately 97%. Unlike analytical methods based on geometric descriptions of the object sought, the method proposed here can cover a large area of shape variation of the artery under study, which normally occurs during examinations as a result of the pressure on the examined tissue, tilt of the probe, setup of the sonographic device, and other factors. This method shows great promise in automating the process of determining circulatory system parameters in the non-invasive clinical diagnostics of cardiovascular diseases. The method employs a Viola-Jones detector that has been specially adapted for efficient detection of transverse sections of the carotid artery. This algorithm is trained on a set of labeled images using the AdaBoost algorithm, Haar-like features and the Matthews coefficient. The training algorithm of the artery detector was modified using evolutionary algorithms. The method for training a cascade of classifiers achieves on a small number of positive and negative training data samples (about 500 images) a high success rate in a computational time that allows implementation of the detector in real time. Testing was performed on images of different patients for whom different ultrasonic instruments were used under different conditions (settings) so that the algorithm developed is applicable in general radiologic practice.",health,311,not included
10.1016/j.eswa.2011.11.016,to_check,Expert Systems with Applications,scopus,2012-04-01,sciencedirect,an intelligent model for the classification of children's occupational therapy problems,https://api.elsevier.com/content/abstract/scopus_id/84855868297,"Objectives
                  In Taiwan, the classification of real problems of children with appropriate occupational therapy is a difficult job for the therapist. The complexities of 127 attribute values to be evaluated in the assessment, the misleading diagnosis which may be made by the pediatrician and the shortage of manpower cause of high workload for the therapist. The design of an easy to use and effective classification model is therefore an important issue in children’s occupational therapy treatment. This study accordingly applies an artificial neural network (ANN) and classification and regression tree (CART) techniques to skeleton an intelligent classification model in order to provide a comprehensive framework to assist the therapist to raise the accuracy when categorizing children’s problems for occupational therapy. These categories with critical attributes under the guidelines of the American Occupational Therapy Association (AOTA) are discussed, in order to assist the therapist for precise assessment and appropriate treatment. To the best of our knowledge, no research has yet been conducted on the problems’ characteristics in children’s occupational therapy.
               
                  Methods
                  Based on the advice and assistance of the therapists and occupational therapy treatment needed, 127 outpatients from a regional hospital in Taiwan between 2007 and 2010 were selected as the data sets for problems in children occupation classification. This study accordingly suggests an intelligent model for the classification which integrates ANN and CART. The major steps in applying the model include: (1) building an ANN higher performance trained model; and (2) adopting CART to the trained model and building in previous steps, to extract the critical attributes of children occupational problems.
               
                  Results
                  The results showed that artificial neural network had a higher accuracy, up to 84%, with evenly distributed datasets. Then high performance of the trained neural network had been extracted for the rules by using the classification tree approach in the classification and regression trees application. Most important of all, this study indicated that some of the rules can correctly identify up to 67% of the problems of the children with 100% confidence, which is much better than the current evaluations being used. Moreover, the tree with a binary variable of age and 8 predicators were found and listed afterward, such as, gross coordination, upper left muscle tone, interpersonal skill, proprioceptive and vestibular, visual, visual stimulus input for influence of emotional and movement, swallowing, and dressing. Actual implementation showed that the intelligent classification model is capable of integrating ANN and CART techniques to clarify children’s occupational therapy problems with considerable accuracy.
               
                  Conclusions
                  The model could be employed as a supporting system in making decisions regarding children problems with occupational therapy classifications and treatment. The rules extracted from CART were helpful to therapists in classifying what category the real problems of the children belonged to. This study expected that more machine learning techniques will certainly play an essential role in future children occupational therapy applications.",health,312,not included
10.1016/j.epsr.2011.06.007,to_check,Electric Power Systems Research,scopus,2011-11-01,sciencedirect,general asset management model in the context of an electric utility: application to power transformers,https://api.elsevier.com/content/abstract/scopus_id/79960994059,"GAMMEU
                        1
                     
                     
                        1
                        GAMMEU: general asset management model for an electric utility.
                      constitutes an integrated approach that covers the different elements related to the asset management of power transformers in the environment of a utility. GAMMEU harmonizes and inter-relates all the relevant subsystems of the asset management that normally are studied as individual entities and not as a system. Concretely, GAMMEU consists of a platform for data integration, an intelligent system for detection and diagnosis of failures, a failure rate estimation model, a module of reliability analysis and an optimisation model for maintenance scheduling. In this work, a brief description of the elements of GAMMEU is presented and the implementation of the intelligent system for detection and diagnosis as well as the failure rate estimation model is exemplified using data of measurements performed in real power transformers. A robust anomaly detection module using prediction models based on artificial intelligence techniques was developed for top oil temperature monitoring and the use of decision trees as classifiers for the assessment of FRA
                        2
                     
                     
                        2
                        Frequency response analysis.
                      measurements is also illustrated. For failure rate estimation, the use of a model based on hidden Markov chains is presented using data of dissolved gas analysis tests. The experience obtained from the implementation of part of the modules of GAMMEU using real data has demonstrated its feasibility.",health,313,included
10.1016/j.ijmedinf.2010.01.012,to_check,International Journal of Medical Informatics,scopus,2010-05-01,sciencedirect,using ontologies for structuring organizational knowledge in home care assistance,https://api.elsevier.com/content/abstract/scopus_id/77949915975,"Purpose
                  Information Technologies and Knowledge-based Systems can significantly improve the management of complex distributed health systems, where supporting multidisciplinarity is crucial and communication and synchronization between the different professionals and tasks becomes essential. This work proposes the use of the ontological paradigm to describe the organizational knowledge of such complex healthcare institutions as a basis to support their management. The ontology engineering process is detailed, as well as the way to maintain the ontology updated in front of changes. The paper also analyzes how such an ontology can be exploited in a real healthcare application and the role of the ontology in the customization of the system. The particular case of senior Home Care assistance is addressed, as this is a highly distributed field as well as a strategic goal in an ageing Europe.
               
                  Materials and methods
                  The proposed ontology design is based on a Home Care medical model defined by an European consortium of Home Care professionals, framed in the scope of the K4Care European project (FP6). Due to the complexity of the model and the knowledge gap existing between the – textual – medical model and the strict formalization of an ontology, an ontology engineering methodology (On-To-Knowledge) has been followed.
               
                  Results
                  After applying the On-To-Knowledge steps, the following results were obtained: the feasibility study concluded that the ontological paradigm and the expressiveness of modern ontology languages were enough to describe the required medical knowledge; after the kick-off and refinement stages, a complete and non-ambiguous definition of the Home Care model, including its main components and interrelations, was obtained; the formalization stage expressed HC medical entities in the form of ontological classes, which are interrelated by means of hierarchies, properties and semantically rich class restrictions; the evaluation, carried out by exploiting the ontology into a knowledge-driven e-health application running on a real scenario, showed that the ontology design and its exploitation brought several benefits with regards to flexibility, adaptability and work efficiency from the end-user point of view; for the maintenance stage, two software tools are presented, aimed to address the incorporation and modification of healthcare units and the personalization of ontological profiles.
               
                  Conclusions
                  The paper shows that the ontological paradigm and the expressiveness of modern ontology languages can be exploited not only to represent terminology in a non-ambiguous way, but also to formalize the interrelations and organizational structures involved in a real and distributed healthcare environment. This kind of ontologies facilitates the adaptation in front of changes in the healthcare organization or Care Units, supports the creation of profile-based interaction models in a transparent and seamless way, and increases the reusability and generality of the developed software components. As a conclusion of the exploitation of the developed ontology in a real medical scenario, we can say that an ontology formalizing organizational interrelations is a key component for building effective distributed knowledge-driven e-health systems.",health,314,not included
10.1016/j.compbiomed.2009.11.021,to_check,Computers in Biology and Medicine,scopus,2010-03-01,sciencedirect,computer validation and statistical correlations of a modern decompression diving algorithm,https://api.elsevier.com/content/abstract/scopus_id/77549086431,"A diving algorithm is a safe combination of model and data to efficiently stage diver ascents following arbitrary underwater exposures. To that end, we detail a modern one, the LANL reduced gradient bubble model (RGBM), dynamical principles, and correlations with the LANL Data Bank data. Table, profile, and meter fit and risk parameters are obtained in statistical likelihood analysis from decompression exposure data. The RGBM algorithm enjoys extensive and utilitarian application in mixed gas diving, both in recreational and technical sectors, and forms the bases for released tables, software, and decompression meters used by scientific, commercial, and research divers. The LANL Data Bank is described, and the methods used to deduce risk are detailed. Risk functions for dissolved gas and bubbles are summarized. Parameters that can be used to estimate profile risk are tallied. To fit data, a modified Levenberg–Marquardt routine is employed. The LANL Data Bank presently contains 2879 profiles with 20 cases of DCS across nitrox, trimix, and heliox deep and decompression diving. This work establishes needed correlation between global mixed gas diving, specific bubble model, and deep stop data. Our objective is operational diving, not clinical science. The fit of bubble model to deep stop data is chi squared significant to 93%, using the logarithmic likelihood ratio of null set (actual set) to fit set. The RGBM algorithm is thus validated within the LANL Data Bank. Extensive and safe utilization of the model reported in field user statistics for tables, meters, and software also suggests real world validation, that is, one without noted nor reported DCS spikes in the field.",health,315,not included
10.3389/fgene.2021.691274,to_check,core,'Frontiers Media SA',2021-07-01 00:00:00,core,imputehr: a visualization tool of imputation for the prediction of biomedical data,https://core.ac.uk/download/475059757.pdf,"Electronic health records (EHRs) have been widely adopted in recent years, but often include a high proportion of missing data, which can create difficulties in implementing machine learning and other tools of personalized medicine. Completed datasets are preferred for a number of analysis methods, and successful imputation of missing EHR data can improve interpretation and increase our power to predict health outcomes. However, use of the most popular imputation methods mainly require scripting skills, and are implemented using various packages and syntax. Thus, the implementation of a full suite of methods is generally out of reach to all except experienced data scientists. Moreover, imputation is often considered as a separate exercise from exploratory data analysis, but should be considered as art of the data exploration process. We have created a new graphical tool, ImputEHR, that is based on a Python base and allows implementation of a range of simple and sophisticated (e.g., gradient-boosted tree-based and neural network) data imputation approaches. In addition to imputation, the tool enables data exploration for informed decision-making, as well as implementing machine learning prediction tools for response data selected by the user. Although the approach works for any missing data problem, the tool is primarily motivated by problems encountered for EHR and other biomedical data. We illustrate the tool using multiple real datasets, providing performance measures of imputation and downstream predictive analysis",health,316,not included
10.11603/mie.1996-1960.2020.2.11171,to_check,core,https://core.ac.uk/download/328038010.pdf,2020-07-13 00:00:00,core,'ternopil state medical university',,"Background. The integrated digitalization of medicine, the use of the Internet of Intelligent Things, and the networks of medical wireless sensors offers ample opportunities to remotely support the appropriate quality of life of chronically ill patients, the elderly, and athletes and professionals with heavy physical or mental workloads.
Materials and methods. Results. The implementation of individual remote means of maintaining quality of life includes the following components: the creation of new and the use of existing miniature or microelectronic medical sensors that directly read medical parameters from the patient's body. Such sensors are designed to monitor body temperature, heart rate, respiration and blood pressure, read heart signals, determine skin moisture, record a fall or abrupt changes in the patient's position and other body parameters in real time, miniature interfaces for data acquisition, analog-to-digital conversion and data preprocessing of medical parameters received from medical sensors located on the patient's body, miniature data communication means to remote medical centers according to modern communication standards, automated and remote diagnostic tools with elements of artificial intelligence, remote-controlled injectors for input of medicines in case of critical condition of the patient.
Conclusions. The current state and prospects for the development of these tools are discussed in the publication.Комплексная цифровизация медицины, применение Интернета интеллектуальных вещей, сети медицинских беспроводных сенсоров открывает широкие возможности для дистанционной поддержки соответствующего качества жизни хронически больных пациентов, людей пожилого возраста, а также спортсменов и специалистов, которые имеют большие физические или психические нагрузки при исполнении служебных обязанностей.
Реализация индивидуальных дистанционных средств поддержки качества жизни включает следующие составляющие: создание новых и использование существующих миниатюрных медицинских сенсоров, которые непосредственно считывают медицинские параметры из организма пациента, миниатюрные интерфейсы для сбора и первичной обработки медицинских параметров, полученных от медицинских сенсоров, средства приема-передачи данных в отдаленные медицинские центры, автоматизированные средства дистанционной диагностики, инжекторы с дистанционным управлением для ввода лекарственных средств в случае критического состояния пациента. Современное состояние и перспективы развития этих средств рассмотрены в данной публикации.Комплексна цифровізація медицини, застосування Інтернету інтелектуальних речей, мережі медичних бездротових сенсорів відкриває широкі можливості для дистанційної підтримки відповідної якості життя хронічно хворих пацієнтів, людей похилого віку, а також спортсменів і фахівців, які мають великі фізичні чи психічні навантаження при виконанні службових обов'язків.
Реалізація індивідуальних дистанційних засобів підтримки якості життя включає такі складові: створення нових і використання існуючих мініатюрних медичних сенсорів, що безпосередньо зчитують медичні параметри з організму пацієнта, мініатюрні інтерфейси для збору та первинного оброблення медичних параметрів, отриманих від медичних сенсорів, засоби приймання-передавання даних у віддалені медичні центри, автоматизовані засоби дистанційної діагностики, інжектори з дистанційним керуванням для вводу лікарських засобів у разі критичного стану пацієнта. Сучасний стан і перспективи розвитку цих засобів розглянуто у даній публікації",health,317,not included
10.1101/2020.05.04.20090258,to_check,core,,2020-05-08 00:00:00,core,'cold spring harbor laboratory',,"How do fine modifications to social distancing measures really affect COVID-19 spread? A major problem for health authorities is that we do not know. In an imaginary world, we might develop a harmless biological virus that spreads just like COVID-19, but is traceable via a cheap and reliable diagnosis. By introducing such an imaginary virus into the population and observing how it spreads, we would have a way of learning about COVID-19 because the benign virus would respond to population behaviour and social distancing measures in a similar manner. Such a benign biological virus does not exist. Instead, we propose a safe and privacy-preserving digital alternative. Our solution is to mimic the benign virus by passing virtual tokens between electronic devices when they move into close proximity. As Bluetooth transmission is the most likely method used for such inter-device communication, and as our suggested ""virtual viruses"" do not harm individuals' software or intrude on privacy, we call these Safe Blues. In contrast to many app-based methods that inform individuals or governments about actual COVID-19 patients or hazards, Safe Blues does not provide information about individuals' locations or contacts. Hence the privacy concerns associated with Safe Blues are much lower than other methods. However, from the point of view of data collection, Safe Blues has two major advantages: - Data about the spread of Safe Blues is uploaded to a central server in real time, which can give authorities a more up-to-date picture in comparison to actual COVID-19 data, which is only available retrospectively. - Sampling of Safe Blues data is not biased by being applied only to people who have shown symptoms or who have come into contact with known positive cases. These features mean that there would be real statistical value in introducing Safe Blues. In the medium term and end game of COVID-19, information from Safe Blues could aid health authorities to make informed decisions with respect to social distancing and other measures. In this paper we outline the general principles of Safe Blues and we illustrate how Safe Blues data together with neural networks may be used to infer characteristics of the progress of the COVID-19 pandemic in real time. Further information is on the Safe Blues website: https://safeblues.org/",health,318,not included
https://core.ac.uk/download/211997756.pdf,to_check,core,Crowdbreaks: Tracking Health Trends Using Public Social Media Data and Crowdsourcing,2019-04-26 00:00:00,core,10.3389/fpubh.2019.00081,,"In the past decade, tracking health trends using social media data has shown great promise, due to a powerful combination of massive adoption of social media around the world, and increasingly potent hardware and software that enables us to work with these new big data streams. At the same time, many challenging problems have been identified. First, there is often a mismatch between how rapidly online data can change, and how rapidly algorithms are updated, which means that there is limited reusability for algorithms trained on past data as their performance decreases over time. Second, much of the work is focusing on specific issues during a specific past period in time, even though public health institutions would need flexible tools to assess multiple evolving situations in real time. Third, most tools providing such capabilities are proprietary systems with little algorithmic or data transparency, and thus little buy-in from the global public health and research community. Here, we introduce Crowdbreaks, an open platform which allows tracking of health trends by making use of continuous crowdsourced labeling of public social media content. The system is built in a way which automatizes the typical workflow from data collection, filtering, labeling and training of machine learning classifiers and therefore can greatly accelerate the research process in the public health domain. This work describes the technical aspects of the platform, thereby covering the functionalities at its current state and exploring its future use cases and extensions",health,319,included
https://riunet.upv.es/bitstream/10251/169535/1/sarabia-jacomeusachpalau%20-%20highly-efficient%20fog-based%20deep%20learning%20aal%20fall%20detection%20system.pdf,to_check,core,Highly-efficient fog-based deep learning AAL fall detection system,2020-09-01 00:00:00,core,10.1016/j.iot.2020.100185,,"[EN] Falls is one of most concerning accidents in aged population due to its high frequency and serious repercussion; thus, quick assistance is critical to avoid serious health consequences. There are several Ambient Assisted Living (AAL) solutions that rely on the technologies of the Internet of Things (IoT), Cloud Computing and Machine Learning (ML). Recently, Deep Learning (DL) have been included for its high potential to improve accuracy on fall detection. Also, the use of fog devices for the ML inference (detecting falls) spares cloud drawback of high network latency, non-appropriate for delay-sensitive applications such as fall detectors. Though, current fall detection systems lack DL inference on the fog, and there is no evidence of it in real environments, nor documentation regarding the complex challenge of the deployment. Since DL requires considerable resources and fog nodes are resource-limited, a very efficient deployment and resource usage is critical. We present an innovative highly-efficient intelligent system based on a fog-cloud computing architecture to timely detect falls using DL technics deployed on resource-constrained devices (fog nodes). We employ a wearable tri-axial accelerometer to collect patient monitoring data. In the fog, we propose a smart-IoT-Gateway architecture to support the remote deployment and management of DL models. We deploy two DL models (LSTM/GRU) employing virtualization to optimize resources and evaluate their performance and inference time. The results prove the effectiveness of our fall system, that provides a more timely and accurate response than traditional fall detector systems, higher efficiency, 98.75% accuracy, lower delay, and service improvement.This research was supported by the Ecuadorian Government through the Secretary of Higher Education, Science, Technology, and Innovation (SENESCYT) and has received funding from the European Union's Horizon 2020 research and innovation program as part of the ACTIVAGE project under Grant 732679.Sarabia-Jácome, D.; Usach, R.; Palau Salvador, CE.; Esteve Domingo, M. (2020). Highly-efficient fog-based deep learning AAL fall detection system. Internet of Things. 11:1-19. https://doi.org/10.1016/j.iot.2020.100185S1191",health,320,included
'georg thieme verlag kg',to_check,core,,2019-04-25 00:00:00,core,"[{'title': 'yearbook of medical informatics', 'identifiers': ['issn:2364-0502', '2364-0502', 'issn:0943-4747', '0943-4747']}]",10.1055/s-0039-1677898,"OBJECTIVES:Artificial Intelligence (AI) offers significant potential for improving healthcare. This paper discusses how an ""open science"" approach to AI tool development, data sharing, education, and research can support the clinical adoption of AI systems. METHOD:In response to the call for participation for the 2019 International Medical Informatics Association (IMIA) Yearbook theme issue on AI in healthcare, the IMIA Open Source Working Group conducted a rapid review of recent literature relating to open science and AI in healthcare and discussed how an open science approach could help overcome concerns about the adoption of new AI technology in healthcare settings. RESULTS:The recent literature reveals that open science approaches to AI system development are well established. The ecosystem of software development, data sharing, education, and research in the AI community has, in general, adopted an open science ethos that has driven much of the recent innovation and adoption of new AI techniques. However, within the healthcare domain, adoption may be inhibited by the use of ""black-box"" AI systems, where only the inputs and outputs of those systems are understood, and clinical effectiveness and implementation studies are missing. CONCLUSIONS:As AI-based data analysis and clinical decision support systems begin to be implemented in healthcare systems around the world, further openness of clinical effectiveness and mechanisms of action may be required by safety-conscious healthcare policy-makers to ensure they are clinically effective in real world use",health,321,not included
10.1590/s2237-93632012000100007,to_check,core,https://core.ac.uk/download/296617134.pdf,2015-11-26 00:00:00,core,'fapunifesp (scielo)',"[{'title': 'Journal of Coloproctology', 'identifiers': ['issn:2237-9363', '2237-9363']}]","OBJECTIVE: Develop a prototype using computer resources to optimize the management process of clinical information and video colonoscopy exams. MATERIALS AND METHODS: Through meetings with medical and computer experts, the following requirements were defined: management of information about medical professionals, patients and exams; video and image captured by video colonoscopes during the exam, and the availability of these videos and images on the Web for further analysis. The technologies used were Java, Flex, JBoss, Red5, JBoss SEAM, MySQL and Flamingo. RESULTS AND DISCUSSION: The prototype contributed to the area of colonocospy by providing resources to maintain the patients' history, tests and images from video colonoscopies. The web-based application allows greater flexibility to physicians and specialists. The resources for remote analysis of data and tests can help doctors and patients in the examination and diagnosis. CONCLUSION: The implemented prototype has contributed to improve colonoscopy-related processes. Future activities include the prototype deployment in the Service of Coloproctology and the utilization of this model to allow real-time monitoring of these exams and knowledge extraction from such structured database using artificial intelligence.OBJETIVO: Desenvolver um protótipo por meio de recursos computacionais para a otimização de processos de gerenciamento de informações clínicas e de exames de videocolonoscopia. MATERIAIS E MÉTODOS: Por meio de reuniões com especialistas médicos e computacionais, definiram-se os seguintes requisitos: gestão de informações sobre profissionais médicos, pacientes e exames complementares; aquisição dos vídeos e captura de imagens a partir do videocolonoscópio durante a realização desse exame, e a disponibilidade por meio da Web para análise posterior dessas imagens. As tecnologias aplicadas foram: Java, Flex, JBOSS, Red5, JBOSS SEAM, MySQL e Flamingo. RESULTADOS E DISCUSSÃO: O protótipo contribuiu para a área de colonocospia disponibilizando recursos para manutenção de histórico de pacientes, exames e imagens. O acesso à aplicação, por meio de browser, permite maior flexibilidade aos médicos e especialistas. Os recursos para análise remota de dados e exames podem auxiliar médicos e pacientes na realização de exames e diagnósticos. CONCLUSÃO: O protótipo implementado contribuiu para melhoria de processos relacionados a exames de videocolonoscopia. Trabalhos futuros incluem implantação do protótipo no serviço de coloproctologia, bem como a extensão do modelo para o acompanhamento dos exames em tempo real e extração de conhecimento dessa base de dados estruturada por meio de inteligência artificial.505",health,323,not included
10.15587/2312-8372.2015.37693,to_check,core,,2015-01-28 00:00:00,core,'private company technology center',,"Cellular automata are widely used in many fields of knowledge for the study of variety of complex real processes: computer engineering and computer science, cryptography, mathematics, physics, chemistry, ecology, biology, medicine, epidemiology, geology, architecture, sociology, theory of neural networks. Thus, cellular automata (CA) and tetra automata are gaining relevance taking into account the hardware and software solutions.Also it is marked a trend towards an increase in the number of possible states of CA that led to the emergence of new types of CA, which are united in this paper under a common name – postbinary cellular automata.This article proposes a variant of generalized structure of CA cell using asynchronous data storage device, software configurable via the user interface. Several ways of hardware implementation of initial values record in register of the cell status and read the results from the current layer of states in the register of reading the results: serial, block, using a layer of initial states through direct serial or block addressing, or cascade addressing on the basis of tetracodes. It is considered the CTA structure when used as a coprocessor in the local computers. At the same time the detailed descriptions are given, problems are identified, corresponding schematic structure are given. It is considered in detail the generalized block diagram of a multi-layer cellular automaton and it is proposed a generalized block diagram of a multi-layer CA cell that improves performance and extended functionality compared with the known CA.The main advantage of the proposed implementation of cellular automata is functional diversity of elements and flexibility, the ability to change the laws of transitions immediately in all cells (matrix elements). Therefore, CA mass production (as well as CTA) on this technology can get a testing ground for numerous experiments in various fields of science.Предложен вариант обобщенной структуры ячейки клеточного автомата с использованием асинхронного запоминающего устройства, программно настраиваемого через пользовательский интерфейс. Рассмотрены несколько способов аппаратной реализации записи начальных значений в регистр состояний ячейки и считывания результатов со слоя текущих состояний в регистр считывания результатов: последовательный, блочный, с использованием слоя начальных состояний через непосредственную последовательную или блочную адресацию, или каскадную адресацию на основе тетракодов. Запропоновано варіант узагальненої структури комірки клітинного автомата з використанням асинхронного устрою запам’ятовування, програмно настоюваного через інтерфейс користувача. Розглянуто кілька способів апаратної реалізації записи початкових значень в регістр станів осередку та зчитування результатів із шару поточних станів в регістр зчитування результатів: послідовний, блоковий, з використанням шару початкових станів через безпосередню послідовну або блокову адресацію, або каскадну адресацію на основі тетракодов.",health,324,not included
10.5937/nbp1503131k,to_check,core,https://core.ac.uk/download/196618218.pdf,2015-01-01 00:00:00,core,'centre for evaluation in education and science (ceon/cees)',"[{'title': 'Nauka bezbednost policija', 'identifiers': ['issn:0354-8872', '0354-8872']}]","Complex real problems increasingly require intelligent systems that combine knowledge, techniques and methodologies from various sources. Intelligent systems based on artificial intelligence techniques that are associated with the behavior of people can perform the processes of learning, reasoning and solving all kinds of problems. Such systems, which automatically can perform tasks set by the user or other software, today thankfully called intelligent agents. Independent, intelligent agents on the Internet can be very successful to perform some search work on behalf of and for the needs of different users. For efficient collection, manipulation and management of data, such software can be very interesting from the standpoint of intelligent data analysis in many areas the police. Analysis of the data collected by an intelligent agent (a software robot-bot) can be successfully utilized, among many jobs in the police, and in the field of crime and in particular manifestation of cyber­crime, traffic safety, emergencies, etc. To make the collection and analysis of data from criminal activities on the Internet effective, it is necessary to examine the existing artificial intelligence techniques to be used for the conclusion of the intelligent agents. On the other hand, using of methods of artificial intelligence in finding data along with intelligent data analysis (data mining) should be used, which has found wide use in the area of business, economics, mechanics, medicine, genetics, transport etc.Kompleksni realni problemi sve češće zahtevaju inteligentne sisteme koji kombinuju znanje, tehnike i metodologije iz različitih izvora. Inteligentni sistemi bazirani na tehnikama veštačke inteligencije koje asociraju na ponašanje ljudi mogu da obavljaju procese učenja, zaključivanja i rešavanje raznovrsnih problema. Ovakvi sistemi, koji automatski mogu da izvrše zadatke zadate od strane korisnika ili drugih softvera, danas se sreću pod imenom inteligentni agenti. Samostalno, inteligentni agenti na Internetu mogu veoma uspešno da izvode neki pretraživački posao u ime i za potrebe raznih korisnika. Zbog efikasnog sakupljanja, manipulisanja i upravljanja podacima, ovakvi softveri mogu biti veoma interesantni sa stanovišta inteligentne analize podataka u mnogim oblastima policije. Analiza podataka sakupljenih od strane inteligentnog agenta (softverskog robota - bota) može se uspešno iskoristiti, između mnogih poslova u policiji, i na polju kriminala i naročito pojavnog oblika sajber kriminala, bezbednosti saobraćaja, vanrednih situacija itd. Kako bi sakupljanje i analiza podataka iz kriminalnih aktivnosti na Internetu bila efikasna, neophodno je sagledati postojeće tehnike veštačke inteligencije koje se koriste za zaključivanje u inteligentnim agentima. S druge strane, treba iskoristiti metode veštačke inteligencije u pronalaženju podataka pri inteligentnoj analizi podataka (data mining-u) koja je našla široku primenu u oblasti poslovanja preduzeća, ekonomije, mehanike, medicine, genetike, saobraćaja i sl",health,325,not included
10.5120/20328-2507,to_check,core,,2015-11-21 00:00:00,core,software design framework for healthcare systems,,"Healthcare is costing lot of money and resources all over the world in all countries; it consumes almost 30-40 percent of their budget in the healthcare industry. The only solution is to make this health care industry automated and online. In this paper we are proposing a software design framework for health care systems which will be based on agent technologies. The proposed system is based on Artificial Intelligence techniques that can support the user for selection and making choices. The proposed software framework suggested that Multi-Agent Systems (MAS) is the most suitable technique for designing such systems. MAS collaborate intelligently for solving this complex problem. Patients can be supported remotely using the proposed framework so that it can reduce the patient load on hospitals, the proposed software framework operate on real time framework. Computing can help in improving the communication process between follow-up doctors and nurses with patients by making appointments the process easier, according to patient preference with a reminder on necessary actions such as taking scheduled prescribed medicine, engaging in exercises, avoiding some kinds of food and harmful habits such as smoking before and after patient",health,326,included
10.1590/1413-81232014195.13932013,to_check,core,Associacao Brasileira de Pos - Graduacao em Saude Coletiva,2015-11-26 00:00:00,core,evaluation of the usability of a mobile digital food guide based on user perception [avaliação da usabilidade do guia alimentar digital móvel segundo a percepção dos usuários],,"The use of digital technology in the form of health care apps has been on the increase. In the nutrition area, apps are now available with a view to lead to behavior change, helping individuals to reflect on their food choices and identify weak points in their dietary routine. The article seeks to evaluate user perception regarding the usability of the Digital Food Guide (DFG), which is a mobile smartphone app with guidelines on healthy eating. A cross-sectional study evaluated the user perception of the app using the Likert scale, built with 24 assertions organized in three dimensions of analysis: the DFG as an intuitive and self-explanatory tool; the DFG as a promoter of healthy food choices; and the DFG as a promoter of the transition to the appropriate weight. The instrument was assessed regarding its reliability through the split-half and validity method in two stages. The 22 assertions were validated; the reliability was 0.93; the average of the assertions in each dimension was 3.10; of the 80 respondents, 58.75% considered the implementation of the DFG to be positive. The application has good usability as perceived by users, considering analysis of the dimensions relating to its performance.19514371446(2013) Obesity and overweight., , http://www.who.int/mediacentre/factsheets/fs311/en/index.html#, Word Health Organization (WHO). [Internet]. [acessado 2014 abr 2]. Disponível em: Disponível emNoronha, J., Hysen, E., Zhang, H., PlateMate: Crowdsourcing nutrition analysis from food photographs (2011) UIST'11 Proceedings og the 24th annual ACM symposium on user interface software and technology., pp. 1-12. , In: Washington: Harvard School of Engineering and Applied SciencesTeixeira, P.D.S., Reis, B.Z., Vieira, D.A.S., Costa, D., Costa, J.O., Raposo, O.F.F., Wartha, E.R.S.A., Netto, R.S.M., Intervenção nutricional educative como ferramenta eficaz para mudanças de hábitos alimentares e peso corporal entre praticantes de atividade física (2013) Cien Saude Colet, 18 (2), pp. 347-356Liu, C., Zhu, Q., Holroyd, K.A., Seng, E.K., Status and trends of mobile-health applications for iOS devices: A developer's perspective (2011) J Syst Software, 84 (11), pp. 2022-2033Parker, A.G., Harper, R., Grinter, R.E., Celebratory health technology (2011) J Diab Science Techny, 5 (2), pp. 319-324Penn, L., Boeing, H., Boushey, C.J., Dragsted, L.O., Kaput, J., Scalbert, A., Welch, A.A., Mathers, J.C., Assessment of dietary intake: NuGO symposium report (2010) Genes Nutr, 5 (3), pp. 205-213Thompson, F.E., Subar, A.F., Loria, C.M., Reedy, J.L., Baranowshi, T., Need for technological innovation in dietary assessment (2011) J Am Diet Assoc, 110 (1), pp. 48-51(2010) Brasil, redes: Linhas telefônicas e assinantes de telefonia celular., , http://www.ibge.gov.br/oaisesat/main.php, Instituto Brasileiro de Geografia e Estatística (IBGE). Brasil. [Internet]. [acessado 2013 jul 15]. Disponível em: Disponível em(2009) Sala de imprensa: Acesso à internet e posse de telefone móvel celular para uso pessoal., , http://www.ibge.gov.br/home/noticias/notica_visualiza.php?id_noticia=1517, Instituto Brasileiro de Geografia e Estatística (IBGE). Brasil. [acessado 2013 jul 13]. Disponível em: Disponível emKenney, M., Pon, B., Structuring the smartphone industry: Is the mobile internet OS platform the key? (2011) J Ind Compet Trade, 11 (3), pp. 239-261Moura, A.M., (2010) Apropriação do telemóvel como ferramenta de mediação em mobile learning-estudos de caso em contexto educativo [dissertação]., , Braga: Universidade do MinhoGrohmann, Z.M., Battistella, L.F., Homens e mulheres ""aceitam"" de maneira diferente? impacto do gênero no modelo (expandido) de aceitação da tecnologia-TAM (2011) Inf & Soc, 21 (1), pp. 175-189Likert, R., Roslow, S., Murphy, G., A simple and reliable method of scoring the thurstone attitude scales (1993) Pers Psychol, 46 (3), pp. 689-690Wakita, T., Ueshima, N., Noguchi, H., Psychological distance between categories in the Likert Scale: Comparing different numbers of options (2012) EPM, 72 (4), pp. 533-546Willett, W.C., Skerrett, P.J., (2002) Eat, drink, and be healthy: The Harvard Medical School Guide to Healthy Eating., , Washington: Harvard School of Medicine Free Press(2011) Guide to eating a healthy meal based on latest science., , http://www.hsph.harvard.edu/news/press-releases/healthy-eating-plate/, Harvard School of Public Health. HSPH News. [Internet]. [acessado 2014 abr 2]. Disponível em: Disponível emBrandalise, L.T., (2006) Modelos de medição de percepção e comportamento [dissertação]., , Cascavel: Universidade Estadual do Oeste do ParanáNata, R.N., Progress in Education (2012) Development and validation of a strategy to assess teaching methods in undergraduate disciplines., pp. 81-107. , In: Moraes SG, Justino ML, Jansen BF, Barbosa EP, Bruno, LFC, Pereira LAV. Nova York: Nova Science Publishers IncSchimidt, M.J., (1975) Understanding and using statistics basic concepts., , Massachusetts: Health and Company(2010) Censo demográfico 2010., , http://www.censo2010.ibge.gov.br/apps/mapa/, Instituto Brasileiro de Geografia e Estatística (IBGE). Brasil. [Internet]. [acessado 2014 abr 2]. Disponível em: Disponível emBoushey, C., Wright, J.K.D.L., Ebert, D.E.J.D., Use of technology in children's dietary assessment (2009) Eur J Clin Nutr, 63 (1), pp. S50-S57Sevick, M.A., Zickmund, S., Korytkowski, M., Piraino, B., Sereika, S., Mihalko, S., Snetselaar, L., Burke, L.E., Design, feasibility, and acceptability of an intervention using personal digital assistant-based self-monitoring in managing type 2 diabetes (2008) Contemp Clin Trials, 29 (3), pp. 396-409Toral, N., Slater, B., Abordagem do modelo transteórico no comportamento alimentar (2007) Cien Saude Colet, 12 (6), pp. 1641-1650Fonseca, A.B., Souza, T.S.N., Frozi, D.S., Pereira, R.A., Modernidade alimentar e consumo de alimentos: Contribuições sócio-antropológicas para a pesquisa e nutrição (2011) Cien Saude Colet, 16 (9), pp. 3853-3862Fabry, P., Hejl, Z., Fodor, J., Braun, T.K.Z., The frequency of meals: Its relation to overweight, hypercholesterolaemia, and decreased glucose tolerance (1964) Lancet, 284 (7360), pp. 614-615La Bounty, P.M., Campbell, B.I., Wilson, J., Galvan, E., Berardi, J., Kleiner, S.M., Kreider, R.B., Antonio, J., International society of sports nutrition position stand: Meal frequency (2011) J Int Soc Sports Nutri, 8 (4), pp. 1-12Neumark-sztainer, D., Wall, M., Fulkerson, J.A., Larson, N., Changes in the frequency of family meals from 1999 to 2010 in the homes of adolescents: Trends by sociodemographic characteristics (2013) J Adol Health, 52 (2), pp. 201-206Chisolm, D.J., Does online health information seeking act like a health behavior?: A test of the behavioral model (2010) Telemed E-Health, 16 (2), pp. 154-160Rowe, S., Alexander, N., Almeida, N., Black, R., Burns, R., Bush, L., Crawford, P., Horn, L.V., Food Science Challenge: Translating the dietary guidelines for americans to bring about real behavior change (2011) JFS, 76 (1), pp. 29-37Rusin, M., Arsand, E., Hartvigsen, G., Functionalities and input methods for recording food intake: A systematic review (2013) Int J Med Inform, 82 (8), pp. 653-66",health,327,not included
https://core.ac.uk/download/18277918.pdf,to_check,core,3D CBIR with sparse coding for image-guided neurosurgery,2013-06-01 00:00:00,core,10.1016/j.sigpro.2012.10.020,,"This research takes an application-specific approach to investigate, extend and implement the state of the art in the fields of both visual information retrieval and machine learning, bridging the gap between theoretical models and real world applications. During an image-guided neurosurgery, path planning remains the foremost and hence the most important step to perform an operation and ensures the maximum resection of an intended target and minimum sacrifice of health tissues. In this investigation, the technique of content-based image retrieval (CBIR) coupled with machine learning algorithms are exploited in designing a computer aided path planning system (CAP) to assist junior doctors in planning surgical paths while sustaining the highest precision. Specifically, after evaluation of approaches of sparse coding and K-means in constructing a codebook, the model of sparse codes of 3D SIFT has been furthered and thereafter employed for retrieving, The novelty of this work lies in the fact that not only the existing algorithms for 2D images have been successfully extended into 3D space, leading to promising results, but also the application of CBIR, that is mainly in a research realm, to a clinical sector can be achieved by the integration with machine learning techniques. Comparison with the other four popular existing methods is also conducted, which demonstrates that with the implementation of sparse coding, all methods give better retrieval results than without while constituting the codebook, implying the significant contribution of machine learning techniques",health,328,not included
"[{'title': none, 'identifiers': ['issn:1307-167x', 'issn:1303-703x', '1307-167x', '1303-703x']}]",to_check,core,https://core.ac.uk/download/47253966.pdf,2010-04-01 00:00:00,core,i̇stanbul teknik üniversitesi,,"İnsanların günlük yaşamında belirli bir sesi, görüntüyü veya analog bir veriyi tanımak için kullandıkları kuralları tanımlamak oldukça karmaşık bir dizi işlem gerektirmektedir ve hatta bu kuralları tanımlamak bazen mümkün olamamaktadır. Oysa pratikte karşılaşılan örüntü tanıma olaylarını, yazılım ve donanım tabanlı tanıma uygulamalarında belirli kriterlere oturtmak mümkündür. Sınıflandırma yöntemleri ilk olarak örüntü sınıflandırma adı altında görülmeye başlanmış ve ilk algoritmalarda basit yapılar ele alınmıştır; ilk gerçeklenen yapıda en yakın komşu yakınsaması kullanılmıştır. Sınıflandırma işlemi, benzer özellik taşıyan objelerin başka farklı özellikte olanlardan ayırt edilmesi şeklinde tanımlanabilir ve otomatik hedef belirleme, yapay zekâ, yapay sinir ağları, analog-sayısal dönüştürücüler, kuantalama, tıbbi tanı, istatistik gibi çeşitli alanlarda kullanılır. Dolaysıyla da, günümüzde, gerek gerçek dünyada gerekse sayısal dünyada verilerin sınıflandırılması büyük önem taşımaktadır. Bugüne kadar sınıflandırma işlemi genellikle çeşitli algoritmalar yardımıyla yazılımsal olarak yapılmaktaydı, oysa birçok uygulamada, sınıflandırma işlemini daha hızlı ve gerçek zamanda yapmak gerektiğinden bu algoritmaların donanımsal olarak gerçeklenmeleri çok daha yararlı olmaktadır. Ayrıca günümüzde portatif cihazların da artmasından dolayı donanımsal olarak gerçeklenecek cihazlarda da güç tüketimi büyük önem kazanmıştır. Dolayısıyla sınıflandırıcı devrelerin de bu ihtiyaçları karşılayacak şekilde tasarlanması gerekmektedir. Bu makalede akım-modlu düşük güçte çalışan bir sınıflandırıcı devresi sunulmaktadır. Önerilen sınıflandırıcı devresi, temel bir bloktan yararlanmaktadır; bu temel bloklar kullanılarak daha gelişmiş sınıflandırıcı yapılarının gerçekleştirilebileceği gösterilmiştir. Önerilen devrenin benzetimleri için 0.35 μm AMS CMOS teknoloji parametreleri kullanılmıştır. Ayrıca çekirdek devre adı verilen temel bloğun, tek boyutlu ve iki boyutlu sınıflandırıcı yapılarının benzetim sonuçları verilmiştir.In the everyday life of humans, to define the rules used to recognize a certain sound, image or an analog data necessitates a sequence of complex processes which sometimes becomes impossible to accomplish. However, to develop well defined software and hardware based criteria in the application of pattern recognition problems, is possible. The aim of classification can be defined as to assign an unknown object to a class containing similar objects (or to distinguish objects having the same properties from those not possessing). Classification is especially important in the real world applications or in the digital world. Basic classification methods using nearest neighbourhood algorithm have first been seen in early sixties under the subject tile"" pattern recognition."" Classification is used in a huge variety of applications such as automatic target identification, artificial neural networks, artificial intelligence, template matching, pattern recognition, analog to digital converters, quantization, medical diagnosis, statistics etc. Therefore nowadays, be it in the real or digital world, data classification is becoming increasingly important. But until recently, major work on classification was on developing algorithms used in software packages whereas, in many applications it is becoming more and more important to classify data much faster and in real time, entailing the need for hardware realization of these algorithms. Software approaches are not practical for real time applications, the processing is computationally very expensive, consuming a lot of Central Processing Unit (CPU) time when implemented as software running on general purpose computers. So in literature hardware implementation of classifier topologies become necessary. Also in literature hardware realized classifiers which are designed to work in low power operation; moreover some of these hardware classifiers do not have custom tunability. So they can only be used for a specific application. The recent developments in electronics technology has created a perfect medium for the hardware realization of classifier structures which, in turn, will render many classifier application prospects feasible in real time. This paper targets the design and application to real world problems of tunable, low power new classifier circuits using CMOS technology. So, a low-power CMOS implementation of a multi-input data classifier with several output levels is presented. The proposed circuit operates in current-mode and can classify several types of analog vector data. An architecture is developed comprising a threshold circuit based on CMOS transistors operating in subthreshold region. To this purpose a one dimensional classifier, called core circuit is proposed. The core circuit also works as a one-dimensional classifier. As this circuit is designed to operate in currentmode the input and the output data is provided to the core circuit with currents. So by interconnecting several core circuits and adding the output currents a multi output classifier can be obtained. Also, combining several core circuits in groups in such a way that each group has identical input current (different from the others), a multi-dimensional, multi-level output classifier can be obtained. Also, numerous efforts in balancing the trade off between power consumption, area and speed have resulted in an acceptable performance. On the other hand, the rapid increasing use of battery operated portable equipment in application areas such as telecommunications and medical electronics increases the importance of low-power and small sized VLSI circuits' technologies. One solution to achieve lowpower and acceptable performance is to operate the transistors in the subthreshold region. The CMOS transistors working in subthreshold region are suitable only for specific applications which need, not very high performance, but low power consumption. The primary aim of this paper is to develop a low power classifier circuit with n inputs and externally tunable decision regions with different output amplitude for each region. Due to the subthreshold operation of the transistors in the proposed core circuit, very low power consumption becomes possible. The proposed core circuit is constructed with two threshold and a subtractor circuit. The SPICE simulation of the threshold circuit, core circuit, one dimensional and two dimensional classifier circuits are given. Using 0.35 μm parameters of AMS CMOS technology, SPICE simulations are performed and a low-power, custom tunable classifier circuit is realized. Because of the parallel processing characteristic of the circuit, it is well suited for real-world applications",health,329,not included
10.1109/syscon.2018.8369547,to_check,2018 Annual IEEE International Systems Conference (SysCon),IEEE,2018-04-26 00:00:00,ieeexplore,an interactive architecture for industrial scale prediction: industry 4.0 adaptation of machine learning,https://ieeexplore.ieee.org/document/8369547/,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",industry,330,included
10.1109/med.2017.7984310,to_check,2017 25th Mediterranean Conference on Control and Automation (MED),IEEE,2017-07-06 00:00:00,ieeexplore,cloud computing for big data analytics in the process control industry,https://ieeexplore.ieee.org/document/7984310/,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0.",industry,331,included
10.1109/icsme.2017.41,to_check,2017 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE,2017-09-22 00:00:00,ieeexplore,predicting and evaluating software model growth in the automotive industry,https://ieeexplore.ieee.org/document/8094464/,"The size of a software artifact influences the software quality and impacts the development process. In industry, when software size exceeds certain thresholds, memory errors accumulate and development tools might not be able to cope anymore, resulting in a lengthy program start up times, failing builds, or memory problems at unpredictable times. Thus, foreseeing critical growth in software modules meets a high demand in industrial practice. Predicting the time when the size grows to the level where maintenance is needed prevents unexpected efforts and helps to spot problematic artifacts before they become critical. Although the amount of prediction approaches in literature is vast, it is unclear how well they fit with prerequisites and expectations from practice. In this paper, we perform an industrial case study at an automotive manufacturer to explore applicability and usability of prediction approaches in practice. In a first step, we collect the most relevant prediction approaches from literature, including both, approaches using statistics and machine learning. Furthermore, we elicit expectations towards predictions from practitioners using a survey and stakeholder workshops. At the same time, we measure software size of 48 software artifacts by mining four years of revision history, resulting in 4,547 data points. In the last step, we assess the applicability of state-of-the-art prediction approaches using the collected data by systematically analyzing how well they fulfill the practitioners' expectations. Our main contribution is a comparison of commonly used prediction approaches in a real world industrial setting while considering stakeholder expectations. We show that the approaches provide significantly different results regarding prediction accuracy and that the statistical approaches fit our data best.",industry,332,not included
10.1109/icrito48877.2020.9198036,to_check,"2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",IEEE,2020-06-05 00:00:00,ieeexplore,state of art: energy efficient protocols for self-powered wireless sensor network in iiot to support industry 4.0,https://ieeexplore.ieee.org/document/9198036/,"Up gradation of manufacturing systems in industries by means of implementing innovative manufacturing techniques that captures real time data, applies machine learning algorithms, makes entire system self-decisive and provides inter connectivity to the whole system is the prime focus of the Industry 4.0. It is aimed at bringing new industrial revolution with the help of internet of things technology due to its considerable influence in the industrial manufacturing process. Though impact of internet of things in industrial sector is huge, a practical implementation incorporates challenges in energy efficiency, self-powered sensor nodes and security. For processing of gathered data self-powered sensor nodes may sinks energy from ambient energy sources. A considerable amount of efforts has been put by researchers to address the challenges for development of energy efficient routing protocol for such nodes. The presented survey is aimed at analyzing the protocols for contribution towards the goals of industry 4.0 Energy efficient protocol will support the system to consume least energy for its operation. Henceforth combination of self-powered wireless sensor network and energy efficient protocol will be useful to power up many industrial IoT applications.",industry,333,not included
10.1109/giots.2018.8534533,to_check,2018 Global Internet of Things Summit (GIoTS),IEEE,2018-06-07 00:00:00,ieeexplore,a car as a semantic web thing: motivation and demonstration,https://ieeexplore.ieee.org/document/8534533/,"Car signal data is usually hard to access, understand and integrate for non automotive domain experts. In this paper, we use semantic technologies for enriching signal data in the automotive industry and access it through Web of Things interactions. This combination allows the access and integration of car data from the web. We built VSSo, a Vehicle Signal ontology based on SOSA/SSN Observations and Actuations, and generated WoT Actions, Events and Properties, enriched with domain metadata. We mapped VSSo to a Web of Things ontology and we developed a Web of Things protocol binding with LwM2M, and made an implementation in a real car. This implementation resulted in a first working prototype, and a number of future improvements required in order to be compliant with automotive standards.",industry,334,not included
10.1109/ssst.1991.138548,to_check,[1991 Proceedings] The Twenty-Third Southeastern Symposium on System Theory,IEEE,1991-03-12 00:00:00,ieeexplore,a neural network based histogramic procedure for fast image segmentation,https://ieeexplore.ieee.org/document/138548/,"The determination of the dimension of a lumber board, the location and extent of surface defects on it, are essential in the construction of a visual inspection station for the lumber industry. The paper presents a neural network based histogramic procedure that performs on the image of a board and can be used to determine the board dimension, the location and extent of surface defects on it, in near real time. The method is based on segmentation of the image based on multiple threshold information derived from a multi-layered neural network. Such a scheme can be applied in general to image analysis and the implementation shows fast processing requiring very little control over the environment. The construction of the network and its training are also discussed.&lt;<ETX>&gt;</ETX>",industry,335,not included
10.1109/caibda53561.2021.00014,to_check,"2021 International Conference on Artificial Intelligence, Big Data and Algorithms (CAIBDA)",IEEE,2021-05-30 00:00:00,ieeexplore,applications of smart energy integrated service platform in optimization of energy utilization of customers,https://ieeexplore.ieee.org/document/9545981/,"In the new era, comprehensive energy service has gradually become an important development direction to promote the high-quality development of energy industry and boost the development of real economy. The construction of smart integrated energy service platform is the internal demand for energy service enterprises to transform into integrated energy service providers, and is a solution to provide comprehensive energy support services for power users. The platform is divided into perception layer, acquisition layer, network layer, platform layer, application layer and display layer from bottom to top. Its main functions include energy regulation, energy efficiency analysis, smart operation, energy trading and smart dispatching. The construction of smart energy comprehensive service platform can provide customers with more high-quality, more convenient and more comprehensive energy value-added services, and constantly create more value for customers. Practice has proved that the implementation of smart energy integrated service platform can greatly reduce the monthly energy cost, monthly power load of the park and monthly coal consumption in park and enhance the indexes of monthly active users and customer satisfaction.",industry,336,not included
10.1109/ijcnn.2016.7727278,to_check,2016 International Joint Conference on Neural Networks (IJCNN),IEEE,2016-07-29 00:00:00,ieeexplore,augmenting adaptation with retrospective model correction for non-stationary regression problems,https://ieeexplore.ieee.org/document/7727278/,"Existing adaptive predictive methods often use multiple adaptive mechanisms as part of their coping strategy in non-stationary environments. We address a scenario when selective deployment of these adaptive mechanisms is possible. In this case, deploying each adaptive mechanism results in different candidate models, and only one of these candidates is chosen to make predictions on the subsequent data. After observing the error of each of candidate, it is possible to revert the current model to the one which had the least error. We call this strategy retrospective model correction. In this work we aim to investigate the benefits of such approach. As a vehicle for the investigation we use an adaptive ensemble method for regression in batch learning mode which employs several adaptive mechanisms to react to changes in the data. Using real world data from the process industry we show empirically that the retrospective model correction is indeed beneficial for the predictive accuracy, especially for the weaker adaptive mechanisms.",industry,337,not included
10.1109/ithet.2012.6246060,to_check,2012 International Conference on Information Technology Based Higher Education and Training (ITHET),IEEE,2012-06-23 00:00:00,ieeexplore,cybernetic education centre: monitoring and control of learner's e-learning study in the field of cybernetics and automation by coloured petri nets model,https://ieeexplore.ieee.org/document/6246060/,"This paper presents the Cybernetic Education Centre (CEDUC) as a hybrid e-learning and training centre for higher education of Cybernetics and Automation fields. If we consider Cybernetics we consider basically (1) controlled systems and (2) control systems. In case of controlled systems learner is focused on the process of analyze, identification, design of the mathematical model and simulation of the controlled system. Therefore this paper deals with controlled models in the laboratories of our department (a) real laboratory models, (b) simulation models and (c) virtual models which creates one integrated hybrid architecture what represents one of new ideas in the paper (Fig. 1). Learner of control systems is focused mainly on design of control parameters, design of control algorithms, design of hardware, software and communication architectures of control systems. Overall control system is represented by Distributed Control System (DCS) which serves for learners to verify designed control systems. The verification of the control systems is very important from safety point of view to prepare learners for real production conditions. Second new idea of the paper is implementation of the Coloured Petri Nets as automata to control access to the resources (not only typical study resources but also access to the components of hybrid DCS architecture) as well as to monitor the learner activities during the study. CEDUC is supported by intensive industry-university partnership. Conclusion of the paper summarizes the results of the study process of learners in DCS environment.",industry,338,not included
10.1109/ipdps.2018.00068,to_check,2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS),IEEE,2018-05-25 00:00:00,ieeexplore,do developers understand ieee floating point?,https://ieeexplore.ieee.org/document/8425212/,"Floating point arithmetic, as specified in the IEEE standard, is used extensively in programs for science and engineering. This use is expanding rapidly into other domains, for example with the growing application of machine learning everywhere. While floating point arithmetic often appears to be arithmetic using real numbers, or at least numbers in scientific notation, it actually has a wide range of gotchas. Compiler and hardware implementations of floating point inject additional surprises. This complexity is only increasing as different levels of precision are becoming more common and there are even proposals to automatically reduce program precision (reducing power/energy and increasing performance) when results are deemed """"good enough.'"""" Are software developers who depend on floating point aware of these issues? Do they understand how floating point can bite them? To find out, we conducted an anonymous study of different groups from academia, national labs, and industry. The participants in our sample did only slightly better than chance in correctly identifying key unusual behaviors of the floating point standard, and poorly understood which compiler and architectural optimizations were non-standard. These surprising results and others strongly suggest caution in the face of the expanding complexity and use of floating point arithmetic.",industry,339,not included
10.1109/bigdata.2017.8258076,to_check,2017 IEEE International Conference on Big Data (Big Data),IEEE,2017-12-14 00:00:00,ieeexplore,empirical evaluations of active learning strategies in legal document review,https://ieeexplore.ieee.org/document/8258076/,"One type of machine learning, text classification, is now regularly applied in the legal matters involving voluminous document populations because it can reduce the time and expense associated with the review of those documents. One form of machine learning - Active Learning - has drawn attention from the legal community because it offers the potential to make the machine learning process even more effective. Active Learning, applied to legal documents, is considered a new technology in the legal domain and is continuously applied to all documents in a legal matter until an insignificant number of relevant documents are left for review. This implementation is slightly different than traditional implementations of Active Learning where the process stops once achieving acceptable model performance. The purpose of this paper is twofold: (i) to question whether Active Learning actually is a superior learning methodology and (ii) to highlight the ways that Active Learning can be most effectively applied to real legal industry data. Unlike other studies, our experiments were performed against large data sets taken from recent, real-world legal matters covering a variety of areas. We conclude that, although these experiments show the Active Learning strategy popularly used in legal document review can quickly identify informative training documents, it becomes less effective over time. In particular, our findings suggest this most popular form of Active Learning in the legal arena, where the highest-scoring documents are selected as training examples, is in fact not the most efficient approach in most instances. Ultimately, a different Active Learning strategy may be best suited to initiate the predictive modeling process but not to continue through the entire document review.",industry,340,not included
10.1109/cdc.2004.1428748,to_check,2004 43rd IEEE Conference on Decision and Control (CDC) (IEEE Cat. No.04CH37601),IEEE,2004-12-17 00:00:00,ieeexplore,grey-box modelling of a motorcycle shock absorber,https://ieeexplore.ieee.org/document/1428748/,"There is an increasing use of virtual prototyping tools in the motorcycle industry, aimed at reducing the development time of new models and speeding up performance optimization, by providing the designer with an in-laboratory virtual test track. Virtual prototyping software is essentially multi body simulation software that requires the availability of models of all the vehicle components. The choice of the model is then of paramount importance, since it heavily affects the accuracy and reliability of the simulation results. Conventional models (like linear models) are often inadequate to describe the behavior of complex nonlinear components, so that it is necessary to appeal to different modelling approaches. This is actually the case when dealing with motorcycle suspension systems, given that their most critical part, the shock absorber, exhibits nonlinear and time-variant behavior. In this paper, a grey-box model of a racing motorcycle mono tube shock absorber is proposed. It consists of a nonlinear parametric model and a black-box, neural network based model. The absorber model has been implemented in a numerical simulation environment, and it has been validated against experimental test data. The results of the validation show that the model is able to reproduce the real behavior of the shock absorber with an accuracy that matches or even beats that of other models previously presented in the literature. The interfacing of the proposed model to the ADAMS virtual prototyping environment is also discussed.",industry,341,not included
10.1109/rtc.2012.6418168,to_check,2012 18th IEEE-NPSS Real Time Conference,IEEE,2012-06-15 00:00:00,ieeexplore,implementation of the disruption predictor apodis in jet real time network using the marte framework,https://ieeexplore.ieee.org/document/6418168/,"The evolution in the past years of Machine learning techniques, as well as the technological evolution of computer architectures and operating systems, are enabling new approaches for complex problems in different areas of industry and research, where a classical approach is nonviable due to lack of knowledge of the problem's nature. A typical example of this situation is the prediction of plasma disruptions in Tokamak devices. This paper shows the implementation of a real time disruption predictor. The predictor is based on a support vector machine (SVM). The implementation was done under the MARTe framework on a six core x86 architecture. The system is connected in JET's Real time Data Network (RTDN). Online results show a high degree of successful predictions and a low rate of false alarms thus, confirming its usefulness in a disruption mitigation scheme. The implementation shows a low computational load, which in an immediate future will be exploited to increase the prediction's temporal resolution.",industry,342,not included
10.1109/isgt.2016.7781159,to_check,2016 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT),IEEE,2016-09-09 00:00:00,ieeexplore,large-scale detection of non-technical losses in imbalanced data sets,https://ieeexplore.ieee.org/document/7781159/,"Non-technical losses (NTL) such as electricity theft cause significant harm to our economies, as in some countries they may range up to 40% of the total electricity distributed. Detecting NTLs requires costly on-site inspections. Accurate prediction of NTLs for customers using machine learning is therefore crucial. To date, related research largely ignore that the two classes of regular and non-regular customers are highly imbalanced, that NTL proportions may change and mostly consider small data sets, often not allowing to deploy the results in production. In this paper, we present a comprehensive approach to assess three NTL detection models for different NTL proportions in large real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and Support Vector Machine. This work has resulted in appreciable results that are about to be deployed in a leading industry solution. We believe that the considerations and observations made in this contribution are necessary for future smart meter research in order to report their effectiveness on imbalanced and large real world data sets.",industry,343,not included
10.1109/ijcnn.2015.7280779,to_check,2015 International Joint Conference on Neural Networks (IJCNN),IEEE,2015-07-17 00:00:00,ieeexplore,on sequences of different adaptive mechanisms in non-stationary regression problems,https://ieeexplore.ieee.org/document/7280779/,"Existing adaptive predictive methods often use multiple adaptive mechanisms as part of their coping strategy in non-stationary environments. These mechanisms are usually deployed in a prescribed order which does not change. In this work we investigate and provide a comparative analysis of the effects of using a flexible order of adaptive mechanisms' deployment resulting in varying adaptation sequences. As a vehicle for this comparison, we use an adaptive ensemble method for regression in batch learning mode which employs several adaptive mechanisms to react to the changes in data. Using real world data from the process industry we demonstrate that such flexible deployment of available adaptive methods embedded in a cross-validatory framework can benefit the predictive accuracy over time.",industry,344,not included
10.1109/iccairo.2017.61,to_check,"2017 International Conference on Control, Artificial Intelligence, Robotics & Optimization (ICCAIRO)",IEEE,2017-05-22 00:00:00,ieeexplore,tuning software based on genetic algorithm applied to industrial pid loops,https://ieeexplore.ieee.org/document/8253004/,"Time-delay processes are frequently found in industry and the most common representations are first order plus delay time (FOPDT) and integrator plus delay time (IPDT) transfer functions. The identification of time delay systems is a challenging task and usually, the performance of its control is not optimal. This work presents a software based on a real coded genetic algorithm to identify the system, using open or close loop information, and to tune the PID controller using several methods. Results on simulation and real industrial loops are presented.",industry,345,not included
10.1109/ithet.2012.6246058,to_check,2012 International Conference on Information Technology Based Higher Education and Training (ITHET),IEEE,2012-06-23 00:00:00,ieeexplore,virtual industrial training: joining innovative interfaces with plant modeling,https://ieeexplore.ieee.org/document/6246058/,"Training in industry is one of the most critical and expensive tasks to be faced by the management. Furthermore, in some cases, it is dangerous or even impossible to directly train operators on the real plants where security and safety problems may arise, making it very difficult to start training programs at low cost. For these reasons, the field of training in industry is rapidly developing using software or hardware solutions coming mainly from the following research areas: i) Human-Computer interaction, i.e., the use of complex and interactive human-machine interfaces, ii) plant simulators, i.e., software systems which are delivered with the plant itself to test and to learn complex tasks and processes, iii) Intelligent Training Systems, i.e., the availability of intelligent and personalized training systems where a virtual tutor guides users through a personalized learning path. In this paper we present the overall architecture of a system for industrial training, embedded into an Intelligent Tutoring System that can provide more effective and personalized training and learning in a context where working directly on real plants can be difficult and very expensive. In particular we present a simulator for training operators in using power plants, based on a multimedia and on interactive interface. This system is particularly suitable to be used for training in industrial electric and oil plants. Moreover, the system allows operators for collaborative problem solving. Currently the system is under delivery to an Italian Electric industry.",industry,346,not included
10.1109/infcom.2009.5062082,to_check,IEEE INFOCOM 2009,IEEE,2009-04-25 00:00:00,ieeexplore,"wi-sh: a simple, robust credit based wi-fi community network",https://ieeexplore.ieee.org/document/5062082/,"Wireless community networks, where users share wireless bandwidth is attracting tremendous interest from academia and industry. Companies such as FON have been successful in attracting large communities of users. However, solutions such as FON either require users to buy specialized FON routers or firmware modifications to existing routers. In this paper we propose a solution which requires no such sophisticated hardware. An alternative is to provide a solution which requires users to download a client software on to their PCs. While the solution appears simple it raises several issues of incentivizing users to share their bandwidth and also issues of preventing users from cheating behaviors which give them an unfair advantage. In this paper, we propose a system and solution which (i) requires only software downloads on PCs, (ii) is robust to tampering of the software, and intermittent monitoring of an access point by the owner, (iii) a credit based mechanism whereby users earn credits for sharing bandwidth and punishment and pricing mechanism whereby users are charged at a higher price whenever they are caught misbehaving. By making simple but plausible assumptions about user behavior, we show via analysis and extensive simulations that the system converges to a Pareto optimal Nash equilibrium. We further validate our system model, by running trace driven simulations on real world data. We believe that the solution provided by Wi-Sh is an attractive and more credible alternative to solutions such as FON.",industry,347,not included
10.1109/tpwrs.2019.2922333,to_check,IEEE Transactions on Power Systems,IEEE,2019-11-01 00:00:00,ieeexplore,a data-driven framework for assessing cold load pick-up demand in service restoration,https://ieeexplore.ieee.org/document/8735925/,"Cold load pick-up (CLPU) has been a critical concern to utilities. Researchers and industry practitioners have underlined the impact of CLPU on distribution system design and service restoration. The recent large-scale deployment of smart meters has provided the industry with a huge amount of data that are highly granular, both temporally and spatially. In this paper, a data-driven framework is proposed for assessing CLPU demand of residential customers using smart meter data. The proposed framework consists of two interconnected layers: 1) At the feeder level, a nonlinear autoregression model is applied to estimate the diversified demand during the system restoration and calculate the CLPU demand ratio. 2) At the customer level, Gaussian mixture models and probabilistic reasoning are used to quantify the CLPU demand increase. The proposed methodology has been verified using real smart meter data and outage cases.",industry,348,not included
10.1109/tr.2019.2895462,to_check,IEEE Transactions on Reliability,IEEE,2020-03-01 00:00:00,ieeexplore,a novel class-imbalance learning approach for both within-project and cross-project defect prediction,https://ieeexplore.ieee.org/document/8648214/,"Software defect prediction (SDP) is an available way to enhance test efficiency and guarantee software reliability. However, there are more clean instances than defective instances in real software projects, and this results in severe class distribution skews and gets the poor performance of classifiers. So solving the class-imbalance problem in SDP has attracted growing attention from industry and academia in software engineering. In this paper, we propose a novel class-imbalance learning approach for both within-project and cross-project class-imbalance problem. We utilize the thought of stratification embedded in nearest neighbor (STr-NN) to produce evolving training datasets with balanced data. For within-project, we directly employ the STr-NN approach for defect prediction. For cross-project, we first introduce transfer component analysis to mitigate the distribution differences between source and target dataset, and then employ the STr-NN approach on the transferred data. We conduct experiments on PROMISE and NASA datasets using ensemble learning based on weight vote. Experimental results indicate that our approach has higher area under curve (AUC), Recall and comparable probability of a false alarm (pf), and F-measure than some existing methods for the class-imbalance problem.",industry,349,not included
10.1109/59.589783,to_check,IEEE Transactions on Power Systems,IEEE,1997-05-01 00:00:00,ieeexplore,security boundary visualization for systems operation,https://ieeexplore.ieee.org/document/589783/,"This paper presents a security assessment approach for operational planning studies that provides the operator with accurate boundary visualization in terms of easily monitored precontingency information. The approach is modeled after traditional security assessment procedures which result in use of a nomogram for characterizing the security boundaries; these procedures are common among many North American utilities today. Therefore, the approach builds on what is already familiar in the industry, but it takes advantage of computer automation and neural networks for generating and understanding large databases. The appeal of the approach is threefold: it provides increased accuracy in boundary representation, it reduces the labor traditionally required in generating security boundaries, and the resulting boundaries, encoded in fast, flexible C subroutines, can be integrated into energy management system software to provide the operator with compact, understandable boundary illustration in real time. These improvements are of particular interest in securely operating transmission systems close to their limits so as to fully utilize existing facilities.",industry,350,not included
10.1109/access.2021.3082934,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,visual product tracking system using siamese neural networks,https://ieeexplore.ieee.org/document/9439511/,"Management of unstructured production data is a key challenge for Industry 4.0. Effective product tracking endorses data integration and productivity improvements throughout the manufacturing processes. Radio-frequency identification (RFID) tags are used in many tracking cases, but in some manufacturing environments, those cannot be used as they might get damaged or removed during processing. In this paper, we propose an alternative visual product tracking system. The physical system uses two cameras placed at the two ends of the tracked process(es). Product pairs are then matched with a Siamese neural network operating on the product images and trained offline on the problem at hand with labeled data. The proposed system can track products solely based on their visual appearance and without any physical interference with the products or production processes. Unlike other existing image-based methods, the proposed system is invariant to major positional and visual changes in the products. As a proof-of-concept, we tested the proposed system with real plywood factory data and were able to track the products with 98.5 % accuracy in a realistic test scenario. The implementation of the proposed method and the Veneer21 dataset are publicly available at https://github.com/TuomasJalonen/visual-product-tracking-system.",industry,351,not included
10.1109/tai.2021.3057027,to_check,IEEE Transactions on Artificial Intelligence,IEEE,2020-12-01 00:00:00,ieeexplore,zju-leaper: a benchmark dataset for fabric defect detection and a comparative study,https://ieeexplore.ieee.org/document/9346038/,"Fabric inspection plays an important role in the process of quality control in textile manufacturing. There is a growing demand in the textile industry to leverage computer vision technology for more efficient quality control in the hope that it will replace the traditional labor-intensive inspection by naked eyes. However, there is an underlying viewpoint in most existing fabric datasets that automatic defect detection is a traditional image classification problem, thus more samples help better, which lacks enough consideration about the problem itself and real application environments. After deep communication with users, we find these facts that an assembly line usually has only a few fixed texture fabrics for a long period, users prefer fast deployment and easily upgradable model to a general model and long-time tuning, and users hope the process of collecting samples, annotating, and deployment affects assembly lines as little as possible. This implies that defect detection is different from popular deep learning problems. Multiple-stage models and fast training become more attractive since users are able to train and deploy models by themselves according to the real conditions of samples that can be obtained. Based on this analysis, we propose a new fabric dataset “ZJU-Leaper”. It provides a series of task settings in accordance with the progressive strategy dealing with the problem, from only normal samples to many defective samples with precise annotations, to facilitate real-world application. To avoid misleading information and inconsistency issues associated with the prior evaluation metrics, we propose a new evaluation protocol by experimental analysis of task-specific indexes, which can tell a truthful comparison between different inspection methods. We also offer some novel solutions to address the new challenges of our dataset, as part of the baseline experiments. It is our hope that ZJU-Leaper can accelerate the research of automated visual inspection and empower the practitioners with more opportunities for manufacturing automation in the textile industry. <p><italic>Impact Statement</italic>—Automatic defect inspection is very important in quality control of the fabric industry by helping manufacturers to identify production problems early, hence improving product quality and production efficiency. Meanwhile, it is able to reduce the high labor cost of manual inspection and boost the productivity of the textile industry. To develop effective mathematical inspection algorithms, the fabric dataset serves as an indispensable component to present a practical application environment and enable fair evaluation for algorithms. This paper proposes a new dataset, called “ZJU-Leaper” designed from a viewpoint of multiple-stage models and fast training, containing threefold novelty: 1) the data collection and organization consider the actual requirements and special characteristics of assembly lines in textile factories; 2) it has several designed task settings in order to meet the different levels of requirements in the practical inspection task; 3) it provides a reasonable evaluation protocol for comprehensive comparisons between different inspection algorithms. The preliminary experiments show that some existing algorithms still cannot reach the satisfying performance by this benchmark, which implies more effort should be made to develop new methods for the real use of automatic defect inspection.</p>",industry,352,not included
10.23919/iconac.2019.8895095,to_check,2019 25th International Conference on Automation and Computing (ICAC),IEEE,2019-09-07 00:00:00,ieeexplore,ant colony optimization algorithm for industrial robot programming in a digital twin,https://ieeexplore.ieee.org/document/8895095/,"Advanced manufacturing that is adaptable to constantly changing product designs often requires dynamic changes on the factory floor to enable manufacture. The integration of robotic manufacture with machine learning approaches offers the possibility to enable such dynamic changes on the factory floor. While ensuring safety and the possibility of losses of components and waste of material are against their usage. Furthermore, developments in design of virtual environments makes it possible to perform simulations in a virtual environment, to enable human-in-the-loop production of parts correctly the first time like never before. Such powerful simulation and control software provides the means to design a digital twin of manufacturing environment in which trials are completed at almost at no cost. In this paper, ant colony optimization is used to program an industrial robot to avoid obstacles and find its way to pick and place objects during an assembly task in an environment containing obstacles that must be avoided. The optimization is completed in a digital twin environment first and movements transferred to the real robot after human inspection. It is shown that the proposed methodology can find the optimal solution, in addition to avoiding collisions, for an assembly task with minimum human intervention.",industry,353,included
10.1109/tencon.2004.1414983,to_check,2004 IEEE Region 10 Conference TENCON 2004.,IEEE,2004-11-24 00:00:00,ieeexplore,certain studies on sample time for a predictive fuzzy logic controller through real time implementation of phenol-formaldehyde manufacturing,https://ieeexplore.ieee.org/document/1414983/,"In polymer manufacturing industries, the automation and control of chemical process incorporating techniques of fuzzy control neural networks, and expert systems had lead to a more secured and stable operation. A sudden and unpredictable heat is often produced by the nonlinear exothermal reaction when phenol and formaldehyde are mixed together. Therefore, the polymerization process has to be controlled with a high level of precision in order to avoid temperature run-away. This paper proposes a design methodology for a sensor based process control system. The duration of ON and OFF time of certain relays are the parameters to be controlled in order to keep the exothermic reaction under control The universe of discourse for the output of the FLC system is the sample time that assigned to the relays where maximum time for heater or valve can be turned on before the next action is applied This paper discusses a detailed real time implementation of the exothermal process control using Matlab-fuzzy logic toolbox. An enhanced predictive FLC structure is developed and compared to a predictive FLC control structure. The obtained practical results thus ensure that the predictive FLC can be enhanced by modifying the rules and the membership Junctions of the universe of discourse, which is proved to be better in controlling the reaction temperature.",industry,354,not included
10.1109/aqtr.2018.8402748,to_check,"2018 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",IEEE,2018-05-26 00:00:00,ieeexplore,time series forecasting for dynamic scheduling of manufacturing processes,https://ieeexplore.ieee.org/document/8402748/,"Manufacturing control systems evolved in the recent decades from pre-programmed rigid systems to adaptable, data driven, cloud based implementations, capable to respond to environment changes and new requirements in real time. A byproduct of this transformation is represented by large amounts of structured and semi-structured information, both historical and real-time data that is made available on various layers of the system. This accumulation of information brings the opportunity to move from the rule based decision making algorithms used traditionally by these control systems towards more intelligent approaches, driven by modern deep learning mechanisms. This paper proposes a time series forecasting model using recursive neural networks (RNN) for operation scheduling and sequencing in a virtual shop floor environment. The time series aspect of the RNN is novel in manufacturing domain, in the sense that the new best prediction produced considers the previous decisions and outcomes. The proposed implementation explains how the RNN can be mapped to the specifics of a manufacturing control system and introduces a bidding mechanism to allow dynamic evaluation of individual forecasts. The pilot implementation, initial experiments on sample data sets and results presented show how using recursive neural networks can optimize resource utilization and energy consumption.",industry,355,not included
10.1109/jiot.2019.2940131,to_check,IEEE Internet of Things Journal,IEEE,2019-12-01 00:00:00,ieeexplore,a two-stage transfer learning-based deep learning approach for production progress prediction in iot-enabled manufacturing,https://ieeexplore.ieee.org/document/8827506/,"In make-to-order manufacturing enterprises, accurate production progress (PP) prediction is an important basis for dynamic production process optimization and on-time delivery of orders. The implementation of Internet of Things (IoT) makes it possible to take real-time production state as an important factor affecting PP. In the IoT-enabled workshop, a two-stage transfer learning-based prediction method using both historical production data and real-time state data is proposed to solve the problem of low-prediction accuracy and poor generalization performance caused by insufficient data of target order. The deep autoencoder (DAE) model with transfer learning is designed to extract the generalized features of target order in the first stage, which uses bootstrap sampling to avoid over fitting. The deep belief network (DBN) model with transfer learning is constructed to fit the nonlinear relation for PP prediction in the second stage. A real case from an IoT enabled machining workshop is taken to validate the performance of the proposed method over the other methods such as DBN, deep neural network.",industry,356,included
10.1109/tase.2020.3044107,to_check,IEEE Transactions on Automation Science and Engineering,IEEE,2021-04-01 00:00:00,ieeexplore,an online policy for energy-efficient state control of manufacturing equipment,https://ieeexplore.ieee.org/document/9308932/,"Machine state control is one of the most promising energy-efficient measures for machining processes. A proper control reduces the energy consumed during idle periods by switching off/on the machines. A critical barrier for practical implementation is related to the knowledge of part arrival process that is affected by uncertainty. The stochastic processes involved in the system are usually assumed to be known. However, real production environments are subject to several sources of randomness that are difficult to model a priori. This work provides an online time-based algorithm that is able to control the machine state. Through a method for the estimation of the stochastic process, the algorithm provides the optimal control parameters based on a collected set of observations. A new policy is formulated to manage the control over time such that changes in the control parameters are applied only under certain conditions. Potential benefits are discussed using realistic numerical cases. Note to Practitioners-This article analyzes the control problem of switching off/on a machine tool for energy saving during machine idle periods. A control policy based on time information is investigated when the machine requires a startup time to resume the service after being switched off. The proposed policy works online while acquiring information from the real system. An algorithm is described for identifying and applying the optimal control parameters. The results of this research will be useful for a practical implementation of a switching policy for energy saving. This implementation requires the estimation of the power adsorbed by the machine in four different states and, therefore, it reduces the implementation effort for practitioners.",industry,357,not included
10.1109/ecai52376.2021.9515185,to_check,"2021 13th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",IEEE,2021-07-03 00:00:00,ieeexplore,automated ceramic plate defect detection using scaledyolov4-large,https://ieeexplore.ieee.org/document/9515185/,"Automated visual inspection has become a popular topic of research in the last couple of decades, as computation power available grew exponentially. Judging by the fact that visual inspection is a critical task for the quality of the products, it would be highly recommended that people only supervise the system. This paper proposes a low cost, rapid development defect detection system based on the Scaled-YOLOv4 object detection model. The original model achieves almost state of the art detection mAP on the COCO dataset with a mAP(mean average precision) of 56.0 for the largest variant, named YOLOv4-P7. Our version is derived from the ScaledYOLOv4-P5 model and is trained on ceramic plate defects and achieves 87.4 mAP at a intersection of union of 0.5, while comfortably processing a frame in 20ms on a consumer RTX3070 GPU. Thus, the real time constraint for the manufacturing system is fulfilled. Hence, the critical aspects of the development process are the: quick development process, fast training, rapid deployment on the factory floor, quick validation and feedback, using images acquired in the lab - not on the factory floor for first iteration and overall low cost of the automated inspection system.",industry,358,not included
10.1109/infoct.2018.8356831,to_check,2018 International Conference on Information and Computer Technologies (ICICT),IEEE,2018-03-25 00:00:00,ieeexplore,fault class prediction in unsupervised learning using model-based clustering approach,https://ieeexplore.ieee.org/document/8356831/,"Manufacturing industries have been on a steady path considering for new methods to achieve near-zero downtime to have flexibility in the manufacturing process and being economical. In the last decade with the availability of industrial internet of things (IIoT) devices, this has made it possible to monitor the machine continuously using wireless sensors, assess the degradation and predict the failures of time. Condition-based predictive maintenance has made a significant influence in monitoring the asset and predicting the failure of time. This has minimized the impact on production, quality, and maintenance cost. Numerous approaches have been in proposed over the years and implemented in supervised learning. In this paper, challenges of supervised learning such as need for historical data and incapable of classifying new faults accurately will be overcome with a new methodology using unsupervised learning for rapid implementation of predictive maintenance activity which includes fault prediction and fault class detection for known and unknown faults using density estimation via Gaussian Mixture Model Clustering and K-means algorithm and compare their results with a real case vibration data.",industry,359,not included
10.1109/iembs.1988.95233,to_check,Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society,IEEE,1988-11-07 00:00:00,ieeexplore,new technical approaches to monitoring and interpreting the dynamics of real neural networks,https://ieeexplore.ieee.org/document/95233/,"Understanding the functional organization of large-scale real neural networks will require the development of techniques capable of predicting, recording, and interpreting the activities of the very large numbers of neurons that participate in their operation. This presents a considerable technical problem for systems-level neurobiologists and associated biomedical engineers. The authors describe their recent work using silicon manufacturing techniques to make multineuron in vivo recording devices, as well as the software tools developed for realistically simulating the neural networks studied.&lt;<ETX>&gt;</ETX>",industry,360,not included
10.1109/ai4i46381.2019.00027,to_check,2019 Second International Conference on Artificial Intelligence for Industries (AI4I),IEEE,2019-09-27 00:00:00,ieeexplore,reinforcement learning of a robot cell control logic using a software-in-the-loop simulation as environment,https://ieeexplore.ieee.org/document/9027783/,"This paper introduces a method for automatic robot programming of industrial robots using reinforcement learning on a Software-in-the-loop simulation. The focus of the the paper is on the higher levels of a hierarchical robot programming problem. While the lower levels the skills are stored as domain specific program code, the combination of the skills into a robot control program to solve a specific task is automated. The reinforcement learning learning approach allows the shopfloor workers and technicians just to define the end result of the manufacturing process through a reward function. The programming and process optimization is done within the learning procedure. The Software-in-the-loop simulation with the robot control software makes it possible to to interpret the real program code and generate the exact motion. The exact motion of the robot is needed in order to find not just an optimal but also a collision-free policy.",industry,361,not included
10.1109/ro-man50785.2021.9515431,to_check,2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),IEEE,2021-08-12 00:00:00,ieeexplore,simplifying the a.i. planning modeling for human-robot collaboration,https://ieeexplore.ieee.org/document/9515431/,"For an effective deployment in manufacturing, Collaborative Robots should be capable of adapting their behavior to the state of the environment and to keep the user safe and engaged during the interaction. Artificial Intelligence (AI) enables robots to autonomously operate understanding the environment, planning their tasks and acting to achieve some given goals. However, the effective deployment of AI technologies in real industrial environments is not straightforward. There is a need for engineering tools facilitating communication and interaction between AI engineers and Domain experts. This paper proposes a novel software tool, called TENANT (Tool fostEriNg Ai plaNning in roboTics) whose aim is to facilitate the use of AI planning technologies by providing domain experts like e.g., production engineers, with a graphical software framework to synthesize AI planning models abstracting from syntactic features of the underlying planning formalism.",industry,362,included
10.1109/sai.2015.7237244,to_check,2015 Science and Information Conference (SAI),IEEE,2015-07-30 00:00:00,ieeexplore,object oriented eco-simulator system as predictor &amp; exploratory system to track impact of human induced activities on environmental resource: a case study: evaluation of unsuitable ground water consumption of petrochemical and power factory of shazand on ecological status of markazi province,https://ieeexplore.ieee.org/document/7237244/,"Environment system studies regardless of main reasons are very difficult whereas they have been constructed from natural ecosystems, manmade infrastructure, socio-economic virtual objects and also their perplexing inner relations. Furthermore, ecosystems as any other elements, have iterative multi-scale structure which makes it even more sophisticated to study. That's why real compound system studies look breathtaking and also too difficult. Gradual depletion of perched water tables in “Shazand” watershed of “Markazi” province in Iran, which mainly stems from bulky extraction of water resources by huge factories and intensive agriculture e.g., doesn't treated well by policy makers not only due to lack of well-based studies but also via devoid of potent monitoring and prediction simulator system to manifest decision side effects to managers and those whom are responsible to keep the environment system balanced. Although GI functionalities have exhibited wonder abilities to handle spatial problems, they seem weak in processing ecosystem real problems as there is few specified environment modules. While combination of GIS and environment modeling software intensively suffers from integration issue, simulator software are mainly confined due to filed limitation, non-spatial functionalities and spatial ontologies. To present a proper solution which makes up the mentioned deficiencies, first of all, a problem tree was designed to decompose real cybernetic correlations between “Shazand” watershed elements including natural or manmade ones. Then, object oriented concept was deployed to simulate sophisticated real environmental systems and also to simplify programming issues regarding to exact spatial ontology. To complete the solution, a category based on “Category Theorem” was defined to support temporal concepts on the backdrop of real ecosystem phenomena regarding Spatio-temporal modeling concepts and object oriented foundations. Likewise group of functors were designed via functional language to allay process burden and also programming of object counteractions. Emerging spatial simulator system, supports monitoring issues as well as prediction missions in compound environment systems. It has been also indicated how to use it as an exploratory analyzing system which conduct real analytical system consisted of virtual smart analytical objects.",industry,363,not included
10.1109/icit.2006.372319,to_check,2006 IEEE International Conference on Industrial Technology,IEEE,2006-12-17 00:00:00,ieeexplore,real time classifier for industrial wireless sensor network using neural networks with wavelet preprocessors,https://ieeexplore.ieee.org/document/4237641/,"Wireless sensor node is embedded of computation unit, sensing unit and a radio unit for communication. Amongst three units communication is the largest consumer of energy. Energy is the prime source for wireless sensor node to function. Hence every aspects of sensor node are designed with energy constraints. Neural Networks in particular the combination of ART1 and FuzzyART(FA) can be used very efficiently for developing Real time Classifier. Wireless sensor networks demand for the real time classification of sensor data. In this paper classification technique using ART1 and Fuzzy ART is discussed. ART1 and FA have very good architectural strategy, which makes it simple for VLSI implementation. The VLSI implementation of the proposed classifier can be a part of embedded microsensor. The paper discusses classification technique, which can reduce the energy need for communication and improves communications bandwidth. The proposed sensor clustering architecture can give distributed storage space for the sensor networks. Wavelet Transform is used as preprocessor for denoising the real word data from sensor node, this makes it much suitable for industrial environment. Many methods of wavelet transforms are available. Simplest Haar 1D transform is used for preprocessing and smoothing the sensor signals. The discrete wavelet transform implemented here helps to extract important feature in the sensor data like sudden changes at various scales.",industry,364,not included
10.1109/iecon.2016.7793206,to_check,IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society,IEEE,2016-10-26 00:00:00,ieeexplore,summer school on intelligent agents in automation: hands-on educational experience on deploying industrial agents,https://ieeexplore.ieee.org/document/7793206/,"Cyber-physical systems constitutes a framework to develop intelligent, distributed, resilient, collaborative and cooperative systems, promoting the fusion of computational entities and physical devices. Agent technology plays a crucial role to develop this kind of systems by offering a decentralized, distributed, modular, robust and reconfigurable control structure. This paper describes the implementation of a summer school aiming to enhance the participants' knowledge in the field of multi-agent systems applied to industrial environments, being able to gain the necessary theoretical and practical skills to develop real industrial agent based applications. This is accomplished in an international framework where individual knowledge and experiences are shared in a complementary level.",industry,365,not included
10.1109/rtsi50628.2021.9597339,to_check,2021 IEEE 6th International Forum on Research and Technology for Society and Industry (RTSI),IEEE,2021-09-09 00:00:00,ieeexplore,towards graph machine learning for smart grid knowledge graphs in industrial scenarios,https://ieeexplore.ieee.org/document/9597339/,"Knowledge Graphs (KGs) demonstrated promising application perspective in different scenarios, especially when combined with Graph Machine Learning (GML) techniques able to interpret and infer over facts. Given the natural network structures of Smart Grid equipment and the exponential growth of electric power data, Smart Grid Knowledge Graphs (SGKGs) provides unprecedented opportunities to manage massive power resources and provide intelligent applications. However, a single representation of the SGKGs is never sufficient to properly exploit GML techniques that leverage different aspects of the KG for various objectives. In this work, we provide a methodology to extract various significant views of the SGKG by iteratively applying a series of transformation to the description of the power network in the IEC CIM standard. Our implementation is based on a declarative approach to guarantee easier portability, and we deploy the transformations as a stateless microservice, facilitating modular integration with the rest of the Smart Grid Semantic Platform. Experimental evaluation on two real power distribution networks demonstrates the efficacy of our approach in highlighting important topological information, without discarding precious additional knowledge present in the SGKG.",industry,366,included
10.1109/fdl53530.2021.9568376,to_check,2021 Forum on specification & Design Languages (FDL),IEEE,2021-09-10 00:00:00,ieeexplore,a container-based design methodology for robotic applications on kubernetes edge-cloud architectures,https://ieeexplore.ieee.org/document/9568376/,"Programming modern Robots' missions and behavior has become a very challenging task. The always increasing level of autonomy of such platforms requires the integration of multi-domain software applications to implement artificial intelligence, cognition, and human-robot/robot-robot interaction applications. In addition, to satisfy both functional and nonfunctional requirements such as reliability and energy efficiency, robotic SW applications have to be properly developed to take advantage of heterogeneous (Edge-Fog-Cloud) architectures. In this context, containerization and orchestration are becoming a standard practice as they allow for better information flow among different network levels as well as increased modularity in the use of software components. Nevertheless, the adoption of such a practice along the design flow, from simulation to the deployment of complex robotic applications by addressing the de-facto development standards (i.e., robotic operating system - ROS - compliancy for robotic applications) is still an open problem. We present a design methodology based on Docker and Kubernetes that enables containerization and orchestration of ROS-based robotic SW applications for heterogeneous and hierarchical HW architectures. The design methodology allows for (i) integration and verification of multi-domain components since early in the design flow, (ii) task-to-container mapping techniques to guarantee minimum overhead in terms of performance and memory footprint, and (iii) multi-domain verification of functional and non-functional constraints before deployment. We present the results obtained in a real case of study, in which the design methodology has been applied to program the mission of a Robotnik RB-Kairos mobile robot in an industrial agile production chain. The source code of the mobile robot is publicly available on GitHub.",industry,367,included
10.1109/aiam48774.2019.00157,to_check,2019 International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM),IEEE,2019-10-18 00:00:00,ieeexplore,a digital twin-based approach for quality control and optimization of complex product assembly,https://ieeexplore.ieee.org/document/8950866/,"To address the problems caused by low ability of quality analysis and decision-making in the process of complex product assembly, in this paper, we proposed a digital twin-based approach for quality control and optimization of complex product assembly, by providing a digital twin system to realize the timely and precisely interactive mapping between the physical world and digital world. Specifically, a quality control and optimization mechanism is presented, which provides the theoretical support to the realization of the digital twin-based approach. A data-driven quality control model is introduced to solve the optimization problem by considering the panoramic assembly quality data. A digital twin system for complex product assembly is elaborated by providing detailed deployment and implementation procedures, which includes (1) building of the digital entity of an assembly line, (2) real-time online sensing in multi-source heterogeneous environment, (3) real-time simulation of equipment and assembly process, (4) realization of the intelligent production scheduling under uncertainty conditions, and (5) dynamical adjustment of the assembly process. Finally, the paper presents the validation results considering the practical applications of the proposed approach in real industrial fields.",industry,368,included
10.1109/ants47819.2019.9117981,to_check,2019 IEEE International Conference on Advanced Networks and Telecommunications Systems (ANTS),IEEE,2019-12-19 00:00:00,ieeexplore,an sd-wan controller for delay jitter minimization in coded multi-access systems,https://ieeexplore.ieee.org/document/9117981/,"For an industrial network that is expected to perform with high reliability and availability, we consider the problem of designing a low-cost approach to continuous monitoring and maintenance of the communication links. A software defined wide area network (SD-WAN) based solution is proposed to adaptively monitor and manage the connectivity links from an end device as per the application requirement. Each such device may have multiple cellular interfaces, and we control the end-to-end delay jitter variation across the different interfaces via an intelligent traffic splitting scheme. We propose use of interstream coding to achieve target delay jitter and also a high reliability/availability. We use a decoupling approach to adapt the probabilistic traffic split into various interfaces and the extent of inter-stream coding. We provide an end-to-end measurement-based traffic splitting scheme that relies on a Machine Learning algorithm. We use a stochastic approximation-like algorithm (operating at a slower timescale) to obtain the right coding level. The various modules developed are pluggable in a pipeline-manner and work with real interfaces as well as simulators like openairinterface. We validate our analysis, and provide traffic split performance results from our working multi-access system (using cellular dongles, and also over openairinterface).",industry,369,not included
10.1109/icnn.1994.374213,to_check,Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94),IEEE,1994-07-02 00:00:00,ieeexplore,an implementation and evaluation of the art1 neural network for pattern recognition,https://ieeexplore.ieee.org/document/374213/,"A key to solving the stability-plasticity dilemma is to add a feedback mechanism between the competitive and the input layer of a network. This feedback mechanism facilitates the learning of new information without destroying old information, automatic switching between stable and plastic modes, and stabilization of the encoding of the classes done by the nodes resulting from this approach we have a neural network architecture that is particularly suited for pattern-classification problems in real world environments. For industrial use, ART1 neural networks have the potential of becoming an important component in a variety of commercial and military systems. Efficient software emulations of these networks are adequate in many of today's low-end applications such as information retrieval or group technology; but for larger applications, special purpose hardware is required to achieve the expected performance requirements.&lt;<ETX>&gt;</ETX>",industry,370,not included
10.1109/iecon.2013.6699377,to_check,IECON 2013 - 39th Annual Conference of the IEEE Industrial Electronics Society,IEEE,2013-11-13 00:00:00,ieeexplore,fuel cells prognostics using echo state network,https://ieeexplore.ieee.org/document/6699377/,"One remaining technological bottleneck to develop industrial Fuel Cell (FC) applications resides in the system limited useful lifetime. Consequently, it is important to develop failure diagnostic and prognostic tools enabling the optimization of the FC. Among all the existing prognostics approaches, datamining methods such as artificial neural networks aim at estimating the process' behavior without huge knowledge about the underlying physical phenomena. Nevertheless, this kind of approach needs huge learning dataset. Also, the deployment of such an approach can be long (trial and error method), which represents a real problem for industrial applications where real-time complying algorithms must be developed. According to this, the aim of this paper is to study the application of a reservoir computing tool (the Echo State Network) as a prognostics system enabling the estimation of the Remaining Useful Life of a Proton Exchange Membrane Fuel Cell. Developments emphasize on the prediction of the mean voltage cells of a degrading FC. Accuracy and time consumption of the approach are studied, as well as sensitivity of several parameters of the ESN. Results appear to be very promising.",industry,371,not included
10.1109/isscs52333.2021.9497411,to_check,"2021 International Symposium on Signals, Circuits and Systems (ISSCS)",IEEE,2021-07-16 00:00:00,ieeexplore,inverted pendulum control with a robotic arm using deep reinforcement learning,https://ieeexplore.ieee.org/document/9497411/,"Inverted pendulum control is a benchmark control problem that researchers have used to test the new control strategies over the past 50 years. Deep Reinforcement Learning Algorithm is used recently on the inverted pendulum on a straightforward form. The inverted pendulum had only one degree of freedom and was moving on a plane. This paper demonstrates a successful implementation of a deep reinforcement learning algorithm on an inverted pendulum that rotates freely on a spherical joint with an industrial 6 degrees freedom robot arm. This research used the Deep Reinforcement Learning algorithm in Robot Operating System (ROS) and Gazebo Simulation. Experimental results show that the proposed method achieved promising outputs and reaches the control objectives. We were able to control the inverted pendulum upward for 30 and 20 seconds in two case studies. Two other significant novelties in this research are using an inertial measurement unit (IMU) on the tip of the pendulum, that will facilitate implementation on the real robot for future work and different reward functions in comparing to past publications that enable continuous learning and mastering control in a vertical position",industry,372,not included
10.1109/etfa.2018.8502485,to_check,2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA),IEEE,2018-09-07 00:00:00,ieeexplore,linear classification of badly conditioned data,https://ieeexplore.ieee.org/document/8502485/,"We present a method for the fast and robust linear classification of badly conditioned data. In our considerations, badly conditioned data are such data which are numerically difficult to handle. Due to, e.g. a large number of features or a large number of objects representing classes as well as noise, outliers or incompleteness, the common software computation of the discriminating linear combination of features between classes fails or is extremely time consuming. The theoretical foundations of our approach are based on the single feature ranking, which allows fast calculation of the approximative initial classification boundary. For the increasing of classification accuracy of this boundary, the refinement is performed in the lower dimensional space. Our approach is tested on several datasets from UCI Reposi-tiory. Experimental results indicate high classification accuracy of the approach. For the modern real industrial applications such a method is especially suitable in the Cyber-Physical-System environments and provides a part of the workflow for the automated classifier design.",industry,373,not included
10.1109/nca.2018.8548330,to_check,2018 IEEE 17th International Symposium on Network Computing and Applications (NCA),IEEE,2018-11-03 00:00:00,ieeexplore,monitoring iot networks for botnet activity,https://ieeexplore.ieee.org/document/8548330/,"The Internet of Things (IoT) has rapidly transitioned from a novelty to a common, and often critical, part of residential, business, and industrial environments. Security vulnerabilities and exploits in the IoT realm have been well documented. In many cases, improving the security of an IoT device by hardening its software is not a realistic option, especially in the cost-sensitive consumer market or in legacy-bound industrial settings. As part of a multifaceted defense against botnet activity on the IoT, this paper explores a method based on monitoring the network activity of IoT devices. A notable benefit of this approach is that it does not require any special access to the devices and adapts well to the addition of new devices. The method is evaluated on a publicly available dataset drawn from a real IoT network.",industry,374,not included
10.1109/iciea.2006.257304,to_check,2006 1ST IEEE Conference on Industrial Electronics and Applications,IEEE,2006-05-26 00:00:00,ieeexplore,performance studies of fuzzy logic based pi-like controller designed for speed control of switched reluctance motor,https://ieeexplore.ieee.org/document/4025905/,"Switched reluctance motor (SRM) has gained significant interest in the field of industrial drive. The controller used to drive the machine is conventional PI controller. But the machine characteristics are very much nonlinear. This poses a problem for conventional controller design as regards to maintaining steady performance. There is also a need to adapt to the variable operating conditions. Fuzzy logic based heuristics is prospective since the exact analytical modelling of the system is difficult. PC implementation of the controller offers great flexibility in both design and maintenance phase. This work implements a PI like fuzzy logic controller (FLC) for SRM, which is found to work successfully in real time conditions. The work compares the performance of the FLC with respect to the conventional PI controller",industry,375,not included
10.1109/isda.2014.7066281,to_check,2014 14th International Conference on Intelligent Systems Design and Applications,IEEE,2014-11-30 00:00:00,ieeexplore,ros-based remote controlled robotic arm workcell,https://ieeexplore.ieee.org/document/7066281/,"This paper describes robotic workplace that is being developed at our faculty. It consists of industrial arm Mitsubishi Melfa RV-6SL, gripper Schunk and six Axis cameras. The purpose of this workplace is to serve students for testing algorithms from artificial intelligence and computer vision. It also gives them the opportunity to work with real industrial manipulator and allow them to test things like kinematics and dynamics of the arm. The arm is inaccessible for students so it can be operated only remotely. The software needed for remote controlling and programming is described in this paper.",industry,376,included
10.1109/avss.2018.8639339,to_check,2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),IEEE,2018-11-30 00:00:00,ieeexplore,scene adaptation for semantic segmentation using adversarial learning,https://ieeexplore.ieee.org/document/8639339/,"Semantic Segmentation algorithms based on the deep learning paradigm have reached outstanding performances. However, in order to achieve good results in a new domain, it is generally demanded to fine-tune a pre-trained deep architecture using new labeled data coming from the target application domain. The fine-tuning procedure is also required when the domain application settings change, e. g., when a camera is moved, or a new camera is installed. This implies the collection and pixel-wise la-beling of images to be used for training, which slows down the deployment of semantic segmentation systems in real industrial scenarios and increases the industrial costs. Taking into account the aforementioned issues, in this paper we propose an approach based on Adversarial Learning to perform scene adaptation for semantic segmentation. We frame scene adaptation as the task of predicting semantic segmentation masks for images belonging to a Target Scene Context given labeled images coming from a Source Scene Context and unlabeled images coming from the Target Scene Context. Experiments highlight that the proposed method achieves promising performances both when the two scenes contain similar content (i.e., they are related to two different points of view of the same scene) and when the observed scenes contain unrelated content (i.e., they account to completely different scenes).",industry,377,not included
10.1109/tase.2020.3032075,to_check,IEEE Transactions on Automation Science and Engineering,IEEE,2021-10-01 00:00:00,ieeexplore,a virtual mechanism approach for exploiting functional redundancy in finishing operations,https://ieeexplore.ieee.org/document/9246671/,"We propose a new approach to programming by the demonstration of finishing operations. Such operations can be carried out by industrial robots in multiple ways because an industrial robot is typically functionally redundant with respect to a finishing task. In the proposed system, a human expert demonstrates a finishing operation, and the demonstrated motion is recorded in the Cartesian space. The robot’s kinematic model is augmented with a virtual mechanism, which is defined according to the applied finishing tool. This way, the kinematic model is expanded with additional degrees of freedom that can be exploited to compute the optimal joint space motion of the robot without altering the essential aspects of the Cartesian space task execution as demonstrated by the human expert. Finishing operations, such as polishing and grinding, occur in contact with the treated workpiece. Since information about the contact point position is needed to control the robot during the operation, we have developed a novel approach for accurate estimation of contact points using the measured forces and torques. Finally, we applied iterative learning control to refine the demonstrated operations and compensate for inaccurate calibration and different dynamics of the robot and human demonstrator. The proposed method was verified on real robots and real polishing and grinding tasks. <italic>Note to Practitioners</italic>—This work was motivated by the need for automation of finishing operations, such as polishing and grinding, on contemporary industrial robots. Existing approaches are both too complex and too time-consuming to be applied in flexible and small-scale production, which often requires the frequent deployment of new applications. Our approach is based on programming by demonstration and enables the programming of finishing operations also for users who are not specialists in robot programming. Programming by demonstration is especially useful for teaching finishing operations because it enables the transfer of expert knowledge about finishing skills to robots without providing lengthy task descriptions or manual coding. Besides the human demonstration of the desired operation, the proposed approach also requires the availability of the kinematic model for the machine tool applied to carry out the finishing operation. We provide several practical examples of grinding and polishing tools and how to integrate them into our approach. Another feature of the proposed system is that user demonstrations of finishing operations can be transferred between different combinations of robots and machine tools.",industry,378,included
10.1109/lra.2020.2969927,to_check,IEEE Robotics and Automation Letters,IEEE,2020-04-01 00:00:00,ieeexplore,augmented lidar simulator for autonomous driving,https://ieeexplore.ieee.org/document/8972449/,"In Autonomous Driving (AD), detection and tracking of obstacles on the roads is a critical task. Deep-learning based methods using annotated LiDAR data have been the most widely adopted approach for this. Unfortunately, annotating 3D point cloud is a very challenging, time- and money-consuming task. In this letter, we propose a novel LiDAR simulator that augments real point cloud with synthetic obstacles (e.g., vehicles, pedestrians, and other movable objects). Unlike previous simulators that entirely rely on CG (Computer Graphics) models and game engines, our augmented simulator bypasses the requirement to create high-fidelity background CAD (Computer Aided Design) models. Instead, we can deploy a vehicle with a LiDAR scanner to sweep the street of interests to obtain the background points cloud, based on which annotated point cloud can be automatically generated. This “scan-and-simulate” capability makes our approach scalable and practical, ready for large-scale industrial applications. In this letter, we describe our simulator in detail, in particular the placement of obstacles that is critical for performance enhancement. We show that detectors with our simulated LiDAR point cloud alone can perform comparably (within two percentage points) with these trained with real data. Mixing real and simulated data can achieve over 95% accuracy.",industry,379,not included
10.1109/jproc.2018.2856739,to_check,Proceedings of the IEEE,IEEE,2018-11-01 00:00:00,ieeexplore,navigating the landscape for real-time localization and mapping for robotics and virtual and augmented reality,https://ieeexplore.ieee.org/document/8436423/,"Visual understanding of 3-D environments in real time, at low power, is a huge computational challenge. Often referred to as simultaneous localization and mapping (SLAM), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, and virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are: 1) tools and methodology for systematic quantitative evaluation of SLAM algorithms; 2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives; 3) end-to-end simulation tools to enable optimization of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches; and 4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",industry,380,not included
10.1109/tim.2021.3092518,to_check,IEEE Transactions on Instrumentation and Measurement,IEEE,2021-01-01 00:00:00,ieeexplore,pipeline safety early warning by multifeature-fusion cnn and lightgbm analysis of signals from distributed optical fiber sensors,https://ieeexplore.ieee.org/document/9541184/,"Energy pipelines are the backbones of global energy systems. Monitoring their safety and automatically identifying and locating third-party damage events are crucial to energy supply. However, most traditional methods lack in-depth consideration of distributed fiber signals and have not been tested on real-world long-distance pipelines, making it difficult to deploy them in operating long-distance pipelines. In this study, we utilize a novel real-time machine-learning method based on phase-sensitive optical time domain reflectometer technology to monitor the safety of oil and gas pipelines. Specifically, we build a multifeature-fusion convolutional neural network and LightGBM fusion model based on two novel complementary spatiotemporal features. The method was applied to a large amount of data collected from real-world oil–gas transportation pipelines of the China National Petroleum Corporation. The proposed method could accurately locate and identify third-party damage events in real-time under conditions of strong noise and various types of system hardware, and could effectively handle signal drift in the time and space dimensions. Our methodology has been deployed at real long-distance energy pipeline sites and our work will contribute to energy pipeline safety and energy supply security. Furthermore, the proposed solution could be generalized to other fields, such as industrial inspection, measurement, and monitoring.",industry,381,not included
10.1109/81.747195,to_check,IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications,IEEE,1999-02-01 00:00:00,ieeexplore,reaction-diffusion cnn algorithms to generate and control artificial locomotion,https://ieeexplore.ieee.org/document/747195/,"In this paper a physiological-behavioral approach to neural processing is used to realize artificial locomotion in mechatronic devices. The task has been realized by using a particular model of reaction-diffusion cellular neural networks (RD-CNN's) generating autowave fronts as well as Turing patterns. Moreover a programmable hardware cellular neural network structure is presented in order to model, generate, and control in real time some biorobots. The programmable hardware implementation gives the possibility of generating locomotion in real time and also to control the transition among several types of locomotion, with particular attention to hexapodes. The approach proposed allows not only the design of walking robots, but also the ability to build structures able to efficiently solve typical problems in industrial automation, such as online routing of objects moved on conveyor belts.",industry,382,not included
10.1109/83.791960,to_check,IEEE Transactions on Image Processing,IEEE,1999-10-01 00:00:00,ieeexplore,real-time dsp implementation for mrf-based video motion detection,https://ieeexplore.ieee.org/document/791960/,"This paper describes the real time implementation of a simple and robust motion detection algorithm based on Markov random field (MRF) modeling, MRF-based algorithms often require a significant amount of computations. The intrinsic parallel property of MRF modeling has led most of implementations toward parallel machines and neural networks, but none of these approaches offers an efficient solution for real-world (i.e., industrial) applications. Here, an alternative implementation for the problem at hand is presented yielding a complete, efficient and autonomous real-time system for motion detection. This system is based on a hybrid architecture, associating pipeline modules with one asynchronous module to perform the whole process, from video acquisition to moving object masks visualization. A board prototype is presented and a processing rate of 15 images/s is achieved, showing the validity of the approach.",industry,383,included
10.1007/978-3-030-69367-1_1,to_check,Impact and Opportunities of Artificial Intelligence Techniques in the Steel Industry,Springer,2021-01-01 00:00:00,springer,challenges and frontiers in implementing artificial intelligence in process industry,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-69367-1_1,"The implementation of artificial intelligence faces different challenges of infrastructural, data related, security related and social scope. These aspects are discussed, reflecting on the requirements of introducing such a technology in a broader way. Machine learning, as subfield of artificial intelligence, benefits from advances in Big Data science. One example is the $$\lambda $$ λ -architecture which can be used for the treatment of streamed process data in industrial applications. Digital twins are shown as further tool providing object-oriented data for machine learning applications. Yet, the increasing freedom of data transfer within a plant, as propagated by Industry 4.0, poses new risks for information technology and automation systems: security of those components is one of the big challenges. Here, artificial intelligence can seen as both risk and solution. A last relevant challenge is acceptance among the staff, as artificial intelligence is associated with fears. Counterstrategies for those fears are presented as a proposed guideline for real applications. Finally, current frontiers at process industry are considered and discussed. These include the need for strengthening the use of high-dimensional data availability, increased roll-out of optimisation concepts and rigorous progresses in semantic modelling of processes and process chains, in order to fully exploit the beneficial scope of artificial intelligence in industry.",industry,384,not included
10.1038/s41598-020-68156-2,to_check,Scientific Reports,Nature,2020-07-08 00:00:00,springer,a machine learning workflow for raw food spectroscopic classification in a future industry,https://www.nature.com/articles/s41598-020-68156-2,"Over the years, technology has changed the way we produce and have access to our food through the development of applications, robotics, data analysis, and processing techniques. The implementation of these approaches by the food industry ensure quality and affordability, reducing at the same time the costs of keeping the food fresh and increase productivity. A system, as the one presented herein, for raw food categorization is needed in future food industries to automate food classification according to type, the process of algorithm approaches that will be applied to every different food origin and also for serving disabled people. The purpose of this work was to develop a machine learning workflow based on supervised PLS regression and SVM classification, towards automated raw food categorization from FTIR. The system exhibited high efficiency in multi-class classification of 7 different types of raw food. The selected food samples, were diverse in terms of storage conditions (temperature, storage time and packaging), while the variability within each food was also taken into account by several different batches; leading in a classifier able to embed this variation towards increased robustness and efficiency, ready for real life applications targeting to the digital transformation of the food industry.",industry,385,not included
http://arxiv.org/abs/1708.02884v1,to_check,arxiv,arxiv,2017-08-09 00:00:00,arxiv,"predicting and evaluating software model growth in the automotive
  industry",http://arxiv.org/abs/1708.02884v1,"The size of a software artifact influences the software quality and impacts
the development process. In industry, when software size exceeds certain
thresholds, memory errors accumulate and development tools might not be able to
cope anymore, resulting in a lengthy program start up times, failing builds, or
memory problems at unpredictable times. Thus, foreseeing critical growth in
software modules meets a high demand in industrial practice. Predicting the
time when the size grows to the level where maintenance is needed prevents
unexpected efforts and helps to spot problematic artifacts before they become
critical.
  Although the amount of prediction approaches in literature is vast, it is
unclear how well they fit with prerequisites and expectations from practice. In
this paper, we perform an industrial case study at an automotive manufacturer
to explore applicability and usability of prediction approaches in practice. In
a first step, we collect the most relevant prediction approaches from
literature, including both, approaches using statistics and machine learning.
Furthermore, we elicit expectations towards predictions from practitioners
using a survey and stakeholder workshops. At the same time, we measure software
size of 48 software artifacts by mining four years of revision history,
resulting in 4,547 data points. In the last step, we assess the applicability
of state-of-the-art prediction approaches using the collected data by
systematically analyzing how well they fulfill the practitioners' expectations.
  Our main contribution is a comparison of commonly used prediction approaches
in a real world industrial setting while considering stakeholder expectations.
We show that the approaches provide significantly different results regarding
prediction accuracy and that the statistical approaches fit our data best.",industry,386,not included
http://arxiv.org/abs/2110.04003v1,to_check,arxiv,arxiv,2021-10-08 00:00:00,arxiv,learning to centralize dual-arm assembly,http://arxiv.org/abs/2110.04003v1,"Even though industrial manipulators are widely used in modern manufacturing
processes, deployment in unstructured environments remains an open problem. To
deal with variety, complexity and uncertainty of real world manipulation tasks
a general framework is essential. In this work we want to focus on assembly
with humanoid robots by providing a framework for dual-arm peg-in-hole
manipulation. As we aim to contribute towards an approach which is not limited
to dual-arm peg-in-hole, but dual-arm manipulation in general, we keep modeling
effort at a minimum. While reinforcement learning has shown great results for
single-arm robotic manipulation in recent years, research focusing on dual-arm
manipulation is still rare. Solving such tasks often involves complex modeling
of interaction between two manipulators and their coupling at a control level.
In this paper, we explore the applicability of model-free reinforcement
learning to dual-arm manipulation based on a modular approach with two
decentralized single-arm controllers and a single centralized policy. We reduce
modeling effort to a minimum by using sparse rewards only. We demonstrate the
effectiveness of the framework on dual-arm peg-in-hole and analyze sample
efficiency and success rates for different action spaces. Moreover, we compare
results on different clearances and showcase disturbance recovery and
robustness, when dealing with position uncertainties. Finally we zero-shot
transfer policies trained in simulation to the real-world and evaluate their
performance.",industry,387,not included
http://arxiv.org/abs/2004.05898v1,to_check,arxiv,arxiv,2020-04-10 00:00:00,arxiv,exposing hardware building blocks to machine learning frameworks,http://arxiv.org/abs/2004.05898v1,"There are a plethora of applications that demand high throughput and low
latency algorithms leveraging machine learning methods. This need for real time
processing can be seen in industries ranging from developing neural network
based pre-distortors for enhanced mobile broadband to designing FPGA-based
triggers in major scientific efforts by CERN for particle physics. In this
thesis, we explore how niche domains can benefit vastly if we look at neurons
as a unique boolean function of the form $f:B^{I} \rightarrow B^{O}$, where $B
= \{0,1\}$. We focus on how to design topologies that complement such a view of
neurons, how to automate such a strategy of neural network design, and
inference of such networks on Xilinx FPGAs. Major hardware borne constraints
arise when designing topologies that view neurons as unique boolean functions.
Fundamentally, realizing such topologies on hardware asserts a strict limit on
the 'fan-in' bits of a neuron due to the doubling of permutations possible with
every increment in input bit-length. We address this limit by exploring
different methods of implementing sparsity and explore activation quantization.
Further, we develop a library that supports training a neural network with
custom sparsity and quantization. This library also supports conversion of
trained Sparse Quantized networks from PyTorch to VERILOG code which is then
synthesized using Vivado, all of which is part of the LogicNet tool-flow. To
aid faster prototyping, we also support calculation of the worst-case hardware
cost of any given topology. We hope that our insights into the behavior of
extremely sparse quantized neural networks are of use to the research community
and by extension allow people to use the LogicNet design flow to deploy highly
efficient neural networks.",industry,388,not included
http://arxiv.org/abs/2109.13602v1,to_check,arxiv,arxiv,2021-09-28 00:00:00,arxiv,"safetynet: safe planning for real-world self-driving vehicles using
  machine-learned policies",http://arxiv.org/abs/2109.13602v1,"In this paper we present the first safe system for full control of
self-driving vehicles trained from human demonstrations and deployed in
challenging, real-world, urban environments. Current industry-standard
solutions use rule-based systems for planning. Although they perform reasonably
well in common scenarios, the engineering complexity renders this approach
incompatible with human-level performance. On the other hand, the performance
of machine-learned (ML) planning solutions can be improved by simply adding
more exemplar data. However, ML methods cannot offer safety guarantees and
sometimes behave unpredictably. To combat this, our approach uses a simple yet
effective rule-based fallback layer that performs sanity checks on an ML
planner's decisions (e.g. avoiding collision, assuring physical feasibility).
This allows us to leverage ML to handle complex situations while still assuring
the safety, reducing ML planner-only collisions by 95%. We train our ML planner
on 300 hours of expert driving demonstrations using imitation learning and
deploy it along with the fallback layer in downtown San Francisco, where it
takes complete control of a real vehicle and navigates a wide variety of
challenging urban driving scenarios.",industry,389,included
http://arxiv.org/abs/1808.03454v1,to_check,arxiv,arxiv,2018-08-10 00:00:00,arxiv,aiq: measuring intelligence of business ai software,http://arxiv.org/abs/1808.03454v1,"Focusing on Business AI, this article introduces the AIQ quadrant that
enables us to measure AI for business applications in a relative comparative
manner, i.e. to judge that software A has more or less intelligence than
software B. Recognizing that the goal of Business software is to maximize value
in terms of business results, the dimensions of the quadrant are the key
factors that determine the business value of AI software: Level of Output
Quality (Smartness) and Level of Automation. The use of the quadrant is
illustrated by several software solutions to support the real life business
challenge of field service scheduling. The role of machine learning and
conversational digital assistants in increasing the business value are also
discussed and illustrated with a recent integration of existing intelligent
digital assistants for factory floor decision making with the new version of
Google Glass. Such hands free AI solutions elevate the AIQ level to its
ultimate position.",industry,390,not included
http://arxiv.org/abs/2009.09926v1,to_check,arxiv,arxiv,2020-09-17 00:00:00,arxiv,"cross-modal alignment with mixture experts neural network for
  intral-city retail recommendation",http://arxiv.org/abs/2009.09926v1,"In this paper, we introduce Cross-modal Alignment with mixture experts Neural
Network (CameNN) recommendation model for intral-city retail industry, which
aims to provide fresh foods and groceries retailing within 5 hours delivery
service arising for the outbreak of Coronavirus disease (COVID-19) pandemic
around the world. We propose CameNN, which is a multi-task model with three
tasks including Image to Text Alignment (ITA) task, Text to Image Alignment
(TIA) task and CVR prediction task. We use pre-trained BERT to generate the
text embedding and pre-trained InceptionV4 to generate image patch embedding
(each image is split into small patches with the same pixels and treat each
patch as an image token). Softmax gating networks follow to learn the weight of
each transformer expert output and choose only a subset of experts conditioned
on the input. Then transformer encoder is applied as the share-bottom layer to
learn all input features' shared interaction. Next, mixture of transformer
experts (MoE) layer is implemented to model different aspects of tasks. At top
of the MoE layer, we deploy a transformer layer for each task as task tower to
learn task-specific information. On the real word intra-city dataset,
experiments demonstrate CameNN outperform baselines and achieve significant
improvements on the image and text representation. In practice, we applied
CameNN on CVR prediction in our intra-city recommender system which is one of
the leading intra-city platforms operated in China.",industry,391,not included
http://arxiv.org/abs/1909.10270v1,to_check,arxiv,arxiv,2019-09-23 00:00:00,arxiv,"pose estimation for texture-less shiny objects in a single rgb image
  using synthetic training data",http://arxiv.org/abs/1909.10270v1,"In the industrial domain, the pose estimation of multiple texture-less shiny
parts is a valuable but challenging task. In this particular scenario, it is
impractical to utilize keypoints or other texture information because most of
them are not actual features of the target but the reflections of surroundings.
Moreover, the similarity of color also poses a challenge in segmentation. In
this article, we propose to divide the pose estimation process into three
stages: object detection, features detection and pose optimization. A
convolutional neural network was utilized to perform object detection.
Concerning the reliability of surface texture, we leveraged the contour
information for estimating pose. Since conventional contour-based methods are
inapplicable to clustered metal parts due to the difficulties in segmentation,
we use the dense discrete points along the metal part edges as semantic
keypoints for contour detection. Afterward, we exploit both keypoint
information and CAD model to calculate the 6D pose of each object in view. A
typical implementation of deep learning methods not only requires a large
amount of training data, but also relies on intensive human labor for labeling
the datasets. Therefore, we propose an approach to generate datasets and label
them automatically. Despite not using any real-world photos for training, a
series of experiments showed that the algorithm built on synthetic data perform
well in the real environment.",industry,392,not included
http://arxiv.org/abs/1908.11863v1,to_check,arxiv,arxiv,2019-08-30 00:00:00,arxiv,systematic analysis of image generation using gans,http://arxiv.org/abs/1908.11863v1,"Generative Adversarial Networks have been crucial in the developments made in
unsupervised learning in recent times. Exemplars of image synthesis from text
or other images, these networks have shown remarkable improvements over
conventional methods in terms of performance. Trained on the adversarial
training philosophy, these networks aim to estimate the potential distribution
from the real data and then use this as input to generate the synthetic data.
Based on this fundamental principle, several frameworks can be generated that
are paragon implementations in several real-life applications such as art
synthesis, generation of high resolution outputs and synthesis of images from
human drawn sketches, to name a few. While theoretically GANs present better
results and prove to be an improvement over conventional methods in many
factors, the implementation of these frameworks for dedicated applications
remains a challenge. This study explores and presents a taxonomy of these
frameworks and their use in various image to image synthesis and text to image
synthesis applications. The basic GANs, as well as a variety of different niche
frameworks, are critically analyzed. The advantages of GANs for image
generation over conventional methods as well their disadvantages amongst other
frameworks are presented. The future applications of GANs in industries such as
healthcare, art and entertainment are also discussed.",industry,393,not included
http://arxiv.org/abs/1904.01719v1,to_check,arxiv,arxiv,2019-04-03 00:00:00,arxiv,"empirical evaluations of active learning strategies in legal document
  review",http://arxiv.org/abs/1904.01719v1,"One type of machine learning, text classification, is now regularly applied
in the legal matters involving voluminous document populations because it can
reduce the time and expense associated with the review of those documents. One
form of machine learning - Active Learning - has drawn attention from the legal
community because it offers the potential to make the machine learning process
even more effective. Active Learning, applied to legal documents, is considered
a new technology in the legal domain and is continuously applied to all
documents in a legal matter until an insignificant number of relevant documents
are left for review. This implementation is slightly different than traditional
implementations of Active Learning where the process stops once achieving
acceptable model performance. The purpose of this paper is twofold: (i) to
question whether Active Learning actually is a superior learning methodology
and (ii) to highlight the ways that Active Learning can be most effectively
applied to real legal industry data. Unlike other studies, our experiments were
performed against large data sets taken from recent, real-world legal matters
covering a variety of areas. We conclude that, although these experiments show
the Active Learning strategy popularly used in legal document review can
quickly identify informative training documents, it becomes less effective over
time. In particular, our findings suggest this most popular form of Active
Learning in the legal arena, where the highest-scoring documents are selected
as training examples, is in fact not the most efficient approach in most
instances. Ultimately, a different Active Learning strategy may be best suited
to initiate the predictive modeling process but not to continue through the
entire document review.",industry,394,not included
http://arxiv.org/abs/2011.07313v1,to_check,arxiv,arxiv,2020-11-14 00:00:00,arxiv,"classification of reverse-engineered class diagram and
  forward-engineered class diagram using machine learning",http://arxiv.org/abs/2011.07313v1,"UML Class diagram is very important to visualize the whole software we are
working on and helps understand the whole system in the easiest way possible by
showing the system classes, its attributes, methods, and relations with other
objects. In the real world, there are two types of Class diagram engineers work
with namely 1) Forward Engineered Class Diagram (FwCD) which are hand-made as
part of the forward-looking development process, and 2). Reverse Engineered
Class Diagram (RECD) which are those diagrams that are reverse engineered from
the source code. In the software industry while working with new open software
projects it is important to know which type of class diagram it is. Which UML
diagram was used in a particular project is an important factor to be known? To
solve this problem, we propose to build a classifier that can classify a UML
diagram into FwCD or RECD. We propose to solve this problem by using a
supervised Machine Learning technique. The approach in this involves analyzing
the features that are useful in classifying class diagrams. Different Machine
Learning models are used in this process and the Random Forest algorithm has
proved to be the best out of all. Performance testing was done on 999 Class
diagrams.",industry,395,not included
http://arxiv.org/abs/2110.00468v1,to_check,arxiv,arxiv,2021-10-01 00:00:00,arxiv,"new evolutionary computation models and their applications to machine
  learning",http://arxiv.org/abs/2110.00468v1,"Automatic Programming is one of the most important areas of computer science
research today. Hardware speed and capability have increased exponentially, but
the software is years behind. The demand for software has also increased
significantly, but it is still written in old fashion: by using humans.
  There are multiple problems when the work is done by humans: cost, time,
quality. It is costly to pay humans, it is hard to keep them satisfied for a
long time, it takes a lot of time to teach and train them and the quality of
their output is in most cases low (in software, mostly due to bugs).
  The real advances in human civilization appeared during the industrial
revolutions. Before the first revolution, most people worked in agriculture.
Today, very few percent of people work in this field.
  A similar revolution must appear in the computer programming field.
Otherwise, we will have so many people working in this field as we had in the
past working in agriculture.
  How do people know how to write computer programs? Very simple: by learning.
Can we do the same for software? Can we put the software to learn how to write
software?
  It seems that is possible (to some degree) and the term is called Machine
Learning. It was first coined in 1959 by the first person who made a computer
perform a serious learning task, namely, Arthur Samuel.
  However, things are not so easy as in humans (well, truth to be said - for
some humans it is impossible to learn how to write software). So far we do not
have software that can learn perfectly to write software. We have some
particular cases where some programs do better than humans, but the examples
are sporadic at best. Learning from experience is difficult for computer
programs. Instead of trying to simulate how humans teach humans how to write
computer programs, we can simulate nature.",industry,396,not included
http://arxiv.org/abs/1609.08018v1,to_check,arxiv,arxiv,2016-09-26 00:00:00,arxiv,"small near-earth asteroids in the palomar transient factory survey: a
  real-time streak-detection system",http://arxiv.org/abs/1609.08018v1,"Near-Earth asteroids (NEAs) in the 1-100 meter size range are estimated to be
$\sim$1,000 times more numerous than the $\sim$15,000 currently-catalogued
NEAs, most of which are in the 0.5-10 kilometer size range. Impacts from 10-100
meter size NEAs are not statistically life-threatening but may cause
significant regional damage, while 1-10 meter size NEAs with low velocities
relative to Earth are compelling targets for space missions. We describe the
implementation and initial results of a real-time NEA-discovery system
specialized for the detection of small, high angular rate (visually-streaked)
NEAs in Palomar Transient Factory (PTF) images. PTF is a 1.2-m aperture,
7.3-deg$^2$ field-of-view optical survey designed primarily for the discovery
of extragalactic transients (e.g., supernovae) in 60-second exposures reaching
$\sim$20.5 visual magnitude. Our real-time NEA discovery pipeline uses a
machine-learned classifier to filter a large number of false-positive streak
detections, permitting a human scanner to efficiently and remotely identify
real asteroid streaks during the night. Upon recognition of a streaked NEA
detection (typically within an hour of the discovery exposure), the scanner
triggers follow-up with the same telescope and posts the observations to the
Minor Planet Center for worldwide confirmation. We describe our ten initial
confirmed discoveries, all small NEAs that passed 0.3-15 lunar distances from
Earth. Lastly, we derive useful scaling laws for comparing
streaked-NEA-detection capabilities of different surveys as a function of their
hardware and survey-pattern characteristics. This work most directly informs
estimates of the streak-detection capabilities of the Zwicky Transient Facility
(ZTF, planned to succeed PTF in 2017), which will apply PTF's current
resolution and sensitivity over a 47-deg$^2$ field-of-view.",industry,397,not included
10.1016/j.energy.2021.120700,to_check,Energy,scopus,2021-09-01,sciencedirect,nonlinear generalized predictive controller based on ensemble of narx models for industrial gas turbine engine,https://api.elsevier.com/content/abstract/scopus_id/85105736036,"New design and operation of modern gas turbine engines (GTEs) are becoming more and more complex where several limitations and control modes should be fulfilled at the same time to accomplish a safe and ideal performance for the engine. For this purpose, a constrained multi-input multi-output (MIMO) non-linear model predictive controller (NMPC) based on neural network model is designed to fulfill the control requirements of a Siemens SGT-A65 three-spool aero-derivative gas turbine engine (ADGTE) used for power generation. However, the implementation of NMPC in real time has two challenges: Firstly, the design of an accurate non-linear model, which can run many times faster than real time. Secondly, the usage of a rapid and reliable optimization algorithm to solve the optimization problem in real time. To solve these issues, the constrained MIMO NMPC is created based on the generalized predictive control (GPC) algorithm as a result of its clarity, ease of use, and capacity to deal with problems in one algorithm. In addition, seven ensembles of eight multi-input single-output (MISO) non-linear autoregressive network with exogenous inputs (NARX) models are used as a base model for the GPC controller to predict the future process outputs. Estimation of free and forced responses of the GPC based on the neural network (NN) model of the plant each sampling time without performing instantaneous linearization is proposed in this study, which reduces the NMPC optimization problem to a linear optimization problem at each sampling step. In addition, the Hildreth's quadratic programming algorithm is used to solve the quadratic optimization problem within the NMPC controller, which offers ease of use and reliability in real time applications. To demonstrate the performance of the NNGPC controller developed in this study, we have compared the performance of the neural network generalized predictive control (NNGPC) controller to the existing controller of the SGT-A65 engine. The simulation results show that the NNGPC has demonstrated output responses with less oscillatory behavior and smoother control actions to the sudden variation in the electric load disturbance than those observed in the existing min-max controller. However, the min-max controller has faster response than that of the NNGPC controller.",industry,398,not included
10.1016/j.jmsy.2021.04.005,to_check,Journal of Manufacturing Systems,scopus,2021-07-01,sciencedirect,learningadd: machine learning based acoustic defect detection in factory automation,https://api.elsevier.com/content/abstract/scopus_id/85106283308,"Defect inspection of glass bottles in the beverage industrial is of significance to prevent unexpected losses caused by the damage of bottles during manufacturing and transporting. The commonly used manual methods suffer from inefficiency, excessive space consumption, and beverage wastes after filling. To replace the manual operations in the pre-filling detection with improved efficiency and reduced costs, this paper proposes a machine learning based Acoustic Defect Detection (LearningADD) system. Moreover, to realize scalable deployment on edge and cloud computing platforms, deployment strategies especially partitioning and allocation of functionalities need to be compared and optimized under realistic constraints such as latency, complexity, and capacity of the platforms. In particular, to distinguish the defects in glass bottles efficiently, the improved Hilbert-Huang transform (HHT) is employed to extend the extracted feature sets, and then Shuffled Frog Leaping Algorithm (SFLA) based feature selection is applied to optimize the feature sets. Five deployment strategies are quantitatively compared to optimize real-time performances based on the constraints measured from a real edge and cloud environment. The LearningADD algorithms are validated by the datasets from a real-life beverage factory, and the F-measure of the system reaches 98.48 %. The proposed deployment strategies are verified by experiments on private cloud platforms, which shows that the Distributed Heavy Edge deployment outperforms other strategies, benefited from the parallel computing and edge computing, where the Defect Detection Time for one bottle is less than 2.061 s in 99 % probability.",industry,399,included
10.1016/j.chemolab.2021.104314,to_check,Chemometrics and Intelligent Laboratory Systems,scopus,2021-06-15,sciencedirect,a scalable approach for the efficient segmentation of hyperspectral images,https://api.elsevier.com/content/abstract/scopus_id/85105360467,"The number of applications of hyperspectral imaging (HSI) is steadily increasing, as technology evolves and cameras become more affordable. However, the volume of data in a hyperspectral image is large (order of Gigabytes) and standard off-the-shelf algorithms for multi-channel image analysis cannot be readily applied, due to the prohibitive computational time and large memory requirements. Therefore, new scalable approaches are required to perform hyperspectral image analysis. In this article we address an efficient methodology for conducting Unsupervised Image Segmentation – one of the basic and most fundamental image analysis operations. In the methodology proposed, unsupervised segmentation is conducted after transforming the spectral and spatial dimensions of the raw hyperspectral image into a more compact representation using multivariate and multiresolution techniques. The clusters identified in the compact image representation are then used to train a discriminative classifier. The classifier is then adapted and transferred for application to the raw image, where it will efficiently label all the original pixels. With the proposed methodology, the computational expensive operations (unsupervised clustering and classifier learning) are minimized, whereas the efficient implementation of the classifier guarantees the analysis at the native resolution. The effectiveness of the proposed methodology was tested on a real case study considering an industrial hyperspectral image capturing the reflectance spectrum for several objects made of different unknown materials. A significant reduction in the computational cost was achieved without compromising the quality of the unsupervised segmentation, demonstrating the potential of the proposed approach.",industry,400,included
10.1016/j.enconman.2021.113856,to_check,Energy Conversion and Management,scopus,2021-04-01,sciencedirect,the mutual benefits of renewables and carbon capture: achieved by an artificial intelligent scheduling strategy,https://api.elsevier.com/content/abstract/scopus_id/85101129959,"Renewable power and carbon capture are key technologies to transfer the power industry into low carbon generation. Renewables have been developed fast, however, the intermittent nature has imposed higher requirement for the flexibility of the power grid. Retrofitting carbon capture technologies to existing fossil-fuel fired power plants is an important solution to avoid the “lock-in” of emissions, but the high operating costs hinders their large scale application. The coexistence of renewable power and carbon capture opens up a new avenue that the deployment of carbon capture can provide additional flexibility for better accommodation of renewable power while excess renewables can be used to reduce the operating costs of carbon capture. To this end, this paper proposes an artificial intelligence based optimal scheduling strategy for the power plant-carbon capture system in the context of renewable power penetration to show that the mutual benefits between carbon capture and renewable power can be achieved when the carbon capture process is made fully adjustable. An artificial intelligent deep belief neural network is used to reflect the complex interactions between carbon, heat and electricity within the power plant carbon capture system. Multiple operating goals are considered in the scheduling such as minimizing the operating costs, renewable power curtailment and carbon emission, and the particle swarm heuristic optimization is employed to find the optimal solution. The impacts of carbon capture constraint mode, carbon emission penalty coefficient, carbon dioxide production constraints and renewable power installed capacity are investigated to provide broader insight on the potential benefit of carbon capture in future low-carbon energy system. A case study using real world data of weather condition and load demand shows that renewable power curtailment can be reduced by 51% with the integration of post-combustion capture systems and 35% of total carbon emission are captured by the use of excess renewable power through optimal scheduling. This paper points out a new way of using artificial intelligent technologies to coordinate the couplings between carbon and electricity for efficient and environmentally friendly operation of future low-carbon energy system.",industry,401,included
10.1016/j.fbp.2020.12.009,to_check,Food and Bioproducts Processing,scopus,2021-03-01,sciencedirect,study of galactooligosaccharides production from dairy waste by ftir and chemometrics as process analytical technology,https://api.elsevier.com/content/abstract/scopus_id/85099356128,"Galactooligosaccharides (GOS) production from whey, a relevant by-product of dairy industry, answers to the Circular Economy principle of extending the life cycle of products. Indeed, it allows the reuse of dairy waste to produce prebiotics to be used in functional food preparations. For this purpose, the effective monitoring of GOS production should be performed in real time and by environmentally friendly techniques. Thus, FTIR spectroscopy, combined with different chemometric approaches, has been tested to assess a Process Analytical Technology to follow GOS production from cheese whey. Partial Least Square regression models were reliable for lactose, glucose and galactose determination (Root Mean Square Error of Prediction of 21.9, 11.1 and 12.4 mg mL−1, respectively). Furthermore, Multivariate Curve Resolution – Alternating Least Square models were proposed to describe trends of the reaction components along the process being an interesting alternative to chromatographic determinations. The real time implementation of the proposed approach will provide the dairy industry with a reliable and green Process Analytical Technology for dairy waste reallocation, avoiding sample pre-processing, large use of organic solvents and long times of analysis.",industry,402,not included
10.1016/j.patrec.2020.06.028,to_check,Pattern Recognition Letters,scopus,2020-10-01,sciencedirect,on the use of a full stack hardware/software infrastructure for sensor data fusion and fault prediction in industry 4.0,https://api.elsevier.com/content/abstract/scopus_id/85087339064,"Aspects related to prognostics are becoming a crucial part in the industrial sector. In this sense, Industry 4.0 is considered as a new paradigm that leverages on the IoT to propose increasingly more solutions to provide an estimate on the working conditions of an industrial plant. However, in context like the industrial sector where the number and heterogeneity of sensors can be very large, and the time requirements are very stringent, emerges the challenge to design effective infrastructures to interact with these complex systems. In this paper, we propose a full stack hardware/software infrastructure to collect, manage, and analyze the data gathered from a set of heterogeneous sensors attached to a real scale replica industrial plant available in our laboratory. On top of the proposed infrastructure we designed and implemented a fault prediction algorithm which exploits sensors data fusion with the aim to assess the working conditions of the industrial plant. The result section shows the obtained results in terms of accuracy from testing our proposed model and provides a comparison with a traditional Deep Neural Network (DNN) topology.",industry,403,not included
10.1016/j.petrol.2020.107087,to_check,Journal of Petroleum Science and Engineering,scopus,2020-07-01,sciencedirect,transformation of academic teaching and research: development of a highly automated experimental sucker rod pumping unit,https://api.elsevier.com/content/abstract/scopus_id/85079611752,"Sucker rod pumps are one of the most popular solutions for artificial lift since their inception in the 19th century with minimum changes in design. Presently, companies are deploying digital technology in the field and, there has been a big push for a networked oilfield in recent years. This means technology is now able to control machines in remote places, evaluate their performances and control safety operating parameters. But these digital solutions are still not available in universities, causing a technological and technical gap for students and researchers.
                  This study presents a prototype of a new dedicated Interactive Digital Sucker Rod Pumping Unit (ID-SRP) system at the University of Oklahoma with representative operating conditions. The prototype mimics sucker rod pump working principles and also imitates different realistic rod string motions. The application and solutions are focused on providing authentic learning experiences for petroleum engineers. The system is also designed to address and optimize SRP well performance and safety through Model Predictive Controller (MPC) implementation and meeting industrial requirements. It connects the physical and virtual interaction with learning technologies. The objective is to bridge the tangible and the abstract for a better understanding of sucker rod concept and implement existing theories into the digital system. Additionally, it aids our future petroleum engineers on how to apply basic industry principles and upsurge their problem-solving skills.
                  The developed unit is capable of simulating any situations in real time and using Internet of Things (IoT) for data acquisition to create tailored diagnostic tools that students and laboratory staff can utilize. The software selected for the system is LabVIEW, which controls all the necessary equipment. This system can build personalized dynocard graphs, intake live data and export them to other programs live Excel, MATLAB, Python or any other programming languages.",industry,404,not included
10.1016/j.cola.2020.100970,to_check,Journal of Computer Languages,scopus,2020-06-01,sciencedirect,"visual programming environments for end-user development of intelligent and social robots, a systematic review",https://api.elsevier.com/content/abstract/scopus_id/85085272330,"Robots are becoming interactive and robust enough to be adopted outside laboratories and in industrial scenarios as well as interacting with humans in social activities. However, the design of engaging robot-based applications requires the availability of usable, flexible and accessible development frameworks, which can be adopted and mastered by researchers and practitioners in social sciences and adult end users as a whole. This paper surveys Visual Programming Environments aimed at enabling a paradigm fostering the so-called End-User Development of applications involving robots with social capabilities. The focus of this article is on those Visual Programming Environments that are designed to support social research goals as well as to cater for professional needs of people not trained in more traditional text-based computer programming languages. This survey excludes interfaces aimed at supporting expert programmers, at allowing industrial robots to perform typical industrial tasks (such as pick and place operations), and at teaching children how to code. After having performed a systematic search, sixteen programming environments have been included in this survey. Our goal is two-fold: first, to present these software tools with their technical features and Authoring Artificial Intelligence modeling approaches, and second, to present open challenges in the development of Visual Programming Environments for end users and social researchers, which can be informative and valuable to the community. The results show that the most recent such tools are adopting distributed and Component-Based Software Engineering approaches and web technologies. However, few of them have been designed to enable the independence of end users from high-tech scribes. Moreover, findings indicate the need for (i) more objective and comparative evaluations, as well as usability and user experience studies with real end users; and (ii) validations of these tools for designing applications aimed at working “in-the-wild” rather than only in laboratories and structured settings.",industry,405,not included
10.1016/j.microrel.2020.113640,to_check,Microelectronics Reliability,scopus,2020-05-01,sciencedirect,two phase cooling with nano-fluid for highly dense electronic systems-on-chip – a pilot study,https://api.elsevier.com/content/abstract/scopus_id/85083093178,"In recent days, electronics gadgets need to design for higher functionalities with dense populated systems in order to meet the demands like lower in size, weight and power consumption. Even industrial electronic component and system design also prefer same slim fashion. On other hand the overheating of electronic components reduces its performance, life and by the way the reliability of such electronic product/system is greatly affected due to overheating. The conventional cooling methods failed to offer best performances. Hence this part of research proposed a effective two phase cooling technique with nano-fluid. The objective of this research is to maintain the maximum temperature at the junction and hot spots in order to break a new ground in the cooling of electronic systems. The Maximum permissible operating temperature for any commercial electronic applications is only upto 70 °C (equal to 343.15 K) and above which most of the inherent electronic circuits may malfunction and destroy the entire application.The HotSpot Simulator-6.0 software employed for establish, verify the simulated model and trial runs to answer many ‘what if’ questions. In the simulation, hottest spot has been found in Int_Reg region, where the steady temperature grows beyond the threshold temperature level. The temperature has to be decreased in order to provide reliable working environment. Hence, HFO 1234ze nano-fluid employed with flow rate of 1100 ml per minute. The nanofliuid minimizes the temperature of the simulated electronic circuit from 351.80 K to 326.86 K in UUT. The proposed two phase nano-fluid cooling system for 3-D Unit-Under Test (UUT) was verified and validated with real system and simulated for experiments. Thus, a high range of temperature difference from the initial and final steady state temperature has been evidently shown in the proposed two phase nano fluid cooling method. The system found outperforms as best of both the worlds. The nanofluid cooling system can be used in thermal-aware systems and highly dense systems to maintain the temperature not much than 343.15 K, even at full load conditions.",industry,406,not included
10.1016/j.adhoc.2019.102047,to_check,Ad Hoc Networks,scopus,2020-03-01,sciencedirect,an intelligent edge-iot platform for monitoring livestock and crops in a dairy farming scenario,https://api.elsevier.com/content/abstract/scopus_id/85076174369,"Today’s globalized and highly competitive world market has broadened the spectrum of requirements in all the sectors of the agri-food industry. This paper focuses on the dairy industry, on its need to adapt to the current market by becoming more resource efficient, environment-friendly, transparent and secure. The Internet of Things (IoT), Edge Computing (EC) and Distributed Ledger Technologies (DLT) are all crucial to the achievement of those improvements because they allow to digitize all parts of the value chain, providing detailed information to the consumer on the final product and ensuring its safety and quality. In Smart Farming environments, IoT and DLT enable resource monitoring and traceability in the value chain, allowing producers to optimize processes, provide the origin of the produce and guarantee its quality to consumers. In comparison to a centralized cloud, EC manages the Big Data generated by IoT devices by processing them at the network edge, allowing for the implementation of services with shorter response times, and a higher Quality of Service (QoS) and security. This work presents a platform oriented to the application of IoT, Edge Computing, Artificial Intelligence and Blockchain techniques in Smart Farming environments, by means of the novel Global Edge Computing Architecture, and designed to monitor the state of dairy cattle and feed grain in real time, as well as ensure the traceability and sustainability of the different processes involved in production. The platform is deployed and tested in a real scenario on a dairy farm, demonstrating that the implementation of EC contributes to a reduction in data traffic and an improvement in the reliability in communications between the IoT-Edge layers and the Cloud.",industry,407,included
10.1016/j.vehcom.2019.100198,to_check,Vehicular Communications,scopus,2020-01-01,sciencedirect,in-vehicle network intrusion detection using deep convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85073150001,"The implementation of electronics in modern vehicles has resulted in an increase in attacks targeting in-vehicle networks; thus, attack detection models have caught the attention of the automotive industry and its researchers. Vehicle network security is an urgent and significant problem because the malfunctioning of vehicles can directly affect human and road safety. The controller area network (CAN), which is used as a de facto standard for in-vehicle networks, does not have sufficient security features, such as message encryption and sender authentication, to protect the network from cyber-attacks. In this paper, we propose an intrusion detection system (IDS) based on a deep convolutional neural network (DCNN) to protect the CAN bus of the vehicle. The DCNN learns the network traffic patterns and detects malicious traffic without hand-designed features. We designed the DCNN model, which was optimized for the data traffic of the CAN bus, to achieve high detection performance while reducing the unnecessary complexity in the architecture of the Inception-ResNet model. We performed an experimental study using the datasets we built with a real vehicle to evaluate our detection system. The experimental results demonstrate that the proposed IDS has significantly low false negative rates and error rates when compared to the conventional machine-learning algorithms.",industry,408,not included
10.1016/j.ifacol.2019.11.172,to_check,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,machine learning framework for predictive maintenance in milling,https://api.elsevier.com/content/abstract/scopus_id/85078904429,"In the Industry 4.0 era, artificial intelligence is transforming the manufacturing industry. With the advent of Internet of Things (IoT) and machine learning methods, manufacturing systems are able to monitor physical processes and make smart decisions through realtime communication and cooperation with humans, machines, sensors, and so forth. Artificial intelligence enables manufacturers to reduce equipment downtime, spot production defects, improve the supply chain, and shorten design times by using machine learning technologies which learn from experiences. One of the last application of these technologies is the development of Predictive Maintenance systems. Predictive maintenance combines Industrial IoT technologies with machine learning to forecast the exact time in which manufacturing equipment will need maintenance, allowing problems to be solved and adaptive decisions to be made in a timely fashion. This study will discuss the implementation of a milling Cutting-tool Predictive Maintenance solution (including Wear Monitoring), applied to a real milling data set as validation of the framework. More generally, this work provides a basic framework for creating a tool to monitor the wear level, preventing the breakdown, of a generic manufacturing tool, in order to improve human-machine interaction and optimize the production process.",industry,409,not included
10.1016/j.scitotenv.2019.02.213,to_check,Science of the Total Environment,scopus,2019-05-20,sciencedirect,passive sampling of volatile organic compounds in industrial atmospheres: uptake rate determinations and application,https://api.elsevier.com/content/abstract/scopus_id/85061829807,"This study describes the implementation of a passive sampling-based method followed by thermal desorption gas-chromatography-mass spectrometry (TD-GC–MS) for the monitoring of volatile organic compounds (VOCs) in industrial atmospheres. However, in order to employ passive sampling as a reliable sampling technique, a specific diffusive uptake rate is required for each compound. Accordingly, the aim of the present study was twofold. First, the experimental diffusive uptake rates of the target VOCs were determined under real industrial air conditions using Carbopack X thermal desorption tubes, and active sampling as reference method. The sampling campaigns carried out between October 2017 and May 2018 provided us of experimental diffusive uptake rates between 0.40 mL min−1 and 0.70 mL min−1 and stable over time (RSD % < 8%) for up to 41 VOCs. Secondly, the uptake rates obtained experimentally were applied for the determination of VOCs concentrations at 16 sampling sites in the North Industrial Complex of Tarragona. The results showed i-pentane, n-pentane and the compounds known as BTEX as the most representative ones. Moreover, some sporadic peaks of 1,3-butadiene, acrylonitrile, ethylbenzene and styrene resulting from certain industrial activities were detected.",industry,410,not included
10.1016/j.ifacol.2019.08.225,to_check,IFAC-PapersOnLine,scopus,2019-01-01,sciencedirect,curriculum change for graduate-level control engineering education at the universidad pontificia bolivariana,https://api.elsevier.com/content/abstract/scopus_id/85076258553,"This paper addresses the graduate-level control engineering curriculum change performed at the Universidad Pontificia Bolivariana (UPB), Medellin, Colombia. New proposed methodologies include active learning activities using a new multipurpose experimental test bed that was developed with industrial components. The renovated graduate-level control engineering related courses include: Continuous Processes, Discrete Processes, Fuzzy Logic, Neural Networks and Genetic Algorithms, Linear Control, Nonlinear Control, and Optimal Estimation. The new experimental station was developed for teaching, research, and industrial training activities for the School of Engineering at the UPB. In this work, we report the use of the station in an Optimal Estimation course to replace a traditional homework/exams evaluation approach with an applied work that required independent study, the implementation of different observers in a real lab-scale industrial plant, and a paper-style written report. Increasing independent study activities resulted in academic discussions that are valuable for the learning process of the student. The use of the experimental station and the real comparison of estimation algorithms, implemented by using industrial controllers and high-level programming environments, provided the student skills that cannot be acquired by using only simulations in which real implementation restrictions/challenges do not appear. This work represents one of the first approaches for the implementation of the new curriculum model at the UPB for graduate education. The methodology used in the Optimal Estimation class promoted independent learning, critical thinking and writing skills through significant learning activities.",industry,411,not included
10.1016/j.promfg.2018.04.026,to_check,Procedia Manufacturing,scopus,2018-01-01,sciencedirect,design and implementation of a low cost rfid track and trace system in a learning factory,https://api.elsevier.com/content/abstract/scopus_id/85052890798,"The factories of the future will make use of actuators, sensors and cyber-physical systems (CPS) to provide an environment in which human beings, machines, and resources will communicate as in a social network. In such a network, communication between various “objects” relay the current state of the physical world. Business decisions are made using the information and it is therefore critical that this information is accurate and in real-time. Information flow is a key enabler of such future factories. Industrial engineers, as designers and improvement agents of such factories of the future, will need to develop better skills in various aspects of data analytics and information communication technologies. This paper describes the development and implementation of a low cost RFID track and trace system (by students) for application in a Learning Factory for teaching undergraduate industrial engineering students key concepts related to Industry 4.0 and “smart factories”. The benefit of this system is not only a demonstrator to be used in the Learning Factory, but also can be used to teach students in a “learning by doing” fashion critical skills related to real time tracking in a manufacturing environment. The system also demonstrates potential low cost implementation of such technologies in SME’s.",industry,412,not included
10.1016/j.neucom.2016.09.005,to_check,Neurocomputing,scopus,2017-05-10,sciencedirect,wood moisture content prediction using feature selection techniques and a kernel method,https://api.elsevier.com/content/abstract/scopus_id/84996497604,"Wood is a renewable, abundant bio-energy and environment friendly resource. Woody biomass Moisture Content (
                        MC
                     ) is a key parameter for controlling the biofuel product qualities and properties. In this paper, we are interested in predicting 
                        MC
                      from data. The input impedance of half-wave dipole antenna when buried in the wood pile varies according to the permittivity of wood. Hence, the measurement of reflection coefficient, that gives information about the input impedance, depends directly on the 
                        MC
                      of wood. The relationship between the reflection coefficient measurements and the 
                        MC
                      is studied. Based upon this relationship, 
                        MC
                      predictive models that use machine learning techniques and feature selection methods are proposed. Numerical experiments using real world data show the relevance of the proposed approach that requires a limited computational power. Therefore, a real-time implementation for industrial processes is feasible.",industry,413,not included
10.1016/j.promfg.2017.04.039,to_check,Procedia Manufacturing,scopus,2017-01-01,sciencedirect,digital twin as enabler for an innovative digital shopfloor management system in the esb logistics learning factory at reutlingen - university,https://api.elsevier.com/content/abstract/scopus_id/85020859111,"Technologies for mapping the “digital twin” have been under development for approximately 20 years. Nowadays increasingly intelligent, individualized products encourages companies to respond innovatively to customer requirements and to handle the rising product variations quickly.
                  An integrated engineering network, spanning across the entire value chain, is operated to intelligently connect various company divisions, and to generate a business ecosystem for products, services and communities. The conditions for the digital twin are thereby determined in which the digital world can be fed into the real, and the real world back into the digital to deal such intelligent products with rising variations.
                  The term digital twin can be described as a digital copy of a real factory, machine, worker etc., that is created and can be independently expanded, automatically updated as well as being globally available in real time. Every real product and production site is permanently accompanied by a digital twin. First prototypes of such digital twins already exist in the ESB Logistics Learning Factory on a cloud- and app-based software that builds on a dynamic, multidimensional data and information model. A standardized language of the robot control systems via software agents and positioning systems has to be integrated. The aspect of the continuity of the real factory in the digital factory as an economical means of ensuring continuous actuality of digital models looks as the basis of changeability.
                  For the indoor localization sensor combinations that in addition to the hardware already contain the software required for the sensor data fusion should be used. Processing systems, scenario-live-simulations and digital shop floor management results in a mandatory procedural combination. Essential to the digital twin is the ability to consistently provide all subsystems with the latest state of all required information, methods and algorithms.",industry,414,not included
10.1016/j.knosys.2016.07.022,to_check,Knowledge-Based Systems,scopus,2016-10-15,sciencedirect,software test quality rating: a paradigm shift in swarm computing for software certification,https://api.elsevier.com/content/abstract/scopus_id/84979704090,"Recently, software quality issues have emerged to be recognized as a fundamental point as we actualize an extensive growth of organizations involved in software industries. Still, these organizations cannot ensure the quality of their products; therefore abandoning customers in uncertainties. Software certification is the branch of quality by means that quality requires to be measured prior to certification admitting process. However, creating an official certification model is difficult due to the deficiency of data in the domain of software engineering. This research participates in solving the problem of assessing software quality by introducing a model that handles a fuzzy inference engine to mix both of the processes–driven and application-driven quality assurance procedures. The fundamental purpose of the suggested model is to enhance the compactness and the interpretability of the system's fuzzy rules via engaging an ant colony optimization algorithm (ACO), which attempts to discover a good rule description by a set of compound rules initially represented with traditional single rules. The proposed model is a fitting one that can be seen as practicing certification models that have already been created from software quality domain data and modifying them to a context-specific data. The model has been tested by a case study and the results have confirmed feasibility and practicality of the model in a real environment.",industry,415,included
10.1016/j.ijfoodmicro.2015.03.010,to_check,International Journal of Food Microbiology,scopus,2015-07-02,sciencedirect,a strategy to establish food safety model repositories,https://api.elsevier.com/content/abstract/scopus_id/84926308733,"Transferring the knowledge of predictive microbiology into real world food manufacturing applications is still a major challenge for the whole food safety modelling community. To facilitate this process, a strategy for creating open, community driven and web-based predictive microbial model repositories is proposed. These collaborative model resources could significantly improve the transfer of knowledge from research into commercial and governmental applications and also increase efficiency, transparency and usability of predictive models. To demonstrate the feasibility, predictive models of Salmonella in beef previously published in the scientific literature were re-implemented using an open source software tool called PMM-Lab. The models were made publicly available in a Food Safety Model Repository within the OpenML for Predictive Modelling in Food community project. Three different approaches were used to create new models in the model repositories: (1) all information relevant for model re-implementation is available in a scientific publication, (2) model parameters can be imported from tabular parameter collections and (3) models have to be generated from experimental data or primary model parameters. All three approaches were demonstrated in the paper. The sample Food Safety Model Repository is available via: http://sourceforge.net/projects/microbialmodelingexchange/files/models and the PMM-Lab software can be downloaded from http://sourceforge.net/projects/pmmlab/. This work also illustrates that a standardized information exchange format for predictive microbial models, as the key component of this strategy, could be established by adoption of resources from the Systems Biology domain.",industry,416,not included
10.1016/j.ifacol.2015.06.228,to_check,IFAC-PapersOnLine,scopus,2015-05-01,sciencedirect,multicast dataset synchronization and agent negotiation in distributed manufacturing control systems,https://api.elsevier.com/content/abstract/scopus_id/84953870369,"Multi agent systems represent an elegant approach for the control architecture of manufacturing systems. Distributed control architectures have the potential to achieve greater flexibility by being capable of local decision making based on real time reasoning. One of the main challenges of these distributed architectures is represented by the capability to synchronize the production data across all execution points in a reliable and consistent fashion. In this context, this paper aims to resolve the problems associated with real time production data synchronization in distributed multi-agent control systems by proposing a common dataset synchronized across all agent entities using multicast network communication. On top of this common dataset approach, an agent negotiation mechanism is proposed that addresses the operation sequencing and resource allocation in decentralized operation model. The pilot implementation is using JADE multi agent platform and JGroups for real time data synchronization and NetLogo for abstract representation of the simulation system. Experimental results gathered from the pilot implementation are discussed.",industry,417,not included
10.1016/j.mejo.2013.12.006,to_check,Microelectronics Journal,scopus,2014-03-01,sciencedirect,enhancing confidence in indirect analog/rf testing against the lack of correlation between regular parameters and indirect measurements,https://api.elsevier.com/content/abstract/scopus_id/84897670549,"The greedy specification testing remains mandatory for analog and radio frequency (RF) integrated circuits because of the accuracy of the sorting based on these measurements. Unfortunately, to be implemented, this kind of testing method often incurs very high costs (expensive instruments, long test time…). A common approach, in the literature, is the so-called indirect/alternate test strategy. This strategy consists in deriving targeted specifications from low-cost Indirect Measurements (IMs). During the industrial test phase, the estimation of regular specifications using IMs is based on a correlation model that has been built previously, during a training phase. Despite the substantial test cost reduction offered by this strategy, its deployment in industry is limited, mainly because of a lack of confidence in the accuracy of estimations made by the correlation model. A solution to increase the confidence in the estimation of specifications using the indirect approach is to implement redundancy in the prediction phase. In this paper, we demonstrate that the redundancy implementation brings more than identifying rare misjudged circuits from a high-correlated model. Indeed redundancy massively increases the accuracy despite of the lack of accurate models that have been assumed in previous implementations of redundant indirect testing. This approach is illustrated on a real case study for which we have experimental measurements on a set of 10,000 devices.",industry,418,not included
10.1016/j.neucom.2012.04.033,to_check,Neurocomputing,scopus,2013-06-03,sciencedirect,applying soft computing techniques to optimise a dental milling process,https://api.elsevier.com/content/abstract/scopus_id/84875966713,"This study presents a novel soft computing procedure based on the application of artificial neural networks, genetic algorithms and identification systems, which makes it possible to optimise the implementation conditions in the manufacturing process of high precision parts, including finishing precision, while saving both time and financial costs and/or energy. This novel intelligent procedure is based on the following phases. Firstly, a neural model extracts the internal structure and the relevant features of the data set representing the system. Secondly, the dynamic system performance of different variables is specifically modelled using a supervised neural model and identification techniques. This constitutes the model for the fitness function of the production process, using relevant features of the data set. Finally, a genetic algorithm is used to optimise the machine parameters from a non parametric fitness function. The proposed novel approach was tested under real dental milling processes using a high-precision machining centre with five axes, requiring high finishing precision of measures in micrometres with a large number of process factors to analyse. The results of the experiment, which validate the performance of the proposed approach, are presented in this study.",industry,419,included
10.1016/j.dss.2012.08.006,to_check,Decision Support Systems,scopus,2012-12-01,sciencedirect,sales forecasting for computer wholesalers: a comparison of multivariate adaptive regression splines and artificial neural networks,https://api.elsevier.com/content/abstract/scopus_id/84868667879,"Artificial neural networks (ANNs) have been found to be useful for sales/demand forecasting. However, one of the main shortcomings of ANNs is their inability to identify important forecasting variables. This study uses multivariate adaptive regression splines (MARS), a nonlinear and non-parametric regression methodology, to construct sales forecasting models for computer wholesalers. Through the outstanding variable screening ability of MARS, important sales forecasting variables for computer wholesalers can be obtained to enable them to make better sales management decisions. Two sets of real sales data collected from Taiwanese computer wholesalers are used to evaluate the performance of MARS. The experimental results show that the MARS model outperforms backpropagation neural networks, a support vector machine, a cerebellar model articulation controller neural network, an extreme learning machine, an ARIMA model, a multivariate linear regression model, and four two-stage forecasting schemes across various performance criteria. Moreover, the MARS forecasting results provide useful information about the relationships between the forecasting variables selected and sales amounts through the basis functions, important predictor variables, and the MARS prediction function obtained, and hence they have important implications for the implementation of appropriate sales decisions or strategies.",industry,420,not included
10.1016/j.asoc.2011.05.011,to_check,Applied Soft Computing Journal,scopus,2011-12-01,sciencedirect,credit risk evaluation using neural networks: emotional versus conventional models,https://api.elsevier.com/content/abstract/scopus_id/80053571498,"Credit scoring and evaluation is one of the key analytical techniques in credit risk evaluation which has been an active research area in financial risk management. Artificial neural networks (NNs) have been considered to be accurate tools for credit analysis among others in the credit industry. Lately, emotional neural networks (EmNNs) have been suggested and applied successfully for pattern recognition. In this paper we investigate the efficiency of EmNNs and compare their performance to conventional NNs when applied to credit risk evaluation. In total 12 neural networks; based equally on emotional and conventional neural models; are arbitrated under three learning schemes to classify whether a credit application is approved or declined. The learning schemes differ in the ratio of training-to-validation data used during training and testing the neural networks. The emotional and conventional neural models are trained using real world credit application cases from the Australian credit approval datasets which has 690 cases; each case with 14 numerical attributes; based on which an application is accepted or rejected. The performance of the 12 neural networks will be evaluated using certain criteria. Experimental results suggest that both emotional and conventional neural models can be used effectively for credit risk evaluations, however the emotional models outperform their conventional counterparts in decision making speed and accuracy, thus, making them ideal for implementation in fast automatic processing of credit applications.",industry,421,included
10.1016/j.neucom.2011.06.027,to_check,Neurocomputing,scopus,2011-11-01,sciencedirect,neural network based controller for cr<sup>6+</sup>-fe<sup>2+</sup> batch reduction process,https://api.elsevier.com/content/abstract/scopus_id/80053311549,An automated pilot plant has been designed and commissioned to carry out online/real-time data acquisition and control for the Cr6+–Fe2+ reduction process. Simulated data from the Cr6+–Fe2+ model derived are validated with online data and laboratory analysis using ICP-AES analysis method. The distinctive trend or patterns exhibited in the ORP profiles for the non-equilibrium model derived have been utilized to train neural network-based controllers for the process. The implementation of this process control is to ensure sufficient Fe2+ solution is dosed into the wastewater sample in order to reduce all Cr6+–Cr3+. The neural network controller has been utilized to compare the capability of set-point tracking with a PID controller in this process. For this process neural network-based controller dosed in less Fe2+ solution compared to the PID controller which hence reduces wastage of chemicals. Industrial Cr6+ wastewater samples obtained from an electro-plating factory has also been tested on the pilot plant using the neural network-based controller to determine its effectiveness to control the reduction process for a real plant. The results indicate the proposed controller is capable of fully reducing the Cr6+–Cr3+ in the batch treatment process with minimal dosage of Fe2+.,industry,422,not included
10.3182/20100831-4-fr-2021.00069,to_check,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2010-01-01,sciencedirect,synthetic target systems in control education: lessons teachers are learning from students,https://api.elsevier.com/content/abstract/scopus_id/84901939930,"In modern Programmable Logic Controllers (PLCs) programming education and training, software packages emulating industrial plants are replacing physical target systems. Whilst this approach is indubitably cost and safety effective, it is important to understand how educationally effective it is, and to what extent can it replace training on real plants. The paper analyses this problem from the feedback that authors got from HMS research and their students in a more than ten years build up experience of control education based on both real and synthetic systems. It concludes that synthetic plants are indeed effective in many training scenarios, but real target systems are still irreplaceable for a cluster of applications. Moreover, despite computer games technology is making synthetic target systems very appealing, both educators and simulation software developers recognize that there is still room for improvement. As such, seizing recent advances in computer technology, software developers are preparing a new generation of synthetic plants.",industry,423,not included
10.3182/20090603-3-ru-2001.0447,to_check,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2009-01-01,sciencedirect,telematics application to optimize operation process of municipal heat and power plant,https://api.elsevier.com/content/abstract/scopus_id/79960930362,"Municipal heat and power plant is a complex and huge industrial system. The main elements of the system are the heating boilers. To perform the operation process of the boilers efficiently it is necessary to monitor a wide range of operation parameters in a real time. Great number of the parameters, short response time and big distance between the controlled objects are the main reasons for telematic systems implementation. But, the basic conditions of telematic system application are: measurement instruments and proper control algorithm. Nowadays, heating boilers are equipped with measurement systems by the producer. The heating boilers are very expensive devices whose operation phase is very long. Therefore, in many municipal heat and power plants, the production process is carried out using old type heating boilers. In such cases telematic systems should operate in spite of limited measurement vector. To do this, special control procedures ought to be implemented. In the paper, the heating boiler control algorithms are presented. Described algorithms are used in case of real industrial objects where a set of monitored parameters is not sufficient for executing full automatic control. An expert system dedicated to support the operation process of heating boiler is also presented. Because of limited information about the heating boiler operation, the process is controlled approximately. To deal with the indicated problem, the idea of fuzzy sets implementation is also described. The presented method of control process fuzzification can increase the quality of heating boiler operation. It will make the implementation of power industry telematics more efficient.",industry,424,not included
10.1016/j.simpat.2007.04.011,to_check,Simulation Modelling Practice and Theory,scopus,2007-09-01,sciencedirect,grey-box modeling of a motorcycle shock absorber for virtual prototyping applications,https://api.elsevier.com/content/abstract/scopus_id/34547773158,"There is an increasing use of virtual prototyping tools in the motorcycle industry, aimed at reducing the development time of new models and speeding up performance optimization, by providing the designer with an in-laboratory virtual test track. Virtual prototyping software are multibody simulation software, which require the availability of models of all the vehicle components. The choice of the model is then of paramount importance, since it heavily affects the accuracy and reliability of the simulation results. Conventional models (like linear models) are often inadequate to describe the behavior of complex nonlinear components, so that it is necessary to appeal to different modeling approaches. This is actually the case when dealing with motorcycle suspension systems, given that their most critical part, the shock absorber, exhibits nonlinear and time-variant behavior.
                  In this paper, a grey-box model of a racing motorcycle mono-tube shock absorber is proposed, which consists of a nonlinear parametric model and a black-box, neural-network-based model. The absorber model has been implemented in a numerical simulation environment, and validated against experimental test data. The results of the validation show that the model is able to reproduce the real behavior of the shock absorber with an accuracy that matches or even beats that of other models previously presented in the literature. The interfacing of the proposed model to the ADAMS virtual prototyping environment is also discussed.",industry,425,not included
10.1016/j.cie.2004.05.005,to_check,Computers and Industrial Engineering,scopus,2004-01-01,sciencedirect,application of neural networks to heuristic scheduling algorithms,https://api.elsevier.com/content/abstract/scopus_id/3242706865,"This paper considers the use of artificial neural networks (ANNs) to model six different heuristic algorithms applied to the n job, m machine real flowshop scheduling problem with the objective of minimizing makespan. The objective is to obtain six ANN models to be used for the prediction of the completion times for each job processed on each machine and to introduce the fuzziness of scheduling information into flowshop scheduling. Fuzzy membership functions are generated for completion, job waiting and machine idle times. Different methods are proposed to obtain the fuzzy parameters. To model the functional relation between the input and output variables, multilayered feedforward networks (MFNs) trained with error backpropagation learning rule are used. The trained network is able to apply the learnt relationship to new problems. In this paper, an implementation alternative to the existing heuristic algorithms is provided. Once the network is trained adequately, it can provide an outcome (solution) faster than conventional iterative methods by its generalizing property. The results obtained from the study can be extended to solve the scheduling problems in the area of manufacturing.",industry,426,not included
10.3390/molecules26144356,to_check,core,'MDPI AG',2021-01-01 00:00:00,core,new composite sorbent for removal of sulfate ions from simulated and real groundwater in the batch and continuous tests,,"The evaluation of groundwater quality in the Dammam formation, Faddak farm, Karbala Governorate, Iraq proved that the sulfate (SO42−) concentrations have high values; so, this water is not suitable for livestock, poultry and irrigation purposes. For reclamation of this water, manufacturing of new sorbent for permeable reactive barrier was required through precipitation of Mg and Fe hydroxides nanoparticles on the activated carbon (AC) surface with best Mg/Fe molar ratio of 7.5/2.5. Mixture of 50% coated AC and 50% scrap iron was applied to eliminate SO42− from contaminated water with efficiency of 59% and maximum capacity of adsorption equals to 9.5 mg/g for a time period of 1 h, sorbent dosage 40 g/L, and initial pH = 5 at 50 mg/L initial SO42− concentration and 200 rpm shaking speed. Characterization analyses certified that the plantation of Mg and Fe nanoparticles onto AC was achieved. Continuous tests showed that the longevity of composite sorbent is increased with thicker bed and lower influent concentration and flow rate. Computer solution (COMSOL) software was well simulated for continuous measurements. The reclamation of real contaminated groundwater was achieved in column set-up with efficiency of 70% when flow rate was 5 mL/min, bed depth was 50 cm and inlet SO42− concentration was 2301 mg/L.Validerad;2021;Nivå 2;2021-07-29 (beamah);Forskningsfinansiär: Taif University (TURSP-2020/49)</p",industry,427,not included
"[{'title': 'mathematical biosciences and engineering', 'identifiers': ['issn:1551-0018', '1551-0018']}]",to_check,core,10.3934/mbe.2020015,2020-01-01 00:00:00,core,'american institute of mathematical sciences (aims)',An intelligent indoor positioning system based on pedestrian directional signage object detection: a case study of Taipei Main Station,"Indoor positioning technologies have gained great interest from both industry and academia. Variety of services and applications can be built based on the availability and accessibility of indoor positioning information, for example indoor navigation and various location-based services. Different approaches have been proposed to provide indoor positioning information to users, in which an underlying system infrastructure is usually assumed to be well deployed in advance to provide the position information to users. Among many others, one common strategy is to deploy a bunch of active sensor nodes, such as WiFi APs and Bluetooth transceivers, to the indoor environment to serve as reference landmarks. The user's current location can thus be obtained directly or indirectly according to the active sensor signals collected by the user. Different from conventional infrastructure-based approaches, which put additional sensor devices to the environment, we utilize available objects in the environment as location landmarks. Leveraging wildly available smartphone devices as customer premises equipment to the user and the cutting-edge deep-learning technology, we investigate the feasibility of an infrastructure-free intelligent indoor positioning system based on visual information only. The proposed scheme has been verified by a real case study, which is to provide indoor positioning information to users in Taipei Main Station, one of the busiest transportation stations in the world. We use available pedestrian directional signage as location landmarks, which include all of the 52 pedestrian directional signs in the testing area. The Google Objection Detection framework is applied for detection and recognition of the pedestrian directional sign. According to the experimental results, we have shown that the proposed scheme can achieve as high as 98% accuracy to successfully identify the 52 pedestrian directional signs for the three test data sets which include 6,341 test images totally. Detailed discussions of the system design and the experiments are also presented in the paper",industry,428,not included
"[{'title': 'journal of translational medicine', 'identifiers': ['issn:1479-5876', '1479-5876']}]",to_check,core,'Springer Science and Business Media LLC',2017-05-01 00:00:00,core,standardization of platelet releasate products for clinical applications in cell therapy: a mathematical approach,,"Abstract Background Standardized animal-free components are required for manufacturing cell-based medicinal products. Human platelet concentrates are sources of growth factors for cell expansion but such products are characterized by undesired variability. Pooling together single-donor products improves consistency, but the minimal pool sample size was never determined. Methods Supernatant rich in growth factors (SRGF) derived from n = 44 single-donor platelet-apheresis was obtained by CaCl2 addition. n = 10 growth factor concentrations were measured. The data matrix was analyzed by a novel statistical algorithm programmed to create 500 groups of random data from single-donor SRGF and to repeat this task increasing group statistical sample size from n = 2 to n = 20. Thereafter, in created groups (n = 9500), the software calculated means for each growth factor and, matching groups with the same sample size, the software retrieved the percent coefficient of variation (CV) between calculated means. A 20% CV was defined as threshold. For validation, we assessed the CV of concentrations measured in n = 10 pools manufactured according to algorithm results. Finally, we compared growth rate and differentiation potential of adipose-derived stromal/stem cells (ASC) expanded by separate SRGF pools. Results Growth factor concentrations in single-donor SRGF were characterized by high variability (mean (pg/ml)–CV); VEGF: 950–81.4; FGF-b: 27–74.6; PDGF-AA: 7883–28.8; PDGF-AB: 107834–32.5; PDGF-BB: 11142–48.4; Endostatin: 305034–16.2; Angiostatin: 197284–32.9; TGF-β1: 68382–53.7; IGF-I: 70876–38.3; EGF: 2411–30.2). In silico performed analysis suggested that pooling n = 16 single-donor SRGF reduced CV below 20%. Concentrations measured in 10 pools of n = 16 single SRGF were not different from mean values measured in single SRGF, but the CV was reduced to or below the threshold. Separate SRGF pools failed to differently affect ASC growth rate (slope pool A = 0.6; R2 = 0.99; slope pool B = 0.7; R2 0.99) or differentiation potential. Discussion Results deriving from our algorithm and from validation utilizing real SRGF pools demonstrated that pooling n = 16 single-donor SRGF products can ameliorate variability of final growth factor concentrations. Different pools of n = 16 single donor SRGF displayed consitent capability to modulate growth and differentiation potential of expanded ASC. Increasing the pool size should not further improve product composition",industry,429,not included
"[{'title': none, 'identifiers': ['1997-7670', '2541-8785', 'issn:1997-7670', 'issn:2541-8785']}]",to_check,core,Irkutsk State University,2017-12-01 00:00:00,core,locally simple models construction: methodology and practice,,"One of the most notable trends associated with the Fourth industrial revolution is a significant strengthening of the role played by semantic methods. They are engaged in artificial intelligence means, knowledge mining in huge flows of big data, robotization, and in the internet of things. Smart contracts also can be mentioned here, although the ’intelligence’ of smart contracts still needs to be seriously elaborated. These trends should inevitably lead to an increased role of logical methods working with semantics, and significantly expand the scope of their application in practice. However, there are a number of problems that hinder this process.

We are developing an approach, which makes the application of logical modeling efficient in some important areas. The approach is based on the concept of locally simple models and is primarily focused on solving tasks in the management of enterprises, organizations, governing bodies. The most important feature of locally simple models is their ability to replace software systems. Replacement of programming by modeling gives huge advantages, for instance, it dramatically reduces development and support costs. Modeling, unlike programming, preserves the explicit semantics of models allowing integration with artificial intelligence and robots. In addition, models are much more understandable to general people than programs.

In this paper we propose the implementation of the concept of locally simple modeling on the basis of so-called document models, which has been developed by us earlier. It is shown that locally simple modeling is realized through document models with finite submodel coverages. In the second part of the paper an example of using document models for solving a management problem of real complexity is demonstrated",industry,430,not included
"[{'title': 'brazilian journal of cardiovascular surgery', 'identifiers': ['issn:1678-9741', '1678-9741']}]",to_check,core,'Sociedade Brasileira de Cirurgia Cardiovascular',,core,development and application of a system based on artificial intelligence for transcatheter aortic prosthesis selection,,"Abstract  Introduction: The interest in Expert systems has increased in the medical area. Some of them are employed even for diagnosis. With the variability of transcatheter prostheses, the most appropriate choice can be complex. This scenario reveals an enabling environment for the use of an Expert system. The goal of the study was to develop an Expert system based on artificial intelligence for supporting the transcatheter aortic prosthesis selection.  Methods: The system was developed on Expert SINTA. The rules were created according to anatomical parameters indicated by the manufacturing company. Annular aortic diameter, aortic area, aortic perimeter, ascending aorta diameter and Valsalva sinus diameter were considered. After performing system accuracy tests, it was applied in a retrospective cohort of 22 patients with submitted to the CoreValve prosthesis implantation. Then, the system indications were compared to the real heart team decisions.  Results: For 10 (45.4%) of the 22 patients there was no concordance between the Expert system and the heart team. In all cases with discordance, the software was right in the indication. Then, the patients were stratified in two groups (same indication vs. divergent indication). The baseline characteristics did not show any significant difference. Mortality, stroke, acute myocardial infarction, atrial fibrillation, atrioventricular block, aortic regurgitation and prosthesis leak did not present differences. Therefore, the maximum aortic gradient in the post-procedure period was higher in the Divergent Indication group (23.9 mmHg vs. 11.9 mmHg, P=0.03), and the mean aortic gradient showed a similar trend.  Conclusion: The utilization of the Expert system was accurate, showing good potential in the support of medical decision. Patients with divergent indication presented high post-procedure aortic gradients and, even without clinical repercussion, these parameters, when elevated, can lead to early prosthesis dysfunction and the necessity of reoperation",industry,431,not included
10.1109/aero47225.2020.9172439,to_check,2020 IEEE Aerospace Conference,IEEE,2020-03-14 00:00:00,ieeexplore,"smart &amp; integrated management system - smart cities, epidemiological control tool using drones",https://ieeexplore.ieee.org/document/9172439/,"This paper describes the development of a real application using Drones over urban regions to help the authorities at epidemiological control through a disruptive solutions based on a customizable Smart &amp; Integrated Management System (SIGI), devices and software based on the Enterprise Resource Planning (ERP) concept. Compound by management software, Drones and specific IoT devices, both referred to as sensors, the sensors collect the data of the interest areas in real time, creating a specified database. Based on the data collected from the interest areas, SIGI software has the ability to show real-time situational analysis of these areas and allows that the administrator can optimize resources (material and human) improving the efficiency of resource allocation in these areas. In addition to the development of the management software, the development of sensors to collect the information in the field and update these information to the database of the management software, are considered. The sensors will be recognized as IoT devices for the collection of meteorological data, images and command / control Drones. Initially the system will be customized, using an Artificial Intelligence tool, to collect data and identify the outbreaks of the dengue mosquito, zika and Chikungunya, nominee by risk areas. After the definition of the potential risk areas, in a complementary way, a totally customized Drone will be used to map these areas of interest, generating aerial photographs, identifying and geotagging the potential “targets”, which will allow the agents to identify potential mosquito breeding sites. After the identification of breeding areas, the next step will be the effective combat of the vectors, using the Drones to fly over the areas of interest, where biological defenses will be “dropped” over the targets to combat mosquitoes. Due some Drone flight restrictions over the cities, the whole process will be monitored by a situation room, that will be able to control the Drone remotely, access the air space controller, reads the sensors installed in the city (field), that will measure, for example, rainfall through weather stations installed in risk areas and subsequently processed by Intelligent System Integrated Management (SIGI), which will result to the information public official reflecting the situational analysis of the areas, which will enable a better management of available resources, helping the public agent, preventively in the decision making.",smart cities,432,included
10.1109/access.2019.2943013,to_check,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,intelligent data delivery approach for smart cities using road side units,https://ieeexplore.ieee.org/document/8846044/,"Smart city progress from classical homogenous technologies with limited facility to heterogeneous interconnected network with immense capabilities. Furthermore, there is a good concern in expanding the scope of application in the smart city. The primary objective of the smart city is to achieve optimization and reinforce the Quality of Service (QoS) of applications by cleverer usage of urban resources. The QoS in the network is measured using several factors like end-end delay, energy consumption, packet loss and throughput. Several pitfalls are experienced in the existing routing innovation. In this proposal, a new technology-based routing structure is proposed. Road Side Units (RSU) will allow the planners to deploy the application without unfamiliar tools for data process and gathering. Data forwarding, acquisition and diffusion are simplified by RSU. K-Nearest Neighbor is used for finding the nearest neighbor nodes and it is optimized using Whale optimization Algorithm (WOA). The evaluation outcomes prove that the intended routing plot provides much spectacle than existing protocols for real time applications.",smart cities,433,not included
10.1109/ica-acca.2018.8609705,to_check,2018 IEEE International Conference on Automation/XXIII Congress of the Chilean Association of Automatic Control (ICA-ACCA),IEEE,2018-10-19 00:00:00,ieeexplore,a survey on intelligent traffic lights,https://ieeexplore.ieee.org/document/8609705/,"Transportation of people and goods has a huge impact on the economy of countries. The exponential grows of vehicles in the cities brings traffic congestion consequences like pollution, high combustible consumption and lowers the quality of life of the citizens. The development of Intelligent Transportation Systems is a key component of Smart Cities; Artificial Intelligence (AI) techniques used to build Intelligent Traffic Light Controllers (ITLC). This paper presents a survey of current technologies being use in the development of ITLC; many of them are experimental and tested in simulators, which show that implanting ITLC systems in real scenarios is difficult and expensive. The main objective of this survey is to find related work and algorithms to control traffic in real-time scenarios with AI. Artificial Vision rises as a promising technology for building reliable Intelligent Traffic Lights (ITL) as the technology evolves the error rate is decreasing, and more applications appear such as vehicle counting and classification, identifying vehicle's moving patterns and giving priority for emergency vehicles. Furthermore, ITL implementation discussed, and the algorithms and different solutions compared.",smart cities,434,not included
10.1109/access.2020.2984127,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,optimal placement of electric vehicle charging stations in the active distribution network,https://ieeexplore.ieee.org/document/9050479/,"Electrification of the transportation sector can play a vital role in reshaping smart cities. With an increasing number of electric vehicles (EVs) on the road, deployment of well-planned and efficient charging infrastructure is highly desirable. Unlike level 1 and level 2 charging stations, level 3 chargers are super-fast in charging EVs. However, their installation at every possible site is not techno-economically justifiable because level 3 chargers may cause violation of critical system parameters due to their high power consumption. In this paper, we demonstrate an optimized combination of all three types of EV chargers for efficiently managing the EV load while minimizing installation cost, losses, and distribution transformer loading. Effects of photovoltaic (PV) generation are also incorporated in the analysis. Due to the uncertain nature of vehicle users, EV load is modeled as a stochastic process. Particle swarm optimization (PSO) is used to solve the constrained nonlinear stochastic problem. MATLAB and OpenDSS are used to simulate the model. The proposed idea is validated on the real distribution system of the National University of Sciences and Technology (NUST) Pakistan. Results show that an optimized combination of chargers placed at judicious locations can greatly reduce cost from $3.55 million to $1.99 million, daily losses from 787kWh to 286kWh and distribution transformer congestion from 58% to 22% when compared to scenario of optimized placement of level 3 chargers for 20% penetration level in commercial feeders. In residential feeder, these statistics are improved from $2.52 to $0.81 million, from 2167kWh to 398kWh and from 106% to 14%, respectively. It is also realized that the integration of PV improves voltage profile and reduces the negative impact of EV load. Our optimization model can work for commercial areas such as offices, university campuses, and industries as well as residential colonies.",smart cities,435,not included
10.1109/eucap.2016.7481737,to_check,2016 10th European Conference on Antennas and Propagation (EuCAP),IEEE,2016-04-15 00:00:00,ieeexplore,integrating composite urban furniture into ray-tracing simulator for 5g small cells and outdoor device-to-device communications,https://ieeexplore.ieee.org/document/7481737/,"Small cells and device-to-device (D2D) communications will play a vital role in the realization of the next generation of wireless mobile communication systems (IMT-2020 or 5G). Both of them involve transmitters and receivers less than 10 m high inducing a more complex urban environment. This paper presents a framework of integrating composite urban furniture, e.g., traffic signs, traffic lights, etc., into ray-tracing tools. The framework starts with the theoretical modeling for radar cross section of furniture components and validation of full-wave analysis simulation in the far field. Then, in order to locally fulfill the far field condition in the small cells or D2D scenarios, the furniture is divided into small segments so that the models in the far field are still applicable. Finally, the decomposed furniture is implemented in a ray-tracing tool and validated by measurements in real scenarios. Under this framework, researchers can improve the ray-tracing tools with more essential elements of urban environment. Last but not least, this paper provides a case study to demonstrate the implementation of the framework, and the results show that the traffic signs indeed influence the vehicle-to-vehicle communications, which is one of the most frequently occurring applications of outdoor D2D systems.",smart cities,436,not included
10.1109/iv48863.2021.9575140,to_check,2021 IEEE Intelligent Vehicles Symposium (IV),IEEE,2021-07-17 00:00:00,ieeexplore,urban traffic surveillance (uts): a fully probabilistic 3d tracking approach based on 2d detections,https://ieeexplore.ieee.org/document/9575140/,"Urban Traffic Surveillance (UTS) is a surveillance system based on a monocular and calibrated video camera that detects vehicles in an urban traffic scenario with dense traffic on multiple lanes and vehicles performing sharp turning maneuvers. UTS then tracks the vehicles using a 3D bounding box representation and a physically reasonable 3D motion model relying on an unscented Kalman filter based approach. Since UTS recovers positions, shape and motion information in a three-dimensional world coordinate system, it can be employed to recognize diverse traffic violations or to supply intelligent vehicles with valuable traffic information. We build on YOLOv3 as a detector yielding 2D bounding boxes and class labels for each vehicle. A 2D detector renders our system much more independent to different camera perspectives as a variety of labeled training data is available. This allows for a good generalization while also being more hardware efficient. The task of 3D tracking based on 2D detections is supported by integrating class specific prior knowledge about the vehicle shape. We quantitatively evaluate UTS using self generated synthetic data and ground truth from the CARLA simulator, due to the non-existence of datasets with an urban vehicle surveillance setting and labeled 3D bounding boxes. Additionally, we give a qualitative impression of how UTS performs on real-world data. Our implementation is capable of operating in real time on a reasonably modern workstation. To the best of our knowledge, UTS is to date the only 3D vehicle tracking system in a surveillance scenario (static camera observing moving targets).",smart cities,437,not included
10.1109/tits.2018.2836141,to_check,IEEE Transactions on Intelligent Transportation Systems,IEEE,2019-03-01 00:00:00,ieeexplore,impact of data loss for prediction of traffic flow on an urban road using neural networks,https://ieeexplore.ieee.org/document/8370052/,"The deployment of intelligent transport systems requires efficient means of assessing the traffic situation. This involves gathering real traffic data from the road network and predicting the evolution of traffic parameters, in many cases based on incomplete or false data from vehicle detectors. Traffic flows in the network follow spatiotemporal patterns and this characteristic is used to suppress the impact of missing or erroneous data. The application of multilayer perceptrons and deep learning networks using autoencoders for the prediction task is evaluated. Prediction sensitivity to false data is estimated using traffic data from an urban traffic network.",smart cities,438,not included
10.1109/hpcc/smartcity/dss.2019.00082,to_check,2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS),IEEE,2019-08-12 00:00:00,ieeexplore,a learning-based vehicle-trajectory generation method for vehicular networking,https://ieeexplore.ieee.org/document/8855534/,"With the rapid development of mobile applications, networking technologies have been constantly evolved to offer a more convenient way of sharing information and online-communication anytime and anywhere. Vehicular networks have the potential to become one of the important carriers of future mobile networks. The performance of current vehicular networks has been widely evaluated through simulation experiments due to the high cost and impracticality of other experimental approaches. The most paramount factors of vehicle networks are the authenticity of simulative evaluation, where the mobility of the vehicles is the first significant feature (i.e., the nodes of the vehicular network) that must be properly considered. However, generating the corresponding real mobility datasets has always been a big challenge although it is vital to the simulations of vehicular networks. Therefore, in this paper, we propose a learning-based generation method that can be used to build the vehicle-trajectory data for variety of vehicle densities. Firstly, with analyzing the road bayonet data, we obtain the hidden pattern between road traffic and time. Secondly, we deploy Vissim (a well-known traffic simulator) to generate the experimental data by considering the urban functional areas for the origins of vehicles. The generated experimental data are learned by Extreme Learning Machine (ELM), and the weight matrix of the parameters is obtained, which presents the impact of the experimental parameters on the simulation results. We prove the effectiveness of our method by comparing the generated vehicle-trajectory datasets with the vehicle density predicted by the weight matrix and the realistic traffic flow model.",smart cities,439,not included
10.1109/icecce49384.2020.9179349,to_check,"2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",IEEE,2020-06-13 00:00:00,ieeexplore,a cloud based smart recycling bin for in-house waste classification,https://ieeexplore.ieee.org/document/9179349/,"Due to the Earth's population rapid growth along with the modern lifestyle the urban waste constantly increases. People consume more and the products are designed to have shorter lifespans. Recycling is the only way to make a sustainable environment. The process of recycling requires the separation of waste materials, which is a time consuming procedure. Most of the proposed research works found in literature are neither budget-friendly nor effective to be practical in real world applications. In this paper, we propose a solution: a low-cost and effective Smart Recycling Bin that utilizes the power of cloud to assist with waste classification for personal in-house usage. A centralized Information System (IS) collects measurements from smart bins that can be deployed virtually anywhere and classifies the waste of each bin using Artificial Intelligence and neural networks. Our implementation is capable of classifying different types of waste with an accuracy of 93.4% while keeping deployment cost and power consumption very low compared to other implementations.",smart cities,440,included
10.1109/mocast49295.2020.9200283,to_check,2020 9th International Conference on Modern Circuits and Systems Technologies (MOCAST),IEEE,2020-09-09 00:00:00,ieeexplore,a cloud based smart recycling bin for waste classification,https://ieeexplore.ieee.org/document/9200283/,"Due to the Earth's population rapid growth along with the modern lifestyle the urban waste constantly increases. People consume more and the products are designed to have shorter lifespans. Recycling is the only way to make a sustainable environment. The process of recycling requires the separation of waste materials, which is a time consuming procedure. However, most of the proposed research works found in literature are neither budget-friendly nor effective to be practical in real world applications. In this paper, we propose a solution: a low-cost and effective Smart Recycling Bin that utilizes the power of cloud to assist with waste classification. A centralized Information System (IS) collects measurements from smart bins that are deployed all around the city and classifies the waste of each bin using Artificial Intelligence and neural networks. Our implementation is capable of classifying different types of waste with an accuracy of 93.4% while keeping deployment cost and power consumption very low.",smart cities,441,included
10.1109/vtcfall.2019.8891257,to_check,2019 IEEE 90th Vehicular Technology Conference (VTC2019-Fall),IEEE,2019-09-25 00:00:00,ieeexplore,fingerprint-based localization using commercial lte signals: a field-trial study,https://ieeexplore.ieee.org/document/8891257/,"Wireless localization for mobile device has attracted more and more interests by increasing the demand for location based services. Fingerprint-based localization is promising, especially in non-Line-of-Sight (NLoS) or rich scattering environments, such as urban areas and indoor scenarios. In this paper, we propose a novel fingerprint-based localization technique based on deep learning framework under commercial long term evolution (LTE) systems. Specifically, we develop a software defined user equipment to collect the real time channel state information (CSI) knowledge from LTE base stations and extract the intrinsic features among CSI observations. On top of that, we propose a time domain fusion approach to assemble multiple positioning estimations. Experimental results demonstrated that the proposed localization technique can significantly improve the localization accuracy and robustness, e.g. achieves Mean Distance Error (MDE) of 0.47 meters for indoor and of 19.9 meters for outdoor scenarios, respectively.",smart cities,442,not included
10.1109/cast.2016.7914958,to_check,"2016 International Conference on Computing, Analytics and Security Trends (CAST)",IEEE,2016-12-21 00:00:00,ieeexplore,intelligent traffic signal synchronization using fuzzy logic and q-learning,https://ieeexplore.ieee.org/document/7914958/,"In the past decade, urban traffic has increased tremendously. As a result, the urban population has to invest more time in traveling. Increased road traffic results in an increased number of road accidents and more consumption of fuel, thus wasting energy. Hence for solving this issue, this paper proposes a traffic signal synchronization system which takes real time traffic signal data as input and with the implementation of multi-agent fuzzy logic, it introduces the design of an intelligent system which would smoothen the overall road traffic of the city. Fuzzy system is capable of handling the various levels of uncertainties found in the input data taken from the traffic signals. Since fuzzy logic system needs expert knowledge for its rule base and the rule base remains unchanged once defined, this paper adds up Q-learning module so that the system learns by itself by updating the set of rule base.",smart cities,443,not included
10.1109/ijcnn.2014.6889658,to_check,2014 International Joint Conference on Neural Networks (IJCNN),IEEE,2014-07-11 00:00:00,ieeexplore,long-term learning behavior in a recurrent neural network for sound recognition,https://ieeexplore.ieee.org/document/6889658/,"In this paper, the long-term learning properties of an artificial neural network model, designed for sound recognition and computational auditory scene analysis in general, are investigated. The model is designed to run for long periods of time (weeks to months) on low-cost hardware, used in a noise monitoring network, and builds upon previous work by the same authors. It consists of three neural layers, connected to each other by feedforward and feedback excitatory connections. It is shown that the different mechanisms that drive auditory attention emerge naturally from the way in which neural activation and intra-layer inhibitory connections are implemented in the model. Training of the artificial neural network is done following the Hebb principle, dictating that ""Cells that fire together, wire together"", with some important modifications, compared to standard Hebbian learning. As the model is designed to be on-line for extended periods of time, also learning mechanisms need to be adapted to this. The learning needs to be strongly attention- and saliency-driven, in order not to waste available memory space for sounds that are of no interest to the human listener. The model also implements plasticity, in order to deal with new or changing input over time, without catastrophically forgetting what it already learned. On top of that, it is shown that also the implementation of short-term memory plays an important role in the long-term learning properties of the model. The above properties are investigated and demonstrated by training on real urban sound recordings.",smart cities,444,not included
10.1109/icimia48430.2020.9074933,to_check,2020 2nd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA),IEEE,2020-03-07 00:00:00,ieeexplore,a hybrid framework for expediting emergency vehicle movement on indian roads,https://ieeexplore.ieee.org/document/9074933/,"Unhindered and smooth movement of emergency vehicles within a city is a crucial aspect of any intelligent transport system. It is common to observe emergency vehicles such as ambulances and fire engines obstructed by traffic snarls on Indian roads, especially in the proximity of busy intersections. Existing literature primarily advocates the deployment of RFID technology to terminate the round-robin sequence of the signal system and switch the signal to green in the required direction. However, this technology has proven to be susceptible to electromagnetic interferences and also the economic feasibility is questionable. This paper proposes a model that employs real time image processing and object detection using a convolutional neural network (CNN) architecture called SSD Mobilenet. Unlike a few other architectures, SSD Mobilenet requires very limited computation, hence enabling swift detection. Furthermore, an acoustic signal (sound) processing (pitch detection) algorithm is employed to detect the sirens of emergency vehicles to nullify the potential false positives (e.g. an ambulance in a non-emergency scenario) that creep into object detection using image processing. Both algorithms work in unison, bolstering the accuracy of detection. Upon detection, the signal instantly switches to green, facilitating the expedited movement of emergency vehicles, even in high traffic conditions.",smart cities,445,not included
10.1109/snpd.2008.90,to_check,"2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing",IEEE,2008-08-08 00:00:00,ieeexplore,an approach to modeling software safety,https://ieeexplore.ieee.org/document/4617469/,"Software for safety-critical systems must deal with the hazards identified by safety analysis in order to make the system safe, risk-free and fail-safe. Software safety is a composite of many factors. Existing software quality models like McCall's and Boehm's and ISO 9126 are inadequate in addressing the software safety issues of real time safety-critical embedded systems. At present there does not exist any standard framework that comprehensively addresses the factors, criteria and metrics (FCM) approach of the quality models in respect of software safety. This paper proposes a new framework for software safety based on the McCall's software quality model that specifically identifies the criteria corresponding to software safety in safety critical applications. The criteria in the proposed software safety model pertains to system hazard analysis, completeness of requirements, identification of software-related safety- critical requirements, safety-constraints based design, runtime issues management, and software safety-critical testing. This framework is then applied to a prototype safety-critical system viz. a software-based road traffic control system (RTCS) commonly used in city traffic, to validate its utility.",smart cities,446,not included
10.1109/aect47998.2020.9194188,to_check,2019 International Conference on Advances in the Emerging Computing Technologies (AECT),IEEE,2020-02-10 00:00:00,ieeexplore,clustering based uav base station positioning for enhanced network capacity,https://ieeexplore.ieee.org/document/9194188/,"Unmanned aerial vehicles (UAVs) are expected to be deployed in a variety of applications in future mobile networks due to several advantages they bring over the deployment of ground base stations. However, despite the recent interest in UAVs in mobile networks, some issues still remain, such as determining the placement of multiple UAVs in different scenarios. In this paper we propose a solution to determine the optimal 3D position of multiple UAVs in a capacity enhancement use-case, or in other words, when the ground network cannot cope with the user traffic demand. For this scenario, real data from the city of Milan, provided by Telecom Italia is utilized to simulate an event. Based on that, a solution based on k-means, a machine learning technique, to position multiple UAVs is proposed and it is compared with two other baseline methods. Results demonstrate that the proposed solution is able to significantly outperform other methods in terms of users covered and quality of service.",smart cities,447,not included
10.1109/icws.2017.76,to_check,2017 IEEE International Conference on Web Services (ICWS),IEEE,2017-06-30 00:00:00,ieeexplore,early air pollution forecasting as a service: an ensemble learning approach,https://ieeexplore.ieee.org/document/8029817/,"Air quality has become a major global concern for human beings involving all social stratums, for both developing and developed countries. Web service of precise and early air pollution forecasting is of great importance as it allows people to pro-actively take preventative and protective measurements. As an endeavor on the course of machine learning based air quality forecasting, this paper presents an initiative and its technological details in solving this challenging problem. Specifically, this work involves three major highlights regarding with both algorithmic innovation and deployment with its impact: 1) We propose a multi-channel ensemble learning framework, 2) We propose a new supervised feature learning and extraction method, i.e. sufficient statistics feature mapping based on Deep Boltzman Machine, which serves as a building block for our learning system, 3) We target our air pollution prediction method to the city of Beijing, China as it is at the forefront for battling against air pollution, which is embodied as a web service for prediction. Extensive experiments of real time air pollution forecasting on the real-world data demonstrates the effectiveness of the proposed method and value of the deployed web service system.",smart cities,448,included
10.1109/icnn.1994.375034,to_check,Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94),IEEE,1994-07-02 00:00:00,ieeexplore,the ordering-oriented hopfield network,https://ieeexplore.ieee.org/document/375034/,"The travelling salesman problem (TSP) is a well known problem which can be solved using Hopfield networks. The TSP solution with Hopfield networks is based on the uniqueness constraint. This is, each city must be visited once and only once while trying to minimize the travelling distance. But in real world applications, usually there are other equally important constraints needing to be considered. For example, an ordering constraint on how cities are to be visited. This paper proposes a modified Hopfield neural network architecture that can solve a new class of optimization problem, called ""the picking stone problem (PSP)"". The PSP requires not only the uniqueness but also the ordering constraints. The neural network implementation to solve PSP tends to turn on neurons which satisfy the ordering constraint and this constraint is essential in solving the stereo correspondence problem in binocular vision and can be applied to many other pattern recognition problems. In this paper the authors define the PSP, formulate its computational complexity, propose the ordering-oriented neural network architecture, discuss the performance of the proposed network by the traditional random initialization method, and propose a new initialization method to improve the performance of the network.&lt;<ETX>&gt;</ETX>",smart cities,449,not included
10.1109/icccn.2019.8847117,to_check,2019 28th International Conference on Computer Communication and Networks (ICCCN),IEEE,2019-08-01 00:00:00,ieeexplore,a user-centric mobility management scheme for high-density fog computing deployments,https://ieeexplore.ieee.org/document/8847117/,"The inherent mobility characterizing users in fog computing environments along with the limited wireless range of their serving fog nodes (FNs) drives the need for designing efficient mobility management (MM) mechanisms. This ensures that users' resource-intensive tasks are always served by the most suitable FNs in their vicinity. However, since MM decisionmaking requires control information which is difficult to predict accurately a-priori, such as the users' mobility patterns and the dynamics of the FNs, researchers have started to shift their attention towards MM solutions based on online learning. Motivated by this approach, in this paper, we consider a bandit learning model to address the mobility-induced FN selection problem, with a particular focus on scenarios with a high FN density. Following this approach, a software agent implemented within the user's device learns the FNs' delay performances via trial and error, by sending them the user's computation tasks and observing the perceived delay, with the goal of minimizing the accumulated delay. This task is particularly challenging when considering a high FN density, since the number of unknown FNs that need to be explored is high, while the time that can be spent on learning their performances is limited, given the user's mobility. Therefore, to address this issue, we propose to limit the number of explorations to a small subset of the FNs. As a result, the user can still have time to be served by the FN that was found to yield the lowest delay performance. Using real world mobility traces and task generation patterns, we found that it pays off to limit the number of explorations in high FN density scenarios. This is shown through significant improvements in the cumulative regret as well as the instantaneous delay, compared to the case where all newly-appeared FNs are explored.",smart cities,450,not included
10.1109/colcaci.2019.8781798,to_check,2019 IEEE Colombian Conference on Applications in Computational Intelligence (ColCACI),IEEE,2019-06-07 00:00:00,ieeexplore,video processing inside embedded devices using ssd-mobilenet to count mobility actors,https://ieeexplore.ieee.org/document/8781798/,"The actual number of surveillance cameras and the different methods for counting vehicles originate the question: What is the best place to process video flows? This work performs the implementation of a counting system for mobility actors like cars, pedestrians, motorcycles, bicycles, buses, and trucks in the context of an Edge computing application using deep learning. However, the implementation of Deep Neural Networks for Object Detection in low-capacity embedded devices make it difficult to perform tasks that require high processing or must be carried out in real time. To solve this problem this study presents the analysis and implementation of different techniques based on the use of an additional hardware element as is the case of a Vision Processing Unit (VPU) in combination with methods that affect the resolution, bit rate, and time of video processing. For this purpose we consider the Mobilenet-SSD model with two approaches: a pre-trained model with known data sets and a trained model with images from our specific scenarios. The use of SSD-Mobilenet's model generates different results in terms of accuracy and time of video processing in the system. Results show that the use of an embedded device in combination with a VPU and video processing techniques reach 18.62 Frames per Second (FPS). Thus, video processing time is slightly superior (5.63 minutes) for a video of 5 minutes. Recall and precision values of 91% and 97% are reported in the best case (class car) for the vehicle counting system.",smart cities,451,not included
10.1109/icmla.2015.65,to_check,2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),IEEE,2015-12-11 00:00:00,ieeexplore,a neural network based handover management strategy for heterogeneous networks,https://ieeexplore.ieee.org/document/7424486/,"One of the key challenges for improvement of quality of services (QoS) in Heterogeneous wireless networks is the design of Vertical Handover (VHO) Management strategy. VHO is required to guide the decision for a mobile terminal (MT) to handoff between different types of networks. This is an essential task to cope with various multimedia services QoS settings. In this paper, we present a machine learning scheme based on Neural Network for calls vertical handover in heterogeneous networks. The Neural Network Based Handover Management Scheme (NNBHMS) of this paper aims toward achieving seamless connectivity and Always Best Connected (ABC) call status for group mobility over a set of heterogeneous networks. The proposed scheme evaluates and creates relationships between different decision criteria related to heterogeneous networks conditions, terminal capabilities, application requirements, and user preferences. Afterward, the estimates of each attribute are forwarded to neural network to select the optimal access network. The proposed scheme is applied for vertical handover Management in heterogeneous networks offering both real time services (voice over IP services), and data Services (packet data traffic). Through the implementation of neural networks based machine learning approach, the proposed research scheme allows solving the complexity of the handover decision process resulting from the multitude dimensions of the decision criteria and the dynamicity of many of its components. The performance results evaluated through simulation show that the use of the a neural network based machine learning scheme to carry out the Handover process can enhance the QoS perceived by both types of voice and data service while fulfilling to great extent the user preference.",smart cities,452,not included
10.1109/vtc2020-fall49728.2020.9348512,to_check,2020 IEEE 92nd Vehicular Technology Conference (VTC2020-Fall),IEEE,2020-12-16 00:00:00,ieeexplore,adaptive deployment of uav-aided networks based on hybrid deep reinforcement learning,https://ieeexplore.ieee.org/document/9348512/,"Unmanned aerial vehicles (UAVs) can be used as air base stations to provide fast wireless connections for ground users. Due to their constraints on both mobility and energy consumption, a key problem is how to deploy UAVs adaptively in a geographic area with changing traffic demand of mobile users, while meeting the aforemetioned constraints. In this paper, we propose an adaptive deployment strategy for UAV-aided networks based on hybrid deep reinforcement learning, where a UAV can adjust its movement direction and distance to serve users who move randomly in the target area. Through hybrid deep reinforcement learning, UAVs can be trained offline to obtain the global state information and learn a completely distributed control strategy, with which each UAV only needs to take actions based on its observed state in the real deployment to be fully adaptive. Moreover, in order to improve the speed and effect of learning, we improve hybrid reinforcement learning, by adding genetic algorithms and TD-error-based resampling optimization mechanism. Simulation results show that the hybrid deep reinforcement learning algorithm has better efficiency and robustness in multi-UAV control, and has better performance in terms of coverage, energy consumption and average throughput, by which average throughput can be increased by 20% to 60%.",smart cities,453,not included
10.1109/apsec.1999.809618,to_check,Proceedings Sixth Asia Pacific Software Engineering Conference (ASPEC'99) (Cat. No.PR00509),IEEE,1999-12-10 00:00:00,ieeexplore,agent-oriented software modeling,https://ieeexplore.ieee.org/document/809618/,"Due to the increased applications of agents, agent-oriented software becomes large and complex. To support systematic developments of such software, the agent-oriented software development methodology needs to be developed. This paper focuses on the modeling phase of agent-oriented software development. For agent-oriented software modeling, the Agent Elicitation method, Intra and Inter Agent modeling method are proposed. Agent Elicitation shows how to extract agents from classes in the real world. Modeling methods of agent's characteristics-Goal, Belief, Plan and Capability-are proposed for Intra Agent modeling. Methods of agent's mobility and communication in multiagent systems are proposed for Inter Agent modeling.",smart cities,454,not included
10.1109/cscn.2017.8088624,to_check,2017 IEEE Conference on Standards for Communications and Networking (CSCN),IEEE,2017-09-20 00:00:00,ieeexplore,big data and machine learning driven handover management and forecasting,https://ieeexplore.ieee.org/document/8088624/,"Handover (HO), as a key aspect of mobility management, plays an important role in improving network quality and mobility performance in mobile networks. Especially, in 5G networks, heterogeneous networks (HetNets) deployment of macro cells and small cells, and the deployment of ultra-dense networks (UDNs) make HO management become more challenging. Besides, the understanding of HO behavior in a cell is quite limited in existing studies, thus the forecasting HO for an individual cell is complicated, even impossible. This challenge led the authors to propose a practical process for managing and forecasting HO for a huge number of cells, based on machinelearning (ML) algorithms and big data. Moreover, based on HO forecasting, the authors also propose an approach to detect any abnormal HO in cells. The performance of the proposed approaches was evaluated by applying it to a real dataset that collected HO KPI of more than 6000 cells of a real network during the years, 2016 and 2017. The results show that the study was successful in identifying, separating HO behavior, forecasting the future number of HO attempts, and detecting abnormal HO behaviors of cells.",smart cities,455,not included
10.1109/vlsid51830.2021.00035,to_check,2021 34th International Conference on VLSI Design and 2021 20th International Conference on Embedded Systems (VLSID),IEEE,2021-02-24 00:00:00,ieeexplore,binary neural network based real time emotion detection on an edge computing device to detect passenger anomaly,https://ieeexplore.ieee.org/document/9407373/,"Passenger safety in public transportation especially while riding in the form of shared cabs, and taxis are often ignored, and not much preventive protocols are devised. In the connected mobility world, emotion recognition from facial expressions is a possibility, however a faster processing and edge computing device to derive anomaly state inferences will be apt for further notifying about the safety of the passenger. FPGA implementation is a viable approach to not only implement in the embedded system automotive electronics, but also accelerate the inference results, hence making it as an ideal real time candidate for passenger anomaly state identification. For the same, a real time emotion detection system using facial features was implemented on FPGA. A Binary Neural Network (BNN) feeded by Local Binary Pattern (LBP) output was designed towards the development of an improved and faster emotion recognition system. LBP is configured as a preprocessing step to extract facial features that is passed on to the BNN layer for successful inference. The preprocessing method utilizes Viola-Jones (VJ) algorithm to extract facial data while removing other background information from the image. The LBP-BNN network is modelled using Facial Expression 2013 (FER-2013) data set for training. The custom hardware accelerator or the overlay is synthesized and the designed IP is implemented on FPGA for the inference. Inference is done using the trained model on FPGA to enable faster classified results. Emotion detection using facial expressions is classified to six states namely: angry, disgust, fear, happy, sad, and surprise. The LBP-BNN network is implemented in FPGA, to realize a real time facial emotion recognition by capturing the image of a person from a web camera interfaced to the FPGA acting as edge computing inference device, with acceptable accuracy. The image processing based emotion detection design is highly suitable for other applications including tracking of emotions for movement disorder patients in hospitals.",smart cities,456,included
10.1109/roman.2005.1513775,to_check,"ROMAN 2005. IEEE International Workshop on Robot and Human Interactive Communication, 2005.",IEEE,2005-08-15 00:00:00,ieeexplore,modularity and integration in the design of a socially interactive robot,https://ieeexplore.ieee.org/document/1513775/,"Designing robots that are capable of interacting with humans in real life settings is a challenging task. One key issue is the integration of multiple modalities (e.g., mobility, physical structure, navigation, vision, audition, dialogue, reasoning) into a coherent framework. Taking the AAAI mobile robot challenge (making a robot attend the national conference on artificial intelligence) as the experimental context, we are currently addressing hardware, software and computation integration issues involved in designing a robot capable of sophisticated interaction with humans. This paper reports on our design solutions and the current status of the work, along with the potential impacts this design on human-robot interaction research.",smart cities,457,not included
10.1109/ccgrid.2019.00048,to_check,"2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)",IEEE,2019-05-17 00:00:00,ieeexplore,multivariate lstm-based location-aware workload prediction for edge data centers,https://ieeexplore.ieee.org/document/8752650/,"Mobile Edge Clouds (MECs) is a promising computing platform to overcome challenges for the success of bandwidth-hungry, latency-critical applications by distributing computing and storage capacity in the edge of the network as Edge Data Centers (EDCs) within the close vicinity of end-users. Due to the heterogeneous distributed resource capacity in EDCs, the application deployment flexibility coupled with the user mobility, MECs bring significant challenges to control resource allocation and provisioning. In order to develop a self-managed system for MECs which efficiently decides how much and when to activate scaling, where to place and migrate services, it is crucial to predict its workload characteristics, including variations over time and locality. To this end, we present a novel location-aware workload predictor for EDCs. Our approach leverages the correlation among workloads of EDCs in a close physical distance and applies multivariate Long Short-Term Memory network to achieve on-line workload predictions for each EDC. The experiments with two real mobility traces show that our proposed approach can achieve better prediction accuracy than a state-of-the art location-unaware method (up to 44%) and a location-aware method (up to 17%). Further, through an intensive performance measurement using various input shaking methods, we substantiate that the proposed approach achieves a reliable and consistent performance.",smart cities,458,not included
10.1109/camad.2018.8515001,to_check,2018 IEEE 23rd International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD),IEEE,2018-09-19 00:00:00,ieeexplore,uncertainty management for wearable iot wristband sensors using laplacian-based matrix completion,https://ieeexplore.ieee.org/document/8515001/,"Contemporary sensing devices provide reliable mechanisms for continuous process monitoring, accommodating use cases related to mHealth and smart mobility, by generating real-time data streams of numerous physiological and vital parameters. Such data streams can be later utilized by machine learning algorithms and decision support systems to predict critical clinical states and motivate users to adopt behaviours that improve the quality of their life and the society as a whole. However, in many cases, even when deployed over highly sophisticated, cutting-edge network infrastructure and deployment paradigms, data may exhibit missing values and non-uniformities due to various reasons, including device malfunction, deliberate data reduction for efficient processing, or data loss due to sensing and communication failures. This work proposes a novel approach to deal with missing entries in heart rate measurements. Benefiting from the low-rank property of the generated data matrices and the proximity of neighbouring measurements, we provide a novel method that combines classical matrix completion approaches with weighted Laplacian interpolation offering high reconstruction accuracy at fast execution times. Extensive evaluation studies carried out with real measurements show that the proposed methods could be effectively deployed by modern wristband-cloud computing systems increasing the robustness, the reliability and the energy efficiency of these systems.",smart cities,459,included
10.1109/tsp.2015.7296288,to_check,2015 38th International Conference on Telecommunications and Signal Processing (TSP),IEEE,2015-07-11 00:00:00,ieeexplore,adaptive noise suppression in voice communication using a neuro-fuzzy inference system,https://ieeexplore.ieee.org/document/7296288/,"This paper describes the implementation of a combination of techniques of the fuzzy system and artificial intelligence in the application area of non-linear suppression of noise and interference. The structure used is called ANFIS (Adaptive Neuro Fuzzy Inference System). This system finds practical use mainly in audio telephone (mobile) communication in a noisy environment (transport, production halls, sports matches, etc.). Within the experiments carried out, the authors created, based on the ANFIS structure, a comprehensive system for the adaptive suppression of unwanted background interference that occurs in audio communication and which degrades the audio signal. The system designed has been tested on real voice signals. Noise cancellation performance of the algorithms has been compared by means of SSNR (Segmental Signal to Noise Ratio) and DTW (Dynamic Time Warping). Also processing durations of the algorithms are determined for evaluating the possibility of real time implementation. The results imply that a system using ANFIS has better experimental results than conventional systems built on adaptive algorithms of the LMS (Least Mean Squares) and RLS (Recursive Least Squares) families.",smart cities,460,not included
10.1109/icmla.2015.209,to_check,2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),IEEE,2015-12-11 00:00:00,ieeexplore,intelligent bus stop identification using smartphone sensors,https://ieeexplore.ieee.org/document/7424444/,"Intelligent transportation systems can be built by developing models that learn from the collected transport data. Data collection and implementation of such systems is often costly, and few countries have support for such systems in their transportation budgets. In places where maintaining currency and accuracy of information is difficult, many problems arise. For instance, in Chennai, India, real time bus transit data is not maintained, there is no proper communication about the bus schedules, bus stops are not regularly updated and inconsistent information about bus stops is observed in the transport authority's website. We are interested in developing models for identifying bus stops from trajectories for situations where accurate and current information is not available and traffic conditions are challenging, such as Chennai, India. We develop a simple yet easily accessible Android mobile application (App) to collect GPS traces of bus routes. We use our App to collect GPS trajectory data from Baltimore, Maryland, a place where there are facilities to access up-to-date information about bus stops. We also collect GPS trajectories from Chennai, India. We then develop a model using machine learning techniques to identify bus stops from the collected trajectories. We experimentally evaluate our model by training it on the Baltimore dataset and testing it on the Chennai dataset, achieving testing accuracy between 85 -- 90%. This is comparable to the accuracy of 95% achieved by both training and testing on the Chennai dataset. This illustrates that our approach is effective in helping maintain an accurate and current transport information system for resource constraint environments.",smart cities,461,included
10.1109/iscas45731.2020.9181034,to_check,2020 IEEE International Symposium on Circuits and Systems (ISCAS),IEEE,2020-10-14 00:00:00,ieeexplore,neuromorphic information processing with nanowire networks,https://ieeexplore.ieee.org/document/9181034/,"Biological neural networks, unlike artificial neural networks (ANNs), can process information from data that is inherently noisy, unstructured, sparse and dynamic. How is this possible? And how can we replicate this? A crucial clue comes from neuroscience: the brain is a complex physical system and the network topology of its neural circuitry is a determinant of its emergent collective properties. Indeed, as the brain's neural network is already entrained in its physical hardware, it clearly does not require an ANN software add-on to learn from natural data. Self-assembled nanowire networks with memristive junctions represent arguably the closest hardware architecture to real biological neural networks and are thus uniquely placed to demonstrate genuinely neuromorphic information processing. Here, we present preliminary results on polymer-coated silver nanowire networks. Their neuromorphic architecture (densely structured network topology, memristive switch junctions and efficient interconnect) gives rise to a rich repertoire of collective nonlinear dynamics manifested through adaptive current transport pathways. The potential for associative learning is demonstrated in a test protocol in which a nanowire network is stimulated by multiple electrodes mapped to different spatial patterns. The capacity to process information in the temporal domain is demonstrated via simulations of a reservoir computing implementation in which nanowire networks are shown to perform tasks such as time series prediction and handwritten digit recognition. Overall, their unique properties and neuromorphic information processing capabilities make nanowire networks promising candidates for emerging applications in cognitive devices in particular, at the edge.",smart cities,462,not included
10.1109/jsac.2021.3064698,to_check,IEEE Journal on Selected Areas in Communications,IEEE,2021-09-01 00:00:00,ieeexplore,autonomous and energy efficient lightpath operation based on digital subcarrier multiplexing,https://ieeexplore.ieee.org/document/9373418/,"The massive deployment of 5G and beyond will require high capacity and low latency connectivity services, so network operators will have either to overprovision capacity in their transport networks or to upgrade the optical network controllers to make decisions nearly in real time; both solutions entail high capital and operational expenditures. A different approach could be to move the decision making toward the nodes and subsystems, so they can adapt dynamically the capacity to the actual needs and thus reduce operational costs in terms of energy consumption. To achieve this, several technological challenges need to be addressed. In this paper, we focus on the autonomous operation of Digital Subcarrier Multiplexing (DSCM) systems, which enable the transmission of multiple and independent subcarriers (SC). Herein, we present several solutions enabling the autonomous DSCM operation, including: i) SC quality of transmission estimation; ii) autonomous SC operation at the transmitter side and blind SC configuration recognition at the receiver side; and iii) intent-based capacity management implemented through Reinforcement Learning. We provide useful guidelines for the application of autonomous SC management supported by the extensive results presented.",smart cities,463,not included
10.1007/978-981-33-6195-9_3,to_check,Nature-Inspired Computing for Smart Application Design,Springer,2021-01-01 00:00:00,springer,environmental sound classification using neural network and deep learning,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-33-6195-9_3,"In this chapter, a method to classify environmental noise in the traffic of urban cities in real time using artificial neural network is discussed. The objective is to design an accurate model for classification of environment sounds present in the city. In fact, the background noise is random in nature and it becomes important to classify them in-order to identify their sources which may use to create awareness among the people. In recent past, neural networks and deep learning have been gaining popularity in signal processing and classification. Therefore, in this chapter, the features of input audio signal is extracted and used for training the neural network using deep learning algorithm for classifying environmental noise. The trained neural network is used to identify different classes of sounds present in the recorded signal. The proposed method focuses on improving model’s prediction accuracy by the latest deep learning algorithm and techniques, for a diverse variety of sounds and a provide an estimation of the source of the sound. The simulation results show that the Bayesian neural network provides better performance over other networks. Further, this project aims to deploy the network model on an android platform so that it would be compatible on most of the smart phones and benefit wide number of people.",smart cities,464,not included
10.1007/978-3-030-79197-1_124,to_check,2021 International Conference on Applications and Techniques in Cyber Intelligence,Springer,2021-01-01 00:00:00,springer,a primary analysis of the role of intellectual media development in promoting city image communication,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-79197-1_124,"Artificial intelligence, big data, VR and other technologies have promoted the intelligent development of media. In the context of intellectual media, intelligent algorithm distribution solves the correlation between audience and content; VR/AR and other technologies provide technical support for the “sense of presence” of city image communication. Intelligent software creates the conditions for content production to be convenient and low standard. These new changes will form the three-dimensional and multi-dimensional communication of city image between offline and online, between real and virtual, and open a new paradigm of city image communication.",smart cities,465,not included
10.1007/978-981-15-6648-6_17,to_check,"Computing Science, Communication and Security",Springer,2020-01-01 00:00:00,springer,retracted chapter: design of a network with vanet sporadic cloud computing applied to traffic accident prevention,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-6648-6_17,"The study analyzes the bandwidth available in a segment of route in the VANET network, since this value directly affects sporadic cloud computing. For this purpose, the bandwidth was tested on a highly complex urban scenario, where a number of mobile nodes were used with random conditions both in mobility and in resources of transmission. The results of the tests show that the stability of the bandwidth available in each region is proportional to the number of real mobile nodes in the region. However, a considerable bandwidth is also reached with a smaller number of mobile nodes, but there is no stability in the region, thus causing the network to collapse. The VANET network simulation tool was NS-3, since it is currently one of the most commonly used free software that allows configure the simulation parameters in a vehicular environment. The urban simulation scenario is the historic center of the City of Bogotá, Colombia, which was created with SUMO for obtaining the mobility traces.",smart cities,466,not included
http://arxiv.org/abs/2011.06144v1,to_check,arxiv,arxiv,2020-11-12 00:00:00,arxiv,i-post: intelligent point of sale and transaction system,http://arxiv.org/abs/2011.06144v1,"We propose a novel solution for the cashier problem. Current cashier
system/Point of Sale (POS) terminals can be inefficient, cumbersome and
time-consuming for the users. There is a need for a solution dependent on
modern technology and ubiquitous computing resources. We present I-POST
(Intelligent Point of Sale and Transaction) as a software system that uses
smart devices, mobile phone and state of the art machine learning algorithms to
process the user transactions in automated and real time manner. I-POST is an
automated checkout system that allows the user to walk in a store, collect his
items and exit the store. There is no need to stand and wait in a queue. The
system uses object detection and facial recognition algorithm to process the
authentication of the client and the state of the object. At point of exit, the
classifier sends the data to the backend server which execute the payments. The
system uses Convolution Neural Network (CNN) for the image recognition and
processing. CNN is a supervised learning model that has found major application
in pattern recognition problem. The current implementation uses two classifiers
that work intrinsically to authenticate the user and track the items. The model
accuracy for object recognition is 97%, the loss is 9.3%. We expect that such
systems can bring efficiency to the market and has the potential for broad and
diverse applications.",smart cities,467,not included
http://arxiv.org/abs/2007.07122v2,to_check,arxiv,arxiv,2020-07-14 00:00:00,arxiv,"energy-efficient resource management for federated edge learning with
  cpu-gpu heterogeneous computing",http://arxiv.org/abs/2007.07122v2,"Edge machine learning involves the deployment of learning algorithms at the
network edge to leverage massive distributed data and computation resources to
train artificial intelligence (AI) models. Among others, the framework of
federated edge learning (FEEL) is popular for its data-privacy preservation.
FEEL coordinates global model training at an edge server and local model
training at edge devices that are connected by wireless links. This work
contributes to the energy-efficient implementation of FEEL in wireless networks
by designing joint computation-and-communication resource management
($\text{C}^2$RM). The design targets the state-of-the-art heterogeneous mobile
architecture where parallel computing using both a CPU and a GPU, called
heterogeneous computing, can significantly improve both the performance and
energy efficiency. To minimize the sum energy consumption of devices, we
propose a novel $\text{C}^2$RM framework featuring multi-dimensional control
including bandwidth allocation, CPU-GPU workload partitioning and speed scaling
at each device, and $\text{C}^2$ time division for each link. The key component
of the framework is a set of equilibriums in energy rates with respect to
different control variables that are proved to exist among devices or between
processing units at each device. The results are applied to designing efficient
algorithms for computing the optimal $\text{C}^2$RM policies faster than the
standard optimization tools. Based on the equilibriums, we further design
energy-efficient schemes for device scheduling and greedy spectrum sharing that
scavenges ""spectrum holes"" resulting from heterogeneous $\text{C}^2$ time
divisions among devices. Using a real dataset, experiments are conducted to
demonstrate the effectiveness of $\text{C}^2$RM on improving the energy
efficiency of a FEEL system.",smart cities,468,not included
http://arxiv.org/abs/1805.00361v1,to_check,arxiv,arxiv,2018-04-30 00:00:00,arxiv,"ultra power-efficient cnn domain specific accelerator with 9.3tops/watt
  for mobile and embedded applications",http://arxiv.org/abs/1805.00361v1,"Computer vision performances have been significantly improved in recent years
by Convolutional Neural Networks(CNN). Currently, applications using CNN
algorithms are deployed mainly on general purpose hardwares, such as CPUs, GPUs
or FPGAs. However, power consumption, speed, accuracy, memory footprint, and
die size should all be taken into consideration for mobile and embedded
applications. Domain Specific Architecture (DSA) for CNN is the efficient and
practical solution for CNN deployment and implementation. We designed and
produced a 28nm Two-Dimensional CNN-DSA accelerator with an ultra
power-efficient performance of 9.3TOPS/Watt and with all processing done in the
internal memory instead of outside DRAM. It classifies 224x224 RGB image inputs
at more than 140fps with peak power consumption at less than 300mW and an
accuracy comparable to the VGG benchmark. The CNN-DSA accelerator is
reconfigurable to support CNN model coefficients of various layer sizes and
layer types, including convolution, depth-wise convolution, short-cut
connections, max pooling, and ReLU. Furthermore, in order to better support
real-world deployment for various application scenarios, especially with
low-end mobile and embedded platforms and MCUs (Microcontroller Units), we also
designed algorithms to fully utilize the CNN-DSA accelerator efficiently by
reducing the dependency on external accelerator computation resources,
including implementation of Fully-Connected (FC) layers within the accelerator
and compression of extracted features from the CNN-DSA accelerator. Live demos
with our CNN-DSA accelerator on mobile and embedded systems show its
capabilities to be widely and practically applied in the real world.",smart cities,469,not included
http://arxiv.org/abs/1610.07862v2,to_check,arxiv,arxiv,2016-10-24 00:00:00,arxiv,intelligence in artificial intelligence,http://arxiv.org/abs/1610.07862v2,"The elusive quest for intelligence in artificial intelligence prompts us to
consider that instituting human-level intelligence in systems may be (still) in
the realm of utopia. In about a quarter century, we have witnessed the winter
of AI (1990) being transformed and transported to the zenith of tabloid fodder
about AI (2015). The discussion at hand is about the elements that constitute
the canonical idea of intelligence. The delivery of intelligence as a
pay-per-use-service, popping out of an app or from a shrink-wrapped software
defined point solution, is in contrast to the bio-inspired view of intelligence
as an outcome, perhaps formed from a tapestry of events, cross-pollinated by
instances, each with its own microcosm of experiences and learning, which may
not be discrete all-or-none functions but continuous, over space and time. The
enterprise world may not require, aspire or desire such an engaged solution to
improve its services for enabling digital transformation through the deployment
of digital twins, for example. One might ask whether the ""work-flow on
steroids"" version of decision support may suffice for intelligence? Are we
harking back to the era of rule based expert systems? The image conjured by the
publicity machines offers deep solutions with human-level AI and preposterous
claims about capturing the ""brain in a box"" by 2020. Even emulating insects may
be difficult in terms of real progress. Perhaps we can try to focus on worms
(Caenorhabditis elegans) which may be better suited for what business needs to
quench its thirst for so-called intelligence in AI.",smart cities,470,not included
http://arxiv.org/abs/1910.05765v1,to_check,arxiv,arxiv,2019-10-13 00:00:00,arxiv,"real-time and embedded deep learning on fpga for rf signal
  classification",http://arxiv.org/abs/1910.05765v1,"We designed and implemented a deep learning based RF signal classifier on the
Field Programmable Gate Array (FPGA) of an embedded software-defined radio
platform, DeepRadio, that classifies the signals received through the RF front
end to different modulation types in real time and with low power. This
classifier implementation successfully captures complex characteristics of
wireless signals to serve critical applications in wireless security and
communications systems such as identifying spoofing signals in signal
authentication systems, detecting target emitters and jammers in electronic
warfare (EW) applications, discriminating primary and secondary users in
cognitive radio networks, interference hunting, and adaptive modulation.
Empowered by low-power and low-latency embedded computing, the deep neural
network runs directly on the FPGA fabric of DeepRadio, while maintaining
classifier accuracy close to the software performance. We evaluated the
performance when another SDR (USRP) transmits signals with different modulation
types at different power levels and DeepRadio receives the signals and
classifies them in real time on its FPGA. A smartphone with a mobile app is
connected to DeepRadio to initiate the experiment and visualize the
classification results. With real radio transmissions over the air, we show
that the classifier implemented on DeepRadio achieves high accuracy with low
latency (microsecond per sample) and low energy consumption (microJoule per
sample), and this performance is not matched by other embedded platforms such
as embedded graphics processing unit (GPU).",smart cities,471,not included
http://arxiv.org/abs/2008.05255v1,to_check,arxiv,arxiv,2020-08-12 00:00:00,arxiv,"identity-aware attribute recognition via real-time distributed inference
  in mobile edge clouds",http://arxiv.org/abs/2008.05255v1,"With the development of deep learning technologies, attribute recognition and
person re-identification (re-ID) have attracted extensive attention and
achieved continuous improvement via executing computing-intensive deep neural
networks in cloud datacenters. However, the datacenter deployment cannot meet
the real-time requirement of attribute recognition and person re-ID, due to the
prohibitive delay of backhaul networks and large data transmissions from
cameras to datacenters. A feasible solution thus is to employ mobile edge
clouds (MEC) within the proximity of cameras and enable distributed inference.
In this paper, we design novel models for pedestrian attribute recognition with
re-ID in an MEC-enabled camera monitoring system. We also investigate the
problem of distributed inference in the MEC-enabled camera network. To this
end, we first propose a novel inference framework with a set of distributed
modules, by jointly considering the attribute recognition and person re-ID. We
then devise a learning-based algorithm for the distributions of the modules of
the proposed distributed inference framework, considering the dynamic
MEC-enabled camera network with uncertainties. We finally evaluate the
performance of the proposed algorithm by both simulations with real datasets
and system implementation in a real testbed. Evaluation results show that the
performance of the proposed algorithm with distributed inference framework is
promising, by reaching the accuracies of attribute recognition and person
identification up to 92.9% and 96.6% respectively, and significantly reducing
the inference delay by at least 40.6% compared with existing methods.",smart cities,472,included
http://arxiv.org/abs/1805.04902v2,to_check,arxiv,arxiv,2018-05-13 00:00:00,arxiv,lmnet: real-time multiclass object detection on cpu using 3d lidar,http://arxiv.org/abs/1805.04902v2,"This paper describes an optimized single-stage deep convolutional neural
network to detect objects in urban environments, using nothing more than point
cloud data. This feature enables our method to work regardless the time of the
day and the lighting conditions.The proposed network structure employs dilated
convolutions to gradually increase the perceptive field as depth increases,
this helps to reduce the computation time by about 30%. The network input
consists of five perspective representations of the unorganized point cloud
data. The network outputs an objectness map and the bounding box offset values
for each point. Our experiments showed that using reflection, range, and the
position on each of the three axes helped to improve the location and
orientation of the output bounding box. We carried out quantitative evaluations
with the help of the KITTI dataset evaluation server. It achieved the fastest
processing speed among the other contenders, making it suitable for real-time
applications. We implemented and tested it on a real vehicle with a Velodyne
HDL-64 mounted on top of it. We achieved execution times as fast as 50 FPS
using desktop GPUs, and up to 10 FPS on a single Intel Core i5 CPU. The deploy
implementation is open-sourced and it can be found as a feature branch inside
the autonomous driving framework Autoware. Code is available at:
https://github.com/CPFL/Autoware/tree/feature/cnn_lidar_detection",smart cities,473,not included
http://arxiv.org/abs/1812.03078v1,to_check,arxiv,arxiv,2018-12-07 00:00:00,arxiv,"evolutionary games, complex networks and nonlinear analysis for
  epileptic seizures forecasting",http://arxiv.org/abs/1812.03078v1,"Epileptic seizures detection and forecasting is nowadays widely recognized as
a problem of great significance and social resonance, and still remains an
open, grand challenge. Furthermore, the development of mobile warning systems
and wearable, non invasive, advisory devices are increasingly and strongly
requested, from the patient community and their families and also from
institutional stakeholders. According to the many recent studies, exploiting
machine learning capabilities upon intracranial EEG (iEEG), in this work we
investigate a combination of novel game theory dynamical model on networks for
brain electrical activity and nonlinear time series analysis based on
recurrences quantification. These two methods are then melted together within a
supervised learning scheme and finally, prediction performances are assessed
using EEG scalp datasets, specifically recorded for this study. Our study
achieved mean sensitivity of 70.9% and a mean time in warning of 20.3%, thus
showing an increase of the improvement over chance metric from 42%, reported in
the most recent study, to 50.5%. Moreover, the real time implementation of the
proposed approach is currently under development on a prototype of a wearable
device.",smart cities,474,not included
http://arxiv.org/abs/2103.05225v3,to_check,arxiv,arxiv,2021-03-09 00:00:00,arxiv,a scavenger hunt for service robots,http://arxiv.org/abs/2103.05225v3,"Creating robots that can perform general-purpose service tasks in a
human-populated environment has been a longstanding grand challenge for AI and
Robotics research. One particularly valuable skill that is relevant to a wide
variety of tasks is the ability to locate and retrieve objects upon request.
This paper models this skill as a Scavenger Hunt (SH) game, which we formulate
as a variation of the NP-hard stochastic traveling purchaser problem. In this
problem, the goal is to find a set of objects as quickly as possible, given
probability distributions of where they may be found. We investigate the
performance of several solution algorithms for the SH problem, both in
simulation and on a real mobile robot. We use Reinforcement Learning (RL) to
train an agent to plan a minimal cost path, and show that the RL agent can
outperform a range of heuristic algorithms, achieving near optimal performance.
In order to stimulate research on this problem, we introduce a publicly
available software stack and associated website that enable users to upload
scavenger hunts which robots can download, perform, and learn from to
continually improve their performance on future hunts.",smart cities,475,not included
http://arxiv.org/abs/2104.13295v1,to_check,arxiv,arxiv,2021-04-27 00:00:00,arxiv,metamorphic detection of repackaged malware,http://arxiv.org/abs/2104.13295v1,"Machine learning-based malware detection systems are often vulnerable to
evasion attacks, in which a malware developer manipulates their malicious
software such that it is misclassified as benign. Such software hides some
properties of the real class or adopts some properties of a different class by
applying small perturbations. A special case of evasive malware hides by
repackaging a bonafide benign mobile app to contain malware in addition to the
original functionality of the app, thus retaining most of the benign properties
of the original app. We present a novel malware detection system based on
metamorphic testing principles that can detect such benign-seeming malware
apps. We apply metamorphic testing to the feature representation of the mobile
app rather than to the app itself. That is, the source input is the original
feature vector for the app and the derived input is that vector with selected
features removed. If the app was originally classified benign and is indeed
benign, the output for the source and derived inputs should be the same class,
i.e., benign, but if they differ, then the app is exposed as likely malware.
Malware apps originally classified as malware should retain that classification
since only features prevalent in benign apps are removed. This approach enables
the machine learning model to classify repackaged malware with reasonably few
false negatives and false positives. Our training pipeline is simpler than many
existing ML-based malware detection methods, as the network is trained
end-to-end to learn appropriate features and perform classification. We
pre-trained our classifier model on 3 million apps collected from the
widely-used AndroZoo dataset. We perform an extensive study on other publicly
available datasets to show our approach's effectiveness in detecting repackaged
malware with more than94% accuracy, 0.98 precision, 0.95 recall, and 0.96 F1
score.",smart cities,476,not included
http://arxiv.org/abs/2101.04930v2,to_check,arxiv,arxiv,2021-01-13 00:00:00,arxiv,"an empirical study on deployment faults of deep learning based mobile
  applications",http://arxiv.org/abs/2101.04930v2,"Deep Learning (DL) is finding its way into a growing number of mobile
software applications. These software applications, named as DL based mobile
applications (abbreviated as mobile DL apps) integrate DL models trained using
large-scale data with DL programs. A DL program encodes the structure of a
desirable DL model and the process by which the model is trained using training
data. Due to the increasing dependency of current mobile apps on DL, software
engineering (SE) for mobile DL apps has become important. However, existing
efforts in SE research community mainly focus on the development of DL models
and extensively analyze faults in DL programs. In contrast, faults related to
the deployment of DL models on mobile devices (named as deployment faults of
mobile DL apps) have not been well studied. Since mobile DL apps have been used
by billions of end users daily for various purposes including for
safety-critical scenarios, characterizing their deployment faults is of
enormous importance. To fill the knowledge gap, this paper presents the first
comprehensive study on the deployment faults of mobile DL apps. We identify 304
real deployment faults from Stack Overflow and GitHub, two commonly used data
sources for studying software faults. Based on the identified faults, we
construct a fine-granularity taxonomy consisting of 23 categories regarding to
fault symptoms and distill common fix strategies for different fault types.
Furthermore, we suggest actionable implications and research avenues that could
further facilitate the deployment of DL models on mobile devices.",smart cities,477,included
http://arxiv.org/abs/1907.00594v1,to_check,arxiv,arxiv,2019-07-01 00:00:00,arxiv,"fingerprint-based localization using commercial lte signals: a
  field-trial study",http://arxiv.org/abs/1907.00594v1,"Wireless localization for mobile device has attracted more and more interests
by increasing the demand for location based services. Fingerprint-based
localization is promising, especially in non-Line-of-Sight (NLoS) or rich
scattering environments, such as urban areas and indoor scenarios. In this
paper, we propose a novel fingerprint-based localization technique based on
deep learning framework under commercial long term evolution (LTE) systems.
Specifically, we develop a software defined user equipment to collect the real
time channel state information (CSI) knowledge from LTE base stations and
extract the intrinsic features among CSI observations. On top of that, we
propose a time domain fusion approach to assemble multiple positioning
estimations. Experimental results demonstrated that the proposed localization
technique can significantly improve the localization accuracy and robustness,
e.g. achieves Mean Distance Error (MDE) of 0.47 meters for indoor and of 19.9
meters for outdoor scenarios, respectively.",smart cities,478,not included
http://arxiv.org/abs/1808.01356v1,to_check,arxiv,arxiv,2018-07-31 00:00:00,arxiv,"deep learning-based multiple object visual tracking on embedded system
  for iot and mobile edge computing applications",http://arxiv.org/abs/1808.01356v1,"Compute and memory demands of state-of-the-art deep learning methods are
still a shortcoming that must be addressed to make them useful at IoT
end-nodes. In particular, recent results depict a hopeful prospect for image
processing using Convolutional Neural Netwoks, CNNs, but the gap between
software and hardware implementations is already considerable for IoT and
mobile edge computing applications due to their high power consumption. This
proposal performs low-power and real time deep learning-based multiple object
visual tracking implemented on an NVIDIA Jetson TX2 development kit. It
includes a camera and wireless connection capability and it is battery powered
for mobile and outdoor applications. A collection of representative sequences
captured with the on-board camera, dETRUSC video dataset, is used to exemplify
the performance of the proposed algorithm and to facilitate benchmarking. The
results in terms of power consumption and frame rate demonstrate the
feasibility of deep learning algorithms on embedded platforms although more
effort to joint algorithm and hardware design of CNNs is needed.",smart cities,479,not included
http://arxiv.org/abs/1702.06329v1,to_check,arxiv,arxiv,2017-02-21 00:00:00,arxiv,"towards a common implementation of reinforcement learning for multiple
  robotic tasks",http://arxiv.org/abs/1702.06329v1,"Mobile robots are increasingly being employed for performing complex tasks in
dynamic environments. Reinforcement learning (RL) methods are recognized to be
promising for specifying such tasks in a relatively simple manner. However, the
strong dependency between the learning method and the task to learn is a
well-known problem that restricts practical implementations of RL in robotics,
often requiring major modifications of parameters and adding other techniques
for each particular task. In this paper we present a practical core
implementation of RL which enables the learning process for multiple robotic
tasks with minimal per-task tuning or none. Based on value iteration methods,
this implementation includes a novel approach for action selection, called
Q-biased softmax regression (QBIASSR), which avoids poor performance of the
learning process when the robot reaches new unexplored states. Our approach
takes advantage of the structure of the state space by attending the physical
variables involved (e.g., distances to obstacles, X,Y,{\theta} pose, etc.),
thus experienced sets of states may favor the decision-making process of
unexplored or rarely-explored states. This improvement has a relevant role in
reducing the tuning of the algorithm for particular tasks. Experiments with
real and simulated robots, performed with the software framework also
introduced here, show that our implementation is effectively able to learn
different robotic tasks without tuning the learning method. Results also
suggest that the combination of true online SARSA({\lambda}) with QBIASSR can
outperform the existing RL core algorithms in low-dimensional robotic tasks.",smart cities,480,included
http://arxiv.org/abs/2007.02351v1,to_check,arxiv,arxiv,2020-07-05 00:00:00,arxiv,offline model guard: secure and private ml on mobile devices,http://arxiv.org/abs/2007.02351v1,"Performing machine learning tasks in mobile applications yields a challenging
conflict of interest: highly sensitive client information (e.g., speech data)
should remain private while also the intellectual property of service providers
(e.g., model parameters) must be protected. Cryptographic techniques offer
secure solutions for this, but have an unacceptable overhead and moreover
require frequent network interaction. In this work, we design a practically
efficient hardware-based solution. Specifically, we build Offline Model Guard
(OMG) to enable privacy-preserving machine learning on the predominant mobile
computing platform ARM - even in offline scenarios. By leveraging a trusted
execution environment for strict hardware-enforced isolation from other system
components, OMG guarantees privacy of client data, secrecy of provided models,
and integrity of processing algorithms. Our prototype implementation on an ARM
HiKey 960 development board performs privacy-preserving keyword recognition
using TensorFlow Lite for Microcontrollers in real time.",smart cities,481,not included
http://arxiv.org/abs/1805.08692v1,to_check,arxiv,arxiv,2018-05-04 00:00:00,arxiv,"assessing a mobile-based deep learning model for plant disease
  surveillance",http://arxiv.org/abs/1805.08692v1,"Convolutional neural network models (CNNs) have made major advances in
computer vision tasks in the last five years. Given the challenge in collecting
real world datasets, most studies report performance metrics based on available
research datasets. In scenarios where CNNs are to be deployed on images or
videos from mobile devices, models are presented with new challenges due to
lighting, angle, and camera specifications, which are not accounted for in
research datasets. It is essential for assessment to also be conducted on real
world datasets if such models are to be reliably integrated with products and
services in society. Plant disease datasets can be used to test CNNs in real
time and gain insight into real world performance. We train a CNN object
detection model to identify foliar symptoms of diseases (or lack thereof) in
cassava (Manihot esculenta Crantz). We then deploy the model on a mobile app
and test its performance on mobile images and video of 720 diseased leaflets in
an agricultural field in Tanzania. Within each disease category we test two
levels of severity of symptoms - mild and pronounced, to assess the model
performance for early detection of symptoms. In both severities we see a
decrease in the F-1 score for real world images and video. The F-1 score
dropped by 32% for pronounced symptoms in real world images (the closest data
to the training data) due to a drop in model recall. If the potential of
smartphone CNNs are to be realized our data suggest it is crucial to consider
tuning precision and recall performance in order to achieve the desired
performance in real world settings. In addition, the varied performance related
to different input data (image or video) is an important consideration for the
design of CNNs in real world applications.",smart cities,482,included
http://arxiv.org/abs/2103.08022v1,to_check,arxiv,arxiv,2021-03-14 00:00:00,arxiv,"success weighted by completion time: a dynamics-aware evaluation
  criteria for embodied navigation",http://arxiv.org/abs/2103.08022v1,"We present Success weighted by Completion Time (SCT), a new metric for
evaluating navigation performance for mobile robots. Several related works on
navigation have used Success weighted by Path Length (SPL) as the primary
method of evaluating the path an agent makes to a goal location, but SPL is
limited in its ability to properly evaluate agents with complex dynamics. In
contrast, SCT explicitly takes the agent's dynamics model into consideration,
and aims to accurately capture how well the agent has approximated the fastest
navigation behavior afforded by its dynamics. While several embodied navigation
works use point-turn dynamics, we focus on unicycle-cart dynamics for our
agent, which better exemplifies the dynamics model of popular mobile robotics
platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present
RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest
collision-free path and completion time from a starting pose to a goal location
in an environment containing obstacles. We experiment with deep reinforcement
learning and reward shaping to train and compare the navigation performance of
agents with different dynamics models. In evaluating these agents, we show that
in contrast to SPL, SCT is able to capture the advantages in navigation speed a
unicycle model has over a simpler point-turn model of dynamics. Lastly, we show
that we can successfully deploy our trained models and algorithms outside of
simulation in the real world. We embody our agents in an real robot to navigate
an apartment, and show that they can generalize in a zero-shot manner.",smart cities,483,included
http://arxiv.org/abs/2007.14545v2,to_check,arxiv,arxiv,2020-07-29 00:00:00,arxiv,"learning object-conditioned exploration using distributed soft actor
  critic",http://arxiv.org/abs/2007.14545v2,"Object navigation is defined as navigating to an object of a given label in a
complex, unexplored environment. In its general form, this problem poses
several challenges for Robotics: semantic exploration of unknown environments
in search of an object and low-level control. In this work we study
object-guided exploration and low-level control, and present an end-to-end
trained navigation policy achieving a success rate of 0.68 and SPL of 0.58 on
unseen, visually complex scans of real homes. We propose a highly scalable
implementation of an off-policy Reinforcement Learning algorithm, distributed
Soft Actor Critic, which allows the system to utilize 98M experience steps in
24 hours on 8 GPUs. Our system learns to control a differential drive mobile
base in simulation from a stack of high dimensional observations commonly used
on robotic platforms. The learned policy is capable of object-guided
exploratory behaviors and low-level control learned from pure experiences in
realistic environments.",smart cities,484,not included
http://arxiv.org/abs/2103.02649v1,to_check,arxiv,arxiv,2021-03-03 00:00:00,arxiv,"self-play learning strategies for resource assignment in open-ran
  networks",http://arxiv.org/abs/2103.02649v1,"Open Radio Access Network (ORAN) is being developed with an aim to
democratise access and lower the cost of future mobile data networks,
supporting network services with various QoS requirements, such as massive IoT
and URLLC. In ORAN, network functionality is dis-aggregated into remote units
(RUs), distributed units (DUs) and central units (CUs), which allows flexible
software on Commercial-Off-The-Shelf (COTS) deployments. Furthermore, the
mapping of variable RU requirements to local mobile edge computing centres for
future centralized processing would significantly reduce the power consumption
in cellular networks. In this paper, we study the RU-DU resource assignment
problem in an ORAN system, modelled as a 2D bin packing problem. A deep
reinforcement learning-based self-play approach is proposed to achieve
efficient RU-DU resource management, with AlphaGo Zero inspired neural
Monte-Carlo Tree Search (MCTS). Experiments on representative 2D bin packing
environment and real sites data show that the self-play learning strategy
achieves intelligent RU-DU resource assignment for different network
conditions.",smart cities,485,not included
http://arxiv.org/abs/2005.00953v1,to_check,arxiv,arxiv,2020-05-03 00:00:00,arxiv,"deep generative adversarial residual convolutional networks for
  real-world super-resolution",http://arxiv.org/abs/2005.00953v1,"Most current deep learning based single image super-resolution (SISR) methods
focus on designing deeper / wider models to learn the non-linear mapping
between low-resolution (LR) inputs and the high-resolution (HR) outputs from a
large number of paired (LR/HR) training data. They usually take as assumption
that the LR image is a bicubic down-sampled version of the HR image. However,
such degradation process is not available in real-world settings i.e. inherent
sensor noise, stochastic noise, compression artifacts, possible mismatch
between image degradation process and camera device. It reduces significantly
the performance of current SISR methods due to real-world image corruptions. To
address these problems, we propose a deep Super-Resolution Residual
Convolutional Generative Adversarial Network (SRResCGAN) to follow the
real-world degradation settings by adversarial training the model with
pixel-wise supervision in the HR domain from its generated LR counterpart. The
proposed network exploits the residual learning by minimizing the energy-based
objective function with powerful image regularization and convex optimization
techniques. We demonstrate our proposed approach in quantitative and
qualitative experiments that generalize robustly to real input and it is easy
to deploy for other down-scaling operators and mobile/embedded devices.",smart cities,486,included
http://arxiv.org/abs/1910.09667v1,to_check,arxiv,arxiv,2019-10-21 00:00:00,arxiv,"combining benefits from trajectory optimization and deep reinforcement
  learning",http://arxiv.org/abs/1910.09667v1,"Recent breakthroughs both in reinforcement learning and trajectory
optimization have made significant advances towards real world robotic system
deployment. Reinforcement learning (RL) can be applied to many problems without
needing any modeling or intuition about the system, at the cost of high sample
complexity and the inability to prove any metrics about the learned policies.
Trajectory optimization (TO) on the other hand allows for stability and
robustness analyses on generated motions and trajectories, but is only as good
as the often over-simplified derived model, and may have prohibitively
expensive computation times for real-time control. This paper seeks to combine
the benefits from these two areas while mitigating their drawbacks by (1)
decreasing RL sample complexity by using existing knowledge of the problem with
optimal control, and (2) providing an upper bound estimate on the
time-to-arrival of the combined learned-optimized policy, allowing online
policy deployment at any point in the training process by using the TO as a
worst-case scenario action. This method is evaluated for a car model, with
applicability to any mobile robotic system. A video showing policy execution
comparisons can be found at https://youtu.be/mv2xw83NyWU .",smart cities,487,not included
http://arxiv.org/abs/1902.00159v1,to_check,arxiv,arxiv,2019-02-01 00:00:00,arxiv,compressing gans using knowledge distillation,http://arxiv.org/abs/1902.00159v1,"Generative Adversarial Networks (GANs) have been used in several machine
learning tasks such as domain transfer, super resolution, and synthetic data
generation. State-of-the-art GANs often use tens of millions of parameters,
making them expensive to deploy for applications in low SWAP (size, weight, and
power) hardware, such as mobile devices, and for applications with real time
capabilities. There has been no work found to reduce the number of parameters
used in GANs. Therefore, we propose a method to compress GANs using knowledge
distillation techniques, in which a smaller ""student"" GAN learns to mimic a
larger ""teacher"" GAN. We show that the distillation methods used on MNIST,
CIFAR-10, and Celeb-A datasets can compress teacher GANs at ratios of 1669:1,
58:1, and 87:1, respectively, while retaining the quality of the generated
image. From our experiments, we observe a qualitative limit for GAN's
compression. Moreover, we observe that, with a fixed parameter budget,
compressed GANs outperform GANs trained using standard training methods. We
conjecture that this is partially owing to the optimization landscape of
over-parameterized GANs which allows efficient training using alternating
gradient descent. Thus, training an over-parameterized GAN followed by our
proposed compression scheme provides a high quality generative model with a
small number of parameters.",smart cities,488,not included
http://arxiv.org/abs/1411.3895v1,to_check,arxiv,arxiv,2014-11-14 00:00:00,arxiv,"learning fuzzy controllers in mobile robotics with embedded
  preprocessing",http://arxiv.org/abs/1411.3895v1,"The automatic design of controllers for mobile robots usually requires two
stages. In the first stage,sensorial data are preprocessed or transformed into
high level and meaningful values of variables whichare usually defined from
expert knowledge. In the second stage, a machine learning technique is applied
toobtain a controller that maps these high level variables to the control
commands that are actually sent tothe robot. This paper describes an algorithm
that is able to embed the preprocessing stage into the learningstage in order
to get controllers directly starting from sensorial raw data with no expert
knowledgeinvolved. Due to the high dimensionality of the sensorial data, this
approach uses Quantified Fuzzy Rules(QFRs), that are able to transform
low-level input variables into high-level input variables, reducingthe
dimensionality through summarization. The proposed learning algorithm, called
Iterative QuantifiedFuzzy Rule Learning (IQFRL), is based on genetic
programming. IQFRL is able to learn rules with differentstructures, and can
manage linguistic variables with multiple granularities. The algorithm has been
testedwith the implementation of the wall-following behavior both in several
realistic simulated environmentswith different complexity and on a Pioneer 3-AT
robot in two real environments. Results have beencompared with several
well-known learning algorithms combined with different data
preprocessingtechniques, showing that IQFRL exhibits a better and statistically
significant performance. Moreover,three real world applications for which IQFRL
plays a central role are also presented: path and objecttracking with static
and moving obstacles avoidance.",smart cities,489,not included
http://arxiv.org/abs/1807.05211v1,to_check,arxiv,arxiv,2018-07-11 00:00:00,arxiv,"learning deployable navigation policies at kilometer scale from a single
  traversal",http://arxiv.org/abs/1807.05211v1,"Model-free reinforcement learning has recently been shown to be effective at
learning navigation policies from complex image input. However, these
algorithms tend to require large amounts of interaction with the environment,
which can be prohibitively costly to obtain on robots in the real world. We
present an approach for efficiently learning goal-directed navigation policies
on a mobile robot, from only a single coverage traversal of recorded data. The
navigation agent learns an effective policy over a diverse action space in a
large heterogeneous environment consisting of more than 2km of travel, through
buildings and outdoor regions that collectively exhibit large variations in
visual appearance, self-similarity, and connectivity. We compare pretrained
visual encoders that enable precomputation of visual embeddings to achieve a
throughput of tens of thousands of transitions per second at training time on a
commodity desktop computer, allowing agents to learn from millions of
trajectories of experience in a matter of hours. We propose multiple forms of
computationally efficient stochastic augmentation to enable the learned policy
to generalise beyond these precomputed embeddings, and demonstrate successful
deployment of the learned policy on the real robot without fine tuning, despite
environmental appearance differences at test time. The dataset and code
required to reproduce these results and apply the technique to other datasets
and robots is made publicly available at rl-navigation.github.io/deployable.",smart cities,490,included
http://arxiv.org/abs/1907.07210v1,to_check,arxiv,arxiv,2019-07-16 00:00:00,arxiv,real-time vision-based depth reconstruction with nvidia jetson,http://arxiv.org/abs/1907.07210v1,"Vision-based depth reconstruction is a challenging problem extensively
studied in computer vision but still lacking universal solution. Reconstructing
depth from single image is particularly valuable to mobile robotics as it can
be embedded to the modern vision-based simultaneous localization and mapping
(vSLAM) methods providing them with the metric information needed to construct
accurate maps in real scale. Typically, depth reconstruction is done nowadays
via fully-convolutional neural networks (FCNNs). In this work we experiment
with several FCNN architectures and introduce a few enhancements aimed at
increasing both the effectiveness and the efficiency of the inference. We
experimentally determine the solution that provides the best
performance/accuracy tradeoff and is able to run on NVidia Jetson with the
framerates exceeding 16FPS for 320 x 240 input. We also evaluate the suggested
models by conducting monocular vSLAM of unknown indoor environment on NVidia
Jetson TX2 in real-time. Open-source implementation of the models and the
inference node for Robot Operating System (ROS) are available at
https://github.com/CnnDepth/tx2_fcnn_node.",smart cities,491,included
http://arxiv.org/abs/1902.06824v2,to_check,arxiv,arxiv,2019-02-18 00:00:00,arxiv,"autonomous airline revenue management: a deep reinforcement learning
  approach to seat inventory control and overbooking",http://arxiv.org/abs/1902.06824v2,"Revenue management can enable airline corporations to maximize the revenue
generated from each scheduled flight departing in their transportation network
by means of finding the optimal policies for differential pricing, seat
inventory control and overbooking. As different demand segments in the market
have different Willingness-To-Pay (WTP), airlines use differential pricing,
booking restrictions, and service amenities to determine different fare classes
or products targeted at each of these demand segments. Because seats are
limited for each flight, airlines also need to allocate seats for each of these
fare classes to prevent lower fare class passengers from displacing higher fare
class ones and set overbooking limits in anticipation of cancellations and
no-shows such that revenue is maximized. Previous work addresses these problems
using optimization techniques or classical Reinforcement Learning methods. This
paper focuses on the latter problem - the seat inventory control problem -
casting it as a Markov Decision Process to be able to find the optimal policy.
Multiple fare classes, concurrent continuous arrival of passengers of different
fare classes, overbooking and random cancellations that are independent of
class have been considered in the model. We have addressed this problem using
Deep Q-Learning with the goal of maximizing the reward for each flight
departure. The implementation of this technique allows us to employ large
continuous state space but also presents the potential opportunity to test on
real time airline data. To generate data and train the agent, a basic
air-travel market simulator was developed. The performance of the agent in
different simulated market scenarios was compared against theoretically optimal
solutions and was found to be nearly close to the expected optimal revenue.",smart cities,492,not included
http://arxiv.org/abs/2003.12228v1,to_check,arxiv,arxiv,2020-03-27 00:00:00,arxiv,mechanism design for wireless powered spatial crowdsourcing networks,http://arxiv.org/abs/2003.12228v1,"Wireless power transfer (WPT) is a promising technology to prolong the
lifetime of the sensors and communication devices, i.e., workers, in completing
crowdsourcing tasks by providing continuous and cost-effective energy supplies.
In this paper, we propose a wireless powered spatial crowdsourcing framework
which consists of two mutually dependent phases: task allocation phase and data
crowdsourcing phase. In the task allocation phase, we propose a Stackelberg
game based mechanism for the spatial crowdsourcing platform to efficiently
allocate spatial tasks and wireless charging power to each worker. In the data
crowdsourcing phase, the workers may have an incentive to misreport its real
working location to improve its utility, which causes adverse effects to the
spatial crowdsourcing platform. To address this issue, we present three
strategyproof deployment mechanisms for the spatial crowdsourcing platform to
place a mobile base station, e.g., vehicle or robot, which is responsible for
transferring the wireless power and collecting the crowdsourced data. As the
benchmark, we first apply the classical median mechanism and evaluate its
worst-case performance. Then, we design a conventional strategyproof deployment
mechanism to improve the expected utility of the spatial crowdsourcing platform
under the condition that the workers' locations follow a known geographical
distribution. For a more general case with only the historical location data
available, we propose a deep learning based strategyproof deployment mechanism
to maximize the spatial crowdsourcing platform's utility. Extensive
experimental results based on synthetic and real-world datasets reveal the
effectiveness of the proposed framework in allocating tasks and charging power
to workers while avoiding the dishonest worker's manipulation.",smart cities,493,not included
10.1016/j.eswa.2021.115080,to_check,Expert Systems with Applications,scopus,2021-10-15,sciencedirect,spectral decision in cognitive radio networks based on deep learning,https://api.elsevier.com/content/abstract/scopus_id/85105355301,"Cognitive radio networks (CRN) have gained great relevance in the efficient use of the radio spectrum, and one of the key aspects of this technology is the spectral decision. The performance of secondary user communication depends largely on the intelligent choice of an appropriate spectral opportunity. The purpose of this research is to propose and assess the performance of a spectral decision model for CRN based on the Deep Learning technique. To achieve this, a classifier was adapted through the feature extraction technique that identifies three levels of traffic (high, medium and low) in a spectral occupation experimental power matrix that models the primary user. The extraction of features is done by Deep Learning and the process of classifying the successful set of features is done by a Support Vector Machine (SVM). These were used along with five evaluation metrics—total handoffs, failed handoffs, bandwidth, delay and throughput—to measure the performance of the proposed spectral decision model based on the Deep Learning technique, and to compare the results with the Multi-Criteria Optimization and Compromise Solution (VIKOR), Technique for Order Preference by Similarity to Ideal Solution (TOPSIS), and Simple Additive Weighting (SAW). This work presents five contributions: incorporation of the real behavior of licensed users, implementation of performance metrics for spectral mobility, proposal of an RGB conversion algorithm based on the threshold level, feedback in the classifier and a methodology based on priorities and scores to establish the channels with the highest availability. The results of this evaluation show that the proposed model has a better performance in the five metrics compared to the other techniques.",smart cities,494,not included
10.1016/j.watres.2021.117012,to_check,Water Research,scopus,2021-05-15,sciencedirect,gas-diffusion-electrode based direct electro-stripping system for gaseous ammonia recovery from livestock wastewater,https://api.elsevier.com/content/abstract/scopus_id/85102581788,"Livestock wastewater (LW) typically contains a substantial amount of NH4
                     + that can potentially be recovered and used in fertilizers or chemicals. In an attempt to recover NH4
                     + from LW, a novel electrochemical approach using a gas diffusion electrode (GDE) was developed and its efficacy was demonstrated in this study. The GDE-based electrochemical device, when operated at an air-flow rate of 20 mL/min, was free of back-diffusion flux, which is a fatal drawback of any membrane-based NH4
                     + separation approach. Continuous operation resulted in a nitrogen flux of 890 g N/m2d with synthetic LW and 770 g N/m2d with real LW at a current density of 10 mA/cm2. The electrochemical energy input was 7.42 kWh/kg N with synthetic LW and 9.44 kWh/kg N with real LW. Compared with the traditional stripping method, the GDE-based electrochemical system has a certain potential to be competitive, in terms of energy consumption. For instance, a rough-cost estimate based only on operating costs regarding chemical usage, air blowing, and water pumping revealed that the system consumed 13.44 kWh/kg N, whereas the conventional stripper required 27.6 kWh/kg N. This analysis showed that an electrochemical approach such as our GDE-based method can recover NH3, (particularly in gaseous form) from LW. In addition, with the future development of a smart operation method, as proposed and demonstrated in this study, the cost-effective implementation of a GDE-based method is feasible.",smart cities,495,not included
10.1016/j.comnet.2020.107573,to_check,Computer Networks,scopus,2020-12-09,sciencedirect,ai-enabled mobile multimedia service instance placement scheme in mobile edge computing,https://api.elsevier.com/content/abstract/scopus_id/85091771160,"Leveraging cloud infrastructure to the mobile edge computing helps the mobile users to get real time multimedia services in Fifth Generation (5G) network system. To ensure higher Quality-of-Experience (QoE), faster migration of mobile multimedia service instances is required to cope up with user mobility. By deploying the mobile multimedia service instances proactively in multiple edge nodes (ENs) helps the users to get higher QoE. However, excessive deployment of service replicas might increase the cost of the overall network. To establish trade-off between these two conflicting objectives, we have formulated the problem as a Multi-objective Integer Linear Programming (MILP) by integrating the users’ path prediction model. This problem is proven to be an NP-hard one for large networks, thus we develop an artificial intelligence (AI) based meta-heuristic Binary Particle Swarm Optimization (BPSO) algorithm to achieve near-optimal solution within polynomial time. The performance analysis results show the significant performance improvement in terms of QoE and user satisfaction as compared to other state-of-the-art works.",smart cities,496,not included
10.1016/j.jnca.2020.102692,to_check,Journal of Network and Computer Applications,scopus,2020-08-15,sciencedirect,mario: a spatio-temporal data mining framework on google cloud to explore mobility dynamics from taxi trajectories,https://api.elsevier.com/content/abstract/scopus_id/85084749934,"With the major advances in location acquisition techniques, deployment of GPS enabled devices and increasing number of mobile users, substantial amount of location traces are generated from different geographical regions. It provides unprecedented opportunities to analyze and derive valuable insights of urban dynamics, specifically, time-dependent mobility patterns and region-specific travel demands. This work proposes an end-to-end mobility association rule mining framework called MARIO, conducive to extract urban mobility dynamics through analysing large taxi trip traces of a city. The MARIO framework consists of (i) generating mobility-dynamics network by spatio-temporal analysis of taxi-trips, (ii) finding travel demand variations in different functional regions of the urban area, (iii) extracting mobility association rules and (iv) predicting travel demands and traffic dynamics using extracted associative rules. The proposed MARIO framework is implemented in Google Cloud Platform and an extensive set of experiments using real GPS trace dataset of NYC Green and Yellow Taxi trace, Roma Taxi Dataset and San Francisco Taxi Dataset have been carried out to demonstrate the effectiveness of the framework. The performance of the proposed approach is significantly better than the baseline methods in predicting travel demands (with the reduction of average MAPE value and execution time by 50%).",smart cities,497,not included
10.1016/j.ast.2019.04.048,to_check,Aerospace Science and Technology,scopus,2019-07-01,sciencedirect,real time estimation of impaired aircraft flight envelope using feedforward neural networks,https://api.elsevier.com/content/abstract/scopus_id/85065791672,"Extensive research in recent years has focused on developing flight envelope estimation methods to improve current loss of control prevention and recovery systems. Such methods are practically efficient only if they are able to evaluate in real time the new flight envelope of damaged aircraft based on the altered dynamics. Due to nonlinear dynamics of aircraft, common approaches to estimate the entire flight envelope of high-fidelity models are numerically intensive and their real time implementation is computationally impossible. So current methods are based on reduced complexity models or flight envelopes are determined locally. This paper presents a novel method to estimate the global flight envelope of impaired aircraft in real-time for any unknown failure degree. In the proposed method, first, numerous flight envelopes are evaluated using a high fidelity model at various failure degrees and different flight conditions and prepared as training data. Then multiple feedforward neural networks are trained offline by a Bayesian regularization backpropagation algorithm. Finally, the trained networks are used to estimate flight envelopes in real time. The method is applied to rudder and aileron failure cases of the NASA Generic Transport Model. Results show that the estimated flight envelopes are good approximations of the high fidelity global flight envelopes.",smart cities,498,not included
10.1016/j.ins.2018.11.028,to_check,Information Sciences,scopus,2019-04-01,sciencedirect,machine learning based privacy-preserving fair data trading in big data market,https://api.elsevier.com/content/abstract/scopus_id/85056879362,"In the era of big data, the produced and collected data explode due to the emerging technologies and applications that pervade everywhere in our daily lives, including internet of things applications such as smart home, smart city, smart grid, e-commerce applications and social network. Big data market can carry out efficient data trading, which provides a way to share data and further enhances the utility of data. However, to realize effective data trading in big data market, several challenges need to be resolved. The first one is to verify the data availability for a data consumer. The second is privacy of a data provider who is unwilling to reveal his real identity to the data consumer. The third is the payment fairness between a data provider and a data consumer with atomic exchange. In this paper, we address these challenges by proposing a new blockchain-based fair data trading protocol in big data market. The proposed protocol integrates ring signature, double-authentication-preventing signature and similarity learning to guarantee the availability of trading data, privacy of data providers and fairness between data providers and data consumers. We show the proposed protocol achieves the desirable security properties that a secure data trading protocol should have. The implementation results with Solidity smart contract demonstrate the validity of the proposed blockchain-based fair data trading protocol.",smart cities,499,not included
10.1016/j.matdes.2018.107577,to_check,Materials and Design,scopus,2019-03-05,sciencedirect,ensemble kalman filter-based data assimilation for three-dimensional multi-phase-field model: estimation of anisotropic grain boundary properties,https://api.elsevier.com/content/abstract/scopus_id/85059744257,"Data assimilation (DA) has been used as a machine learning approach to estimate a system's state and the unknown parameters in its numerical model by integrating observed data into model predictions. In this paper, we propose using the DA methodology based on the ensemble Kalman filter (EnKF) to improve the accuracy of microstructure prediction using three-dimensional multi-phase-field (3D-MPF) model and estimate the model parameters simultaneously. To demonstrate the applicability of the DA methodology, we performed numerical experiments in which a priori assumed true parameters related to the grain boundary (GB) energy cusp and GB mobility peak of Σ7 coincidence site lattice GB were estimated from synthetic data of time-evolving polycrystalline microstructure. Four model parameters related to the Σ7 GB properties were successfully estimated by assimilating the synthetic microstructure data to the 3D-MPF model predictions using the EnKF-based DA method. Furthermore, we accurately reproduced the preliminarily assumed true shapes of GB energy cusp and GB mobility peak by using the estimated parameters. The results suggest that implementation of the EnKF-based DA method in the MPF model has great potential for identifying unknown material properties and estimating unmeasurable microstructure evolutions in polycrystalline materials based on real time-series 3D microstructure observation data.",smart cities,500,not included
10.1016/j.future.2017.08.009,to_check,Future Generation Computer Systems,scopus,2019-03-01,sciencedirect,agra: ai-augmented geographic routing approach for iot-based incident-supporting applications,https://api.elsevier.com/content/abstract/scopus_id/85028541738,"Applications that cater to the needs of disaster incident response generate large amount of data and demand large computational resource access. Such datasets are usually collected in real-time at the incident scenes using different Internet of Things (IoT) devices. Hierarchical clouds, i.e., core and edge clouds, can help these applications’ real-time data orchestration challenges as well as with their IoT operations scalability, reliability and stability by overcoming infrastructure limitations at the ad-hoc wireless network edge. Routing is a crucial infrastructure management orchestration mechanism for such systems. Current geographic routing or greedy forwarding approaches designed for early wireless ad-hoc networks lack efficient solutions for disaster incident-supporting applications, given the high-speed and low-latency data delivery that edge cloud gateways impose. In this paper, we present a novel Artificial Intelligent (AI)-augmented geographic routing approach, that relies on an area knowledge obtained from the satellite imagery (available at the edge cloud) by applying deep learning. In particular, we propose a stateless greedy forwarding that uses such an environment learning to proactively avoid the local minimum problem by diverting traffic with an algorithm that emulates electrostatic repulsive forces. In our theoretical analysis, we show that our Greedy Forwarding achieves in the worst case a 
                        3
                        .
                        291
                      path stretch approximation bound with respect to the shortest path, without assuming presence of symmetrical links or unit disk graphs. We evaluate our approach with both numerical and event-driven simulations, and we establish the practicality of our approach in a real incident-supporting hierarchical cloud deployment to demonstrate improvement of application level throughput due to a reduced path stretch under severe node failures and high mobility challenges of disaster response scenarios.",smart cities,501,not included
10.1016/j.biosystems.2019.04.006,to_check,BioSystems,scopus,2019-01-01,sciencedirect,bin-ct: urban waste collection based on predicting the container fill level,https://api.elsevier.com/content/abstract/scopus_id/85064636834,"The fast demographic growth, together with the population concentration in cities and the increasing amount of daily waste, are factors that are pushing to the limit the ability of waste assimilation by Nature. Therefore, we need technological means to optimally manage of the waste collection process, which represents 70% of the operational cost in waste treatment. In this article, we present a free intelligent software system called BIN-CT (BIN for the CiTy), based on computational learning algorithms, which plans the best routes for waste collection supported by past (historical) and future (predictions) data.
                  The objective of the system is to reduction the cost of the waste collection service minimizing the distance traveled by a truck to collect the waste from a container, thereby reducing the fuel consumption. At the same time the quality of service for the citizen is increased, avoiding the annoying overflows of containers thanks to the accurate fill-level predictions given by BIN-CT. In this article we show the features of our software system, illustrating its operation with a real case study of a Spanish city. We conclude that the use of BIN-CT avoids unnecessary trips to containers, reduces the distance traveled to collect a container by 20%, and generates routes 33.2% shorter than the routes used by the company. Therefore it enables a considerable reduction of total costs and harmful emissions thrown up into the atmosphere.",smart cities,502,not included
10.1016/j.eswa.2018.06.037,to_check,Expert Systems with Applications,scopus,2018-12-15,sciencedirect,two-echelon logistics delivery and pickup network optimization based on integrated cooperation and transportation fleet sharing,https://api.elsevier.com/content/abstract/scopus_id/85049438774,"The optimization of the two-echelon logistics delivery and pickup network (2E-LDPN) is a strategical and tactical task which can efficiently be achieved by establishing cooperative alliances. Under the coordination of logistics services providers or logistics facilities of the existing network, high operating costs caused by cross and long distance transportation can be reduced via the inclusive reorganization of the entire network. In order to minimize the total cost, this study simultaneously considers semitrailer truck and vehicle sharing, and establishes a linear mathematical model capable of interpreting real world practices under single or multiple alliances scenarios. An Improved Particle Swarm Optimization (IPSO) algorithm and the Ant Colony Optimization (ACO) algorithm are reasonably combined into a hybrid meta-heuristics to solve the cooperative 2E-LDPN optimization problem. This algorithm combines the merits of IPSO and ACO with local and global search capabilities, and redistributes customer zones on the basis of region partitioning solutions in order to rationalize transportation activities. Finally, an Improved Shapley value model is applied to guarantee profits allocation's fairness and is proved reliable in term of alliance stability. Empirical results out of a case study in Chongqing city show that the IPSO–ACO hybrid algorithm is superior to three well-known algorithms on the cost solution and the number of iterations. Using the Improved Shapley value model and strictly monotonic path (SMP) selection principles, optimal adhesion sequences for two alliances and a grand alliance are yielded. The implemented transition from two sub-alliances based network to the grand alliance is in line with real-world's practices and provides decision makers with a useful tool for the design of cooperative alliances. In addition, the proposed cooperation strategy and profit allocation method enable companies to increase cost savings and the logistics network's efficiency. Besides, semitrailer truck and vehicle sharing as feature of collaboration conditional clauses reduces the size of transportation fleets, and promotes greener logistics operations.",smart cities,503,not included
10.1016/j.compag.2018.09.037,to_check,Computers and Electronics in Agriculture,scopus,2018-11-01,sciencedirect,a decision support tool to enhance agricultural growth in the mékrou river basin (west africa),https://api.elsevier.com/content/abstract/scopus_id/85054181612,"We describe in this paper the implementation of E-Water, an open software Decision Support System (DSS), designed to help local managers assess the Water Energy Food Environment (WEFE) nexus. E-Water aims at providing optimal management solutions to enhance food crop production at river basin level. The DSS was applied in the transboundary Mékrou river basin, shared among Benin, Burkina Faso and Niger. The primary sector for local economy in the region is agriculture, contributing significantly to income generation and job creation. Fostering the productivity of regional agricultural requires the intensification of farming practices, promoting additional inputs (mainly nutrient fertilizers and water irrigation) but, also, a more efficient allocation of cropland.
                  In order to cope with the heterogeneity of data, and the analyses and issues required by the WEFE nexus approach, our DSS integrates the following modules: (1) the EPIC biophysical agricultural model; (2) a simplified regression metamodel, linking crop production with external inputs; (3) a linear programming and a multiobjective genetic algorithm optimization routines for finding efficient agricultural strategies; and (4) a user-friendly interface for input/output analysis and visualization.
                  To test the main features of the DSS, we apply it to various real and hypothetical scenarios in the Mékrou river basin. The results obtained show how food unavailability due to insufficient local production could be reduced by, approximately, one third by enhancing the application and optimal distribution of fertilizers and irrigation. That would also affect the total income of the farming sector, eventually doubling it in the best case scenario. Furthermore, the combination of optimal agricultural strategies and modified optimal cropland allocation across the basin would bring additional moderate increases in food self-sufficiency, and more substantial gains in the total agricultural income.
                  The proposed software framework proves to be effective, enabling decision makers to identify efficient and site-specific agronomic management strategies for nutrients and water. Such practices would augment crop productivity, which, in turn, would allow to cope with increasing future food demands, and find a balanced use of natural resources, also taking other economic sectors—like livestock, urban or energy—into account.",smart cities,504,included
10.1016/j.engappai.2018.03.016,to_check,Engineering Applications of Artificial Intelligence,scopus,2018-06-01,sciencedirect,proa: an intelligent multi-criteria personalized route assistant,https://api.elsevier.com/content/abstract/scopus_id/85045475454,"Personalization of pedestrian routes becomes a necessity due to the wide variety of user profiles that may differ on preferences or requirements to choose a route. Several software applications offer routes usually based on single criterion like distance or time; however, these criteria do not often fit the pedestrian needs.
                  Here, we will first focus on the Personalized Routes Problem and then we will approach the specific case of designing accessible and green pedestrian routes.
                  The proposal is implemented as a freely available Android application (named as PRoA, by intelligent multi-criteria Personalized Route Assistant), which automatically obtains geographical data and information for the decision criteria from open datasets.
                  The proposal is evaluated using real cases at the city of Granada, Spain.",smart cities,505,included
10.1016/j.procs.2018.07.138,to_check,Procedia Computer Science,scopus,2018-01-01,sciencedirect,building an anomaly detection engine (ade) for iot smart applications,https://api.elsevier.com/content/abstract/scopus_id/85051386119,"Data Analytics is by far the component with more added value in Internet of Things (IoT) networks. One aspect of data analytics is anomaly detection within data points received in some cases in real time that help to conduct predictive maintenance, weather monitoring or cyber security forensics for instance. Although there exists a number of web dashboards that allow IoT users to visualize data in time domain and perform statistical analysis, anomaly detection is often absent else if present not that straightforward, reliable and accurate. The development and implementation of Anomaly Detection Engine (ADE) poses a number of challenges that are in fact addressed in this paper. The research work exposes the multifaceted aspect of IoT networks and applications based on real life use cases and the difficulties engendered in mounting an ADE from both software system engineering and network convergence perspectives. Moreover a comparative description of diverse time series models adopted in anomaly detection is undertaken. It was noticed that there is neither one size fit all solution nor a plug n play alternative and that the unsupervised mode in machine learning as a model for time series analysis is the most versatile and efficient technique for IoT analytics developers.",smart cities,506,not included
10.1016/j.egypro.2017.03.271,to_check,Energy Procedia,scopus,2017-03-01,sciencedirect,predicting large scale fine grain energy consumption,https://api.elsevier.com/content/abstract/scopus_id/85017254936,"Today a large volume of energy-related data have been continuously collected. Extracting actionable knowledge from such data is a multi-step process that opens up a variety of interesting and novel research issues across two domains: energy and computer science. The computer science aim is to provide energy scientists with cutting-edge and scalable engines to effectively support them in their daily research activities. This paper presents SPEC, a scalable and distributed predictor of fine grain energy consumption in buildings. SPEC exploits a data stream methodology analysis over a sliding time window to train a prediction model tailored to each building. The building model is then exploited to predict the upcoming energy consumption at a time instant in the near future. SPEC currently integrates the artificial neural networks technique and the random forest regression algorithm. The SPEC methodology exploits the computational advantages of distributed computing frameworks as the current implementation runs on Spark. As a case study, real data of thermal energy consumption collected in a major city have been exploited to preliminarily assess the SPEC accuracy. The initial results are promising and represent a first step towards predicting fine grain energy consumption over a sliding time window.",smart cities,507,not included
10.1016/j.procs.2015.05.489,to_check,Procedia Computer Science,scopus,2015-01-01,sciencedirect,towards a design support system for urban walkability,https://api.elsevier.com/content/abstract/scopus_id/84939191805,"In the paper we present an urban design support tool centered on pedestrian accessibility and walkability of places. Differently from standard decision support systems developed for the purpose of evaluating given pre-defined urban projects and designs, we address the inverse problem of having the software system itself generate hypotheses of projects and designs, given some (user-provided) objectives and constraints. Taking as a starting point a model for evaluating walkability, we adapt the NSGA-II multi-objective genetic algorithm to produce the front of non-dominated design alternatives to satisfy certain predefined constraints. By way of example, we briefly present an application of the system to a real urban area.",smart cities,508,not included
10.1016/j.procs.2014.05.151,to_check,Procedia Computer Science,scopus,2014-01-01,sciencedirect,evaluation of in-vehicle decision support system for emergency evacuation,https://api.elsevier.com/content/abstract/scopus_id/84902825749,"One of the most important issues in Decision Support Systems (DSS) technology is in ensuring their effectiveness and efficiency for future implementations and use. DSS is prominent tool in disaster information system, which allows the authority to provide life safety information directly to the mobile devices of anyone physically located in the evacuation area. After that a personal DSS guides users to a safe point. Due to the large uncertainty in initial conditions and assumptions on underlying process the implementation and evaluation of such DSS are extremely hard, particularly in real environment. We propose a simulation methodology for the evaluation of in-vehicle DSS for emergency evacuation based on transport system and human decision-making modeling.",smart cities,509,not included
10.1016/j.aap.2013.04.026,to_check,Accident Analysis and Prevention,scopus,2013-06-03,sciencedirect,characterization and simulation of optical sensors,https://api.elsevier.com/content/abstract/scopus_id/84887043277,"Numerical simulation is gradually becoming an advantage in active safety. This is why the development of realistic numerical models enabling to substitute real truth by simulated truth is primordial. In order to provide an accurate and cost effective solution to simulate real optical sensor behavior, the software Pro-SiVIC™ has been developed. Simulations with the software Pro-SiVIC™ can replace real tests with optical sensors and hence allow substantial cost and time savings during the development of solutions for driver assistance systems. An optical platform has been developed by IFSTTAR (French Institute of Science and Technology for Transport, Development and Networks) to characterize and validate any existing camera, in order to measure their characteristics as distortion, vignetting, focal length, etc. By comparing real and simulated sensors with this platform, this paper demonstrates that Pro-SiVIC™ accurately reproduces real sensors’ behavior.",smart cities,510,not included
10.1016/j.jhydrol.2013.02.022,to_check,Journal of Hydrology,scopus,2013-01-01,sciencedirect,runoff forecasting using a takagi-sugeno neuro-fuzzy model with online learning,https://api.elsevier.com/content/abstract/scopus_id/84886101097,"A study using local learning Neuro-Fuzzy System (NFS) was undertaken for a rainfall–runoff modeling application. The local learning model was first tested on three different catchments: an outdoor experimental catchment measuring 25m2 (Catchment 1), a small urban catchment 5.6km2 in size (Catchment 2), and a large rural watershed with area of 241.3km2 (Catchment 3). The results obtained from the local learning model were comparable or better than results obtained from physically-based, i.e. Kinematic Wave Model (KWM), Storm Water Management Model (SWMM), and Hydrologiska Byråns Vattenbalansavdelning (HBV) model. The local learning algorithm also required a shorter training time compared to a global learning NFS model. The local learning model was next tested in real-time mode, where the model was continuously adapted when presented with current information in real time. The real-time implementation of the local learning model gave better results, without the need for retraining, when compared to a batch NFS model, where it was found that the batch model had to be retrained periodically in order to achieve similar results.",smart cities,511,not included
10.1016/j.neunet.2012.02.036,to_check,Neural Networks,scopus,2012-08-01,sciencedirect,metamodeling and the critic-based approach to multi-level optimization,https://api.elsevier.com/content/abstract/scopus_id/84861762153,"Large-scale networks with hundreds of thousands of variables and constraints are becoming more and more common in logistics, communications, and distribution domains. Traditionally, the utility functions defined on such networks are optimized using some variation of Linear Programming, such as Mixed Integer Programming (MIP). Despite enormous progress both in hardware (multiprocessor systems and specialized processors) and software (Gurobi) we are reaching the limits of what these tools can handle in real time. Modern logistic problems, for example, call for expanding the problem both vertically (from one day up to several days) and horizontally (combining separate solution stages into an integrated model). The complexity of such integrated models calls for alternative methods of solution, such as Approximate Dynamic Programming (ADP), which provide a further increase in the performance necessary for the daily operation. In this paper, we present the theoretical basis and related experiments for solving the multistage decision problems based on the results obtained for shorter periods, as building blocks for the models and the solution, via Critic-Model-Action cycles, where various types of neural networks are combined with traditional MIP models in a unified optimization system. In this system architecture, fast and simple feed-forward networks are trained to reasonably initialize more complicated recurrent networks, which serve as approximators of the value function (Critic). The combination of interrelated neural networks and optimization modules allows for multiple queries for the same system, providing flexibility and optimizing performance for large-scale real-life problems. A MATLAB implementation of our solution procedure for a realistic set of data and constraints shows promising results, compared to the iterative MIP approach.",smart cities,512,not included
10.1016/j.advwatres.2009.01.001,to_check,Advances in Water Resources,scopus,2009-04-01,sciencedirect,pumping optimization of coastal aquifers based on evolutionary algorithms and surrogate modular neural network models,https://api.elsevier.com/content/abstract/scopus_id/62349136438,"Pumping optimization of coastal aquifers involves complex numerical models. In problems with many decision variables, the computational burden for reaching the optimal solution can be excessive. Artificial Neural Networks (ANN) are flexible function approximators and have been used as surrogate models of complex numerical models in groundwater optimization. However, this approach is not practical in cases where the number of decision variables is large, because the required neural network structure can be very complex and difficult to train. The present study develops an optimization method based on modular neural networks, in which several small subnetwork modules, trained using a fast adaptive procedure, cooperate to solve a complex pumping optimization problem with many decision variables. The method utilizes the fact that salinity distribution in the aquifer, depends more on pumping from nearby wells rather than from distant ones. Each subnetwork predicts salinity in only one monitoring well, and is controlled by relatively few pumping wells falling within certain control distance from the monitoring well. While the initial control area is radial, its shape is adaptively improved using a Hermite interpolation procedure. The modular neural subnetworks are trained adaptively during optimization, and it is possible to retrain only the ones not performing well. As optimization progresses, the subnetworks are adapted to maximize performance near the current search space of the optimization algorithm. The modular neural subnetwork models are combined with an efficient optimization algorithm and are applied to a real coastal aquifer in the Greek island of Santorini. The numerical code SEAWAT was selected for solving the partial differential equations of flow and density dependent transport. The decision variables correspond to pumping rates from 34 wells. The modular subnetwork implementation resulted in significant reduction in CPU time and identified an even better solution than the original numerical model.",smart cities,513,included
10.3182/20070904-3-kr-2922.00028,to_check,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2007-01-01,sciencedirect,interacting with gestures and facial expressions - implementation and applications,https://api.elsevier.com/content/abstract/scopus_id/79961041301,"Developments in software and hardware technologies as, e.g. in microelectronics, mechatronics, speech technology, computer linguistics, computer vision, and artificial intelligence are continuously driving new embedded applications for work, leisure, and mobility. Since usability of such appliances turns out to be the main factor limiting complexity, new approaches to interface design are needed. Promising measures for building enhanced usability interfaces are non-intrusive means of interaction as, e.g. gestures and mimics. This paper describes how suitable features can be extracted from camera pictures in real time and presents some real world applications.",smart cities,514,not included
10.1016/j.annemergmed.2004.02.037,to_check,Annals of Emergency Medicine,scopus,2004-09-01,sciencedirect,effects of neural network feedback to physicians on admit/discharge decision for emergency department patients with chest pain,https://api.elsevier.com/content/abstract/scopus_id/4344692494,"Study objective
                  Neural networks can risk-stratify emergency department (ED) patients with potential acute coronary syndromes with a high specificity, potentially facilitating ED discharge of patients to home. We hypothesized that the use of “real-time” neural networks would decrease the admission rate for ED chest pain patients.
               
                  Methods
                  We conducted a before-and-after trial. Consecutive ED patients with chest pain were evaluated before and after implementation of a neural network in an urban university ED. Data included 40 variables used in neural networks for acute myocardial infarction and acute coronary syndrome. Data were obtained in real time, and neural network outputs were provided to the treating physician while patients were in the ED. On hospital discharge, attending physicians received feedback, including neural network output, their initial clinical impression, cardiac test results, and final diagnosis. The main outcome was the actual admit/discharge decision made before versus after the implementation of the neural network.
               
                  Results
                  Before implementation, 4,492 patients were enrolled; after implementation, 432 patients were enrolled. Implementation of the neural network did not decrease the hospital admission rate (before: 62.7% [95% confidence interval (CI) 61.3% to 64.1%] versus after: 66.6% [95% CI 62.2% to 71.0%]). Additionally, the ICU admission rates were not different (11.4% [95% CI 10.5% to 12.3%] versus 9.3% [95% CI 6.6% to 12.0%]). Physician query found that the neural network changed management in only 2 cases (<1%).
               
                  Conclusion
                  The use of real-time neural network feedback did not influence the admission decision for ED patients with chest pain, most likely because the neural network output was delayed until the return of cardiac markers, and the disposition decision had already been made by that time.",smart cities,515,not included
10.12759/hsr.46.2021.3.124-150,to_check,core,DEU,2021-01-01 00:00:00,core,the body as the border: a new era,https://doi.org/10.12759/hsr.46.2021.3.124-150,"COVID-19 has reminded us of the significance of borders. In 1989, with the fall of the Berlin Wall, many predicted that sealed gates would soon become relics of a bygone era. Today, we find a different reality. Instead of disappearing, borders are transforming. In this article, we build upon the shifting border logic to explore how responses to the global pandemic have accelerated processes of detachment of mobility control from a fixed territorial marker. From global travel bans to mandating prearrival proof of a negative test result taken within 48 or 72 hours prior to departure to requiring digital registration of a passenger's travel history to enforcing strict post-arrival mandatory quarantine orders that arrest mobility, the shifting border paradigm has provided a template for policymakers to respond to a mounting global crisis. In addition to regulating movement across international borders and within countries, we trace the surprising return of subnational and interregional division lines in managing mobility, the erosion of the once taken for granted right to return to one’s home country, and the spatial and legal techniques used to block refugees from reaching terra firma during the pandemic. Next, we critically evaluate the authorization given under emergency regulations to deploy novel biometric and AI technologies, big data, and predictive algorithms to surveil moving bodies at real time and reprimand those deemed to have breached their quarantine or related governmental emergency measures. While drastic times call for drastic measures, techniques of movement control that ""scan"" and trace our bodies raise serious questions about justice, fairness, and the risk of discrimination, which may well remain with us even long after the pandemic is over",smart cities,516,not included
https://core.ac.uk/download/327690899.pdf,to_check,core,10.28925/2663-4023.2020.8.97112,2020-06-25 00:00:00,core,'borys grinchenko kyiv university',ЗАСТОСУВАННЯ ЗГОРТКОВИХ НЕЙРОННИХ МЕРЕЖ ДЛЯ БЕЗПЕКИ РОЗПІЗНАВАННЯ ОБ'ЄКТІВ У ВІДЕОПОТОЦІ,"The article is devoted to analyzing methods for recognizing images and finding them in the video stream. The evolution of the structure of convolutional neural networks used in the field of computer video flow diagnostics is analyzed. The performance of video flow diagnostics algorithms and car license plate recognition has been evaluated. The technique of recognizing the license plates of cars in the video stream of transport neural networks is described. The study focuses on the creation of a combined system that combines artificial intelligence and computer vision based on fuzzy logic. To solve the problem of license plate image recognition in the video stream of the transport system, a method of image recognition in a continuous video stream with its implementation based on the composition of traditional image processing methods and neural networks with convolutional and periodic layers is proposed. The structure and peculiarities of functioning of the intelligent distributed system of urban transport safety, which feature is the use of mobile devices connected to a single network, are described.
A practical implementation of a software application for recognizing car license plates by mobile devices on the Android operating system platform has been proposed and implemented. Various real-time vehicle license plate recognition scenarios have been developed and stored in a database for further analysis and use. The proposed application uses two different specialized neural networks: one for detecting objects in the video stream, the other for recognizing text from the selected image. Testing and analysis of software applications on the Android operating system platform for license plate recognition in real time confirmed the functionality of the proposed mathematical software and can be used to securely analyze the license plates of cars in the scanned video stream by comparing with license plates in the existing database. The authors have implemented the operation of the method of convolutional neural networks detection and recognition of license plates, personnel and critical situations in the video stream from cameras of mobile devices in real time. The possibility of its application in the field of safe identification of car license plates has been demonstrated.Стаття присвячена аналізу методів розпізнавання зображень та пошуку їх у відеопотоці. Проаналізовано еволюцію структури згорткових нейронних мереж, що використовуються в області діагностики комп'ютерних відеопотоків. Здійснено оцінку ефективності алгоритмів діагностики відеопотоків та розпізнавання номерних знаків автомобілів. Описана методика розпізнавання номерних знаків автомобілів, що знаходяться у відеопотоці транспортних нейронних мереж. В дослідженні приділено увагу створенню комбінованої системи, яка поєднує в собі технологію штучного інтелекту та комп'ютерного зору на основі нечіткої логіки. Для вирішення проблеми розпізнавання зображень номерних знаків у відеопотоці транспортної системи запропоновано метод розпізнавання зображень у безперервному відеопотоці з його реалізацією на основі складу традиційних методів обробки зображень та нейронних мереж із згортковими та періодичними шарами. Описано структуру та особливості функціонування інтелектуальної розподіленої системи безпеки міського транспорту, особливістю якої є використання мобільних пристроїв, підключених до єдиної мережі. Запропоновано та здійснено практичну реалізацію програмного застосування для розпізнавання автомобільних номерних знаків мобільними пристроями на платформі операційної системи Android. Розроблено різні сценарії розпізнавання номерних знаків автомобілів у реальному часі та збереження їх у базі даних для подальшого аналізу та використання. В запропонованому застосуванні використовуються дві різні спеціалізовані нейромережі: одна - для детектування об’єктів у відеопотоці, інша – для розпізнавання тексту з виділеного зображення. Проведене випробовування та аналіз програмного застосування на платформі операційної системи Android для розпізнавання автомобільних номерних знаків у реальному часі підтвердив працездатність запропонованого математичного забезпечення і може використовуватися для безпечного аналізу номерних знаків автомобілів у сканованому відео потоці шляхом порівняння з номерними знаками в існуючій базі даних. Авторами реалізовано функціонування метод згорткових нейронних мереж виявлення та розпізнавання номерних знаків, персоналу та критичних ситуацій у відеопотоці з камер мобільних пристроїв у режимі реального часу. Продемонстрована можливість його застосування у сфері безпечної ідентифікації номерних знаків автомобілів",smart cities,517,not included
"'universidade de sao paulo, agencia usp de gestao da informacao academica (aguia)'",to_check,core,,2018-03-07 00:00:00,core,"automation of the reduction of technical in reticulated distribution systems using artificial neural netwarks, te4chnical losses power factor.",10.11606/T.3.2018.tde-05032018-102829,"Este trabalho apresenta a metodologia, o desenvolvimento e testes de um sistema de automação independente, baseado em Redes Neurais Artificiais, para redução de perdas técnicas em redes de distribuição subterrâneas reticuladas por meio do controle ótimo dos bancos de capacitores presentes na rede. A metodologia proposta contempla funcionalidades típicas de Redes Inteligentes, incluindo soluções práticas para o posicionamento de sensores de corrente em redes subterrâneas, coleta de medições de campo e transmissão para o Centro de Operação da Distribuição e controle em tempo real dos equipamentos de campo (bancos de capacitores). Portanto este trabalho consiste na implementação da solução através de baixo custo de investimento na mitigação do controle do fator de potência nos pontos de entrega ao consumidor, sendo que com isto ocorrem melhorias nos indicadores de qualidade e confiabilidade atendendo aos requisitos regulamentares e contratuais de fornecimento das distribuidoras. Para validação da metodologia proposta, foram utilizados os dados da concessionária de energia AES Eletropaulo sobre a Rede de Distribuição Subterrânea Reticulada do centro da cidade de São Paulo. As etapas da metodologia proposta e os principais aspectos do desenvolvimento do sistema são também descritos, bem como os testes realizados para comprovação dos resultados e validação do sistema.This work presents the methodology, development and testing of an independent automation system, based on Artificial Neural Networks, to reduce technical losses in reticulated underground distribution networks by means of the optimal control of the capacitor banks present in the network. The proposed methodology includes typical functionalities of Intelligent Networks, including practical solutions for the positioning of current sensors in underground networks, collection of field measurements and transmission to the Distribution Operation Center and real-time control of field equipment (capacitors banks). Therefore, this work consists in the implementation of the solution through a low cost of investment in the mitigation of the control of the power factor in the points of delivery to the consumer, and with this there are improvements in the indicators of quality and reliability taking into account the regulatory and contractual requirements of supply of the distributors. The energy concessionaire AES Eletropaulo had great participation in this research project, providing the necessary data of the Reticulated Underground Distribution Network of the city center of São Paulo. The steps of the proposed methodology and the main aspects of system development are also described, as well as the tests performed to prove the results and validate the system",smart cities,518,not included
14932249c7e25082013909f47760131f45ec114e,to_check,semantic_scholar,,,semantic_scholar,research statement for allen miu mit computer science and artificial intelligence laboratory,https://www.semanticscholar.org/paper/14932249c7e25082013909f47760131f45ec114e,"I share the vision that ubiquitous wireless networks and mobile systems of the future will help foster new applications that increase productivity, facilitate new modes of communication, and introduce new forms of convenience and entertainment that do not exist today. My doctoral dissertation focuses on the problem of improving coverage and reducing transmission latency and losses in wireless local area networks (WLANs). The solutions in my dissertation improve the performance and quality of a variety of delaysensitive wireless applications that do not run well on today’s WLANs—these applications include voice over IP, video conferencing, and online gaming. During my career as a graduate student at MIT, I have also worked on several systems projects related to managing mobility in wireless networks, characterizing the performance of the Cricket location system, implementing a programming library for location-aware applications, developing a personal indoor navigation system, and building a system that extends high bandwidth Internet connectivity to moving vehicles in a city via open 802.11 access points. In the future, I would like to continue to explore problems in wireless networks and mobile systems, and develop new applications for these systems. One of the big challenges in designing effective wireless networks and mobile systems is that their performance is often affected by the limitations of device hardware and by the vulgarities of the physical environment. For example, location sensors and wireless networking interfaces may work perfectly in a large open field. But harmful effects such as attenuation, interference, reflections, and mobility that are commonly found in real operational settings can lead to incorrect position estimates and unsuccessful packet transmissions. Because the complexity of the physical environment are intractable, the simple analytical and simulation models used to design and study these systems can often lead to unexpected and undesirable results in practice. My early experience in working with the Cricket location system led me to realize that running experiments using testbeds and building system prototypes are necessary to develop robust wireless networks and mobile systems that work effectively in the real world, and to conduct system evaluations that are representative and meaningful. For example, by building the Cricket indoor navigation system and studying it in a real environment, we found numerous problems such as inaccuracies, fragile components, and the lack of debugging facility in the original Cricket devices that eventually led to a series of recommendations in the design of the much-improved Cricket v2 system. By analyzing the data collected in our testbed, we gained new insights that led to an improved position estimation algorithm with reduced delay and improved accuracy. Building on my successful experience in Cricket, my doctoral thesis work also follows the approach of hands-on system-building and experimentation. In my dissertation, I explore the problem of increasing loss resilience and reducing transmission delay in wireless local area networks. By conducting a wide range of experiments, I discovered that transmission losses in WLANs, while often bursty, are also independent between different transmission paths, particularly when nodes are moving. These results motivated me to revisit some of the traditional approaches used to combat transmission losses in current WLAN systems and to seek creative yet practical ways to improve them. To obtain good coverage inside a building, WLAN operators typically deploy multiple access points (APs), which are network elements that relay packets for a WLAN client. In today’s WLAN, each AP operates independently and limits each WLAN client to communicate with a single AP. Thus, when the channel quality of the transmission path deteriorates, the transmitter must either repeat retransmissions until the channel quality improves or reduce the transmission rate to compensate for the poor channel. The consequence is manifested as transmission loss (which occurs when the transmitter exhausts the retransmission limit) or transmission delay, which can drastically reduce the perceived quality of many wireless applications. My thesis attempts to tackle the problem from a systems’ perspective. I hypothesized that APs with",smart cities,519,not included
10.1109/cloudcom.2016.0060,to_check,2016 IEEE International Conference on Cloud Computing Technology and Science (CloudCom),IEEE,2016-12-15 00:00:00,ieeexplore,animation rendering on multimedia fog computing platforms,https://ieeexplore.ieee.org/document/7830701/,"Modern distributed multimedia applications are resource-hungry, and they often leverage on-demand cloud services to reduce their expenses. Existing cloud services deploy many servers in a few data centers, which consume a lot of electricity to power up and cold down, and thus are expensive and environmentally unfriendly. In this paper, we present a multimedia fog computing platform that utilizes resources from public crowds, edge networks, and data centers to serve distributed multimedia applications at lower costs. We use animation rendering as a case study, and identify several challenges for optimizing it on our multimedia fog computing platform. Among these challenges, we focus on the problem of predicting the completion time of each rendering job. We propose an efficient algorithm based on state-of-the-art machine learning algorithms. We also fine-tune the algorithm using multi-fold cross-validation for higher prediction accuracy. With real datasets, we conduct trace-driven simulations to quantify the performance of our prediction algorithm and that of the whole platform. The simulation results show that our proposed algorithm outperforms a state-of the-art statistical model in several aspects: completed job ratio by 20%, makespan by 2 times, and normalized deviation by 30 times, on average. Moreover, the overall performance of the platform with our proposed algorithm is fairly close to that with an Oracle of the actual job completion time: a small factor of 1.48 in terms of makespan is observed.",multimedia,520,not included
10.1109/ictta.2008.4529967,to_check,2008 3rd International Conference on Information and Communication Technologies: From Theory to Applications,IEEE,2008-04-11 00:00:00,ieeexplore,multimedia learning in advanced computer-based contexts: `discovering trier',https://ieeexplore.ieee.org/document/4529967/,"Edutainment is a neologism that expresses the marriage of education and entertainment. In particular edutainment is a form of entertainment designed to educate as well as to amuse and typically seeks to instruct or socialize its audience by embedding lessons in some familiar form of entertainment: television programs, computer and video games, films, music, websites or multimedia software. On the other hand, the design and the implementation of not boring or repetitive edutainment are not trivial or easy tasks. In this paper we propose a new approach for the design of an edutainment. The proposed environment ""translates"" the storyboards of the games in a Dynamic Bayesian Networks that are the extension of Bayesian Networks for modelling times-series data. The Bayesian approach allows a dynamic adaptation of the game to the user's profile and the establishment of several paths in the same game. We furnish some results obtained by the use of a first prototype of our tool in real academic courses.",multimedia,521,not included
10.1109/fpl.2010.22,to_check,2010 International Conference on Field Programmable Logic and Applications,IEEE,2010-09-02 00:00:00,ieeexplore,real-time classification of multimedia traffic using fpga,https://ieeexplore.ieee.org/document/5694221/,"Real-time classification of Internet traffic according to application types is vital for network management and surveillance. Identifying emerging applications based on well-known port numbers is no longer reliable. While deep packet inspection (DPI) solutions can be accurate, they require constant updates of signatures and become infeasible for encrypted payload especially in multimedia applications (e.g. Skype). Statistical approaches based on machine learning have thus been considered more promising and robust to encryption, privacy, protocol obfuscation, etc. However, the computation complexity of traffic classification using those statistical solutions is high, which prevents them being deployed in systems that need to manage Internet traffic in real time. This paper proposes a FPGA-based parallel architecture to accelerate the statistical identification of multimedia applications while maintaining high classification accuracy. Specifically, we base our design on the k-Nearest Neighbors (k-NN) algorithm which has been shown to be one of the most accurate machine learning algorithms for Internet traffic classification. To enable high-rate data streaming for real-time classification, we adopt the locality sensitive hashing (LSH) for approximate k-NN. The LSH scheme is carefully designed to achieve high accuracy while being efficient for implementation on FPGA. Processing components in the architecture are optimized to realize high throughput. Extensive experiments and FPGA implementation results show that our design can achieve high accuracy above 99% for classifying three main categories of multimedia applications from Internet traffic while sustaining 80 Gbps throughput for minimum size (40 bytes) packets.",multimedia,522,not included
10.1109/nof.2017.8251223,to_check,2017 8th International Conference on the Network of the Future (NOF),IEEE,2017-11-24 00:00:00,ieeexplore,prediction of active ue number with bayesian neural networks for self-organizing lte networks,https://ieeexplore.ieee.org/document/8251223/,"Internet-empowered electronic gadgets and content rich multimedia applications have expanded exponentially in recent years. As a consequence, heterogeneous network structures introduced with Long Term Evolution (LTE) Advanced have increasingly gaining momentum in order to handle with data explosion. On the other hand, the deployment of new network equipment is resulting in increasing both capital and operating expenditures. These deployments are done under the consideration of the busy hour periods which the network experiences the highest amount of traffic. However, these periods refer to only a couple of hours over a 24-hour period. In relation to this, accurate prediction of active user equipment (UE) number is significant for efficient network operations and results in decreasing energy consumption. In this paper, we investigate a Bayesian technique to design an optimal feed-forward neural network for shortterm predictor executed at the network management entity and providing proactivity to Energy Saving, a Self-Organizing Network function. We first demonstrate prediction results of active UE number collected from real LTE network. Then, we evaluate the prediction accuracy of the Bayesian neural network as comparing with low complex naive prediction method, Holt- Winter's exponential smoothing method, a deterministic feedforward neural network without Bayesian regularization term.",multimedia,523,not included
10.1109/aivr50618.2020.00025,to_check,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),IEEE,2020-12-18 00:00:00,ieeexplore,unmasking communication partners: a low-cost ai solution for digitally removing head-mounted displays in vr-based telepresence,https://ieeexplore.ieee.org/document/9319106/,"Face-to-face conversation in Virtual Reality (VR) is a challenge when participants wear head-mounted displays (HMD). A significant portion of a participant's face is hidden and facial expressions are difficult to perceive. Past research has shown that high-fidelity face reconstruction with personal avatars in VR is possible under laboratory conditions with high-cost hardware. In this paper, we propose one of the first low-cost systems for this task which uses only open source, free software and affordable hardware. Our approach is to track the user's face underneath the HMD utilizing a Convolutional Neural Network (CNN) and generate corresponding expressions with Generative Adversarial Networks (GAN) for producing RGBD images of the person's face. We use commodity hardware with low-cost extensions such as 3Dprinted mounts and miniature cameras. Our approach learns end-to-end without manual intervention, runs in real time, and can be trained and executed on an ordinary gaming computer. We report evaluation results showing that our low-cost system does not achieve the same fidelity of research prototypes using high-end hardware and closed source software, but it is capable of creating individual facial avatars with personspecific characteristics in movements and expressions.",multimedia,524,not included
10.1109/acie51979.2021.9381092,to_check,2021 IEEE Asia Conference on Information Engineering (ACIE),IEEE,2021-01-31 00:00:00,ieeexplore,end-to-end behavior simulation for multi-access edge computing,https://ieeexplore.ieee.org/document/9381092/,"Multi-Access edge computing (MEC) is an emerging network architecture that enables cloud computing at the edge of the network characterized by ultra-low latency, high-bandwidth, and direct access to real-time network information. However, the network heterogeneity (4G, 5G, WiFi, etc.) and quality requirement diversity (Internet of Things, Artificial Intelligence, Augmented Reality, etc.) make the deployment of MEC ever more complicated and expensive. In this paper, we introduce a novel end-to-end simulation tool- Behavior Simulation from User Equipment to Edge and Cloud (BSUEEC)- which can offload computing, communication, and traffic simulation to different simulators for the deployment and resource planning of MEC, applicable to various real world scenarios. It's a co-simulation tool that can: 1) seamlessly integrate with heterogeneous access network simulators via a self-developed adapter, 2) extend the simulation from UE(user equipments)-to-edge to UE-to-multi-layer-MEC, even the Cloud 3) assign the UE with real traffic patterns, 4) consider the interaction among access network, MEC/Cloud processing, and road traffic in one simulation. The effectiveness and applicability of our simulation framework (“BSUEEC”) three Vehicle to Everything (V2X) scenarios.",multimedia,525,not included
10.1109/ectc32862.2020.00276,to_check,2020 IEEE 70th Electronic Components and Technology Conference (ECTC),IEEE,2020-06-30 00:00:00,ieeexplore,extracting power supply current profile by using interposer-based low-noise probing technique for pdn design of high-density pop,https://ieeexplore.ieee.org/document/9159523/,"Firmly understanding the power supply current profile (PSCP) of various scenarios used in real use cases is essential for the simulation and design of power delivery network (PDN) of system-on-chip (SOC) to maximize processor's performance within limited cost budget for die, package, and system, because the low power hard-ware implementation of leading-edge SOC including high performance computing cores for video data processing, 3D graphics, augmented reality, artificial intelligence, and 5G data communication with battery powered portable electronic devices, whose primary concern is the low power consumption, has been concentrated on reducing the minimum allowable power supply voltage for high performance computing cores including CPU, GPU, NPU and CP. The objective of this work is presenting the method to precisely probe power supply voltage fluctuation (PSVF) of whole power domains for power supply current profile (PSCP) extraction of entire cores, for which the authors present an concrete analysis methodology, based on which a test interposer scheme targeted for probing core logic blocks at the proper position of PDN is implemented and demonstrated when in operation. The proposed low noise probing system for acquiring PSCP is constructed by a test interposer designed with rigorous PI analyses.",multimedia,526,not included
10.1109/robot.1997.606781,to_check,Proceedings of International Conference on Robotics and Automation,IEEE,1997-04-25 00:00:00,ieeexplore,real-time pose estimation of 3d objects from camera images using neural networks,https://ieeexplore.ieee.org/document/606781/,This paper deals with the problem of obtaining a rough estimate of three dimensional object position and orientation from a single two dimensional camera image. Such an estimate is required by most 3-D to 2-D registration and tracking methods that can efficiently refine an initial value by numerical optimization to precisely recover 3-D pose. However the analytic computation of an initial pose guess requires the solution of an extremely complex correspondence problem that is due to the large number of topologically distinct aspects that arise when a three dimensional opaque object is imaged by a camera. Hence general analytic methods fail to achieve real-time performance and most tracking and registration systems are initialized interactively or by ad hoc heuristics. To overcome these limitations we present a novel method for approximate object pose estimation that is based on a neural net and that can easily be implemented in real-time. A modification of Kohonen's self-organizing feature map is systematically trained with computer generated object views such that it responds to a preprocessed image with one or more sets of object orientation parameters. The key idea proposed here is to choose network topology in accordance with the representation of 3-D orientation. Experimental results from both simulated and real images demonstrate that a pose estimate within the accuracy requirements can be found in more than 81% of all cases. The current implementation operates at 10 Hz on real world images.,multimedia,527,not included
10.1109/secon.2017.7925321,to_check,SoutheastCon 2017,IEEE,2017-04-02 00:00:00,ieeexplore,towards real-time segmentation of 3d point cloud data into local planar regions,https://ieeexplore.ieee.org/document/7925321/,"This article describes an algorithm for efficient segmentation of point cloud data into local planar surface regions. This is a problem of generic interest to researchers in the computer graphics, computer vision, artificial intelligence and robotics community where it plays an important role in applications such as object recognition, mapping, navigation and conversion from point clouds representations to 3D surface models. Prior work on the subject is either computationally burdensome, precluding real time applications such as robotic navigation and mapping, prone to error for noisy measurements commonly found at long range or requires availability of coregistered color imagery. The approach we describe consists of 3 steps: (1) detect a set of candidate planar surfaces, (2) cluster the planar surfaces merging redundant plane models, and (3) segment the point clouds by imposing a Markov Random Field (MRF) on the data and planar models and computing the Maximum A-Posteriori (MAP) of the segmentation labels using Bayesian Belief Propagation (BBP). In contrast to prior work which relies on color information for geometric segmentation, our implementation performs detection, clustering and estimation using only geometric data. Novelty is found in the fast clustering technique and new MRF clique potentials that are heretofore unexplored in the literature. The clustering procedure removes redundant detections of planes in the scene prior to segmentation using BBP optimization of the MRF to improve performance. The MRF clique potentials dynamically change to encourage distinct labels across depth discontinuities. These modifications provide improved segmentations for geometry-only depth images while simultaneously controlling the computational cost. Algorithm parameters are tunable to enable researchers to strike a compromise between segmentation detail and computational performance. Experimental results apply the algorithm to depth images from the NYU depth dataset which indicate that the algorithm can accurately extract large planar surfaces from depth sensor data.",multimedia,528,not included
10.1109/aero50100.2021.9438232,to_check,2021 IEEE Aerospace Conference (50100),IEEE,2021-03-13 00:00:00,ieeexplore,a pipeline for vision-based on-orbit proximity operations using deep learning and synthetic imagery,https://ieeexplore.ieee.org/document/9438232/,"Deep learning has become the gold standard for image processing over the past decade. Simultaneously, we have seen growing interest in orbital activities such as satellite servicing and debris removal that depend on proximity operations between spacecraft. However, two key challenges currently pose a major barrier to the use of deep learning for vision-based on-orbit proximity operations. Firstly, efficient implementation of these techniques relies on an effective system for model development that streamlines data curation, training, and evaluation. Secondly, a scarcity of labeled training data (images of a target spacecraft) hinders creation of robust deep learning models. This paper presents an open-source deep learning pipeline, developed specifically for on-orbit visual navigation applications, that addresses these challenges. The core of our work consists of two custom software tools built on top of a cloud architecture that interconnects all stages of the model development process. The first tool leverages Blender, an open-source 3D graphics toolset, to generate labeled synthetic training data with configurable model poses (positions and orientations), lighting conditions, backgrounds, and commonly observed in-space image aberrations. The second tool is a plugin-based framework for effective dataset curation and model training; it provides common functionality like metadata generation and remote storage access to all projects while giving complete independence to project-specific code. Time-consuming, graphics-intensive processes such as synthetic image generation and model training run on cloud-based computational resources which scale to any scope and budget and allow development of even the largest datasets and models from any machine. The presented system has been used in the Texas Spacecraft Laboratory with marked benefits in development speed and quality. Remote development, scalable compute, and automatic organization of data and artifacts have dramatically decreased iteration time while increasing reproducibility and system comprehension. Diverse, high-fidelity synthetic images that more closely replicate the real environment have improved model performance against real-world data. These results demonstrate that the presented pipeline offers tangible benefits to the application of deep learning for vision-based on-orbit proximity operations.",multimedia,529,included
10.1109/icebe.2011.28,to_check,2011 IEEE 8th International Conference on e-Business Engineering,IEEE,2011-10-21 00:00:00,ieeexplore,a rfid-based intelligent warehouse management system design and implementation,https://ieeexplore.ieee.org/document/6104615/,"A RFID-based intelligent warehouse management system (RFID-IWMS) is proposed in this paper. The RFID-IWMS helps to achieve better inventory control, as well as to improve operation efficiency. To this purpose, it automates the manual warehouse operation and provides tight integration with current warehouse management system (WMS). In this system, RFID tags are embedded in the pallets and shelves. In addition, forklifts are equipped with intelligent terminal as well as RFID reader and antenna to support automatic data scanning and storage location checking. Moreover, a middle layer software component is design to facilitate the communication between WMS, portable terminals and forklift terminals through wireless LAN. Besides, it also supports additional powerful functions such as forklift scheduling, picking sequence management and 3D shelves monitoring. The design of the system makes full use of the existing equipments and facilities and has the feature of low cost and quick inaction. Through real working practice in the distribution center of Bailian Group, the system is proved to be feasible in the aspects of both technology and cost.",multimedia,530,included
10.1049/icp.2021.1447,to_check,11th International Conference of Pattern Recognition Systems (ICPRS 2021),IET,2021-03-19 00:00:00,ieeexplore,asymmetric u-net for brain tumor segmentation: transfer to an independent database,https://ieeexplore.ieee.org/document/9569013/,"An automatic and accurate brain tumor segmentation software for magnetic resonance imaging is crucial for clinical assessment, follow-up, and subsequent gliomas treatment. Convolutional Neural Networks (CNN) is the state-of-the-art in this task. One of the fundamental challenges for the inclusion of CNN's into clinical practice is the networks' ability to generalize their performance on a different dataset, other than the one in which the model was trained. Most of the proposed methods only evaluate their models on public databases and do not test them in real clinical images. We present a 3D Asymmetric U-Net for brain tumor segmentation from MRI images in patients with glioma. Our model has been trained on the BraTS 2020 public database. Besides, our model performance was evaluated on an independent cohort of 12 patients from the Bretonneau Hospital.",multimedia,531,not included
10.1109/humanoids.2014.7041373,to_check,2014 IEEE-RAS International Conference on Humanoid Robots,IEEE,2014-11-20 00:00:00,ieeexplore,footstep planning on uneven terrain with mixed-integer convex optimization,https://ieeexplore.ieee.org/document/7041373/,"We present a new method for planning footstep placements for a robot walking on uneven terrain with obstacles, using a mixed-integer quadratically-constrained quadratic program (MIQCQP). Our approach is unique in that it handles obstacle avoidance, kinematic reachability, and rotation of footstep placements, which typically have required non-convex constraints, in a single mixed-integer optimization that can be efficiently solved to its global optimum. Reachability is enforced through a convex inner approximation of the reachable space for the robot's feet. Rotation of the footsteps is handled by a piecewise linear approximation of sine and cosine, designed to ensure that the approximation never overestimates the robot's reachability. Obstacle avoidance is ensured by decomposing the environment into convex regions of obstacle-free configuration space and assigning each footstep to one such safe region. We demonstrate this technique in simple 2D and 3D environments and with real environments sensed by a humanoid robot. We also discuss computational performance of the algorithm, which is currently capable of planning short sequences of a few steps in under one second or longer sequences of 10-30 footsteps in tens of seconds to minutes on common laptop computer hardware. Our implementation is available within the Drake MATLAB toolbox [1].",multimedia,532,included
10.1109/iaeac.2018.8577729,to_check,"2018 IEEE 3rd Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)",IEEE,2018-10-14 00:00:00,ieeexplore,image segmentation based on blob analysis and quad-tree algorithm,https://ieeexplore.ieee.org/document/8577729/,"Image segmentation is one of the most popular topic in recent research and studies, there are lots of different method to solve this problem. Some existing method works pretty well, and others not. This paper proposed to implement two improved existing method, Split and merge and Blob coloring algorithm, and compared their segmentation results in both 2D and 3D. Meanwhile, to clarify our achievements following scientific method, we have to establish our evaluation function FMI to give a score to tell what level of goodness our implementation could achieve. The first task is implementation of different region growing algorithms. We implement a general split and merge algorithm and the Blob Coloring algorithm that can work both in 2D and in 3D with different homogeneity criteria. For the split and merge algorithm, we will use quad-tree algorithm to split and merge our image based on the homogeneity. For the blob algorithm, we will check the L shape, then merge the pixels with similar homogeneity, and ignore those not. According to the experimental results, We found the split merge algorithm is generally better than the blob algorithm. Although for some case, the blob algorithm can also reach to more than 0.8 FMI score in evaluating, the overall performance is still bad. Split and merge algorithm works well on all the image cases. As we think, we decide to apply our implementation in some real images to achieve some goals. For example, we choose a group of interior design and a group of animals as 3D images. The result coming from our implementation could be used in biomedical field such as cancer detection, as well as in public management such as suspects outline detection. However, there are real problems for us to use region growing techniques to detect boundary of objects with some specific indexes because if the target has obvious difference inside, the algorithm will not treat it as a whole. However, from a visual point of view, we treat it as a complete object.",multimedia,533,not included
10.1109/iv48863.2021.9576020,to_check,2021 IEEE Intelligent Vehicles Symposium (IV),IEEE,2021-07-17 00:00:00,ieeexplore,in-cabin vehicle synthetic data to test deep learning based human pose estimation models,https://ieeexplore.ieee.org/document/9576020/,"The use of vehicle in-cabin monitoring has been increasing to fulfil the specifications of European safety regulations. These regulations present several requirements for detecting driver distraction, and more complex requirements are soon to be expected in higher automation levels. Today's restraint systems provide optimal protection in standard frontal seat positions and deviations to this might cause severe airbag-induced injuries. This makes in-cabin monitoring critical to improve safety and mitigate dangerous situations in case of a crash, and especially in high levels of autonomous driving. Defining the best sensor positioning inside the vehicle's cabin is a challenge due to its constraints and limitations. The main aim of this work was to verify if simulated 3D human models integrated into a 3D modelled vehicle interior environment can be used to run Deep Learning based human pose estimation models. To perform such task, we utilized the software MakeHuman combined with Blender, to build the virtual environment and create photorealistic scenes containing selected front occupants' postures use cases and then feed into Openpose and Mask R-CNN models. The results showed that using a 2D HPE (Human Pose Estimation) network pre-trained on real data, can detect successfully photorealistic synthetic data of humans under complex scenarios. It is also shown that complex and rare postures can cause failure on 2D HPE detections, as shown in the literature review. This work helps to define the most suitable camera positions which, in combination with specific camera lenses, can deliver quality images for a robust pose detection.",multimedia,534,not included
10.1109/robot.2010.5509682,to_check,2010 IEEE International Conference on Robotics and Automation,IEEE,2010-05-07 00:00:00,ieeexplore,indoor scene recognition through object detection,https://ieeexplore.ieee.org/document/5509682/,"Scene recognition is a highly valuable perceptual ability for an indoor mobile robot, however, current approaches for scene recognition present a significant drop in performance for the case of indoor scenes. We believe that this can be explained by the high appearance variability of indoor environments. This stresses the need to include high-level semantic information in the recognition process. In this work we propose a new approach for indoor scene recognition based on a generative probabilistic hierarchical model that uses common objects as an intermediate semantic representation. Under this model, we use object classifiers to associate low-level visual features to objects, and at the same time, we use contextual relations to associate objects to scenes. As a further contribution, we improve the performance of current state-of-the-art category-level object classifiers by including geometrical information obtained from a 3D range sensor that facilitates the implementation of a focus of attention mechanism within a Monte Carlo sampling scheme. We test our approach using real data, showing significant advantages with respect to previous state-of-the-art methods.",multimedia,535,not included
10.1109/fg47880.2020.00049,to_check,2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020),IEEE,2020-11-20 00:00:00,ieeexplore,reenactnet: real-time full head reenactment,https://ieeexplore.ieee.org/document/9320178/,"Video-to-video synthesis is a challenging problem aiming at learning a translation function between a sequence of semantic maps and a photo-realistic video depicting the characteristics of a driving video. We propose a head-to-head system of our own implementation capable of fully transferring the human head 3D pose, facial expressions and eye gaze from a source to a target actor, while preserving the identity of the target actor. Our system produces high-fidelity, temporally-smooth and photo-realistic synthetic videos faithfully transferring the human time-varying head attributes from the source to the target actor. Our proposed implementation: 1) works in real time (~20 fps), 2) runs on a commodity laptop with a webcam as the only input, 3) is interactive, allowing the participant to drive a target person, e.g. a celebrity, politician, etc, instantly by varying their expressions, head pose, and eye gaze, and visualising the synthesised video concurrently.",multimedia,536,not included
10.1109/wowmom51794.2021.00053,to_check,"2021 IEEE 22nd International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM)",IEEE,2021-06-11 00:00:00,ieeexplore,"similarity measures for location-dependent mmimo, 5g base stations on/off switching using radio environment map",https://ieeexplore.ieee.org/document/9469498/,"The Massive Multiple-Input Multiple-Output (MMIMO) technique together with Heterogeneous Network (Het-Net) deployment enables high throughput of 5G and beyond networks. However, a high number of antennas and a high number of Base Stations (BSs) can result in significant power consumption. Previous studies have shown that the energy efficiency (EE) of such a network can be effectively increased by turning off some BSs depending on User Equipments (UEs) positions. Such mapping is obtained by using Reinforcement Learning. Its results are stored in a so-called Radio Environment Map (REM). However, in a real network, the number of UEs' positions patterns would go to infinity. This paper aims to determine how to match the current set of UEs' positions to the most similar pattern, i.e., providing the same optimal active BSs set, saved in REM. We compare several state-of-the-art distance metrics using a computer simulator: an accurate 3D-Ray-Tracing model of the radio channel and an advanced system-level simulator of MMIMO Het-Net. The results have shown that the so-called Sum of Minimums Distance provides the best matching between REM data and UEs' positions, enabling up to 56% EE improvement over the scenario without EE optimization.",multimedia,537,not included
10.1109/icaie50891.2020.00083,to_check,2020 International Conference on Artificial Intelligence and Education (ICAIE),IEEE,2020-06-28 00:00:00,ieeexplore,virtualized circuit welding and simulation experiment system based on unity3d,https://ieeexplore.ieee.org/document/9262558/,"Circuit welding and simulation experiment is a compulsory course for electronic information students in college. To make students more intuitively learn the principles of circuits and exercise their practical ability, we use Unity3D and 3DMax software to develop an immersive 3D virtual experiment system, and give the system's overall structure design and some key technologies. The system serves as an auxiliary teaching and learning tool, simulating a real experimental environment, and providing necessary conditions for students* autonomous experiments.",multimedia,538,not included
10.1109/fg.2019.8756520,to_check,2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019),IEEE,2019-05-18 00:00:00,ieeexplore,when computers decode your social intention,https://ieeexplore.ieee.org/document/8756520/,"In this demo session, we will propose our framework that is based on our paper [1] . In real time, we proposed to analyze the trajectories of the human arm to predict social intention (personal or social intention). The trajectories of different 3D markers acquired by Mocap system are defined in shape spaces of open curves, thus analyze in a Riemannian manifold. The results obtained in the experiments on a new dataset show an average recognition of about 68% for the proposed method, which is comparable with the average score produced by human evaluation. The experimental results show also that the classification rate could be used to improve social communication between human and virtual agents. To the best of our knowledge, this is the first demo in real time, which uses computer vision techniques to analyze the effect of social intention on motor action for improving the social communication between human and avatar. The main goal is to categorize the user intention among two classes denote {personal, social}. This experimentation contains 3 parts: a) data acquisitions and a learning step; b) classification; c) Kinematic analysis of the evolution of subjects to interact with the avatar. To successfully drive our study, all the using scripts are writing under Matlab and C/C++. Then the using equipments are: 1) Qualisys motion capture camera (qualisys system). The qualisys system is delivered with a desk computer with 8 GB, a processor Intel core i7-4770k (8 CPUs) at 3.5 GHz. The frequency of those cameras can varies from 100 to 500 Hz. A black glove equipped with infrared reflective markers, all those equipments are also provided by qualisys system. 2) A Matlab software (version R2014a) installed on a desk computer (qualisys system); the Qualisys system provide a specific driver that allow to couple all the Matlab scripts with their system. Thus, it is possible to command all the cameras directly from Matlab for real time analysis, see Fig. 1 .",multimedia,539,not included
10.1109/tpami.2018.2863285,to_check,IEEE Transactions on Pattern Analysis and Machine Intelligence,IEEE,2019-08-01 00:00:00,ieeexplore,deep supervision with intermediate concepts,https://ieeexplore.ieee.org/document/8434117/,"Recent data-driven approaches to scene interpretation predominantly pose inference as an end-to-end black-box mapping, commonly performed by a Convolutional Neural Network (CNN). However, decades of work on perceptual organization in both human and machine vision suggest that there are often intermediate representations that are intrinsic to an inference task, and which provide essential structure to improve generalization. In this work, we explore an approach for injecting prior domain structure into neural network training by supervising hidden layers of a CNN with intermediate concepts that normally are not observed in practice. We formulate a probabilistic framework which formalizes these notions and predicts improved generalization via this deep supervision method. One advantage of this approach is that we are able to train only from synthetic CAD renderings of cluttered scenes, where concept values can be extracted, but apply the results to real images. Our implementation achieves the state-of-the-art performance of 2D/3D keypoint localization and image classification on real image benchmarks including KITTI, PASCALVOC, PASCAL3D+, IKEA, and CIFAR100. We provide additional evidence that our approach outperforms alternative forms of supervision, such as multi-task networks.",multimedia,540,not included
10.1109/tie.2012.2183833,to_check,IEEE Transactions on Industrial Electronics,IEEE,2013-01-01 00:00:00,ieeexplore,real-time nonlinear parameter estimation using the levenberg–marquardt algorithm on field programmable gate arrays,https://ieeexplore.ieee.org/document/6129410/,"The Levenberg-Marquardt (LM) algorithm is a nonlinear parameter learning algorithm that converges accurately and quickly. This paper demonstrates for the first time to our knowledge, a real-time implementation of the LM algorithm on field programmable gate arrays (FPGAs). It was used to train neural networks to solve the eXclusive Or function (XOR), and for 3D-to-2D camera calibration parameter estimation. A Xilinx Virtex-5 ML506 was used to implement the LMA as a hardware-in-the-loop system. The XOR function was approximated in only 13 iterations from zero initial conditions, usually the same function is approximated in thousands of iterations using the error backpropagation algorithm. Also, this type of training not only reduced the number of iterations but also achieved a speed up in excess of 3 ×10<sup>6</sup> when compared to the software implementation. A real-time camera calibration and parameter estimation was performed successfully on FPGAs. Compared to the software implementation the FPGA implementation led to an increase in the mean squared error and standard deviation by only 17.94% and 8.04% respectively. The FPGA increased the calibration speed by a factor of 1.41 × 10<sup>6</sup>. There are a wide range of systems problems solved via nonlinear parameter optimization, this study demonstrated that a hardware solution for systems such as automated optical inspection systems or systems dealing with projective geometry estimation and motion compensation systems in robotic vision systems is possible in real time.",multimedia,541,not included
10.1109/icfec51620.2021.00018,to_check,2021 IEEE 5th International Conference on Fog and Edge Computing (ICFEC),IEEE,2021-05-13 00:00:00,ieeexplore,a privacy preserving system for ai-assisted video analytics,https://ieeexplore.ieee.org/document/9458893/,"The emerging Edge computing paradigm facilitates the deployment of distributed AI-applications and hardware, capable of processing video data in real time. AI-assisted video analytics can provide valuable information and benefits for parties in various domains. Face recognition, object detection, or movement tracing are prominent examples enabled by this technology. However, the widespread deployment of such mechanism in public areas are a growing cause of privacy and security concerns. Data protection strategies need to be appropriately designed and correctly implemented in order to mitigate the associated risks. Most existing approaches focus on privacy and security related operations of the video stream itself or protecting its transmission. In this paper, we propose a privacy preserving system for AI-assisted video analytics, that extracts relevant information from video data and governs the secure access to that information. The system ensures that applications leveraging extracted data have no access to the video stream. An attribute-based authorization scheme allows applications to only query a predefined subset of extracted data. We demonstrate the feasibility of our approach by evaluating an application motivated by the recent COVID-19 pandemic, deployed on typical edge computing infrastructure.",multimedia,542,included
10.5594/m001243,to_check,International Workshop on HDTV '96,SMPTE,1996-10-09 00:00:00,ieeexplore,a programmable and scalable architecture for real time audio and video processing,https://ieeexplore.ieee.org/document/7263296/,"Current Real Time Audio Video Processing demands flexible architectures and programmability in order to better match the application. — According to these requirements a fully programmable system, which extensively uses DSP technology, is presented. It is based on a VME bus structure and implements a scalable architecture. — Through extendible modular layers the computational power can be progressively increased and easily managed to perform different tasks. These features are particularly suited for implementing algorithms which are specified for different levels of quality/complexity like in the present MPEG2 standard (from MP@ML to HP@HL) [3]. — The software programmability of the system also enable to further improve and refine the current algorithm implementation without any hardware change. — The system has been developed within the framework of the EUREKA 1187 – ADTT project to allow, in particular, real time experimentation of Audio Video Non Broadcast Applications like Electronic Movie and Video Communication for professional environments.",multimedia,543,not included
10.1109/cnna.2010.5430245,to_check,2010 12th International Workshop on Cellular Nanoscale Networks and their Applications (CNNA 2010),IEEE,2010-02-05 00:00:00,ieeexplore,a multi-fpga distributed embedded system for the emulation of multi-layer cnns in real time video applications,https://ieeexplore.ieee.org/document/5430245/,"This paper describes the design and the implementation of an embedded system based on multiple FPGAs that can be used to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more CNN layers. The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates.",multimedia,544,included
10.1109/icassp.1996.550796,to_check,"1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings",IEEE,1996-05-09 00:00:00,ieeexplore,a probabilistic decision-based neural network for locating deformable objects and its applications to surveillance system and video browsing,https://ieeexplore.ieee.org/document/550796/,"Detection of a (deformable) pattern or object is an important machine learning and computer vision problem. The task involves finding specific (but locally deformable) patterns in images, such as human faces and eyes/mouths. There are many important commercial applications. This paper presents a decision-based neural network for finding such patterns with specific applications to detecting human faces and locating eyes in the faces. The system built upon the proposal has been demonstrated to be applicable under reasonable variations of orientation and/or lighting, and with the possibility of eye glasses. This method has been shown to be very robust against a large variation of face features and eye shapes. The algorithm takes only 200 ms on a SUN Sparc20 workstation to find human faces in an image with 320/spl times/240 pixels. For a facial image with 320/spl times/240 pixels, the algorithm takes 500 ms to locate two eyes on a SUN Sparc20 workstation. Furthermore, the algorithm can be easily implemented via specialised hardware for real time performance. We have applied this technique to two applications (surveillance system, video browsing) and this paper provides experimental results. Although we have only shown its successful implementation on face detection and eye localization, the proposed technique is meant for more general applications of detection of any (locally deformable) object.",multimedia,545,not included
10.1109/cbms.2019.00041,to_check,2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS),IEEE,2019-06-07 00:00:00,ieeexplore,action recognition in real homes using low resolution depth video data,https://ieeexplore.ieee.org/document/8787393/,"We report work in progress from interdisciplinary research on Assisted Living Technology in smart homes for older adults with mild cognitive impairments or dementia. We present our field trial, the set-up for collecting and storing data from real homes, and preliminary results on action recognition using low resolution depth video cameras. The data have been collected from seven apartments with one resident each over a period of two weeks. We propose a pre-processing of the depth videos by applying an Infinite Response Filter (IIR) for extracting the movements in the frames prior to classification. In this work we classify four actions: TV interaction (turn it on/ off and switch over), standing up, sitting down, and no movement. Our first results indicate that using the IIR filter for movement information extraction improves accuracy and can be an efficient method for recognizing actions. Our current implementation uses a convolutional long short-term memory (ConvLSTM) neural network, and achieved an average peak accuracy of 86%.",multimedia,546,included
10.1109/icra40945.2020.9196582,to_check,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,adversarial skill networks: unsupervised robot skill learning from video,https://ieeexplore.ieee.org/document/9196582/,"Key challenges for the deployment of reinforcement learning (RL) agents in the real world are the discovery, representation and reuse of skills in the absence of a reward function. To this end, we propose a novel approach to learn a task-agnostic skill embedding space from unlabeled multi-view videos. Our method learns a general skill embedding independently from the task context by using an adversarial loss. We combine a metric learning loss, which utilizes temporal video coherence to learn a state representation, with an entropy-regularized adversarial skill-transfer loss. The metric learning loss learns a disentangled representation by attracting simultaneous viewpoints of the same observations and repelling visually similar frames from temporal neighbors. The adversarial skill-transfer loss enhances re-usability of learned skill embeddings over multiple task domains. We show that the learned embedding enables training of continuous control policies to solve novel tasks that require the interpolation of previously seen skills. Our extensive evaluation with both simulation and real world data demonstrates the effectiveness of our method in learning transferable skills from unlabeled interaction videos and composing them for new tasks. Code, pretrained models and dataset are available at http://robotskills.cs.uni-freiburg.de.",multimedia,547,not included
10.1109/nitc.2017.8285657,to_check,2017 National Information Technology Conference (NITC),IEEE,2017-09-15 00:00:00,ieeexplore,automatic video descriptor for human action recognition,https://ieeexplore.ieee.org/document/8285657/,"Assistive software such as screen readers are unable to describe images or videos for visually impaired people. Although recent research have found ways to describe an image automatically, describing the content of a video is still an ongoing issue. Visually impaired people find it difficult to understand video content without an indication of sound. The current solution of video description is only provided through digital television and for selected programs and movies. As an initiative to describe video content for visually impaired people, the solution acts as a video player which automatically understands the ongoing human action on screen, associates textual descriptions and narrates it to the blind user. The human actions in the video should be recognized in real time, hence fast, reliable feature extraction and classification methods must be adopted. A feature set is extracted for each frame and is obtained from the projection histograms of the foreground mask. The number of moving pixels for each row and column of the frame is used to identify the instant position of a person. Support Vector Machine (SVM) is used to classify extracted features of each frame. The final classification is given by analyzing frames in segments. The classified actions will be converted from text to speech.",multimedia,548,not included
10.1109/dsmp.2018.8478575,to_check,2018 IEEE Second International Conference on Data Stream Mining & Processing (DSMP),IEEE,2018-08-25 00:00:00,ieeexplore,development and implementation of human face alignment and tracking in video streams,https://ieeexplore.ieee.org/document/8478575/,"The paper presents a method that allows detection, alignment and tracking of a human face in a real time in video streams. To detect and to align face on an image a face shape regression approach is used. The developed method uses scanning window, a cascade of ensembles of regression and classification trees, and adaptive boosting. The same trees are used for classification whether the given window contains a face and for regression of a face shape. For face tracking a starting position for face search is taken from the found shape on the previous frame. Conducted analysis of the proposed method implementation gave good performance results but revealed shortcomings and directions for future work. Sensitivity of face detection is 78% and accuracy of face alignment is 95%. The implementation can track faces in real time with a speed of at least 20 frames per second.",multimedia,549,not included
10.1109/icmlc.2008.4620899,to_check,2008 International Conference on Machine Learning and Cybernetics,IEEE,2008-07-15 00:00:00,ieeexplore,gpu based video stylization,https://ieeexplore.ieee.org/document/4620899/,"In this paper, we present a GPU based video stylization framework that can artistically stylize video stream in real time. In this framework, firstly, we use a separable implementation of bilateral filter as an adaptive and iterative smoothing operation that selectively simplifies image color, leading to an abstracted look. Secondly, we perform a soft color quantization step on the abstracted video. A significant advantage of the soft color quantization implementation is preserving temporal coherence and reducing computation time as well. Successively, some optional approaches are designed to generate different artistic styles. We evaluate the effectiveness of our stylization framework with the experiment results.",multimedia,550,not included
10.1145/3326285.3329051,to_check,2019 IEEE/ACM 27th International Symposium on Quality of Service (IWQoS),IEEE,2019-06-25 00:00:00,ieeexplore,leap: learning-based smart edge with caching and prefetching for adaptive video streaming,https://ieeexplore.ieee.org/document/9068647/,"Dynamic Adaptive Streaming over HTTP (DASH) has emerged as a popular approach for video transmission, which brings a potential benefit for the Quality of Experience (QoE) because of its segment-based flexibility. However, the Internet can only provide no guaranteed delivery. The high dynamic of the available bandwidth may cause bitrate switching or video rebuffering, thus inevitably damaging the QoE. Besides, the frequently requested popular videos are transmitted for multiple times and contribute to most of the bandwidth consumption, which causes massive transmission redundancy. Therefore, we propose a Learning-based Edge with cAching and Prefetching (LEAP) to improve the online user QoE of adaptive video streaming. LEAP introduces caching into the edge to reduce the redundant video transmission and employs prefetching to fight against network jitters. Taking the state information of users into account, LEAP intelligently makes the most beneficial decisions of caching and prefetching by a QoE-oriented deep neural network model. To demonstrate the performance of our scheme, we deploy the implemented prototype of LEAP in both the simulated scenario and the real Internet. Compared with all selected schemes, LEAP at least raises average bitrate by 34.4% and reduces video rebuffering by 42.7%, which leads to at least 15.9% improvement in the user QoE in the simulated scenario. The results in the real Internet scenario further confirm the superiority of LEAP.",multimedia,551,included
10.1109/icnnsp.2008.4590383,to_check,2008 International Conference on Neural Networks and Signal Processing,IEEE,2008-06-11 00:00:00,ieeexplore,video object matching based on sift algorithm,https://ieeexplore.ieee.org/document/4590383/,"SIFT (scale invariant feature transform) is used to solve visual tracking problem, where the appearances of the tracked object and scene background change during tracking. The implementation of this algorithm has five major stages: scale-space extrema detection; keypoint localization; orientation assignment; keypoint descriptor; keypoint matching. From the beginning frame, object is selected as the template, its SIFT features are computed. Then in the following frames, the SIFT features are computed. Euclidean distance between the object's SIFT features and the frames' SIFT features can be used to compute the accurate position of the matched object. The experimental results on real video sequences demonstrate the effectiveness of this approach and show this algorithm is of higher robustness and real-time performance. It can solve the matching problem with translation, rotation and affine distortion between images. It plays an important role in video object tracking and video object retrieval.",multimedia,552,not included
10.1109/cnna.2002.1035038,to_check,Proceedings of the 2002 7th IEEE International Workshop on Cellular Neural Networks and Their Applications,IEEE,2002-07-24 00:00:00,ieeexplore,watermarking for the authentication of video on cnn-um,https://ieeexplore.ieee.org/document/1035038/,"Digital watermarks have been proposed for authentication of both video and still images. In such applications, the watermark is embedded within a host image such that subsequent alteration to the watermarked image can be detected with high probability. In this paper the possibility of implementing real time watermarking on a video stream is presented. In fact the new CNN-UM implementation offers time operation of only microseconds working on 64/spl times/64 images.",multimedia,553,not included
10.1109/tnsm.2019.2929511,to_check,IEEE Transactions on Network and Service Management,IEEE,2019-09-01 00:00:00,ieeexplore,itelescope: softwarized network middle-box for real-time video telemetry and classification,https://ieeexplore.ieee.org/document/8765778/,"Video continues to dominate network traffic, yet operators today have poor visibility into the number, duration, and resolutions of the video streams traversing their domain. Current monitoring approaches are inaccurate, expensive, or unscalable, as they rely on statistical sampling, middle-box hardware, or packet inspection software. We present iTelescope, the first intelligent, inexpensive, and scalable softwarized network middle-box solution for identifying and classifying video flows in realtime. Our solution is novel in combining dynamic flow rules with telemetry and machine learning, and is built on commodity OpenFlow switches and open-source software. We develop a fully functional system, train it in the lab using multiple machine learning algorithms, and validate its performance to show over 95% accuracy in identifying and classifying video streams from many providers, including YouTube and Netflix. Lastly, we conduct tests to demonstrate its scalability to tens of thousands of concurrent streams, and deploy it live on a campus network serving several hundred real users. Our traffic monitoring system gives unprecedented fine-grained real-time visibility of video streaming performance to operators of enterprise and carrier networks at very low cost.",multimedia,554,included
10.1109/icsmc.2003.1244605,to_check,"SMC'03 Conference Proceedings. 2003 IEEE International Conference on Systems, Man and Cybernetics. Conference Theme - System Security and Assurance (Cat. No.03CH37483)",IEEE,2003-10-08 00:00:00,ieeexplore,a novel approach for person authentication and content-based tracking in videos using kernel methods and active appearance models,https://ieeexplore.ieee.org/document/1244605/,"A novel integration of methods for person authentication and tracking is proposed for real time security systems. The implementation of the idea for this real time implementation follows a three step procedure-face detection, recognition and content-based tracking. Instead of analyzing continuous videos we sample the frame based on a method derived from Shannon's information theory model. The Face-detector detects multi-viewed faces in a video using feature-based kernel methods in a reduced feature space obtained using ICA. The identified ""face regions"" are then passed on to the face recognition system which is based on Active Appearance Models (AAM). Once the subject is recognized, it can be tracked in the video using kernel based object tracking method. Several space reduction techniques have been used like ICA, PCA and skin-color segmentation.",multimedia,555,not included
10.1109/igeht.2017.8094075,to_check,2017 International Conference on Innovations in Green Energy and Healthcare Technologies (IGEHT),IEEE,2017-03-18 00:00:00,ieeexplore,a study on cognitive social data fusion,https://ieeexplore.ieee.org/document/8094075/,"Traditional data mining usually deals with data from a single domain. In the big data era, we are facing a diversity of datasets from different sources in different domains. These datasets consist of multiple modalities, each of which has a different representation, distribution, scale and density. Big data have volumes in range Exabyte's ten to the power of eighteen. A large number of data's are stored in Big Data storage every second. For instance in YouTube for every second a video of size 72 hours are being uploaded. It shows that big data have a big scope in handling of large data. Big data for learning, intelligence, data fusion, social network, mining and so many plays a vital role in it. The big data technologies along with machine learning algorithm have developed lots of advanced development in the field of social mining, network and social Medias. It has also developed so many challenges in data storage, handling, representation, mining, analysing the user behaviours and so many. In Social mining along with text the symbols are also analysed for effective mining of users. This paper does not only introduce high-level principles of each category of methods, but also give examples in which these techniques are used to handle real big data problems. The data storage size can be optimized by using the map reduce algorithm in a effective way for data storage in big data. When the repeateddata's are replaced with its reference then the storage will be optimized. Thus this method implementation will leads to the effective data mining and the analysing of persons behaviour more effectively.",multimedia,556,not included
10.1109/icpr.2016.7900142,to_check,2016 23rd International Conference on Pattern Recognition (ICPR),IEEE,2016-12-08 00:00:00,ieeexplore,a temporally coherent neural algorithm for artistic style transfer,https://ieeexplore.ieee.org/document/7900142/,"Within the fields of visual effects and animation, humans have historically spent painstaking hours mastering the skill of drawing frame-by-frame animations. One such animation technique that has been widely used is called “rotoscoping” and has allowed uniquely stylized animations to capture the motion of real life action sequences. Automating this arduous process would free animators from performing frame by frame stylization to concentrate on artistic contributions. We introduce a new artificial system based on an existing neural style transfer method which creates artistically stylized animations that simultaneously reproduce both the motion of the original videos that they are derived from and the unique style of a given artistic work. This system utilizes a convolutional neural network framework to extract a hierarchy of image features used for generating images that appear visually similar to a given artistic style while at the same time faithfully preserving temporal content. The use of optical flow allows the combination of style and content to be integrated directly with the apparent motion over frames of a video to produce smooth and visually appealing transitions. This implementation demonstrates how biologically-inspired systems such as convolutional neural networks are rapidly approaching human-level behavior in tasks that were once thought impossible. Further, this research provides unique insights into the way that humans who produce artistically stylized animations perceive temporal information.",multimedia,557,not included
10.1109/iccas.2013.6703875,to_check,"2013 13th International Conference on Control, Automation and Systems (ICCAS 2013)",IEEE,2013-10-23 00:00:00,ieeexplore,active relearning for robust on-road vehicle detection and tracking,https://ieeexplore.ieee.org/document/6703875/,This paper aims to introduce a novel robust real time system capable of rapidly detecting and tracking vehicles in a video stream using a monocular vision system. The framework used for this purpose is an actively relearned implementation of the Haar-like feature based Viola-Jones classifier capable of classifying image frame regions as a vehicle or non-vehicle. A passively trained supervised system (based on Adaboost) is initially built by cascading a set of weak classifiers working with Rectangular Haar-like features. An actively learned model is then generated from the initial passive classifier by querying misclassified instances when the model is evaluated on an independent dataset. This classifier is integrated with a Lucas-Kanade Optical Flow Tracker and an empirical distance estimation algorithm to evolve the system into a complete real-time detection and tracking system. The built model is then evaluated extensively on static as well as real world data and results are presented.,multimedia,558,not included
10.1109/iria53009.2021.9588707,to_check,2021 International Symposium of Asian Control Association on Intelligent Robotics and Industrial Automation (IRIA),IEEE,2021-09-22 00:00:00,ieeexplore,automatic license plate recognition system using ssd,https://ieeexplore.ieee.org/document/9588707/,"Automatic License Plate Recognition (ALPR) is a very widely used system in applications such as parking management, theft detection, traffic control and management etc. Most of the existing ALPR systems fail to showcase acceptable performance on real time images/video scenes. This work proposes and demonstrates implementation of a deep learning-based approach to locate license plates of four wheeler vehicles thereby enabling optical character recognition (OCR) to recognize the characters and numbers on the located plates in real time. The proposed system is decomposed into three sub-blocks viz. Vehicle image/video acquisition, License plate localization and OCR. A simple setup using a reasonable resolution webcam has been designed to capture images/videos of vehicles at some entry point. We propose to utilize Single Shot Detector (SSD) based Mobilenet V1 architecture to localize the license plates. The hyper parameters of this architecture are selected with rigorous experimentation so as to avoid over-fitting. We have compared performance of two OCRs viz. Tesseract OCR, Easy OCR and found the superiority of Easy OCR since it utilizes deep learning approach for character recognition. NVIDIA Jetson Nano and Raspberry Pi 3B hardware platforms have been used to implement the entire system. The parameters of these three sub-blocks have been optimized to yield real time performance of ALPR with acceptable accuracy. The proposed and implemented system on Jetson Nano allows processing of videos for ALPR having accuracy more than 95%.",multimedia,559,included
10.1109/iros40897.2019.8967592,to_check,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),IEEE,2019-11-08 00:00:00,ieeexplore,can a robot become a movie director? learning artistic principles for aerial cinematography,https://ieeexplore.ieee.org/document/8967592/,"Aerial filming is constantly gaining importance due to the recent advances in drone technology. It invites many intriguing, unsolved problems at the intersection of aesthetical and scientific challenges. In this work, we propose a deep reinforcement learning agent which supervises motion planning of a filming drone by making desirable shot mode selections based on aesthetical values of video shots. Unlike most of the current state-of-the-art approaches that require explicit guidance by a human expert, our drone learns how to make favorable viewpoint selections by experience. We propose a learning scheme that exploits aesthetical features of retrospective shots in order to extract a desirable policy for better prospective shots. We train our agent in realistic AirSim simulations using both a hand-crafted reward function as well as reward from direct human input. We then deploy the same agent on a real DJI M210 drone in order to test the generalization capability of our approach to real world conditions. To evaluate the success of our approach in the end, we conduct a comprehensive user study in which participants rate the shot quality of our methods. Videos of the system in action can be seen at https://youtu.be/qmVw6mfyEmw.",multimedia,560,included
10.1109/cvpr.2019.00346,to_check,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),IEEE,2019-06-20 00:00:00,ieeexplore,densefusion: 6d object pose estimation by iterative dense fusion,https://ieeexplore.ieee.org/document/8953386/,"A key technical challenge in performing 6D object pose estimation from RGB-D image is to fully leverage the two complementary data sources. Prior works either extract information from the RGB image and depth separately or use costly post-processing steps, limiting their performances in highly cluttered scenes and real-time applications. In this work, we present DenseFusion, a generic framework for estimating 6D pose of a set of known objects from RGB-D images. DenseFusion is a heterogeneous architecture that processes the two data sources individually and uses a novel dense fusion network to extract pixel-wise dense feature embedding, from which the pose is estimated. Furthermore, we integrate an end-to-end iterative pose refinement procedure that further improves the pose estimation while achieving near real-time inference. Our experiments show that our method outperforms state-of-the-art approaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed method to a real robot to grasp and manipulate objects based on the estimated pose.",multimedia,561,included
10.1109/sai.2017.8252178,to_check,2017 Computing Conference,IEEE,2017-07-20 00:00:00,ieeexplore,gwvt: a gpu maritime vessel tracker based on the wisard weightless neural network,https://ieeexplore.ieee.org/document/8252178/,"Maritime surveillance systems increase the security of ports and ships. The video tracking is an important and challenging component of a surveillance system. Difficulties can arise due to weather conditions, target trajectory and appearance, occlusions, lighting conditions and noise. The tracker locates the vessel frame by frame in real time. This paper proposes the GPU WiSARD Vessel Tracker GWVT, a maritime vessel tracker that uses the WiSARD weightless neural network and implemented on a GPU. The GPU parallel processing feature allows the tracking algorithm to be executed very fast. CUDA easy the implementation of the parallel WiSARD tracker because the network discriminators fit well in the thread and block layout. The implementation of a WiSARD based vessel tracker on a GPU is innovative on literature. The GWVT realizes the vessel tracking at only 1 millisecond allowing other tracking techniques to be executed in parallel to rise the performance. Discounting the kernel function call time from GWVT average tracking time, GWVT becomes 13,95 faster than the CPU tracker version.",multimedia,562,not included
10.1109/cloudnet.2018.8549558,to_check,2018 IEEE 7th International Conference on Cloud Networking (CloudNet),IEEE,2018-10-24 00:00:00,ieeexplore,hi-clust: unsupervised analysis of cloud latency measurements through hierarchical clustering,https://ieeexplore.ieee.org/document/8549558/,"Latency is nowadays one of the most relevant network and service performance metrics reflecting end-user experience. With the wide adoption and deployment of delay-sensitive applications in the Cloud (e.g., gaming, interactive video conferencing, corporate services, etc.), monitoring and analysis of Cloud service latency is becoming increasingly relevant for Cloud service providers, tenants and even users. Traditional network monitoring approaches based on time-series analysis and thresholding are capable of raising alarms when anomalous events arise, but are not applicable to detect correlations among multiple monitored dimensions, necessary to provide an adequate interpretation of an anomaly. In this paper we present Hi-Clust, an unsupervised-based approach for analyzing and interpreting anomalies in multi-dimensional network data, through the application of hierarchical clustering techniques. While Hi-Clust is applicable to the analysis of different types of nested or hierarchically structured data, we particularly focus on the analysis of Cloud service latency, using active measurements collected from geographically distributed vantage points. We implement and benchmark multiple density-based clustering approaches for Hi-Clust over four weeks of real multidimensional Cloud service latency measurements. Using the most robust underlying clustering algorithm from the benchmark, we show how to automatically extract and interpret anomalous Cloud service behavior with Hi-Clust. In addition, we show the advantages of Hi-Clust over traditional threshold-based approaches for detecting and interpreting anomalous behavior, through practical examples over the collected measurements.",multimedia,563,not included
10.1109/qomex48832.2020.9123142,to_check,2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX),IEEE,2020-05-28 00:00:00,ieeexplore,how deep is your encoder: an analysis of features descriptors for an autoencoder-based audio-visual quality metric,https://ieeexplore.ieee.org/document/9123142/,The development of audio-visual quality assessment models poses a number of challenges in order to obtain accurate predictions. One of these challenges is the modelling of the complex interaction that audio and visual stimuli have and how this interaction is interpreted by human users. The No-Reference Audio-Visual Quality Metric Based on a Deep Autoencoder (NAViDAd) deals with this problem from a machine learning perspective. The metric receives two sets of audio and video features descriptors and produces a low-dimensional set of features used to predict the audio-visual quality. A basic implementation of NAViDAd was able to produce accurate predictions tested with a range of different audio-visual databases. The current work performs an ablation study on the base architecture of the metric. Several modules are removed or re-trained using different configurations to have a better understanding of the metric functionality. The results presented in this study provided important feedback that allows us to understand the real capacity of the metric's architecture and eventually develop a much better audio-visual quality metric.,multimedia,564,not included
10.1109/conielecomp.2014.6808580,to_check,"2014 International Conference on Electronics, Communications and Computers (CONIELECOMP)",IEEE,2014-02-28 00:00:00,ieeexplore,implementation of an embedded system on a ts7800 board for robot control,https://ieeexplore.ieee.org/document/6808580/,"Growing Functional Modules (GFM) learning based controllers need to be experimented on real robots. In 2009, looking to develop a flexible and generic embedded interface for such robots, we decided to use a TS-7800 single board computer (SBC) with a Debian Linux operating system. Despite the many advantages of this board, implementing the embedded system has been a complex task. This paper describes the implementation of protocols through the TS-7800 different ports (RS232, TCP/IP, USB, analog and digital pins) as well as the connection of external boards (TS-ADC24, TS-DIO64, SSC-32 and LCD display). This implementation was required to connect a large range of actuators, sensors and other peripherals. Furthermore, the architecture of the embedded system is exposed in detail, including topics such as the XML configuration file that specifies the peripherals connected to the SBC, the concept of virtual sensors, the implementation of parallelism and the embedded system interface launcher. Technical aspects such as the optimization of video capture and processing are detailed because their execution required specific compilers versions, EABI emulation and extra libraries (openCV libjpg and libpngand libv4l). The final embedded system was implemented in a humanoid robot and connected to the GFM controller in charge of developing its equilibrium subsystem.",multimedia,565,included
10.1109/cvpr.2019.00437,to_check,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),IEEE,2019-06-20 00:00:00,ieeexplore,learning to film from professional human motion videos,https://ieeexplore.ieee.org/document/8953663/,"We investigate the problem of 6 degrees of freedom (DOF) camera planning for filming professional human motion videos using a camera drone. Existing methods either plan motions for only a pan-tilt-zoom (PTZ) camera, or adopt ad-hoc solutions without carefully considering the impact of video contents and previous camera motions on the future camera motions. As a result, they can hardly achieve satisfactory results in our drone cinematography task. In this study, we propose a learning-based framework which incorporates the video contents and previous camera motions to predict the future camera motions that enable the capture of professional videos. Specifically, the inputs of our framework are video contents which are represented using subject-related feature based on 2D skeleton and scene-related features extracted from background RGB images, and camera motions which are represented using optical flows. The correlation between the inputs and output future camera motions are learned via a sequence-to-sequence convolutional long short-term memory (Seq2Seq ConvLSTM) network from a large set of video clips. We deploy our approach to a real drone cinematography system by first predicting the future camera motions, and then converting them to the drone's control commands via an odometer. Our experimental results on extensive datasets and showcases exhibit significant improvements in our approach over conventional baselines and our approach can successfully mimic the footage of a professional cameraman.",multimedia,566,included
10.1109/ictai.2011.89,to_check,2011 IEEE 23rd International Conference on Tools with Artificial Intelligence,IEEE,2011-11-09 00:00:00,ieeexplore,multi-agent simulation design driven by real observations and clustering techniques,https://ieeexplore.ieee.org/document/6103379/,"The multi-agent simulation consists in using a set of interacting agents to reproduce the dynamics and the evolution of the phenomena that we seek to simulate. It is considered now as an alternative to classical simulations based on analytical models. But, its implementation remains difficult, particularly in terms of behaviors extraction and agents modelling. This task is usually performed by the designer who has some expertise and available observation data on the process. In this paper, we propose a novel way to make use of the observations of real world agents to model simulated agents. The modelling is based on clustering techniques. Our approach is illustrated through an example in which the behaviors of agents are extracted as trajectories and destinations from video sequences analysis. This methodology is investigated with the aim to apply it, in particular, in a retail space simulation for the evaluation of marketing strategies. This paper presents experiments of our methodology in the context of a public area modelling.",multimedia,567,not included
10.1109/camp.2000.875968,to_check,Proceedings Fifth IEEE International Workshop on Computer Architectures for Machine Perception,IEEE,2000-09-13 00:00:00,ieeexplore,multi-sensors and environment simulator for collision avoidance applications,https://ieeexplore.ieee.org/document/875968/,"The ""PICAR"" project is based on ""Prometheus Prochip"" experience and results. The goal is to design an embedded multi sensors collision avoidance system for automotive application. The system includes sensors like video camera, ultrasonic sensors, a PC hardware computer, a CAN/sup 2/ network and a dedicated software for signal and image processing, data fusion and AI expert system. The design of such a system is difficult under real time constraints. Therefore, a simulator is a good solution to test different parts of the system and then to help to choose the overall architecture. However simulating all the embedded architecture in real-time becomes a very complex work. It is necessary to have an environment simulator where sensors can be virtually implemented. The data are then processed leading to results without hardware costs. The designed environment software allows the simulation of physical sensors, and also the emulation of these sensors. The simulator is interfaced to the physical embedded hardware by a network bridge. So, we can emulate some sensors to experiment data processing on the physical embedded PICAR computer. This paper presents the physical smart car PICAR and its embedded system, the virtual world simulator. We explain how we can mix the virtual world (produced by virtual sensors) and real world (physical embedded system) to implement some scenarios (like automatic parking) and to validate the physical architecture. An alternate goal can be to design customized sensors.",multimedia,568,not included
10.1109/ic4me247184.2019.9036531,to_check,"2019 International Conference on Computer, Communication, Chemical, Materials and Electronic Engineering (IC4ME2)",IEEE,2019-07-12 00:00:00,ieeexplore,object detection based security system using machine learning algorthim and raspberry pi,https://ieeexplore.ieee.org/document/9036531/,"Conventional security systems that use surveillance cameras to monitor the property lacks the ability to notify the security administrator in the event of trespassing. A security camera when used along with a digital video recorder (DVR) is only effective as a source to gather evidence unless the video feed is constantly being monitored by a dedicated personnel. This paper discusses the implementation of a cost effective, intelligent security system that overcomes drawbacks of conventional security cameras by utilizing a machine learning and Viola-Jones algorithm under image processing literature to identify trespassers and multiple object detection in real time. The paper presents the design and implementation details of the intelligent object detection based security system in two different computing environment, MATLAB and Python respectively using Raspberry Pi 3 B single board computer. The security system is capable of alerting the security administrator through email via internet while activating an alarm locally.",multimedia,569,included
10.1109/infcomw.2016.7562053,to_check,2016 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),IEEE,2016-04-14 00:00:00,ieeexplore,resource provisioning and profit maximization for transcoding in information centric networking,https://ieeexplore.ieee.org/document/7562053/,"Adaptive bitrate streaming (ABR) has been widely adopted to support video streaming services over heterogeneous devices and varying network conditions. With ABR, each video content is transcoded into multiple representations in different bitrates and resolutions. However, video transcoding is computing intensive, which requires the transcoding service providers to deploy a large number of servers for transcoding the video contents published by the content producers. As such, a natural question for the transcoding service provider is how to provision the computing resource for transcoding the video contents while maximizing service profit. To address this problem, we design a cloud video transcoding system by taking the advantage of cloud computing technology to elastically allocate computing resource. We propose a method for jointly considering the task scheduling and resource provisioning problem in two timescales, and formulate the service profit maximization as a two-timescale stochastic optimization problem. We derive some approximate policies for the task scheduling and resource provisioning. Based on our proposed methods, we implement our open source cloud video transcoding system Morph and evaluate its performance in a real environment. The experiment results demonstrate that our proposed method can reduce the resource consumption and achieve a higher profit compared with the baseline schemes.",multimedia,570,included
10.1109/infocom41043.2020.9155467,to_check,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications,IEEE,2020-07-09 00:00:00,ieeexplore,rldish: edge-assisted qoe optimization of http live streaming with reinforcement learning,https://ieeexplore.ieee.org/document/9155467/,"Recent years have seen a rapidly increasing traffic demand for HTTP-based high-quality live video streaming. The surging traffic demand, as well as the real-time property of live videos, make it challenging for content delivery networks (CDNs) to guarantee the Quality-of-Experiences (QoE) of viewers. The initial video segment (IVS) of live streaming plays an important role in the QoE of live viewers, particularly when users require fast join time and smooth view experience. State-of-the-art research on this regard estimates network throughput for each viewer and thus may incur a large overhead that offsets the benefit. To tackle the problem, we propose Rldish, a scheme deployed at the edge CDN server, to dynamically select a suitable IVS for new live viewers based on Reinforcement Learning (RL). Rldish is transparent to both the client and the streaming server. It collects the real-time QoE observations from the edge without any client-side assistance, then uses these QoE observations as real-time rewards in RL. We deploy Rldish as a virtualized network function (VNF) in a real HTTP cache server, and evaluate its performance using streaming servers distributed over the world. Our experiments show that Rldish improves the state- of-the-art IVS selection scheme w.r.t. the average QoE of live viewers by up to 22%.",multimedia,571,included
10.1109/ecai.2013.6636188,to_check,"Proceedings of the International Conference on ELECTRONICS, COMPUTERS and ARTIFICIAL INTELLIGENCE - ECAI-2013",IEEE,2013-06-29 00:00:00,ieeexplore,surveillance system using ip camera and face-detection algorithm,https://ieeexplore.ieee.org/document/6636188/,"This paper presents an efficient video surveillance system which consists of a network camera and an algorithm for automatically detection of the human faces in the monitoring area via real time video contents analysis. The main contribution of this research consists in a software application which is able to process the images received from the camera in order to detect human faces and trigger the process of saving the live stream as a video file. The algorithm for face-detection is based on the integral image (summed area table) - a model which allows the calculation of the sum of all pixels values from any rectangular area in the original image by doing only four operations of addition or subtraction. In addition, the application also includes a file containing data about needed tests in the detection algorithm by analyzing multiple images as templates. The application is written in C# language and experimental results are presented for images with different sizes and backgrounds.",multimedia,572,not included
10.1109/cnna.2002.1035109,to_check,Proceedings of the 2002 7th IEEE International Workshop on Cellular Neural Networks and Their Applications,IEEE,2002-07-24 00:00:00,ieeexplore,test-bed board for 16/spl times/64 stereo vision cnn chip,https://ieeexplore.ieee.org/document/1035109/,"The implementation of an artificial vision algorithm in real time is really attractive in such an application as the field of environment sensing. The SVCNN (stereo vision cellular neural network) chip is an analogue circuit able to compute in real time the Disparity Map from a couple of images by using a stereo visual system algorithm. A ""test-bed"" board for the 16/spl times/64 SVCNN chip is presented in this paper. This board is composed of an analogue processing core implemented by two 16/spl times/64 SVCNN chips together with a digital high performance pre-processing unit and a video grabbing section.",multimedia,573,not included
10.1109/pccc.2018.8710767,to_check,2018 IEEE 37th International Performance Computing and Communications Conference (IPCCC),IEEE,2018-11-19 00:00:00,ieeexplore,the frame latency of personalized livestreaming can be significantly slowed down by wifi,https://ieeexplore.ieee.org/document/8710767/,"The popular personalized livestreaming (PL) in China, arguably the largest PL market in the world, is more monetized than PL in US and hence demands much lower interactive latencies to ensure a good quality of user experience. However, our pilot experiment shows that the video frame latency, dominant component of PL's interactive latency, can be significantly slowed down by WiFi, the primary Internet access method for PL. Understanding and further improving the frame latency over WiFi, however, have difficulties in 1) measuring end-to-end latency; 2) parsing encrypted PL's traffic and 3) modeling complex relationships between WiFi radio factors and the latency. To tackle these challenges, we design and prototype Latency Doctor (LTDr), a practical system which aims to model and optimize PL's video frame latency over WiFi. We deploy LTDr in our campus and obtain several key observations based on 13.9M video frames extracted from 12K individual views on three leading PLs in China. We observe that 40% frame latencies over WiFi hop are more than 30ms, and channel utilization should be less than 64% for low latency. Then we build a predictive model based on the dataset using the machine learning methodologies. Two real cases show that the median frame latencies are decreased by LTDr from 130ms to 22ms, and 50ms to 12ms respectively over WiFi networks.",multimedia,574,not included
10.1109/icmlc.2010.5580745,to_check,2010 International Conference on Machine Learning and Cybernetics,IEEE,2010-07-14 00:00:00,ieeexplore,the study and implementation of real-time face recognition and tracking system,https://ieeexplore.ieee.org/document/5580745/,"During the past several years, face recognition in video has received significant attention. For the video monitoring and artificial vision, real time face recognition has very important meaning. The current method is still very susceptible to the illumination condition, non-real time and very common to fail to track the target face especially when partly covered or moving fast. In this paper, we propose to use AdaBoost Cascade for face detection and then in order to recognize the candidate faces, they will be analyzed by the hybrid Wavelet and LDA method. After that, Mean shift will be invoked to track the face. The implementation shows that the algorithm has quite good performance in terms of real-time and the tracking procedure is triggered accurately.",multimedia,575,not included
10.1109/icmlc.2009.5212738,to_check,2009 International Conference on Machine Learning and Cybernetics,IEEE,2009-07-15 00:00:00,ieeexplore,time series analysis during the releasing arrow stage,https://ieeexplore.ieee.org/document/5212738/,"In this paper, during the releasing arrow stage, the relationship between the time series trajectory and the archery performance has been studied. With the aid of the high sampling rate 1200 frames/second of video camera, the time series aiming trajectory during the releasing stage can be captured for analysis, especially the releasing string motion. The linear time invariant auto-regressive exogenous (ARX) process is adopted to model the intended aiming trajectory before the releasing arrow stage. Then based on the model the estimated trajectory during the releasing arrow stage is evaluated as the reference, so its difference between the real one is the main focus in the paper. According to these discrepancies some key parameters are defined first, and the implementation of correlation approach can result in some significant relationships which can characterize his particular releasing string motion during this stage. For example, his releasing arrow trajectory always has an upper linear trend along the vertical direction, and the vertical deviation on the target plays more important role than the radial deviation.",multimedia,576,not included
10.1109/cvpr.2005.348,to_check,2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05),IEEE,2005-06-25 00:00:00,ieeexplore,tracking multiple colored blobs with a moving camera,https://ieeexplore.ieee.org/document/1467577/,"This paper concerns a method for tracking multiple blobs exhibiting certain color distributions in images acquired by a possibly moving camera. The method encompasses a collection of techniques that enable modeling and detecting the blobs possessing the desired color distribution(s), as well as inferring their temporal association across image sequences. Appropriately colored blobs are detected with a Bayesian classifier, which is bootstrapped with a small set of training data. Then, an online iterative training procedure is employed to refine the classifier using additional training images. Online adaptation of color probabilities is used to enable the classifier to cope with illumination changes. Tracking over time is realized through a novel technique, which can handle multiple colored blobs. Such blobs may move in complex trajectories and occlude each other in the field of view of a possibly moving camera, while their number may vary over time. A prototype implementation of the developed system running on a conventional Pentium IV processor at 2.5 GHz operates on 320/spl times/240 live video in real time (30Hz). It is worth pointing out that currently, the cycle time of the tracker is determined by the maximum acquisition frame rate that is supported by our IEEE 1394 camera, rather than the latency introduced by the computational overhead for tracking blobs.",multimedia,577,not included
10.1109/tmscs.2015.2513741,to_check,IEEE Transactions on Multi-Scale Computing Systems,IEEE,2015-12-01 00:00:00,ieeexplore,"an ultra-low power, “always-on” camera front-end for posture detection in body worn cameras using restricted boltzman machines",https://ieeexplore.ieee.org/document/7368929/,"The Internet of Things (IoTs) has triggered rapid advances in sensors, surveillance devices, wearables and body area networks with advanced Human-Computer Interfaces (HCI). One such application area is the adoption of Body Worn Cameras (BWCs) by law enforcement officials. The need to be ‘always-on’ puts heavy constraints on battery usage in these camera front-ends, thus limiting their widespread adoption. Further, the increasing number of such cameras is expected to create a data deluge, which requires large processing, transmission and storage capabilities. Instead of continuously capturing and streaming or storing videos, it is prudent to provide “smartness” to the camera front-end. This requires hardware assisted image recognition and template matching in the front-end, capable of making judicious decisions on when to trigger video capture or streaming. Restricted Boltzmann Machines (RBMs) based neural networks have been shown to provide high accuracy for image recognition and are well suited for low power and re-configurable systems. In this paper we propose an RBM based “always-on’’ camera front-end capable of detecting human posture. Aggressive behavior of the human being in the field of view will be used as a wake-up signal for further data collection and classification. The proposed system has been implemented on a Xilinx Virtex 7 XC7VX485T platform. A minimum dynamic power of 19.18 mW for a target recognition accuracy while maintaining real time constraints has been measured. The hardware-software co-design illustrates the trade-offs in the design with respect to accuracy, resource utilization, processing time and power. The results demonstrate the possibility of a true “always-on” body-worn camera system in the IoT environment.",multimedia,578,included
10.1109/jiot.2019.2902141,to_check,IEEE Internet of Things Journal,IEEE,2019-06-01 00:00:00,ieeexplore,deep learning-based multiple object visual tracking on embedded system for iot and mobile edge computing applications,https://ieeexplore.ieee.org/document/8653851/,"Compute and memory demands of state-of-the-art deep learning methods are still a shortcoming that must be addressed to make them useful at Internet of Things (IoT) end-nodes. In particular, recent results depict a hopeful prospect for image processing using convolutional neural networks, CNNs, but the gap between software and hardware implementations is already considerable for IoT and mobile edge computing applications due to their high power consumption. This proposal performs low-power and real time deep learning-based multiple object visual tracking implemented on an NVIDIA Jetson TX2 development kit. It includes a camera and wireless connection capability and it is battery powered for mobile and outdoor applications. A collection of representative sequences captured with the on-board camera, dETRUSC video dataset, is used to exemplify the performance of the proposed algorithm and to facilitate benchmarking. The results in terms of power consumption and frame rate demonstrate the feasibility of deep learning algorithms on embedded platforms although more effort in the joint algorithm and hardware design of CNNs is needed.",multimedia,579,not included
10.1109/tnn.2006.872253,to_check,IEEE Transactions on Neural Networks,IEEE,2006-05-01 00:00:00,ieeexplore,on algorithmic rate-coded aer generation,https://ieeexplore.ieee.org/document/1629098/,"This paper addresses the problem of converting a conventional video stream based on sequences of frames into the spike event-based representation known as the address-event-representation (AER). In this paper we concentrate on rate-coded AER. The problem is addressed as an algorithmic problem, in which different methods are proposed, implemented and tested through software algorithms. The proposed algorithms are comparatively evaluated according to different criteria. Emphasis is put on the potential of such algorithms for a) doing the frame-based to event-based representation in real time, and b) that the resulting event streams resemble as much as possible those generated naturally by rate-coded address-event VLSI chips, such as silicon AER retinae. It is found that simple and straightforward algorithms tend to have high potential for real time but produce event distributions that differ considerably from those obtained in AER VLSI chips. On the other hand, sophisticated algorithms that yield better event distributions are not efficient for real time operations. The methods based on linear-feedback-shift-register (LFSR) pseudorandom number generation is a good compromise, which is feasible for real time and yield reasonably well distributed events in time. Our software experiments, on a 1.6-GHz Pentium IV, show that at 50% AER bus load the proposed algorithms require between 0.011 and 1.14 ms per 8 bit-pixel per frame. One of the proposed LFSR methods is implemented in real time hardware using a prototyping board that includes a VirtexE 300 FPGA. The demonstration hardware is capable of transforming frames of 64times64 pixels of 8-bit depth at a frame rate of 25 frames per second, producing spike events at a peak rate of 10<sup>7</sup>events per second",multimedia,580,not included
10.1109/tnsm.2021.3085097,to_check,IEEE Transactions on Network and Service Management,IEEE,2021-09-01 00:00:00,ieeexplore,seta++: real-time scalable encrypted traffic analytics in multi-gbps networks,https://ieeexplore.ieee.org/document/9444314/,"The security and privacy of the end-users are a few of the most important components of a communication network. Though end-to-end encryption (e.g., TLS/SSL) fulfils this requirement, it makes inspecting network traffic with legacy solutions such as Deep Packet Inspection difficult. Recent Machine Learning techniques have shown outstanding performance in encrypted traffic classification. Nevertheless, such approaches require efficient flow sampling at real enterprise-scale networks due to the sheer volume of transferred data. Through this paper, we propose a holistic architecture to extract flow information of encrypted data at multi Gbps line rate using sampling and sketching mechanisms, enabling network operators to estimate flow size distribution accurately and understand the behavior of VPN-obfuscated traffic. Using over 6000 video traffic traces, under three main evaluation scenarios based on trace duration and starting time point, we show that it is possible to achieve 99% accuracy for service provider classification and over 90% accuracy for content classification for a given service provider in the best case. We also deploy our solution at an operational enterprise-scale network leveraging kernel bypassing to demonstrate its capability to efficiently sample live traffic for analytics.",multimedia,581,included
10.1109/cerma.2007.4367696,to_check,"Electronics, Robotics and Automotive Mechanics Conference (CERMA 2007)",IEEE,2007-09-28 00:00:00,ieeexplore,image recognition processor based on morphological associative memories,https://ieeexplore.ieee.org/document/4367696/,"In this paper, the description of an image recognition processor based on morphological associative memories (MAM) is presented. The combination of the MAM features with its implementation in a programmable logic device grants to the system high speeds of processing, high immunity to the present noise in the images and great capacity of storage; these features make of our proposal a robust system and ideal to work in applications with autonomous real time systems. The described processor, with aid of a binary process of the image, realizing the filter functions, and the use of morphological heteroassociative memories min demonstrated a high performance in the recognition of images corrupted with diverse types of noises.",multimedia,582,not included
10.1109/access.2021.3051625,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,development of effective methods for structural image recognition using the principles of data granulation and apparatus of fuzzy logic,https://ieeexplore.ieee.org/document/9324755/,"The processes of intelligent data processing in computer vision systems have been researched. The problem of structural image recognition is relevant. This is a promising way to assess the degree of similarity of objects. This approach provides the simplicity of construction and the high reliability of decision making. The main problem of an effective description of characteristic features is the distortion of fragments of analyzed objects. The reasons for changing the input data can be the actions of geometric transformations, the influence of background or interference. The elements of false objects with similar characteristics are formed. The problem of ensuring high-quality recognition requires the implementation of effective means of image processing. Methods of statistical modeling, granulation of data and fuzzy sets, detection and comparison of keypoints on the image, classification and clustering of data, and simulation modelling are used in this research. The implementation of the proposed approaches provides the formation of a concise description of features or a vector representation of unique keypoints. The verification of theoretical foundations and evaluation of the effectiveness of the proposed data processing methods for real image bases is performed using the OpenCV library. The applied significance of the work is substantiated according to the criterion of data processing time without reducing the characteristics of reliability and interference immunity. The developed methods allow to increase the structural recognition of images by several times. Perspectives of research may involve identifying the optimal number of keypoints of the base set.",multimedia,583,not included
10.1109/iceeccot46775.2019.9114716,to_check,"2019 4th International Conference on Electrical, Electronics, Communication, Computer Technologies and Optimization Techniques (ICEECCOT)",IEEE,2019-12-14 00:00:00,ieeexplore,facial recognition using machine learning algorithms on raspberry pi,https://ieeexplore.ieee.org/document/9114716/,"Facial recognition is a non-invasive method of biometric authentication and useful for numerous applications. The real time implementation of the algorithm with adequate accuracy is required, with hardware timing into consideration. This paper deals with the implementation of machine learning algorithm for real time facial image recognition. Two dominant methods out of many facial recognition methods are discussed, simulated and implemented using Raspberry Pi. A rigorous comparative analysis is presented considering various limitations which may be the case required for innumerable application which utilize facial recognition. The drawbacks and different use cases of each method is highlighted. The facial recognition software uses algorithms to compare a digital image captured through a camera, to the stored face print so as to authenticate a person's identity. The Haar-Cascade method was one of the first methods developed for facial recognition. The HOG (Histogram of Oriented Gradients) method has worked very effectively for object recognition and thus suitable for facial recognition also. Both the methods are compared with Eigen feature-based face recognition algorithm. Various important features are experimented like speed of operation, lighting condition, frontal face profile, side profiles, distance of image, size of image etc. The facial recognition model is implemented to detect and recognize faces in real-time by means of Raspberry Pi and Pi camera for the user defined database in addition to the available databases.",multimedia,584,included
10.1109/itsc.2019.8917249,to_check,2019 IEEE Intelligent Transportation Systems Conference (ITSC),IEEE,2019-10-30 00:00:00,ieeexplore,trade-off analysis using synthetic training data for neural networks in the automotive development process,https://ieeexplore.ieee.org/document/8917249/,"Applications in the field of Deep Learning are constantly increasing the need for extensive, annotated data sets. Simulation software offers the possibility to create data sets of almost any size and thus the potential to cover this need. In the context of autonomous driving the test effort in the development processes increases to an extent that the use of virtual driving environments comes into focus. Deep Learning offers the opportunity to learn advanced driving strategies, starting from image recognition up to trajectory implementation.In this paper different driving simulation environments are used as data sources for the training of a neural network and examined for applicability on the basis of the working example of vehicle detection in image data.The visualizations include CarMaker, Carla and Grand Theft Auto V with different procedures for exporting ground truth information. For object detection, a pre-trained Convolutional Neural Network (CNN) is used and the effects on the detector quality of the data source and data size are investigated.An important part of the task of generating suitable data is the annotation process. The quality of the generated data is a decisive factor in achieving high performance training results. The test executions show that virtual data sources in training achieve lower detection rates in contrast to real data, but are more cost-effective in the overall context by scales.",multimedia,585,not included
10.1109/jssc.2016.2617317,to_check,IEEE Journal of Solid-State Circuits,IEEE,2017-01-01 00:00:00,ieeexplore,a 502-gops and 0.984-mw dual-mode intelligent adas soc with real-time semiglobal matching and intention prediction for smart automotive black box system,https://ieeexplore.ieee.org/document/7744546/,"The advanced driver assistance system (ADAS) for adaptive cruise control and collision avoidance is strongly dependent upon the robust image recognition technology such as lane detection, vehicle/pedestrian detection, and traffic sign recognition. However, the conventional ADAS cannot realize more advanced collision evasion in real environments due to the absence of intelligent vehicle/pedestrian behavior analysis. Moreover, accurate distance estimation is essential in ADAS applications and semiglobal matching (SGM) is most widely adopted for high accuracy, but its system-on-chip (SoC) implementation is difficult due to the massive external memory bandwidth. In this paper, an ADAS SoC with behavior analysis with Artificial Intelligence functions and hardware implementation of SGM is proposed. The proposed SoC has dual-mode operations of high-performance operation for intelligent ADAS with real-time SGM in D-Mode (d-mode) and ultralow-power operation for black box system in parking-mode. It features: 1) task-level pipelined SGM processor to reduce external memory bandwidth by 85.8%; 2) region-of-interest generation processor to reduce 86.2% of computation; 3) mixed-mode intention prediction engine for dual-mode intelligence; and 4) dynamic voltage and frequency scaling control to save 36.2% of power in d-mode. The proposed ADAS processor achieves 862 GOPS/W energy efficiency and 31.4GOPS/mm<sup>2</sup> area efficiency, which are 1.53× and 1.75× improvements than the state of the art, with 30 frames/s throughput under 720p stereo inputs.",multimedia,586,not included
10.1109/tcyb.2019.2912205,to_check,IEEE Transactions on Cybernetics,IEEE,2021-04-01 00:00:00,ieeexplore,memristive quantized neural networks: a novel approach to accelerate deep learning on-chip,https://ieeexplore.ieee.org/document/8705375/,"Existing deep neural networks (DNNs) are computationally expensive and memory intensive, which hinder their further deployment in novel nanoscale devices and applications with lower memory resources or strict latency requirements. In this paper, a novel approach to accelerate on-chip learning systems using memristive quantized neural networks (M-QNNs) is presented. A real problem of multilevel memristive synaptic weights due to device-to-device (D2D) and cycle-to-cycle (C2C) variations is considered. Different levels of Gaussian noise are added to the memristive model during each adjustment. Another method of using memristors with binary states to build M-QNNs is presented, which suffers from fewer D2D and C2C variations compared with using multilevel memristors. Furthermore, methods of solving the sneak path issues in the memristive crossbar arrays are proposed. The M-QNN approach is evaluated on two image classification datasets, that is, ten-digit number and handwritten images of mixed National Institute of Standards and Technology (MNIST). In addition, input images with different levels of zero-mean Gaussian noise are tested to verify the robustness of the proposed method. Another highlight of the proposed method is that it can significantly reduce computational time and memory during the process of image recognition.",multimedia,587,not included
10.1109/icra48506.2021.9561941,to_check,2021 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2021-06-05 00:00:00,ieeexplore,a robot walks into a bar: automatic robot joke success assessment,https://ieeexplore.ieee.org/document/9561941/,"Effective social robots should leverage humor’s unique ability to improve relationship connections and dispel stress, but current robots possess limited (if any) humorous abilities. In this paper, we aim to supplement one aspect of autonomous robots by giving robotic systems the ability to ""read the room"" to assess how their humorous statements are received by nearby people in real time. Using a dataset of the audio of crowd responses to a robotic comedian over multiple performances (first presented in past work), we establish human-labeled joke success ground truths and compare individual human rater accuracy against the outputs of lightweight Machine Learning (ML) approaches that are easy to deploy in real-time joke assessment. Our results indicate that all three ML approaches (naïve Bayes, support vector machines, and single-hidden-layer feedforward neural networks) performed significantly better than the baseline approach used in our past work. In particular, support vector machines and neural network approaches are comparable to a human rater in the task of assessing if a joke failed or not in certain cases. The products of this work will inform self-assessment techniques for robots and help social robotics researchers test their own assessment methods on realistic data from human crowds.",multimedia,588,not included
10.1109/ijcnn.2008.4634309,to_check,2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),IEEE,2008-06-08 00:00:00,ieeexplore,a new intelligent digital right management technique for e-learning content,https://ieeexplore.ieee.org/document/4634309/,"The digitalization of e-learning sources makes it an easy target for frauds, conterfeiting and content stealing. In this paper we present a new technique to deal with the security problems of e-learning content, its authentication and digital right management. The proposed technique is done by inserting a digital logo image, which serves as watermark signals, in the audio stream of e-learning material. This technique is based on modulated complex lapped transform that was selected for its audio reconstruction properties and the extraction of the watermark is performed using an independent component analysis algorithm. To demonstrate the effectiveness of the proposed method, a real world implementation has been done and the algorithm shows quite good visual and audible quality in watermarked content, as well as a high robustness against common signal processing attacks.",multimedia,589,not included
10.1109/icassp.2018.8462273,to_check,"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",IEEE,2018-04-20 00:00:00,ieeexplore,improving accuracy of nonparametric transfer learning via vector segmentation,https://ieeexplore.ieee.org/document/8462273/,"Transfer learning using deep neural networks as feature extractors has become increasingly popular over the past few years. It allows to obtain state-of-the-art accuracy on datasets too small to train a deep neural network on its own, and it provides cutting edge descriptors that, combined with nonparametric learning methods, allow rapid and flexible deployment of performing solutions in computationally restricted settings. In this paper, we are interested in showing that the features extracted using deep neural networks have specific properties which can be used to improve accuracy of downstream nonparametric learning methods. Namely, we demonstrate that for some distributions where information is embedded in a few coordinates, segmenting feature vectors can lead to better accuracy. We show how this model can be applied to real datasets by performing experiments using three mainstream deep neural network feature extractors and four databases, in vision and audio.",multimedia,590,not included
10.1109/dese51703.2020.9450744,to_check,2020 13th International Conference on Developments in eSystems Engineering (DeSE),IEEE,2020-12-17 00:00:00,ieeexplore,machine vision intelligent travel aid for the visually impaired (itavi) in developing countries<sup>*</sup>,https://ieeexplore.ieee.org/document/9450744/,"The visually impaired have little or no effective visual sensory input and have to rely on external assistance for navigation. Several electronic travel aids have been developed to aid independent navigation of the visually impaired, however they are not without limitations and dependence on third-parties. This paper describes the design and implementation of an Intelligent Travel Aid for the Visually Impaired, it combines the detection and recognition of objects in real-time with audio feedback to provide aid to the visually impaired users. This assistive device uses machine vision for object recognition detection, a camera for capturing object images respectively and a speaker all collectively form the core of the system. The system notifies users of obstacles and objects via synthesized speech. Using a quantized MobileNet based Single Shot multibox object detection model pre-trained on the Common Objects in Context dataset, the device was able to detect objects/obstacles, as well as determine the relative position and approximate distance. The device, when tested, was found to achieve real time performance of up to 70.56 frames per second for detections. Audio feedback was also achieved using the eSpeak Text to Speech engine to provide real time voice instructions to the user. All algorithms were implemented using Python language. The device is user friendly, allowing the visually impaired to enjoy easier navigation. However, other features such as extra object classes as well as language variety could be added in order to boost the robustness of the device.",multimedia,591,included
10.1109/iseee48094.2019.9136152,to_check,2019 6th International Symposium on Electrical and Electronics Engineering (ISEEE),IEEE,2019-10-20 00:00:00,ieeexplore,compact isolated speech recognition on raspberry-pi based on reaction diffusion transform,https://ieeexplore.ieee.org/document/9136152/,"A low complexity solution for speech recognition is proposed and its implementation on a resources constrained platform, namely the Raspberry-Pi is evaluated. In order to achieve good performance with limited resources, both the feature extractor and the classifier are specially designed. A special form of feature extractor, called RDT (reaction-diffusion transform) was optimized and evaluated in conjunction with a specially designed ELM (extreme learning machine) classifier. Optimization of parameters led to very good recognition rates (up to 100%) for a small dictionary of isolated sounds representing vocal commands for automotive applications. The real time factor is sub-unitary, ensuring the realization of real time identification using the proposed method.",multimedia,592,included
10.1109/ijcnn.1999.835990,to_check,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),IEEE,1999-07-16 00:00:00,ieeexplore,fuzzy speech recognition,https://ieeexplore.ieee.org/document/835990/,"Speech recognition is a major topic in speech signal processing. Many algorithms based on results of speech analysis, among which dynamic time warping) and hidden Markov models are the most important, have been advanced. However, these algorithms generally turn out to be too complicated to be implemented in real time systems. The proposed algorithm in this paper, which uses a fuzzy logic recognition approach based on the power distribution pattern of a segment of a speech, allows the implementation of real-time speech recognition.",multimedia,593,not included
10.1109/icm48031.2019.9021904,to_check,2019 31st International Conference on Microelectronics (ICM),IEEE,2019-12-18 00:00:00,ieeexplore,low power cnn hardware fpga implementation,https://ieeexplore.ieee.org/document/9021904/,"A convolution Neural Networks (CNN) goes under the wide umbrella of Deep Neural Networks (DNN) whose applications are widely used. For example, the later are used in robotics and different applications of recognition like speech recognition and facial recognition, also nowadays in autonomous cars. Therefore the aim of implementing the CNN is to be used in real time applications. As a result of that, Graphics processing units (GPUs) are used but their worst disadvantage is it's high power consumption which can't be used in daily used equipments. The target of this paper is to solve the power consumption problem by using Field Programmable Array (FPGA) which has low power consumption, and flexible architecture. The implementation architecture of Alex Network, which consists of three fully connected layers and five convolution layers, on FPGA will depend on two main techniques parallelism of resources, and pipelining inside of some layers.",multimedia,594,not included
10.1109/picst47496.2019.9061308,to_check,"2019 IEEE International Scientific-Practical Conference Problems of Infocommunications, Science and Technology (PIC S&T)",IEEE,2019-10-11 00:00:00,ieeexplore,speech to mind map conversion in infocommunication systems,https://ieeexplore.ieee.org/document/9061308/,"The task of speech recognition and analysis is one of the most important in the modern infocommunication systems. Currently, a wide range of people doing business uses such systems. For rational use of time in carrying out planning or responsible negotiations, many modern companies use mind maps. Mind maps are a display of an effective way to think, remember, solve creative tasks and an opportunity to present and visually express the internal processes of information processing. However, the process of mind maps creating is not sufficiently automated; it requires special software and some time. Thus, the creation of memory cards requires a certain amount of human resources and in the case of an active discussion or speech, the actual memory card will be somewhat behind in time. Implementation of the infocommunication system proposed in the article will allow automating the process of speech to mind map conversion. Besides the fact that such a process proceeds in real time, the determination of the weight of separate nodes can be performed without human intervention, which will increase the objectivity of the result.",multimedia,595,not included
10.1109/tcsi.2018.2852260,to_check,IEEE Transactions on Circuits and Systems I: Regular Papers,IEEE,2018-11-01 00:00:00,ieeexplore,real-time embedded machine learning for tensorial tactile data processing,https://ieeexplore.ieee.org/document/8439040/,"Machine learning (ML) has increasingly been recently employed to provide solutions for difficult tasks, such as image and speech recognition, and tactile data processing achieving a near human decision accuracy. However, the efficient hardware implementation of ML algorithms in particular for real time applications is still a challenge. This paper presents the hardware architectures and implementation of a real time ML method based on tensorial kernel approach dealing with multidimensional input tensors. Two different hardware architectures are proposed and assessed. Results demonstrate the feasibility of the proposed implementations for real time classification. The proposed parallel architecture achieves a peak performance of 302 G-ops while consuming 1.14 W for the Virtex-7 XC7VX980T FPGA device overcoming state of the art solutions.",multimedia,596,not included
10.1109/pdcat.2011.40,to_check,"2011 12th International Conference on Parallel and Distributed Computing, Applications and Technologies",IEEE,2011-10-22 00:00:00,ieeexplore,fast estimation of gaussian mixture model parameters on gpu using cuda,https://ieeexplore.ieee.org/document/6118944/,"Gaussian Mixture Models (GMMs) are widely used among scientists e.g. in statistics toolkits and data mining procedures. In order to estimate parameters of a GMM the Maximum Likelihood (ML) training is often utilized, more precisely the Expectation-Maximization (EM) algorithm. Nowadays, a lot of tasks works with huge datasets, what makes the estimation process time consuming (mainly for complex mixture models containing hundreds of components). The paper presents an efficient and robust implementation of the estimation of GMM statistics used in the EM algorithm on GPU using NVIDIA's Compute Unified Device Architecture (CUDA). Also an augmentation of the standard CPU version is proposed utilizing SSE instructions. Time consumptions of presented methods are tested on a large dataset of real speech data from the NIST Speaker Recognition Evaluation (SRE) 2008. Estimation on GPU proves to be more than 400 times faster than the standard CPU version and 130 times faster than the SSE version, thus a huge speed up was achieved without any approximations made in the estimation formulas. Proposed implementation was also compared to other implementations developed by other departments over the world and proved to be the fastest (at least 5 times faster than the best implementation published recently).",multimedia,597,not included
10.1109/waspaa.2013.6701825,to_check,2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics,IEEE,2013-10-23 00:00:00,ieeexplore,spectral feature-based nonlinear residual echo suppression,https://ieeexplore.ieee.org/document/6701825/,"We propose a method for nonlinear residual echo suppression that consists of extracting spectral features from the far-end signal, and using an artificial neural network to model the residual echo magnitude spectrum from these features. We compare the modeling accuracy achieved by realizations with different features and network topologies, evaluating the mean squared error of the estimated residual echo magnitude spectrum. We also present a low complexity real-time implementation combining an offline-trained network with online adaptation, and investigate its performance in terms of echo suppression and speech distortion for real mobile phone recordings.",multimedia,598,not included
http://arxiv.org/abs/2110.04697v1,to_check,arxiv,arxiv,2021-10-10 00:00:00,arxiv,"an augmented reality platform for introducing reinforcement learning to
  k-12 students with robots",http://arxiv.org/abs/2110.04697v1,"Interactive reinforcement learning, where humans actively assist during an
agent's learning process, has the promise to alleviate the sample complexity
challenges of practical algorithms. However, the inner workings and state of
the robot are typically hidden from the teacher when humans provide feedback.
To create a common ground between the human and the learning robot, in this
paper, we propose an Augmented Reality (AR) system that reveals the hidden
state of the learning to the human users. This paper describes our system's
design and implementation and concludes with a discussion on two directions for
future work which we are pursuing: 1) use of our system in AI education
activities at the K-12 level; and 2) development of a framework for an AR-based
human-in-the-loop reinforcement learning, where the human teacher can see
sensory and cognitive representations of the robot overlaid in the real world.",multimedia,599,not included
http://arxiv.org/abs/2006.01804v2,to_check,arxiv,arxiv,2020-06-02 00:00:00,arxiv,"practical sensorless aberration estimation for 3d microscopy with deep
  learning",http://arxiv.org/abs/2006.01804v2,"Estimation of optical aberrations from volumetric intensity images is a key
step in sensorless adaptive optics for 3D microscopy. Recent approaches based
on deep learning promise accurate results at fast processing speeds. However,
collecting ground truth microscopy data for training the network is typically
very difficult or even impossible thereby limiting this approach in practice.
Here, we demonstrate that neural networks trained only on simulated data yield
accurate predictions for real experimental images. We validate our approach on
simulated and experimental datasets acquired with two different microscopy
modalities, and also compare the results to non-learned methods. Additionally,
we study the predictability of individual aberrations with respect to their
data requirements and find that the symmetry of the wavefront plays a crucial
role. Finally, we make our implementation freely available as open source
software in Python.",multimedia,600,not included
http://arxiv.org/abs/1709.04909v1,to_check,arxiv,arxiv,2017-09-14 00:00:00,arxiv,shared learning : enhancing reinforcement in $q$-ensembles,http://arxiv.org/abs/1709.04909v1,"Deep Reinforcement Learning has been able to achieve amazing successes in a
variety of domains from video games to continuous control by trying to maximize
the cumulative reward. However, most of these successes rely on algorithms that
require a large amount of data to train in order to obtain results on par with
human-level performance. This is not feasible if we are to deploy these systems
on real world tasks and hence there has been an increased thrust in exploring
data efficient algorithms. To this end, we propose the Shared Learning
framework aimed at making $Q$-ensemble algorithms data-efficient. For achieving
this, we look into some principles of transfer learning which aim to study the
benefits of information exchange across tasks in reinforcement learning and
adapt transfer to learning our value function estimates in a novel manner. In
this paper, we consider the special case of transfer between the value function
estimates in the $Q$-ensemble architecture of BootstrappedDQN. We further
empirically demonstrate how our proposed framework can help in speeding up the
learning process in $Q$-ensembles with minimum computational overhead on a
suite of Atari 2600 Games.",multimedia,601,not included
http://arxiv.org/abs/2011.03630v1,to_check,arxiv,arxiv,2020-11-06 00:00:00,arxiv,"unmasking communication partners: a low-cost ai solution for digitally
  removing head-mounted displays in vr-based telepresence",http://arxiv.org/abs/2011.03630v1,"Face-to-face conversation in Virtual Reality (VR) is a challenge when
participants wear head-mounted displays (HMD). A significant portion of a
participant's face is hidden and facial expressions are difficult to perceive.
Past research has shown that high-fidelity face reconstruction with personal
avatars in VR is possible under laboratory conditions with high-cost hardware.
In this paper, we propose one of the first low-cost systems for this task which
uses only open source, free software and affordable hardware. Our approach is
to track the user's face underneath the HMD utilizing a Convolutional Neural
Network (CNN) and generate corresponding expressions with Generative
Adversarial Networks (GAN) for producing RGBD images of the person's face. We
use commodity hardware with low-cost extensions such as 3D-printed mounts and
miniature cameras. Our approach learns end-to-end without manual intervention,
runs in real time, and can be trained and executed on an ordinary gaming
computer. We report evaluation results showing that our low-cost system does
not achieve the same fidelity of research prototypes using high-end hardware
and closed source software, but it is capable of creating individual facial
avatars with person-specific characteristics in movements and expressions.",multimedia,602,not included
http://arxiv.org/abs/2103.07220v1,to_check,arxiv,arxiv,2021-03-12 00:00:00,arxiv,real-time timbre transfer and sound synthesis using ddsp,http://arxiv.org/abs/2103.07220v1,"Neural audio synthesis is an actively researched topic, having yielded a wide
range of techniques that leverages machine learning architectures. Google
Magenta elaborated a novel approach called Differential Digital Signal
Processing (DDSP) that incorporates deep neural networks with preconditioned
digital signal processing techniques, reaching state-of-the-art results
especially in timbre transfer applications. However, most of these techniques,
including the DDSP, are generally not applicable in real-time constraints,
making them ineligible in a musical workflow. In this paper, we present a
real-time implementation of the DDSP library embedded in a virtual synthesizer
as a plug-in that can be used in a Digital Audio Workstation. We focused on
timbre transfer from learned representations of real instruments to arbitrary
sound inputs as well as controlling these models by MIDI. Furthermore, we
developed a GUI for intuitive high-level controls which can be used for
post-processing and manipulating the parameters estimated by the neural
network. We have conducted a user experience test with seven participants
online. The results indicated that our users found the interface appealing,
easy to understand, and worth exploring further. At the same time, we have
identified issues in the timbre transfer quality, in some components we did not
implement, and in installation and distribution of our plugin. The next
iteration of our design will address these issues. Our real-time MATLAB and
JUCE implementations are available at https://github.com/SMC704/juce-ddsp and
https://github.com/SMC704/matlab-ddsp , respectively.",multimedia,603,not included
http://arxiv.org/abs/1802.08960v2,to_check,arxiv,arxiv,2018-02-25 00:00:00,arxiv,"bonnet: an open-source training and deployment framework for semantic
  segmentation in robotics using cnns",http://arxiv.org/abs/1802.08960v2,"The ability to interpret a scene is an important capability for a robot that
is supposed to interact with its environment. The knowledge of what is in front
of the robot is, for example, relevant for navigation, manipulation, or
planning. Semantic segmentation labels each pixel of an image with a class
label and thus provides a detailed semantic annotation of the surroundings to
the robot. Convolutional neural networks (CNNs) are popular methods for
addressing this type of problem. The available software for training and the
integration of CNNs for real robots, however, is quite fragmented and often
difficult to use for non-experts, despite the availability of several
high-quality open-source frameworks for neural network implementation and
training. In this paper, we propose a tool called Bonnet, which addresses this
fragmentation problem by building a higher abstraction that is specific for the
semantic segmentation task. It provides a modular approach to simplify the
training of a semantic segmentation CNN independently of the used dataset and
the intended task. Furthermore, we also address the deployment on a real
robotic platform. Thus, we do not propose a new CNN approach in this paper.
Instead, we provide a stable and easy-to-use tool to make this technology more
approachable in the context of autonomous systems. In this sense, we aim at
closing a gap between computer vision research and its use in robotics
research. We provide an open-source codebase for training and deployment. The
training interface is implemented in Python using TensorFlow and the deployment
interface provides a C++ library that can be easily integrated in an existing
robotics codebase, a ROS node, and two standalone applications for label
prediction in images and videos.",multimedia,604,included
http://arxiv.org/abs/2104.08002v1,to_check,arxiv,arxiv,2021-04-16 00:00:00,arxiv,efficient and generic 1d dilated convolution layer for deep learning,http://arxiv.org/abs/2104.08002v1,"Convolutional neural networks (CNNs) have found many applications in tasks
involving two-dimensional (2D) data, such as image classification and image
processing. Therefore, 2D convolution layers have been heavily optimized on
CPUs and GPUs. However, in many applications - for example genomics and speech
recognition, the data can be one-dimensional (1D). Such applications can
benefit from optimized 1D convolution layers. In this work, we introduce our
efficient implementation of a generic 1D convolution layer covering a wide
range of parameters. It is optimized for x86 CPU architectures, in particular,
for architectures containing Intel AVX-512 and AVX-512 BFloat16 instructions.
We use the LIBXSMM library's batch-reduce General Matrix Multiplication
(BRGEMM) kernel for FP32 and BFloat16 precision. We demonstrate that our
implementation can achieve up to 80% efficiency on Intel Xeon Cascade Lake and
Cooper Lake CPUs. Additionally, we show the generalization capability of our
BRGEMM based approach by achieving high efficiency across a range of
parameters. We consistently achieve higher efficiency than the 1D convolution
layer with Intel oneDNN library backend for varying input tensor widths, filter
widths, number of channels, filters, and dilation parameters. Finally, we
demonstrate the performance of our optimized 1D convolution layer by utilizing
it in the end-to-end neural network training with real genomics datasets and
achieve up to 6.86x speedup over the oneDNN library-based implementation on
Cascade Lake CPUs. We also demonstrate the scaling with 16 sockets of
Cascade/Cooper Lake CPUs and achieve significant speedup over eight V100 GPUs
using a similar power envelop. In the end-to-end training, we get a speedup of
1.41x on Cascade Lake with FP32, 1.57x on Cooper Lake with FP32, and 2.27x on
Cooper Lake with BFloat16 over eight V100 GPUs with FP32.",multimedia,605,not included
http://arxiv.org/abs/2007.15152v2,to_check,arxiv,arxiv,2020-07-29 00:00:00,arxiv,"accelerating multi-attribute unsupervised seismic facies analysis with
  rapids",http://arxiv.org/abs/2007.15152v2,"Classification of seismic facies is done by clustering seismic data samples
based on their attributes. Year after year, 3D datasets used by exploration
geophysics increase in size, complexity, and number of attributes, requiring a
continuous rise in the classification performance. In this work, we explore the
use of Graphics Processing Units (GPUs) to perform the classification of
seismic surveys using the well-established Machine Learning (ML) method
k-means. We show that the high-performance distributed implementation of the
k-means algorithm available at the RAPIDS library can be used to classify
facies in large seismic datasets much faster than a classical parallel CPU
implementation (up to 258-fold faster in NVIDIA V100 GPUs), especially for
large seismic blocks. We tested the algorithm with different real seismic
volumes, including Netherlands, Parihaka, and Kahu (from 12GB to 66GB).",multimedia,606,not included
http://arxiv.org/abs/2003.11100v1,to_check,arxiv,arxiv,2020-03-24 00:00:00,arxiv,"how deep is your encoder: an analysis of features descriptors for an
  autoencoder-based audio-visual quality metric",http://arxiv.org/abs/2003.11100v1,"The development of audio-visual quality assessment models poses a number of
challenges in order to obtain accurate predictions. One of these challenges is
the modelling of the complex interaction that audio and visual stimuli have and
how this interaction is interpreted by human users. The No-Reference
Audio-Visual Quality Metric Based on a Deep Autoencoder (NAViDAd) deals with
this problem from a machine learning perspective. The metric receives two sets
of audio and video features descriptors and produces a low-dimensional set of
features used to predict the audio-visual quality. A basic implementation of
NAViDAd was able to produce accurate predictions tested with a range of
different audio-visual databases. The current work performs an ablation study
on the base architecture of the metric. Several modules are removed or
re-trained using different configurations to have a better understanding of the
metric functionality. The results presented in this study provided important
feedback that allows us to understand the real capacity of the metric's
architecture and eventually develop a much better audio-visual quality metric.",multimedia,607,not included
http://arxiv.org/abs/1804.09914v1,to_check,arxiv,arxiv,2018-04-26 00:00:00,arxiv,"itelescope: intelligent video telemetry and classification in real-time
  using software defined networking",http://arxiv.org/abs/1804.09914v1,"Video continues to dominate network traffic, yet operators today have poor
visibility into the number, duration, and resolutions of the video streams
traversing their domain. Current approaches are inaccurate, expensive, or
unscalable, as they rely on statistical sampling, middle-box hardware, or
packet inspection software. We present {\em iTelescope}, the first intelligent,
inexpensive, and scalable SDN-based solution for identifying and classifying
video flows in real-time. Our solution is novel in combining dynamic flow rules
with telemetry and machine learning, and is built on commodity OpenFlow
switches and open-source software. We develop a fully functional system, train
it in the lab using multiple machine learning algorithms, and validate its
performance to show over 95\% accuracy in identifying and classifying video
streams from many providers including Youtube and Netflix. Lastly, we conduct
tests to demonstrate its scalability to tens of thousands of concurrent
streams, and deploy it live on a campus network serving several hundred real
users. Our system gives unprecedented fine-grained real-time visibility of
video streaming performance to operators of enterprise and carrier networks at
very low cost.",multimedia,608,included
http://arxiv.org/abs/2103.11052v1,to_check,arxiv,arxiv,2021-03-19 00:00:00,arxiv,"a first step towards automated species recognition from camera trap
  images of mammals using ai in a european temperate forest",http://arxiv.org/abs/2103.11052v1,"Camera traps are used worldwide to monitor wildlife. Despite the increasing
availability of Deep Learning (DL) models, the effective usage of this
technology to support wildlife monitoring is limited. This is mainly due to the
complexity of DL technology and high computing requirements. This paper
presents the implementation of the light-weight and state-of-the-art YOLOv5
architecture for automated labeling of camera trap images of mammals in the
Bialowieza Forest (BF), Poland. The camera trapping data were organized and
harmonized using TRAPPER software, an open source application for managing
large-scale wildlife monitoring projects. The proposed image recognition
pipeline achieved an average accuracy of 85% F1-score in the identification of
the 12 most commonly occurring medium-size and large mammal species in BF using
a limited set of training and testing data (a total 2659 images with animals).
  Based on the preliminary results, we concluded that the YOLOv5 object
detection and classification model is a promising light-weight DL solution
after the adoption of transfer learning technique. It can be efficiently
plugged in via an API into existing web-based camera trapping data processing
platforms such as e.g. TRAPPER system. Since TRAPPER is already used to manage
and classify (manually) camera trapping datasets by many research groups in
Europe, the implementation of AI-based automated species classification may
significantly speed up the data processing workflow and thus better support
data-driven wildlife monitoring and conservation. Moreover, YOLOv5 developers
perform better performance on edge devices which may open a new chapter in
animal population monitoring in real time directly from camera trap devices.",multimedia,609,included
http://arxiv.org/abs/1912.10609v1,to_check,arxiv,arxiv,2019-12-23 00:00:00,arxiv,one-shot imitation filming of human motion videos,http://arxiv.org/abs/1912.10609v1,"Imitation learning has been applied to mimic the operation of a human
cameraman in several autonomous cinematography systems. To imitate different
filming styles, existing methods train multiple models, where each model
handles a particular style and requires a significant number of training
samples. As a result, existing methods can hardly generalize to unseen styles.
In this paper, we propose a framework, which can imitate a filming style by
""seeing"" only a single demonstration video of the same style, i.e., one-shot
imitation filming. This is done by two key enabling techniques: 1) feature
extraction of the filming style from the demo video, and 2) filming style
transfer from the demo video to the new situation. We implement the approach
with deep neural network and deploy it to a 6 degrees of freedom (DOF) real
drone cinematography system by first predicting the future camera motions, and
then converting them to the drone's control commands via an odometer. Our
experimental results on extensive datasets and showcases exhibit significant
improvements in our approach over conventional baselines and our approach can
successfully mimic the footage with an unseen style.",multimedia,610,included
http://arxiv.org/abs/2105.03668v2,to_check,arxiv,arxiv,2021-05-08 00:00:00,arxiv,"real-time prediction of probabilistic crack growth with a reduced-order
  digital twin of a helicopter component",http://arxiv.org/abs/2105.03668v2,"To deploy the airframe Digital Twin or to conduct probabilistic evaluations
of the remaining life of a structural component, a (near) real-time crack
growth simulation method is critical. In this paper, a reduced-order simulation
approach is developed to achieve this goal by leveraging two methods. On one
hand, the SGBEM super element - FEM coupling method is combined with parametric
modeling to generate the database of computed Stress Intensity Factors for
cracks with various sizes/shapes in a complex structural component, by which
hundreds of samples are automatically simulated within a day. On the other
hand, machine learning methods are applied to establish the relation between
crack sizes/shapes and crack front SIFs. By combining the reduced-order
computational model with load inputs and fatigue growth laws, a real time
prediction of probabilistic crack growth in complex structures with minimum
computational burden is realized. In an example of a round-robin helicopter
component, even though the fatigue crack growth is simulated cycle by cycle,
the simulation is faster than real-time (as compared to the physical test). The
proposed approach is a key simulation technology towards realizing the Digital
Twin of complex structures, which further requires fusion of model predictions
with flight/inspection/monitoring data.",multimedia,611,not included
http://arxiv.org/abs/2104.02306v1,to_check,arxiv,arxiv,2021-04-06 00:00:00,arxiv,binary neural network for speaker verification,http://arxiv.org/abs/2104.02306v1,"Although deep neural networks are successful for many tasks in the speech
domain, the high computational and memory costs of deep neural networks make it
difficult to directly deploy highperformance Neural Network systems on
low-resource embedded devices. There are several mechanisms to reduce the size
of the neural networks i.e. parameter pruning, parameter quantization, etc.
This paper focuses on how to apply binary neural networks to the task of
speaker verification. The proposed binarization of training parameters can
largely maintain the performance while significantly reducing storage space
requirements and computational costs. Experiment results show that, after
binarizing the Convolutional Neural Network, the ResNet34-based network
achieves an EER of around 5% on the Voxceleb1 testing dataset and even
outperforms the traditional real number network on the text-dependent dataset:
Xiaole while having a 32x memory saving.",multimedia,612,not included
http://arxiv.org/abs/2105.05873v1,to_check,arxiv,arxiv,2021-05-12 00:00:00,arxiv,out of the box: embodied navigation in the real world,http://arxiv.org/abs/2105.05873v1,"The research field of Embodied AI has witnessed substantial progress in
visual navigation and exploration thanks to powerful simulating platforms and
the availability of 3D data of indoor and photorealistic environments. These
two factors have opened the doors to a new generation of intelligent agents
capable of achieving nearly perfect PointGoal Navigation. However, such
architectures are commonly trained with millions, if not billions, of frames
and tested in simulation. Together with great enthusiasm, these results yield a
question: how many researchers will effectively benefit from these advances? In
this work, we detail how to transfer the knowledge acquired in simulation into
the real world. To that end, we describe the architectural discrepancies that
damage the Sim2Real adaptation ability of models trained on the Habitat
simulator and propose a novel solution tailored towards the deployment in
real-world scenarios. We then deploy our models on a LoCoBot, a Low-Cost Robot
equipped with a single Intel RealSense camera. Different from previous work,
our testing scene is unavailable to the agent in simulation. The environment is
also inaccessible to the agent beforehand, so it cannot count on scene-specific
semantic priors. In this way, we reproduce a setting in which a research group
(potentially from other fields) needs to employ the agent visual navigation
capabilities as-a-Service. Our experiments indicate that it is possible to
achieve satisfying results when deploying the obtained model in the real world.
Our code and models are available at https://github.com/aimagelab/LoCoNav.",multimedia,613,included
http://arxiv.org/abs/2104.14236v1,to_check,arxiv,arxiv,2021-04-29 00:00:00,arxiv,learning multi-attention context graph for group-based re-identification,http://arxiv.org/abs/2104.14236v1,"Learning to re-identify or retrieve a group of people across non-overlapped
camera systems has important applications in video surveillance. However, most
existing methods focus on (single) person re-identification (re-id), ignoring
the fact that people often walk in groups in real scenarios. In this work, we
take a step further and consider employing context information for identifying
groups of people, i.e., group re-id. We propose a novel unified framework based
on graph neural networks to simultaneously address the group-based re-id tasks,
i.e., group re-id and group-aware person re-id. Specifically, we construct a
context graph with group members as its nodes to exploit dependencies among
different people. A multi-level attention mechanism is developed to formulate
both intra-group and inter-group context, with an additional self-attention
module for robust graph-level representations by attentively aggregating
node-level features. The proposed model can be directly generalized to tackle
group-aware person re-id using node-level representations. Meanwhile, to
facilitate the deployment of deep learning models on these tasks, we build a
new group re-id dataset that contains more than 3.8K images with 1.5K annotated
groups, an order of magnitude larger than existing group re-id datasets.
Extensive experiments on the novel dataset as well as three existing datasets
clearly demonstrate the effectiveness of the proposed framework for both
group-based re-id tasks. The code is available at
https://github.com/daodaofr/group_reid.",multimedia,614,not included
http://arxiv.org/abs/1610.10042v2,to_check,arxiv,arxiv,2016-10-31 00:00:00,arxiv,confocalgn : a minimalistic confocal image simulator,http://arxiv.org/abs/1610.10042v2,"SUMMARY : We developed a user-friendly software to generate synthetic
confocal microscopy images from a ground truth specified as a 3D bitmap with
pixels of arbitrary size. The software can analyze a real confocal stack to
derivate noise parameters and will use them directly to generate new images
with similar noise characteristics. Such synthetic images can then be used to
assert the quality and robustness of an image analysis pipeline, as well as be
used to train machine-learning image analysis procedures. We illustrate the
approach with closed curves corresponding to the microtubule ring present in
blood platelet. AVAILABILITY AND IMPLEMENTATION: ConfocalGN is written in
Matlab but does not require any toolbox. The source code is distributed under
the GPL 3.0 licence on https://github.com/SergeDmi/ConfocalGN.",multimedia,615,not included
http://arxiv.org/abs/1209.6393v1,to_check,arxiv,arxiv,2012-09-27 00:00:00,arxiv,learning robust low-rank representations,http://arxiv.org/abs/1209.6393v1,"In this paper we present a comprehensive framework for learning robust
low-rank representations by combining and extending recent ideas for learning
fast sparse coding regressors with structured non-convex optimization
techniques. This approach connects robust principal component analysis (RPCA)
with dictionary learning techniques and allows its approximation via trainable
encoders. We propose an efficient feed-forward architecture derived from an
optimization algorithm designed to exactly solve robust low dimensional
projections. This architecture, in combination with different training
objective functions, allows the regressors to be used as online approximants of
the exact offline RPCA problem or as RPCA-based neural networks. Simple
modifications of these encoders can handle challenging extensions, such as the
inclusion of geometric data transformations. We present several examples with
real data from image, audio, and video processing. When used to approximate
RPCA, our basic implementation shows several orders of magnitude speedup
compared to the exact solvers with almost no performance degradation. We show
the strength of the inclusion of learning to the RPCA approach on a music
source separation application, where the encoders outperform the exact RPCA
algorithms, which are already reported to produce state-of-the-art results on a
benchmark database. Our preliminary implementation on an iPad shows
faster-than-real-time performance with minimal latency.",multimedia,616,not included
http://arxiv.org/abs/2109.14549v1,to_check,arxiv,arxiv,2021-09-29 00:00:00,arxiv,"vision-guided quadrupedal locomotion in the wild with multi-modal delay
  randomization",http://arxiv.org/abs/2109.14549v1,"Developing robust vision-guided controllers for quadrupedal robots in complex
environments, with various obstacles, dynamical surroundings and uneven
terrains, is very challenging. While Reinforcement Learning (RL) provides a
promising paradigm for agile locomotion skills with vision inputs in
simulation, it is still very challenging to deploy the RL policy in the real
world. Our key insight is that aside from the discrepancy in the domain gap, in
visual appearance between the simulation and the real world, the latency from
the control pipeline is also a major cause of difficulty. In this paper, we
propose Multi-Modal Delay Randomization (MMDR) to address this issue when
training RL agents. Specifically, we simulate the latency of real hardware by
using past observations, sampled with randomized periods, for both
proprioception and vision. We train the RL policy for end-to-end control in a
physical simulator without any predefined controller or reference motion, and
directly deploy it on the real A1 quadruped robot running in the wild. We
evaluate our method in different outdoor environments with complex terrains and
obstacles. We demonstrate the robot can smoothly maneuver at a high speed,
avoid the obstacles, and show significant improvement over the baselines. Our
project page with videos is at https://mehooz.github.io/mmdr-wild/.",multimedia,617,included
http://arxiv.org/abs/2109.07165v1,to_check,arxiv,arxiv,2021-09-15 00:00:00,arxiv,3d annotation of arbitrary objects in the wild,http://arxiv.org/abs/2109.07165v1,"Recent years have produced a variety of learning based methods in the context
of computer vision and robotics. Most of the recently proposed methods are
based on deep learning, which require very large amounts of data compared to
traditional methods. The performance of the deep learning methods are largely
dependent on the data distribution they were trained on, and it is important to
use data from the robot's actual operating domain during training. Therefore,
it is not possible to rely on pre-built, generic datasets when deploying robots
in real environments, creating a need for efficient data collection and
annotation in the specific operating conditions the robots will operate in. The
challenge is then: how do we reduce the cost of obtaining such datasets to a
point where we can easily deploy our robots in new conditions, environments and
to support new sensors? As an answer to this question, we propose a data
annotation pipeline based on SLAM, 3D reconstruction, and 3D-to-2D geometry.
The pipeline allows creating 3D and 2D bounding boxes, along with per-pixel
annotations of arbitrary objects without needing accurate 3D models of the
objects prior to data collection and annotation. Our results showcase almost
90% Intersection-over-Union (IoU) agreement on both semantic segmentation and
2D bounding box detection across a variety of objects and scenes, while
speeding up the annotation process by several orders of magnitude compared to
traditional manual annotation.",multimedia,618,included
http://arxiv.org/abs/1710.08637v1,to_check,arxiv,arxiv,2017-10-24 00:00:00,arxiv,"improving accuracy of nonparametric transfer learning via vector
  segmentation",http://arxiv.org/abs/1710.08637v1,"Transfer learning using deep neural networks as feature extractors has become
increasingly popular over the past few years. It allows to obtain
state-of-the-art accuracy on datasets too small to train a deep neural network
on its own, and it provides cutting edge descriptors that, combined with
nonparametric learning methods, allow rapid and flexible deployment of
performing solutions in computationally restricted settings. In this paper, we
are interested in showing that the features extracted using deep neural
networks have specific properties which can be used to improve accuracy of
downstream nonparametric learning methods. Namely, we demonstrate that for some
distributions where information is embedded in a few coordinates, segmenting
feature vectors can lead to better accuracy. We show how this model can be
applied to real datasets by performing experiments using three mainstream deep
neural network feature extractors and four databases, in vision and audio.",multimedia,619,not included
http://arxiv.org/abs/1904.02579v2,to_check,arxiv,arxiv,2019-04-04 00:00:00,arxiv,"can a robot become a movie director? learning artistic principles for
  aerial cinematography",http://arxiv.org/abs/1904.02579v2,"Aerial filming is constantly gaining importance due to the recent advances in
drone technology. It invites many intriguing, unsolved problems at the
intersection of aesthetical and scientific challenges. In this work, we propose
a deep reinforcement learning agent which supervises motion planning of a
filming drone by making desirable shot mode selections based on aesthetical
values of video shots. Unlike most of the current state-of-the-art approaches
that require explicit guidance by a human expert, our drone learns how to make
favorable viewpoint selections by experience. We propose a learning scheme that
exploits aesthetical features of retrospective shots in order to extract a
desirable policy for better prospective shots. We train our agent in realistic
AirSim simulations using both a hand-crafted reward function as well as reward
from direct human input. We then deploy the same agent on a real DJI M210 drone
in order to test the generalization capability of our approach to real world
conditions. To evaluate the success of our approach in the end, we conduct a
comprehensive user study in which participants rate the shot quality of our
methods. Videos of the system in action can be seen at
https://youtu.be/qmVw6mfyEmw.",multimedia,620,not included
http://arxiv.org/abs/2002.10718v2,to_check,arxiv,arxiv,2020-02-25 00:00:00,arxiv,"denoising imu gyroscopes with deep learning for open-loop attitude
  estimation",http://arxiv.org/abs/2002.10718v2,"This paper proposes a learning method for denoising gyroscopes of Inertial
Measurement Units (IMUs) using ground truth data, and estimating in real time
the orientation (attitude) of a robot in dead reckoning. The obtained algorithm
outperforms the state-of-the-art on the (unseen) test sequences. The obtained
performances are achieved thanks to a well-chosen model, a proper loss function
for orientation increments, and through the identification of key points when
training with high-frequency inertial data. Our approach builds upon a neural
network based on dilated convolutions, without requiring any recurrent neural
network. We demonstrate how efficient our strategy is for 3D attitude
estimation on the EuRoC and TUM-VI datasets. Interestingly, we observe our dead
reckoning algorithm manages to beat top-ranked visual-inertial odometry systems
in terms of attitude estimation although it does not use vision sensors. We
believe this paper offers new perspectives for visual-inertial localization and
constitutes a step toward more efficient learning methods involving IMUs. Our
open-source implementation is available at
https://github.com/mbrossar/denoise-imu-gyro.",multimedia,621,not included
http://arxiv.org/abs/2105.01948v1,to_check,arxiv,arxiv,2021-05-05 00:00:00,arxiv,"similarity measures for location-dependent mmimo, 5g base stations
  on/off switching using radio environment map",http://arxiv.org/abs/2105.01948v1,"The Massive Multiple-Input Multiple-Output (MMIMO) technique together with
Heterogeneous Network (Het-Net) deployment enables high throughput of 5G and
beyond networks. However, a high number of antennas and a high number of Base
Stations (BSs) can result in significant power consumption. Previous studies
have shown that the energy efficiency (EE) of such a network can be effectively
increased by turning off some BSs depending on User Equipments (UEs) positions.
Such mapping is obtained by using Reinforcement Learning. Its results are
stored in a so-called Radio Environment Map (REM). However, in a real network,
the number of UEs' positions patterns would go to infinity. This paper aims to
determine how to match the current set of UEs' positions to the most similar
pattern, i.e., providing the same optimal active BSs set, saved in REM. We
compare several state-of-the-art distance metrics using a computer simulator:
an accurate 3D-Ray-Tracing model of the radio channel and an advanced
system-level simulator of MMIMO Het-Net. The results have shown that the
so-called Sum of Minimums Distance provides the best matching between REM data
and UEs' positions, enabling up to 56% EE improvement over the scenario without
EE optimization.",multimedia,622,not included
10.1016/j.media.2021.102171,to_check,Medical Image Analysis,scopus,2021-10-01,sciencedirect,automatic skull defect restoration and cranial implant generation for cranioplasty,https://api.elsevier.com/content/abstract/scopus_id/85111529504,"A fast and fully automatic design of 3D printed patient-specific cranial implants is highly desired in cranioplasty - the process to restore a defect on the skull. We formulate skull defect restoration as a 3D volumetric shape completion task, where a partial skull volume is completed automatically. The difference between the completed skull and the partial skull is the restored defect; in other words, the implant that can be used in cranioplasty. To fulfill the task of volumetric shape completion, a fully data-driven approach is proposed. Supervised skull shape learning is performed on a database containing 167 high-resolution healthy skulls. In these skulls, synthetic defects are injected to create training and evaluation data pairs. We propose a patch-based training scheme tailored for dealing with high-resolution and spatially sparse data, which overcomes the disadvantages of conventional patch-based training methods in high-resolution volumetric shape completion tasks. In particular, the conventional patch-based training is applied to images of high resolution and proves to be effective in tasks such as segmentation. However, we demonstrate the limitations of conventional patch-based training for shape completion tasks, where the overall shape distribution of the target has to be learnt, since it cannot be captured efficiently by a sub-volume cropped from the target. Additionally, the standard dense implementation of a convolutional neural network tends to perform poorly on sparse data, such as the skull, which has a low voxel occupancy rate. Our proposed training scheme encourages a convolutional neural network to learn from the high-resolution and spatially sparse data. In our study, we show that our deep learning models, trained on healthy skulls with synthetic defects, can be transferred directly to craniotomy skulls with real defects of greater irregularity, and the results show promise for clinical use. Project page: https://github.com/Jianningli/MIA.",multimedia,623,not included
10.1016/j.csl.2020.101180,to_check,Computer Speech and Language,scopus,2021-07-01,sciencedirect,identification of related languages from spoken data: moving from off-line to on-line scenario,https://api.elsevier.com/content/abstract/scopus_id/85098984452,"The accelerating flow of information we encounter around the world today makes many companies deploy speech recognition systems that, to an ever-growing extent, process data on-line rather than off-line. These systems, e.g., for real-time 24/7 broadcast transcription, often work with input-stream data containing utterances in more than one language. This multilingual data can correctly be transcribed in real-time only if the language used is identified with just a small latency for each input frame. For this purpose, a novel approach to on-line spoken language identification is proposed in this work. Its development is documented within a series of consecutive experiments starting in the off-line mode for 11 Slavic languages, going through artificially prepared multilingual data for the on-line scenario, and ending with real bilingual TV programs containing utterances in mutually similar Czech and Slovak. The resulting scheme that we propose operates frame-by-frame; it takes in a multilingual stream of speech frames and outputs a stream of the corresponding language labels. It utilizes a weighted finite-state transducer as a decoder, which smooths the output from a language classifier fed by multilingual and augmented bottleneck features. An essential factor from the accuracy point of view is that these features, as well as the classifier itself, are based on deep neural network architectures that allow the modeling of long-term time dependencies. The obtained results show that our scheme allows us to determine the language spoken in real-world bilingual TV shows with an average latency of around 2.5 seconds and with an increase in word error rate by a mere 2.9% over the reference 18.1% value yielded by using manually prepared language labels.",multimedia,624,not included
10.1016/j.wasman.2020.09.032,to_check,Waste Management,scopus,2021-01-01,sciencedirect,detecting glass and metal in consumer trash bags during waste collection using convolutional neural networks,https://api.elsevier.com/content/abstract/scopus_id/85092313671,"We present a proof-of-concept method to classify the presence of glass and metal in consumer trash bags. With the prevalent utilization of waste collection trucks in municipal solid waste management, the aim of this method is to help pinpoint the locations where waste sorting quality is below accepted standards, making it possible and more efficient to develop tailored procedures that can improve the waste sorting quality in areas with the most urgent needs. Using trash bags containing various amounts of glass and metal, in addition to common waste found in households, we use a combination of sound recording and a beat-frequency oscillation metal detector as inputs to a machine learning algorithm to identify the occurrence of glass and metal in trash bags. A custom-built test rig was developed to mimic a real waste collection truck, which was used to test different sensors and build the datasets. Convolutional neural networks were trained for the classification task, achieving accuracies of up to 98%. These promising results support this method’s potential implementation in real waste collection trucks, enabling location-specific and long-term monitoring of consumer waste sorting quality, which can provide decision support for waste management systems, and research on consumer behavior.",multimedia,625,not included
10.1016/j.physa.2019.123151,to_check,Physica A: Statistical Mechanics and its Applications,scopus,2020-02-15,sciencedirect,early warning system: from face recognition by surveillance cameras to social media analysis to detecting suspicious people,https://api.elsevier.com/content/abstract/scopus_id/85074532417,"Surveillance security cameras are increasingly deployed in almost every location for monitoring purposes, including watching people and their actions for security purposes. For criminology, images collected from these cameras are usually used after an incident occurs to analyze who could be the people involved. While this usage of the cameras is important for a post crime action, there exists the need for real time monitoring to act as an early warning to prevent or avoid an incident before it occurs. In this paper, we describe the development and implementation of an early warning system that recognizes people automatically in a surveillance camera environment and then use data from various sources to identify these people and build their profile and network. The current literature is still missing a complete workflow from identifying people/criminals from a video surveillance to building a criminal information extraction framework and identifying those people and their interactions with others We train a feature extraction model for face recognition using convolutional neural networks to get a good recognition rate on the Chokepoint dataset collected using surveillance cameras. The system also provides the function to record people appearance in a location, such that unknown people passing through a scene excessive number of times (above a threshold decided by a security expert) will then be further analyzed to collect information about them. We implemented a queue based system to record people entrance. We try to avoid missing relevant individuals passing through as in some cases it is not possible to add every passing person to the queue which is maintained using some cache handling techniques. We collect and analyze information about unknown people by comparing their images from the cameras to a list of social media profiles collected from Facebook and intelligent services archives. After locating the profile of a person, traditional news and other social media platforms are crawled to collect and analyze more information about the identified person. The analyzed information is then presented to the analyst where a list of keywords and verb phrases are shown. We also construct the person’s network from individuals mentioned with him/her in the text. Further analysis will allow security experts to mark this person as a suspect or safe. This work shows that building a complete early warning system is feasible to tackle and identify criminals so that authorities can take the required actions on the spot.",multimedia,626,not included
10.1016/j.autcon.2019.103012,to_check,Automation in Construction,scopus,2020-02-01,sciencedirect,on-demand monitoring of construction projects through a game-like hybrid application of bim and machine learning,https://api.elsevier.com/content/abstract/scopus_id/85075291648,"While unavoidable, inspections, progress monitoring, and comparing as-planned with as-built conditions in construction projects do not readily add tangible intrinsic value to the end-users. In large-scale construction projects, the process of monitoring the implementation of every single part of buildings and reflecting them on the BIM models can become highly labour intensive and error-prone, due to the vast amount of data produced in the form of schedules, reports and photo logs. In order to address the mentioned methodological and technical gap, this paper presents a framework and a proof of concept prototype for on-demand automated simulation of construction projects, integrating some cutting edge IT solutions, namely image processing, machine learning, BIM and Virtual Reality. This study utilised the Unity game engine to integrate data from the original BIM models and the as-built images, which were processed via various computer vision techniques. These methods include object recognition and semantic segmentation for identifying different structural elements through supervised training in order to superimpose the real world images on the as-planned model. The proposed framework leads to an automated update of the 3D virtual environment with states of the construction site. This framework empowers project managers and stockholders with an advanced decision-making tool, highlighting the inconsistencies in an effective manner. This paper contributes to body knowledge by providing a technical exemplar for the integration of ML and image processing approaches with immersive and interactive BIM interfaces, the algorithms and program codes of which can help replicability of these approaches by other scholars.",multimedia,627,not included
10.1016/j.cogsys.2019.09.015,to_check,Cognitive Systems Research,scopus,2020-01-01,sciencedirect,multi-agent neurocognitive models of semantics of spatial localization of events,https://api.elsevier.com/content/abstract/scopus_id/85072851037,"The purpose of the study is to develop a learning system for internal representation of the events localization space to realize orientation and navigation of autonomous mobile systems. The task of the research is the development of simulation models of the semantics of the event localization space based on multi-agent neurocognitive architectures. The paper proves that the multi-agent neurocognitive architecture is an effective formalism for describing the semantics of the spatial localization of events. Main theoretical foundations have been developed for the simulation of spatial relations using the so-called multi-agent facts, consisting of software agents-concepts, reflecting semantic categories corresponding to parts of speech. It is shown that locative software agents that describe the spatial location of objects and events, forming homogeneous connections, compose the so-called field locations. The latter describes a holistic view of the intellectual agent about the environment. The paper defines conceptual foundations of multi-agent modeling of the semantics of subjective reflexive mapping of the interaction between real objects, space and time.",multimedia,628,not included
10.1016/j.isatra.2019.01.026,to_check,ISA Transactions,scopus,2019-08-01,sciencedirect,a new multi-agent particle swarm algorithm based on birds accents for the 3d indoor deployment problem,https://api.elsevier.com/content/abstract/scopus_id/85061324166,"The 3D indoor deployment of sensor nodes is a complex real world problem, proven to be NP-hard and difficult to resolve using classical methods. In this context, we propose a hybrid approach relying on a novel bird’s accent-based many objective particle swarm optimization algorithm (named acMaPSO) to resolve the problem of 3D indoor deployment on the Internet of Things collection networks. The new concept of bird’s accent is presented to assess the search ability of particles in their local areas. To conserve the diversity of the population during searching, particles are separated into different accent groups by their regional habitation and are classified into different categories of birds/particles in each cluster according to their common manner of singing. A particle in an accent-group can select other particles as its neighbors from its group or from other groups (which sing differently) if the selected particles have the same expertise in singing or are less experienced compared to this particle. To allow the search escaping from local optima, the most expert particles (parents) “die” and are regularly replaced by a novice (newborn) randomly generated ones. Moreover, the hybridization of the proposed acMaPSO algorithm with multi-agent systems is suggested. The new variant (named acMaMaPSO) takes advantage of the distribution and interactivity of particle agents. Experimental, numerical and statistical found results show the effectiveness of the two proposed variants compared to different other recent state-of-the-art of many-objective evolutionary algorithms.",multimedia,629,not included
10.1016/bs.adcom.2019.02.004,to_check,Advances in Computers,scopus,2019-01-01,sciencedirect,ssim and ml based qoe enhancement approach in sdn context,https://api.elsevier.com/content/abstract/scopus_id/85063000805,"Today, video streaming rose above all other traffic types over the internet. While providing this service with a high quality is the most challenging task, researchers are trying to solve the challenge by giving a more efficient network where congestion, broadband limitations and unsatisfied users are limited. In new multimedia based networks, new challenges move from technology-oriented services to user-oriented services which prove the importance of QoE. Service providers' growth depends nowadays not only on QoS parameters but also on clients' feeling and expectation. That is why; service providers must measure received QoE and this becomes a challenge regarding the evaluation of users' feeling. For users, qualitative perception differs from one user to another and service providers affront difficulties to transform the qualitative values into the quantitative one. The QoE evaluation needs more sophisticated methods to describe the real expectation of users. New multimedia applications provide at any user locations media who are sharing video, and communicating together in virtual network. These applications require providing the best possible QoE to consumers. When it comes to us, we present in this paper a machine learning approach combined with adaptive coding in order to provide a better QoE for video streaming services. This solution will be established using SDN architecture. We can justify this choice because we need a centralized architecture, where the totality of the network is known, to predict its status. So, we will implement a machine learning algorithm in the controller: this algorithm, called ML-based SSIM, will calculate approximately the quality needed for a video to be streamed. Finally, the quality found by the ML-based SSIM Algorithm will be combined with the network situation to choose the right coding. First part of the paper deals with an introduction of QoE requirement, metrics and protocols used especially in streaming services, then we give a complete study around Machine learning algorithms and other fields used in literature to enhance QoE. We define in this paper, at first, QoS and QoE, then the serving environment such as mobile cloud computing and software Defined Network (SDN). Then, we give both objective and subjective metrics, expose mathematical approaches used in modeling, predicting and evaluating QoE. Second, we expose the SSIM approach and explain how our proposed one is based on. The last part of the paper deals with experiments: we describe SDN environment deployment, describe scenarios and finally simulate on SDN emulator some topologies to demonstrate the impact of SDN components helping QoE measurement. At the end, we give the results and values. We highlight the future of our proposition.",multimedia,630,not included
10.1016/j.future.2017.06.002,to_check,Future Generation Computer Systems,scopus,2018-02-01,sciencedirect,automated multi-level malware detection system based on reconstructed semantic view of executables using machine learning techniques at vmm,https://api.elsevier.com/content/abstract/scopus_id/85023600868,"In order to fulfill the requirements like stringent timing restraints and demand on resources, Cyber–Physical System (CPS) must deploy on the virtualized environment such as cloud computing. To protect Virtual Machines (VMs) in which CPSs are functioning against malware-based attacks, malware detection and mitigation technique is emerging as a highly crucial concern. The traditional VM-based anti-malware software themselves a potential target for malware-based attack since they are easily subverted by sophisticated malware. Thus, a reliable and robust malware monitoring and detection systems are needed to detect and mitigate rapidly the malware based cyber-attacks in real time particularly for virtualized environment. The Virtual Machine Introspection (VMI) has emerged as a fine-grained out-of-VM security solution to detect malware by introspecting and reconstructing the volatile memory state of the live guest Operating System (OS) by functioning at the Virtual Machine Monitor (VMM) or hypervisor. However, the reconstructed semantic details by the VMI are available in a combination of benign and malicious states at the hypervisor. In order to distinguish between these two states, extensive manual analysis is required by the existing out-of-VM security solutions. To address the foremost issue, in this paper, we propose an advanced VMM-based guest-assisted Automated Multilevel Malware Detection System (AMMDS) that leverages both VMI and Memory Forensic Analysis (MFA) techniques to predict early symptoms of malware execution by detecting stealthy hidden processes on a live guest OS. More specifically, the AMMDS system detects and classifies the actual running malicious executables from the semantically reconstructed process view of the guest OS. The two sub-components of the AMMDS are: Online Malware Detector (OMD) and Offline Malware Classifier (OFMC). The OMD recognizes whether the running processes are benign or malicious using its Local Malware Signature Database (LMSD) and online malware scanner and the OFMC classify unknown malware by adopting machine learning techniques at the hypervisor. The AMMDS has been evaluated by executing large real-world malware and benign executables on to the live guest OSs. The evaluation results achieved 100% of accuracy and zero False Positive Rate (FPR) on the 10-fold cross-validation in classifying unknown malware with maximum performance overhead of 5.8%.",multimedia,631,not included
10.26083/tuprints-00017606,to_check,core,,2021-01-01 00:00:00,core,automation for camera-only 6d object detection,,"Today a widespread deployment of Augmented Reality (AR) systems is only possible by means of computer vision frameworks like ARKit and ARCore, which abstract from specific devices, yet restrict the set of devices to the respective vendor.
This thesis therefore investigates how to allow deploying AR systems to any device with an attached camera.

One crucial part of an AR system is the detection of arbitrary objects in the camera frame and naturally accompanying the estimation of their 6D-pose.
This increases the degree of scene understanding that AR applications require for placing augmentations in the real world. Currently, this is limited by a coarse segmentation of the scene into planes as provided by the aforementioned frameworks.
Being able to reliably detect individual objects, allows attaching specific augmentations as required by e.g. AR maintenance applications.
For this, we employ convolutional neural networks (CNNs) to estimate the 6D-pose of all visible objects from a single RGB image.
Here, the addressed challenge is the automated training of the respective CNN models, given only the CAD geometry of the target object.
First, we look at reconstructing the missing surface data in real-time before we turn to the more general problem of bridging the domain gap between the non-photorealistic representation and the real world appearance.
To this end, we build upon generative adversarial network (GAN) models to formulate the domain gap as an unsupervised learning problem.
Our evaluation shows an improvement in model performance, while providing a simplified handling compared to alternative solutions.

Furthermore, the calibration data of the used camera must be known for precise pose estimation. This data, again, is only available for the restricted set of devices, that the proprietary frameworks support.
To lift this restriction, we propose a web-based camera calibration service that not only aggregates calibration data, but also guides users in the calibration of new cameras.
Here, we first present a novel calibration-pose selection framework that reduces the number of required calibration images by 30% compared to existing solutions, while ensuring a repeatable and reliable calibration outcome.
Then, we present an evaluation of different user-guidance strategies, which allows choosing a setting suitable for most users.
This enables even novice users to perform a precise camera calibration in about 2 minutes.
Finally, we propose an efficient client-server architecture to deploy the aforementioned guidance on the web, making it available to the widest possible range of devices.
This service is not restricted to AR systems, but allows the general deployment of computer vision algorithms on the web that rely on camera calibration data, which was previously not possible.

These elements combined, allow a semi-automatic deployment of AR systems with any camera to detect any object",multimedia,632,included
10.1145/3444693,to_check,core,'Association for Computing Machinery (ACM)',2021-05-01 00:00:00,core,"fuzzy logic in surveillance big video data analysis: comprehensive review, challenges, and research directions",https://core.ac.uk/download/365180882.pdf,"CCTV cameras installed for continuous surveillance generate enormous amounts of data daily, forging the term “Big Video Data” (BVD). The active practice of BVD includes intelligent surveillance and activity recognition, among other challenging tasks. To efficiently address these tasks, the computer vision research community has provided monitoring systems, activity recognition methods, and many other computationally complex solutions for the purposeful usage of BVD. Unfortunately, the limited capabilities of these methods, higher computational complexity, and stringent installation requirements hinder their practical implementation in real-world scenarios, which still demand human operators sitting in front of cameras to monitor activities or make actionable decisions based on BVD. The usage of human-like logic, known as fuzzy logic, has been employed emerging for various data science applications such as control systems, image processing, decision making, routing, and advanced safety-critical systems. This is due to its ability to handle various sources of real world domain and data uncertainties, generating easily adaptable and explainable data-based models. Fuzzy logic can be effectively used for surveillance as a complementary for huge-sized artificial intelligence models and tiresome training procedures. In this paper, we draw researchers’ attention towards the usage of fuzzy logic for surveillance in the context of BVD. We carry out a comprehensive literature survey of methods for vision sensory data analytics that resort to fuzzy logic concepts. Our overview highlights the advantages, downsides, and challenges in existing video analysis methods based on fuzzy logic for surveillance applications. We enumerate and discuss the datasets used by these methods, and finally provide an outlook towards future research directions derived from our critical assessment of the efforts invested so far in this exciting field",multimedia,633,not included
http://arxiv.org/abs/2107.04974,to_check,core,,2021-07-11 00:00:00,core,non-linear visual knowledge discovery with elliptic paired coordinates,,"It is challenging for humans to enable visual knowledge discovery in data
with more than 2-3 dimensions with a naked eye. This chapter explores the
efficiency of discovering predictive machine learning models interactively
using new Elliptic Paired coordinates (EPC) visualizations. It is shown that
EPC are capable to visualize multidimensional data and support visual machine
learning with preservation of multidimensional information in 2-D. Relative to
parallel and radial coordinates, EPC visualization requires only a half of the
visual elements for each n-D point. An interactive software system EllipseVis,
which is developed in this work, processes high-dimensional datasets, creates
EPC visualizations, and produces predictive classification models by
discovering dominance rules in EPC. By using interactive and automatic
processes it discovers zones in EPC with a high dominance of a single class.
The EPC methodology has been successful in discovering non-linear predictive
models with high coverage and precision in the computational experiments. This
can benefit multiple domains by producing visually appealing dominance rules.
This chapter presents results of successful testing the EPC non-linear
methodology in experiments using real and simulated data, EPC generalized to
the Dynamic Elliptic Paired Coordinates (DEPC), incorporation of the weights of
coordinates to optimize the visual discovery, introduction of an alternative
EPC design and introduction of the concept of incompact machine learning
methodology based on EPC/DEPC.Comment: 29 pages, 29 figures, 12 table",multimedia,634,not included
https://core.ac.uk/download/429740636.pdf,to_check,core,"'Institute of Economics, Technologies and Entrepreneurship'",2021-05-19 00:00:00,core,стратегічні напрями розвитку інноваційного маркетингу на ринку високих технологій,10.37332/2309-1533.2021.1-2.17,"Ковшова І.О., Бабич Ю.В. СТРАТЕГІЧНІ НАПРЯМИ РОЗВИТКУ ІННОВАЦІЙНОГО МАРКЕТИНГУ НА РИНКУ ВИСОКИХ ТЕХНОЛОГІЙМета. Визначення стратегічних напрямів розвитку інноваційного маркетингу на ринку високих технологій задля збільшення обсягів продажів товарів чи послуг, підвищення ефективності діяльності підприємств та аналізу потенційних ринків збуту.Методика дослідження. В процесі дослідження використано наступні методи: абстрактно-логічний - для формулювання понять інноваційного маркетингу, високих технологій та стратегій інноваційного маркетингу; аналізу та синтезу - для виділення окремих частин інноваційної маркетингової стратегії та правильне застосування їх на ринку високих технологій; економіко-статистичний - задля вивчення стратегічних напрямів інноваційного маркетингу та встановлення кількісного впливу на ринок високих технологій; порівняльний - для співставлення тенденції розвитку стратегічних напрямків інноваційного маркетингу у 2016 та 2020 роках.Результати дослідження. Поглиблено сутність характеристики інноваційного маркетингу на ринку високих технологій та визначено найефективніші стратегічні напрями розвитку інноваційного маркетингу сьогодення: програмне просування, штучний інтелект, відеомаркетинг, персоналізацію та розмовний маркетинг. Обґрунтовано, що для забезпечення інноваційної маркетингової діяльності підприємству важливо постійно проводити моніторинг ринку та нових інструментів, які дозволять вивести товари чи послуги на новий рівень. Встановлено, що виділені п’ять напрямів розвитку інноваційного маркетингу значно впливають на комплекс маркетингових інструментів високотехнологічних підприємств, програми штучного інтелекту включені у всі сфери маркетингового комплексу і активно використовуються маркетологами для обробки, розпізнавання і аналізу голосу, тексту, зображень і процесу прийняття рішень. Обґрунтовано варіанти застосування штучного інтелекту у маркетинговій діяльності підприємств та сформовано основні тренди подальшого розвитку.Наукова новизна результатів дослідження. Обґрунтовано, що зазначені стратегічні напрями розвитку інноваційного маркетингу сьогодення (програмне просування, штучний інтелект, відеомаркетинг, персоналізація та розмовний маркетинг), на відміну від існуючих стратегічних напрямків розвитку, збільшують продажі товарів і послуг підприємств, підвищують поінформованість про бренд, формують довіру та покращують процес комунікації з потенційними і реальними клієнтами.Практична значущість результатів дослідження. Результати проведеного дослідження можуть будуть застосовані високотехнологічними підприємствами в Україні та світі задля підвищення конкурентоспроможності на ринку та виведення продукту чи послуги на новий рівень. Також це дасть змогу збільшити обсягів продажів товарів чи послуг та підвищити ефективність діяльності підприємств.Ключові слова: інноваційний маркетинг, підприємство, напрями розвитку, ринок технологій, відеомаркетинг, штучний інтелект, маркетинг.Kovshova I.O., Babych Yu.V. STRATEGIC DIRECTIONS OF DEVELOPMENT OF INNOVATIVE MARKETING IN THE MARKET OF HIGH TECHNOLOGIESPurpose. The aim of the article is defining strategic directions for the development of innovative marketing in the high-tech market in order to increase sales of goods or services, increase the efficiency of enterprises and analyse potential markets.Methodology of research. The following methods were used in the research process: abstract and logical - to formulate the concepts of innovative marketing, high technologies and strategies of innovative marketing; analysis and synthesis - to identify certain parts of the innovative marketing strategy and their proper application in the high technology market; economic and statistical - to study the strategic directions of innovative marketing and to establish a quantitative impact on the high technology market; comparative - to compare the development trends of strategic areas of innovative marketing in 2016 and 2020.Findings. The essence of the characteristics of innovative marketing in the high-tech market is deepened and the most effective strategic directions of innovative marketing development of today are identified: software promotion, artificial intelligence, video marketing, personalization and conversational marketing. It is substantiated that to ensure innovative marketing activities, it is important for the company to constantly monitor the market and new tools that will bring goods or services to a new level. It is established that the five areas of innovative marketing have been identified as significantly influencing the marketing tools of high-tech enterprises, artificial intelligence programs are included in all areas of the marketing industry and are actively used by marketers to process, recognize and analyse voice, text, images and decision making. Options for the use of artificial intelligence in the marketing activities of enterprises are substantiated and the main trends of further development are formed.Originality. It is substantiated that these strategic directions of development of innovative marketing today (software promotion, artificial intelligence, video marketing, personalization and conversational marketing), in contrast to existing strategic directions of development, increase sales of goods and services, increase brand awareness, build trust and improve communication with potential and real customers.Practical value. The results of the conducted study can be used by high-tech companies in Ukraine and around the world to increase market competitiveness and bring the product or service to a new level. It will also increase sales of goods or services and increase the efficiency of enterprises.Key words: innovative marketing, enterprise, directions of development, technology market, video marketing, artificial intelligence, marketing",multimedia,635,not included
http://arxiv.org/abs/2106.09357,to_check,core,'Institute of Electrical and Electronics Engineers (IEEE)',2021-06-17 00:00:00,core,"cat-like jumping and landing of legged robots in low-gravity using deep
  reinforcement learning",10.1109/TRO.2021.3084374,"In this article, we show that learned policies can be applied to solve legged
locomotion control tasks with extensive flight phases, such as those
encountered in space exploration. Using an off-the-shelf deep reinforcement
learning algorithm, we trained a neural network to control a jumping quadruped
robot while solely using its limbs for attitude control. We present tasks of
increasing complexity leading to a combination of three-dimensional
(re-)orientation and landing locomotion behaviors of a quadruped robot
traversing simulated low-gravity celestial bodies. We show that our approach
easily generalizes across these tasks and successfully trains policies for each
case. Using sim-to-real transfer, we deploy trained policies in the real world
on the SpaceBok robot placed on an experimental testbed designed for
two-dimensional micro-gravity experiments. The experimental results demonstrate
that repetitive, controlled jumping and landing with natural agility is
possible.Comment: Published in IEEE Transactions on Robotics:
  https://ieeexplore.ieee.org/document/9453856 Video:
  https://youtu.be/KQhlZa42fe",multimedia,636,included
10.23919/date48585.2020.9116560,to_check,core,Offline Model Guard: Secure and Private ML on Mobile Devices,2020-07-05 00:00:00,core,http://arxiv.org/abs/2007.02351,'Institute of Electrical and Electronics Engineers (IEEE)',"Performing machine learning tasks in mobile applications yields a challenging
conflict of interest: highly sensitive client information (e.g., speech data)
should remain private while also the intellectual property of service providers
(e.g., model parameters) must be protected. Cryptographic techniques offer
secure solutions for this, but have an unacceptable overhead and moreover
require frequent network interaction. In this work, we design a practically
efficient hardware-based solution. Specifically, we build Offline Model Guard
(OMG) to enable privacy-preserving machine learning on the predominant mobile
computing platform ARM - even in offline scenarios. By leveraging a trusted
execution environment for strict hardware-enforced isolation from other system
components, OMG guarantees privacy of client data, secrecy of provided models,
and integrity of processing algorithms. Our prototype implementation on an ARM
HiKey 960 development board performs privacy-preserving keyword recognition
using TensorFlow Lite for Microcontrollers in real time.Comment: Original Publication (in the same form): DATE 202",multimedia,638,included
10.13130/2464-8914/12652,to_check,core,"Il diritto per i minori, i diritti dei minori. Itinerari nell'Italia del Novecento",2019-01-01 00:00:00,core,https://core.ac.uk/download/288644199.pdf,,"La   questione   minorile   è   stata   ripercorsa   dalla storiografia soprattutto attraverso il prisma della repressione e prevenzione; nell’Italia  del  Novecento  il  bisogno  di  controllo  sociale,  ‘pensiero dominante’, ha complicato la costruzione del diritto peri minori e deiminori.La codificazione civile, ‘a misura di adulto’, assegnava infatti i diritti al soggetto razionale ed autonomo, quasi a riproporre l’impostazione del paternalismo liberale di Stuart Mill, a proposito di minori da proteggere in primo luogo da se stessi. L’ordinamento contemplava i minori soprattutto per imporgli l’‘antico onora il padre’;anche se il codice civile del 1942 non prevedeva  una  soggezione  totale  dei  figli  rispetto  alla  potestà  genitoriale, prima  del  fascismo  ed  ancoranegli  anni  Cinquanta  la  patria  potestà  era intesa come ‘rimedio’ all’incapacità di agire del figlio, ostacolo all’«azione dello Stato» nell’ordine e disordine della famiglia. Agli inizi del secolo la «protezione giuridica dei minorenni» pareva «sconosciuta al nostro diritto e fuori della legislazione», inadeguata soprattutto nell’impietoso paragone con la legislazione europea e d’oltreoceano; la comparazione con un ‘altrove felice’ –in primo luogo il Children Actdel 1908 –sarebbe stata una costante nelle politiche nazionali per l’infanzia ed adolescenza, dalla Circolaredel guardasigilli V.E. Orlando al Progetto di Codice per i minorenni.Lontano dallo ‘specialismo’ –cifra della scientia iurisnazionale –il diritto minorile aveva vocazione ‘interdisciplinare’, nel poggiare soprattutto sul legame tra diritto e pedagogia; nel 1910 Orlando parlava all’Istituto pedago-gico forense di Milano del diritto‘da sempre e per sempre’ preminente per i minori, quello alla «protezione educativa», all’«educazione, siapure  for-zata».  Nell’Italia  liberale  era  ampio  il  dibattito  e  significativa  la  pro-gettazione; ne era un ‘ultimo atto’ il ProgettoFerri, inteso a distinguere tra bambini e adolescenti e a costruire una giustizia penale speciale rispetto a quella per gli adulti. Negli anni del fascismo –anche in questa materia tutt’al-tro  che  parentesi –era invece serrata la legislazione, con l’istituzione dell’Opera nazionale maternità e infanzia –attiva fino al 1975 –l’Opera na-zionale Balilla, il Tribunale per i minorenni (Rd. 1404/1934). Il regime inten-deva  marcare  il  passaggiodella  questione  minorile  dal  campo  penale  a quello «sociale»; era centrale la «rieducazione», come ribadito nel 1941 dal guardasigilli Dino Grandi. Il Rd. 140/1934 si ricollegava alla codificazione pe-nale e penal-processuale (1930), ed era ‘apripista’ della codificazione civile (1942). Nella dottrina degli anni Cinquanta l’«autonomia»del diritto minorile na-sceva con il Tribunale per i minori; cenni sporadici erano riservati alla Costi-tuzione  ed  alla  Dichiarazionedi  Ginevra  dei  diritti  universali  dei  bambini (1924). Pur in un orizzonte meramente esortatorio, in seguito le fonti inter-nazionali avrebbero avrebbero ‘incalzato’ il legislatore italiano,soprattutto sul piano, problematico, dell’attuazione dei principi. Il  diritto  minorile  si sviluppava inoltre come sorta di judge made law–costruito sopratutto dai presidenti del Tribunale dei minori, Radaelli, Baviera, Cividali, Meucci, Moro, Vercellone,  Occhiogrosso,  Fadiga –profilo  problematico  per  la  prevalente cultura giuridica legalista italiana, ostile all’ampio potere discrezionale, eser-citato dal giudice specializzato in materia giurisdizionale ed amministrativa. Nel 1951 nasceva inoltre l’Unione italiana giudici per minorenni, poi Associa-zione, motore del processo legislativo; nel 1971 era istituita la pianta orga-nica del «giudice specializzato».Il  diritto  minorile  era  complicato  dai  cambiamenti  di  costume,  che investivano  la  società:  nel  primoNovecento  il  dibattito tra  giuristi,  giudici, politici, scienziati sociali era iscritto nell’orizzonte della autorità,paterna e/o dello  Stato,  non  in  quello  della  libertà  ed  autonomia  del  minore.  Con  un cambio di prospettiva, all’Assemblea Costituente Aldo Moro tematizzava un diritto  «problematico»,  stante  l’incapacità  di  agire  del  soggetto,  ma «autentico». Quasi ad anticipare il best interest of the child, la legge 431/67 –detta  dell’adozione  speciale –attribuiva   al   Tribunale   il   compito   di «promuovere e difendere i diritti del minore». Elia –relatore di una densa sentenza  della  Consulta  del  1981 –sostenne che il “centro di gravità” del sistema passava dall’“interesse dall’adottante a quello dell’adottato”. Ma non    mancavano    problemi:    davanti    ai    concretissimi    drammi    sulla destinazione del bambino –madre sola e problematica o Istituto –Jemolo si chiedeva «dov’è l’interesse del minore», interrogativo a suo avviso non sciolto dalla legge, che aveva affidato al giudice «un compito che non è suo». Anche il mondo del diritto era lambito da nuove sensibilità e trasformazioni sociali: alla metà degli anni Settanta siponevail tema del «metodo», con la trasformazione  del  «diritto  minorile»,  da  «diritto  dei  minori  a  diritto  dei diritti dei minori». Con la riforma del diritto di famiglia del 1975 il «vecchio» diritto  minorile  pareva  destinato  a  lasciare  il  posto  ad  uno  «nuovo»;perAlfredo  Carlo  Moro  tramontava  il  soggetto  «unificato»,ed  entravano  in scena «Pierino, Maria»; alla luce della Convenzione internazionale dei diritti del fanciullo del 1989, ratificata in Italia due anni dopo, Moro affermava che il discorso sui «diritti» doveva poggiare sulla «attuazione». D’altro canto si osservava  che  il  legislatore  non  poteva  stabilire  per  legge  i  contenuti  dei diritti –primo di tutti quello all’educazione –se  non  con  criteri  generali  e astratti,   e   che   il  minore  non  era  considerato  dall’ordinamento  come soggetto di diritto, ma come destinatario di decisioni prese da altri, genitori, tutori, giudici. Nel 1970 Cividali metteva in scena un «giudice nuovo», impegnato a pro-muovere i diritti del minore, empatico con la «persona», più che intento ad applicare ‘freddamente’ la legge. La polarità tra il rispetto delle norme, a tu-tela di tuttii bambini, e la decisione nel migliore interessedi quelparticolare bambino, anche contravvenendo alla legge, irrompeva con il ‘caso celebre’ di Serena Cruz, tolta dal Tribunale per i minorenni di Torino alla famiglia, che l’aveva adottata illegalmente; quel ‘caso celebre’ metteva in scena il corto circuito tra legalità e «vera giustizia». Quanto al rapporto tra minori e istitu-zioni, intese alla rieducazione, la prassi mostra che, dal fascismo alla repub-blica, alla «discontinuità  politica»è  corrisposta  una «continuità  istituzio-nale», anche nella violenza esercitata sui minori; da qui una ‘provocazione’, la creazione di un «Tribunale per la difesa dei minori». Neppure il codice di procedura penale minorile del 1988, inteso a «educare responsabilizzando», è parso assicurare ai minorenni una efficace «garanzia».The juvenile question has been traced by historiography above all through the prism of repression and prevention; in twentieth-cen-tury Italy the need for social control, ""dominant thought"", has complicated the construction of the right for minors and minors. In fact, civil codification, ""adult-friendly"", assigned the rights to the rational and autonomous subject, as if to re-propose the liberal paternalism of Stuart Mill, about minors to be protected  in  the  first  place  by  themselves.  The  system  envisaged  minors above all to impose the ancient on it honors the father; even if the civil code of  1942  did  not  foresee  a  total  subjection  of  the  children  with  respect  to parental authority, before fascism and still in the fifties the parental author-ity was understood as 'remedy' to the inability to act of the child, obstacle to the action of the State in the order and disorder of the family. At the be-ginning of the century the ""legal protection of minors"" seemed ""unknown to our law and outside the law"", inadequate especially in the pitiless compari-son with European and overseas legislation; the comparison with a ‘happy elsewhere’ -first and foremost the Children Act of 1908 -would have been a constant in national policies for children and adolescents, from the Circular of the Minister of Justice V.E. Orlando to the Code Project for minors.Far from ‘specialism’ -a figure of the italian scientia iuris-the juvenile law had an ‘interdisciplinary’ figure, based above all on the link between law and pedagogy; in 1910 Orlando was speaking at the Milan Forensic Pedagogical Institute  of the  ""always  and forever""  pre-eminent  right for  minors, that of ""educational protection"", ""education, albeit forced"". In liberal Italy the plan-ning was significant and the debate was significant; the Ferri Project was a 'final act', intended to distinguish between children and adolescents and to build a special criminal justice system with respect to that for adults. In the years of fascism -even in this matter anything but parenthesis -legislation was tightened, with the institution of the Maternity and Childhood National Opera -active until 1975 -the Balilla National Opera, the Juvenile Court (Rd.1404/1934). The regime intended to mark the passage of the juvenile issue from the criminal to the ""social"" field; ""re-education"" was central, as reiter-ated in 1941 by the Minister of Justice Dino Grandi. Rd. 140/1934 was linked to the penal  and  penal-procedural  codification  (1930)  and  was  a  ""forerun-ner"" of civil codification (1942). In the doctrine of the 1950s the ""autonomy"" of juvenile law was born with the juvenile court; sporadic notes were reserved for the Constitution and the Geneva Declaration of Universal Children's Rights (1924). Despite a merely exhortatory  horizon,  the  international  sources  later  allegedly  ""urged""  the Italian legislator, above all on the problematic level of implementation of the principles.  The  juvenile  law  developed  as  a  sort  of  judge  made  law -built above all by the presidents of the juvenile court, Radaelli, Baviera, Cividali, Meucci, Moro, Vercellone, Occhiogrosso, Fadiga -problematic profile for the prevailing  legalistic  Italian  legal  culture,  hostile  to  the  wide  discretion, exercised  by  the  judge  specialized  in  jurisdictional  and  administrative matters. In 1951 the Italian Union of judges for minors was also born, then the  Association,  the  engine  of  the  legislative  process;  in  1971  the  organic plan of the ""specialized judge"" was established.The  juvenile  law  was  complicated  by  the  changes  of  custom,  which invested  the  society:  in  the  early  twentieth  century  the  debate  between jurists, judges, politicians, social scientists was inscribed in the horizon of the paternal and / or state authority, not in that of freedom and autonomy of the minor. With a change of perspective, at the Constituent Assembly Aldo Moro discussed a ""problematic"" right, given the subject's inability to act, but ""authentic"".  Asif  to  anticipate  the  best  interest  of  the  child,  law  431/67 -called  special  adoption -gave  the  Court  the  task  of  ""promoting  and defending  the  rights  of  the  child"".  Elia -who  was  the  speaker  of  a  dense sentence of the Consulta in 1981 -argued that the ""center of gravity"" of the system shifted ""from the interest of the adopter to the one adopted"". But there were problems: in front of the very real dramas about the destination of  the  child -single  and  problematic  mother  or  Institute -Jemolo  asked himself  ""where  is  the  minor's  interest"",  a  question  in  his  opinion  not dissolved by the law, which he had entrusted to the judge a task that is not his. Even  the  world  of  law  was  surrounded  by  new  sensitivities  and  social transformations:  in  the  mid-seventies  the  theme  of  the  ""method""  was posed,  with  the  transformation  of  the  ""juvenile  right"",  from  ""the  right  of minors to the rights of minors' rights"". With the family law reform of 1975, the ""old"" child law seemed destined to give way to a ""new"" one; for Alfredo Carlo Moro set the ""unified"" subject, and ""Pierino, Maria"" entered the scene; in the light of the 1989 International Convention on the Rights of the Child, ratified in Italy two years later, Moro affirmed that the discourse on ""rights"" had  to  rest  on  ""implementation"".  On  the  other  hand  it  was  observed  that the legislator could not establish by law the contents of the rights -first of all that to education -if not with general and abstract criteria, and that the minor was not considered by law as a subjectof right, but as a recipient of decisions made by others, parents, guardians, judges.In 1970 Cividali staged a ""new judge"", committed to promoting the rights of  the  child,  empathetic  with  the  ""person"",  rather  than  intent  on  applying the  law  ""coldly"".  Thepolarity  between  the  respect  of  the  rules,  for  the protection  of  all  children,  and  the  decision  in  the  best  interest  of  that particular  child,  even  in  contravention  of  the  law,  burst  with  the  'famous case' of Serena Cruz, removed by the Turin Juvenile Court to the family, who had  adopted  it  illegally;  that  ""celebrated  case""  staged  the  gap  between legality  and  ""true  justice"".  As  for  the  relationship  between  minors  and institutions, meant for re-education, the practice shows that, from fascism to the republic, political discontinuity is matched by institutional continuity, even  in  the  violence  exercised  on  minors;  hence  the  ""provocation"",  the creation of a ""Tribunal for the defense of minors"". Not even the 1988 Code of  Juvenile  Criminal  Procedure,  intended  to""educate  by  empowering"", seemed to ensure an effective ""guarantee"" for minors",multimedia,639,not included
http://www.mdpi.com/2071-1050/10/9/3142,to_check,core,'MDPI AG',2018-09-03 00:00:00,core,"a systematic review of smart real estate technology: drivers of, and barriers to, the use of digital disruptive technologies and online platforms",10.3390/su10093142,"Real estate needs to improve its adoption of disruptive technologies to move from traditional to smart real estate (SRE). This study reviews the adoption of disruptive technologies in real estate. It covers the applications of nine such technologies, hereby referred to as the Big9. These are: drones, the internet of things (IoT), clouds, software as a service (SaaS), big data, 3D scanning, wearable technologies, virtual and augmented realities (VR and AR), and artificial intelligence (AI) and robotics. The Big9 are examined in terms of their application to real estate and how they can furnish consumers with the kind of information that can avert regrets. The review is based on 213 published articles. The compiled results show the state of each technology’s practice and usage in real estate. This review also surveys dissemination mechanisms, including smartphone technology, websites and social media-based online platforms, as well as the core components of SRE: sustainability, innovative technology and user centredness. It identifies four key real estate stakeholders—consumers, agents and associations, government and regulatory authorities, and complementary industries—and their needs, such as buying or selling property, profits, taxes, business and/or other factors. Interactions between these stakeholders are highlighted, and the specific needs that various technologies address are tabulated in the form of a what, who and how analysis to highlight the impact that the technologies have on key stakeholders. Finally, stakeholder needs as identified in the previous steps are matched theoretically with six extensions of the traditionally accepted technology adoption model (TAM), paving the way for a smoother transition to technology-based benefits for consumers. The findings pertinent to the Big9 technologies in the form of opportunities, potential losses and exploitation levels (OPLEL) analyses highlight the potential utilisation of each technology for addressing consumers’ needs and minimizing their regrets. Additionally, the tabulated findings in the form of what, how and who links the Big9 technologies to core consumers’ needs and provides a list of resources needed to ensure proper information dissemination to the stakeholders. Such high-quality information can bridge the gap between real estate consumers and other stakeholders and raise the state of the industry to a level where its consumers have fewer or no regrets. The study, being the first to explore real estate technologies, is limited by the number of research publications on the SRE technologies that has been compensated through incorporation of online reports",multimedia,640,not included
https://core.ac.uk/download/199450757.pdf,to_check,core,'MDPI AG',2018-01-01 00:00:00,core,an accelerated tool for flood modelling based on iber,10.3390/w10101459,"Este artigo inclúese no número especial ""Selected Papers from the 1st International Electronic Conference on the Hydrological Cycle (ChyCle-2017)""[Abstract:] This paper presents Iber+, a new parallel code based on the numerical model Iber for two-dimensional (2D) flood inundation modelling. The new implementation, which is coded in C++ and takes advantage of the parallelization functionalities both on CPUs (central processing units) and GPUs (graphics processing units), was validated using different benchmark cases and compared, in terms of numerical output and computational efficiency, with other well-known hydraulic software packages. Depending on the complexity of the specific test case, the new parallel implementation can achieve speedups up to two orders of magnitude when compared with the standard version. The speedup is especially remarkable for the GPU parallelization that uses Nvidia CUDA (compute unified device architecture). The efficiency is as good as the one provided by some of the most popular hydraulic models. We also present the application of Iber+ to model an extreme flash flood that took place in the Spanish Pyrenees in October 2012. The new implementation was used to simulate 24 h of real time in roughly eight minutes of computing time, while the standard version needed more than 15 h. This huge improvement in computational efficiency opens up the possibility of using the code for real-time forecasting of flood events in early-warning systems, in order to help decision making under hazardous events that need a fast intervention to deploy countermeasures.Water JPI—WaterWorks Programme, project Improving
Drought and Flood Early Warning, Forecasting and Mitigation, IMDROFLOOD; PCIN-2015-243European Commission; project RISC_ML 034_RISC_ML_6_EXunta de Galicia; ED431C 2017/64-GRCXunta de Galicia; ED481A-2017/314Xunta de Galicia; ED481B-2018/020European Commission; IMDROFLOOD PCIN-2015-24",multimedia,641,not included
https://core.ac.uk/download/296283456.pdf,to_check,core,Università degli Studi di Milano,2019-12-19 00:00:00,core,"il diritto per i minori, i diritti dei minori. itinerari nell’italia del novecento",10.13130/2464-8914/12652,"The juvenile question has been traced by historiography above all through the prism of repression and prevention; in twentieth-century Italy the need for social control, ""dominant thought"", has complicated the construction of the right for minors and minors. In fact, civil codification, ""adult-friendly"", assigned the rights to the rational and autonomous subject, as if to re-propose the liberal paternalism of Stuart Mill, about minors to be protected in the first place by themselves. The system envisaged minors above all to impose the ancient on it honors the father; even if the civil code of 1942 did not foresee a total subjection of the children with respect to parental authority, before fascism and still in the fifties the parental authority was understood as 'remedy' to the inability to act of the child, obstacle to the action of the State in the order and disorder of the family. At the beginning of the century the ""legal protection of minors"" seemed ""unknown to our law and outside the law"", inadequate especially in the pitiless comparison with European and overseas legislation; the comparison with a ‘happy elsewhere’ - first and foremost the Children Act of 1908 - would have been a constant in national policies for children and adolescents, from the Circular of the Minister of Justice V.E. Orlando to the Code Project for minors.
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Far from ‘specialism’ - a figure of the italian scientia iuris - the juvenile law had an ‘interdisciplinary’ figure, based above all on the link between law and pedagogy; in 1910 Orlando was speaking at the Milan Forensic Pedagogical Institute of the ""always and forever"" pre-eminent right for minors, that of ""educational protection"", ""education, albeit forced"". In liberal Italy the planning was significant and the debate was significant; the Ferri Project was a 'final act', intended to distinguish between children and adolescents and to build a special criminal justice system with respect to that for adults. In the years of fascism - even in this matter anything but parenthesis - legislation was tightened, with the institution of the Maternity and Childhood National Opera - active until 1975 - the Balilla National Opera, the Juvenile Court (Rd . 1404/1934). The regime intended to mark the passage of the juvenile issue from the criminal to the ""social"" field; ""re-education"" was central, as reiterated in 1941 by the Minister of Justice Dino Grandi. Rd. 140/1934 was linked to the penal and penal-procedural codification (1930) and was a ""forerunner"" of civil codification (1942).
In the doctrine of the 1950s the ""autonomy"" of juvenile law was born with the juvenile court; sporadic notes were reserved for the Constitution and the Geneva Declaration of Universal Children's Rights (1924). Despite a merely exhortatory horizon, the international sources later allegedly ""urged"" the Italian legislator, above all on the problematic level of implementation of the principles. The juvenile law developed as a sort of judge made law - built above all by the presidents of the juvenile court, Radaelli, Baviera, Cividali, Meucci, Moro, Vercellone, Occhiogrosso, Fadiga - problematic profile for the prevailing legalistic Italian legal culture, hostile to the wide discretion, exercised by the judge specialized in jurisdictional and administrative matters. In 1951 the Italian Union of judges for minors was also born, then the Association, the engine of the legislative process; in 1971 the organic plan of the ""specialized judge"" was established.
The juvenile law was complicated by the changes of custom, which invested the society: in the early twentieth century the debate between jurists, judges, politicians, social scientists was inscribed in the horizon of the paternal and / or state authority, not in that of freedom and autonomy of the minor. With a change of perspective, at the Constituent Assembly Aldo Moro discussed a ""problematic"" right, given the subject's inability to act, but ""authentic"". As if to anticipate the best interest of the child, law 431/67 - called special adoption - gave the Court the task of ""promoting and defending the rights of the child"". Elia - who was the speaker of a dense sentence of the Consulta in 1981 - argued that the ""center of gravity"" of the system shifted ""from the interest of the adopter to the one adopted"". But there were problems: in front of the very real dramas about the destination of the child - single and problematic mother or Institute - Jemolo asked himself ""where is the minor's interest"", a question in his opinion not dissolved by the law, which he had entrusted to the judge a task that is not his. Even the world of law was surrounded by new sensitivities and social transformations: in the mid-seventies the theme of the ""method"" was posed, with the transformation of the ""juvenile right"", from ""the right of minors to the rights of minors' rights"". With the family law reform of 1975, the ""old"" child law seemed destined to give way to a ""new"" one; for Alfredo Carlo Moro set the ""unified"" subject, and ""Pierino, Maria"" entered the scene; in the light of the 1989 International Convention on the Rights of the Child, ratified in Italy two years later, Moro affirmed that the discourse on ""rights"" had to rest on ""implementation"". On the other hand it was observed that the legislator could not establish by law the contents of the rights - first of all that to education - if not with general and abstract criteria, and that the minor was not considered by law as a subject of right, but as a recipient of decisions made by others, parents, guardians, judges.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; In 1970 Cividali staged a ""new judge"", committed to promoting the rights of the child, empathetic with the ""person"", rather than intent on applying the law ""coldly"". The polarity between the respect of the rules, for the protection of all children, and the decision in the best interest of that particular child, even in contravention of the law, burst with the 'famous case' of Serena Cruz, removed by the Turin Juvenile Court to the family, who had adopted it illegally; that ""celebrated case"" staged the gap between legality and ""true justice"". As for the relationship between minors and institutions, meant for re-education, the practice shows that, from fascism to the republic, political discontinuity is matched by institutional continuity, even in the violence exercised on minors; hence the ""provocation"", the creation of a ""Tribunal for the defense of minors"". Not even the 1988 Code of Juvenile Criminal Procedure, intended to ""educate by empowering"", seemed to ensure an effective ""guarantee"" for minors.La questione minorile è stata ripercorsa dalla storiografia soprattutto attraverso il prisma della repressione e prevenzione; nell’Italia del Novecento il bisogno di controllo sociale, ‘pensiero dominante’, ha complicato la costruzione del diritto per i minori e dei minori. La codificazione civile, ‘a misura di adulto’, assegnava infatti i diritti al soggetto razionale ed autonomo, quasi a riproporre l’impostazione del paternalismo liberale di Stuart Mill, a proposito di minori da proteggere in primo luogo da se stessi. L’ordinamento contemplava i minori soprattutto per imporgli l’‘antico onora il padre’;&nbsp;anche se il codice civile del 1942 non prevedeva una soggezione totale dei figli rispetto alla potestà genitoriale, prima del fascismo ed ancora negli anni Cinquanta la patria potestà era intesa come ‘rimedio’ all’incapacità di agire del figlio, ostacolo all’«azione dello Stato» nell’ordine e disordine della famiglia. Agli inizi del secolo la «protezione giuridica dei minorenni» pareva «sconosciuta al nostro diritto e fuori della legislazione», inadeguata soprattutto nell’impietoso paragone con la legislazione europea e d’oltreoceano; la comparazione con un ‘altrove felice’ – in primo luogo il Children Act del 1908 – sarebbe stata una costante nelle politiche nazionali per l’infanzia ed adolescenza, dalla Circolare del guardasigilli V.E. Orlando al Progetto di Codice per i minorenni.
Lontano dallo ‘specialismo’ – cifra della scientia iuris nazionale – il diritto minorile aveva vocazione ‘interdisciplinare’, nel poggiare soprattutto sul legame tra diritto e pedagogia; nel 1910 Orlando parlava all’Istituto pedagogico forense di Milano del diritto&nbsp; ‘da sempre e per sempre’ preminente per i minori, quello alla «protezione educativa», all’«educazione, sia pure forzata». Nell’Italia liberale era ampio il dibattito e significativa la progettazione; ne era un ‘ultimo atto’ il Progetto Ferri, inteso a distinguere tra bambini e adolescenti e a costruire una giustizia penale speciale rispetto a quella per gli adulti. Negli anni del fascismo – anche in questa materia tutt’altro che parentesi – era invece serrata la legislazione, con l’istituzione dell’Opera nazionale maternità e infanzia – attiva fino al 1975 – l’Opera nazionale Balilla, il Tribunale per i minorenni (Rd. 1404/1934). Il regime intendeva marcare il passaggio della questione minorile dal campo penale a quello «sociale»; era centrale la «rieducazione», come ribadito nel 1941 dal guardasigilli Dino Grandi. Il Rd. 140/1934 si ricollegava alla codificazione penale e penal-processuale (1930), ed era ‘apripista’ della codificazione civile (1942).
Nella dottrina degli anni Cinquanta l’«autonomia» del diritto minorile nasceva con il Tribunale per i minori; cenni sporadici erano riservati alla Costituzione ed alla Dichiarazione di Ginevra dei diritti universali dei bambini (1924). Pur in un orizzonte meramente esortatorio, in seguito le fonti internazionali avrebbero avrebbero ‘incalzato’ il legislatore italiano, soprattutto sul piano, problematico, dell’attuazione dei principi. Il diritto minorile si sviluppava inoltre come sorta di judge made law – costruito sopratutto dai presidenti del Tribunale dei minori, Radaelli, Baviera, Cividali, Meucci, Moro, Vercellone, Occhiogrosso, Fadiga – profilo problematico per la prevalente cultura giuridica legalista italiana, ostile all’ampio potere discrezionale, esercitato dal giudice specializzato in materia giurisdizionale ed amministrativa. Nel 1951 nasceva inoltre l’Unione italiana giudici per minorenni, poi Associazione, motore del processo legislativo; nel 1971 era istituita la pianta organica del «giudice specializzato».
Il diritto minorile era complicato dai cambiamenti di costume, che investivano la società: nel primo Novecento il dibattito tra giuristi, giudici, politici, scienziati sociali era iscritto nell’orizzonte della autorità, paterna e/o dello Stato, non in quello della libertà ed autonomia del minore. Con un cambio di prospettiva, all’Assemblea Costituente Aldo Moro tematizzava un diritto «problematico», stante l’incapacità di agire del soggetto, ma «autentico». Quasi ad anticipare il best interest of the child, la legge 431/67 – detta dell’adozione speciale – attribuiva al Tribunale il compito di «promuovere e difendere i diritti del minore». Elia – relatore di una densa sentenza della Consulta del 1981 – sostenne che il “centro di gravità” del sistema passava dall’“interesse dall’adottante a quello dell’adottato”. Ma non mancavano problemi: davanti ai concretissimi drammi sulla destinazione del bambino – madre sola e problematica o Istituto – Jemolo si chiedeva «dov’è l’interesse del minore», interrogativo a suo avviso non sciolto dalla legge, che aveva affidato al giudice «un compito che non è suo». Anche il mondo del diritto era lambito da nuove sensibilità e trasformazioni sociali: alla metà degli anni Settanta si poneva il tema del «metodo», con la trasformazione del «diritto minorile», da «diritto dei minori a diritto dei diritti dei minori». Con la riforma del diritto di famiglia del 1975 il «vecchio» diritto minorile pareva destinato a lasciare il posto ad uno «nuovo»;&nbsp; per Alfredo Carlo Moro tramontava il soggetto «unificato»,&nbsp;ed entravano in scena «Pierino, Maria»; alla luce della Convenzione internazionale dei diritti del fanciullo del 1989, ratificata in Italia due anni dopo, Moro affermava che il discorso sui «diritti» doveva poggiare sulla «attuazione». D’altro canto si osservava che il legislatore non poteva stabilire per legge i contenuti dei diritti – primo di tutti quello all’educazione – se non con criteri generali e astratti, e che il minore non era considerato dall’ordinamento come soggetto di diritto, ma come destinatario di decisioni prese da altri, genitori, tutori, giudici.
Nel 1970 Cividali metteva in scena un «giudice nuovo», impegnato a promuovere i diritti del minore, empatico con la «persona», più che intento ad applicare ‘freddamente’ la legge. La polarità tra il rispetto delle norme, a tutela di tutti i bambini, e la decisione nel migliore interesse di quel particolare bambino, anche contravvenendo alla legge, irrompeva con il ‘caso celebre’ di Serena Cruz, tolta dal Tribunale per i minorenni di Torino alla famiglia, che l’aveva adottata illegalmente; quel ‘caso celebre’ metteva in scena il corto circuito tra legalità e «vera giustizia». Quanto al rapporto tra minori e istituzioni, intese alla rieducazione, la prassi mostra che, dal fascismo alla repubblica, alla «discontinuità politica» è corrisposta una «continuità istituzionale», anche nella violenza esercitata sui minori; da qui una ‘provocazione’, la creazione di un «Tribunale per la difesa dei minori». Neppure il codice di procedura penale minorile del 1988, inteso a «educare responsabilizzando», è parso assicurare ai minorenni una efficace «garanzia»",multimedia,642,not included
"[{'title': 'frontiers in neuroscience', 'identifiers': ['issn:1662-453x', 'issn:1662-4548', '1662-453x', '1662-4548']}]",to_check,core,'Frontiers Media SA',2018-10-31 00:00:00,core,"a neuro-inspired system for online learning and recognition of parallel spike trains, based on spike latency, and heterosynaptic stdp",,"Humans perform remarkably well in many cognitive tasks including pattern recognition. However, the neuronal mechanisms underlying this process are not well understood. Nevertheless, artificial neural networks, inspired in brain circuits, have been designed and used to tackle spatio-temporal pattern recognition tasks. In this paper we present a multi-neuronal spike pattern detection structure able to autonomously implement online learning and recognition of parallel spike sequences (i.e., sequences of pulses belonging to different neurons/neural ensembles). The operating principle of this structure is based on two spiking/synaptic neurocomputational characteristics: spike latency, which enables neurons to fire spikes with a certain delay and heterosynaptic plasticity, which allows the own regulation of synaptic weights. From the perspective of the information representation, the structure allows mapping a spatio-temporal stimulus into a multi-dimensional, temporal, feature space. In this space, the parameter coordinate and the time at which a neuron fires represent one specific feature. In this sense, each feature can be considered to span a single temporal axis. We applied our proposed scheme to experimental data obtained from a motor-inhibitory cognitive task. The results show that out method exhibits similar performance compared with other classification methods, indicating the effectiveness of our approach. In addition, its simplicity and low computational cost suggest a large scale implementation for real time recognition applications in several areas, such as brain computer interface, personal biometrics authentication, or early detection of diseases.GS acknowledges financial support by the Spanish Ministry of Economy and Competitiveness (PTA-2015-10395-I).



Research by author LC is supported by Viera y Clavijo fellowship from Tenerife, Spain.



ML is supported by a postdoctoral fellowship from the Spanish Ministry of Economy and Competitiveness (IJCI-2016-30662).CM and EP acknowledge support from the Spanish Ministry of Economy and Competitiveness and Fondo Europeo de Desarrollo Regional (FEDER) through projects TEC2016-80063-C3-3-R (AEI/FEDER, UE).



CM acknowledges the Spanish State Research Agency, through the María de Maeztu Program for Units of Excellence in R&D (MDM-2018-2022).Peer reviewe",multimedia,643,not included
rit scholar works,to_check,core,A Temporally Coherent Neural Algorithm for Artistic Style Transfer,2016-07-01 00:00:00,core,10.1109/icpr.2016.7900142,,"Within the fields of visual effects and animation, humans have historically spent countless painstaking hours mastering the skill of drawing frame-by-frame animations. One such animation technique that has been widely used in the animation and visual effects industry is called \u22rotoscoping\u22 and has allowed uniquely stylized animations to capture the motion of real life action sequences, however it is a very complex and time consuming process. Automating this arduous technique would free animators from performing frame by frame stylization and allow them to concentrate on their own artistic contributions. This thesis introduces a new artificial system based on an existing neural style transfer method which creates artistically stylized animations that simultaneously reproduce both the motion of the original videos that they are derived from and the unique style of a given artistic work. This system utilizes a convolutional neural network framework to extract a hierarchy of image features used for generating images that appear visually similar to a given artistic style while at the same time faithfully preserving temporal content. The use of optical flow allows the combination of style and content to be integrated directly with the apparent motion over frames of a video to produce smooth and visually appealing transitions. The implementation described in this thesis demonstrates how biologically-inspired systems such as convolutional neural networks are rapidly approaching human-level behavior in tasks that were once thought impossible for computers. Such a complex task elucidates the current and future technical and artistic capabilities of such biologically-inspired neural systems as their horizons expand exponentially. Further, this research provides unique insights into the way that humans perceive and utilize temporal information in everyday tasks. A secondary implementation that is explored in this thesis seeks to improve existing convolutional neural networks using a biological approach to the way these models adapt to their inputs. This implementation shows how these pattern recognition systems can be greatly improved by integrating recent neuroscience research into already biologically inspired systems. Such a novel hybrid activation function model replicates recent findings in the field of neuroscience and shows significant advantages over existing static activation functions",multimedia,644,not included
"lausanne, epfl",to_check,core,Computational Analysis of Urban Places Using Mobile Crowdsensing,2016-11-09 00:00:00,core,10.5075/epfl-thesis-7243,https://core.ac.uk/download/148025514.pdf,"In cities, urban places provide a socio-cultural habitat for people to counterbalance the daily grind of urban life, an environment away from home and work. Places provide an environment for people to communicate, share perspectives, and in the process form new social connections. Due to the active role of places to the social fabric of city life, it is important to understand how people perceive and experience places. One fundamental construct that relates place and experience is ambiance, i.e., the impressions we ubiquitously form when we go out. Young people are key actors of urban life, specially at night, and as such play an equal role in co-creating and appropriating the urban space. Understanding how places and their youth inhabitants interact at night is a relevant urban issue. Until recently, our ability to assess the visual and perceptual qualities of urban spaces and to study the dynamics surrounding youth experiences in those spaces have been limited partly due to the lack of quantitative data. However, the growth of computational methods and tools including sensor-rich mobile devices, social multimedia platforms, and crowdsourcing tools have opened ways to measure urban perception at scale, and to deepen our understanding of nightlife as experienced by young people. In this thesis, as a first contribution, we present the design, implementation and computational analysis of four mobile crowdsensing studies involving youth populations from various countries to understand and infer phenomena related to urban places and people. We gathered a variety of explicit and implicit crowdsourced data including mobile sensor data and logs, survey responses, and multimedia content (images and videos) from hundreds of crowdworkers and thousands of users of mobile social networks. Second, we showed how crowdsensed images can be used for the computational characterization and analysis of urban perception in indoor and outdoor places. For both place types, urban perception impressions were elicited for several physical and psychological constructs using online crowdsourcing. Using low-level and deep learning features extracted from images, we automatically inferred crowdsourced judgments of indoor ambiance with a maximum R2 of 0.53 and outdoor perception with a maximum R2 of 0.49. Third, we demonstrated the feasibility to collect rich contextual data to study the physical mobility, activities, ambiance context, and social patterns of youth nightlife behavior. Fourth, using supervised machine learning techniques, we automatically classified drinking behavior of young people in an urban, real nightlife setting. Using features extracted from mobile sensor data and application logs, we obtained an overall accuracy of 76.7%. While this thesis contributes towards understanding urban perception and youth nightlife patterns in specific contexts, our research also contributes towards the computational understanding of urban places at scale with high spatial and temporal resolution, using a combination of mobile crowdsensing, social media, machine learning, multimedia analysis, and online crowdsourcing",multimedia,645,not included
10.1007/978-3-319-20883-1_22,to_check,core,,2015-05-22 00:00:00,core,gptips 2: an open-source software platform for symbolic data mining,http://arxiv.org/abs/1412.4690,"GPTIPS is a free, open source MATLAB based software platform for symbolic
data mining (SDM). It uses a multigene variant of the biologically inspired
machine learning method of genetic programming (MGGP) as the engine that drives
the automatic model discovery process. Symbolic data mining is the process of
extracting hidden, meaningful relationships from data in the form of symbolic
equations. In contrast to other data-mining methods, the structural
transparency of the generated predictive equations can give new insights into
the physical systems or processes that generated the data. Furthermore, this
transparency makes the models very easy to deploy outside of MATLAB. The
rationale behind GPTIPS is to reduce the technical barriers to using,
understanding, visualising and deploying GP based symbolic models of data,
whilst at the same time remaining highly customisable and delivering robust
numerical performance for power users. In this chapter, notable new features of
the latest version of the software are discussed with these aims in mind.
Additionally, a simplified variant of the MGGP high level gene crossover
mechanism is proposed. It is demonstrated that the new functionality of GPTIPS
2 (a) facilitates the discovery of compact symbolic relationships from data
using multiple approaches, e.g. using novel gene-centric visualisation analysis
to mitigate horizontal bloat and reduce complexity in multigene symbolic
regression models (b) provides numerous methods for visualising the properties
of symbolic models (c) emphasises the generation of graphically navigable
libraries of models that are optimal in terms of the Pareto trade off surface
of model performance and complexity and (d) expedites real world applications
by the simple, rapid and robust deployment of symbolic models outside the
software environment they were developed in.Comment: 26 pages, accepted for publication in the Springer Handbook of
  Genetic Programming Applications (2015, in press",multimedia,646,not included
10.12681/eadd/40089,to_check,core,'National Documentation Centre (EKT)',2015-01-01 00:00:00,core,development of chemical sensors for the detection of toxic compounds,,"The present dissertation was focused on the topic of chemical sensors for rapid detection of toxic substances, an investigation that was made for first time in the literature. For this purpose, biosensors using printed circuits composed from graphene microelectrodes using stabilized lipid membranes were constructed and developed.A number of particular methodologies were used to embody various receptors such as enzymes (urease and cholesterol oxidase) and an antibody (antibody D-dimer) in the sensors.Various physicochemical and instrumental methods of analysis, eg., differential scanning calorimetry (DSC), scanning electron microscopy (SEM), Raman spectrophotometry, etc., were used to locate the position of the receptor (eg. antibody, enzyme, etc.) into the stabilized lipid membrane of the sensor and the mechanism of signal generation.The response of the sensor to various potentially toxic substrates eg. urea (urease immobilization), cholesterol (cholesterol oxidase immobilization) and D-dimer (immobilizing antibody D-dimer) and its stability with time were studied.For example, the response of urea biosensor towards various urea concentrations was found to have high sensitivity (ca. 70 mV/ concentration decade) over a range of urea concentrations between 1 Χ 10−6 Μ - 1 Χ 10−3 Μ. The response of cholesterol biosensor towards various concentrations of the substrate were also found to be highly sensitive (ca. 64 mV/ concentration decade) over a concentration range between 1 Χ 10−6 Μ - 1 Χ 10−3 Μ; this sensitivity is larger than those provided up to date in the literature. Finally the response of the D-dimer immunosensor was found satisfactory over a wide concentration range of analyte (10-6 μg/ mL - 10-3 μg/ mL) with a sensitivity of ca. 59 mV/ concentration decade with a rapid response of 15 s.The selectivity of the sensor construction method and on how to limit interferences were studied so that the sensor could be applied in real samples. The conditions for satisfactory stability and reproducibility of the chemical sensors were thoroughly studied and their construction into nanoscale was investigated.The results of the implementation of the constructed devices in real samples were satisfactory and show the possibility of construction of portable chemical sensors for the detection of toxicants for first time in the literature with many advantages, which can be used in the future as alternative systems instead of the time consuming, costly and complex chromatographic methods of analysis.Η παρούσα διατριβή εμβάθυνε για πρώτη φορά στην κατασκευή χημικών αισθητήρων για την ταχεία ανίχνευση τοξικών ουσιών. Για το σκοπό αυτό κατασκευάσθηκαν και αναπτύχθηκαν βιαισθητήρες που στηρίζονται σε τυπωμένα κυκλώματα γραφενίου με ακινητοποιημένες σταθεροποιημένες λιπιδικές μεμβράνες.Χρησιμοποιώντας συγκεκριμένες μεθοδολογίες, έγινε ενσωμάτωση στους αισθητήρες διαφόρων υποδοχέων, όπως ενζύμων (ουρεάση και οξειδάση της χοληστερόλης) και αντισωμάτων (αντισώματος του D-dimer).Στη συνέχεια, με χρήση ενόργανων μεθόδων ανάλυσης όπως, διαφορικής θερμιδομετρίας σαρώσεως (DSC), ηλεκτρονικής μικροσκοπίας σαρώσεως (SEM), φασματοφωτομετρίας Raman, κλπ., προσδιορίστηκε πειραματικά η θέση του υποδοχέα (π.χ. αντίσωμα, ένζυμο, κτλ.) μέσα στη σταθεροποιημένη λιπιδική μεμβράνη του αισθητήρα και εξερευνήθηκε ο μηχανισμός παραγωγής του σήματος.Μελετήθηκε η απόκριση σε διάφορα υποστρώματα με τοξικές εν δυνάμει ενώσεις π.χ. ουρία (ακινητοποίηση ουρεάσης), χοληστερόλη (ακινητοποίηση οξειδάσης της χοληστερόλης) και D-dimer (ακινητοποίηση αντισώματος D-dimer), καθώς και η σταθερότητα του συστήματος των αισθητήρων με το χρόνο.Χαρακτηριστικά αναφέρεται ότι η απόκριση του αισθητήρα της ουρίας σε διάφορες συγκεντρώσεις ουρίας, βρέθηκε να παρουσίαζει υψηλή ευαισθησία (~70 mV/ δεκάδα συγκεντρώσεων) για μια ευρεία περιοχή συγκεντρώσεων της ουρίας που κυμαινόταν από 1 Χ 10−6 Μ ως 1 Χ 10−3 Μ. Επίσης η απόκριση του αισθητήρα της χοληστερόλης σε διάφορες συγκεντρώσεις χοληστερόλης, βρέθηκε να παρουσιάζει υψηλή ευαισθησία (~64 mV/ δεκάδα συγκεντρώσεων) για μια ευρεία περιοχή συγκεντρώσεων της χοληστερόλης που κυμαινόταν από 1 Χ 10−6 Μ ως 1 Χ 10−3 Μ, και είναι καλύτερη από βιβλιογραφικά αναφερθέντες συσκευές βιοαισθητήρες χοληστερόλης. Τέλος η απόκριση του ανοσοαισθητήρα του D-dimer βρέθηκε ικανοποιητική για ένα ευρύ συγκέντρωσεων D-dimer (10-6 μg/ mL έως 10-3 μg/ mL) με ευαισθησία ~59 mV/ δεκάδα συγκεντρώσεων και με μια γρήγορη απόκριση 15s.Εν συνεχεία μελετήθηκε η εκλεκτικότητα της μεθόδου της κατασκευής των αισθητήρων και η άρση των παρεμποδίσεων για να εφαρμοστούν σε πραγματικά δείγματα. Παράλληλα μελετήθηκαν οι συνθήκες για ύπαρξη ικανοποιητικής σταθερότητας και επαναληπτικότητας των χημικών αισθητήρων και ερευνήθηκε η κατασκευή τους σε νανοκλίμακα.Τα αποτελέσματα της εφαρμογής των αισθητήρων που κατασκευάστηκαν σε πραγματικά δείγματα ήταν ικανοποιητικά με αποτέλεσμα για πρώτη φορά στην παγκόσμια βιβλιογραφία να υπάρχει η δυνατότητα να κατασκευαστούν φορητοί χημικοί αισθητήρες για την ανίχνευση τοξικών ουσιών με πολλά πλεονεκτήματα και στο μέλλον να μπορούν να χρησιμοποιηθούν σαν εναλλακτικά συστήματα αντί των χρονοβόρων, πανάκριβων και πολύπλοκων χρωματογραφικών μεθόδων ανάλυσης",multimedia,647,not included
10.1145/1363686.1363695,to_check,core,,2015-11-26 00:00:00,core,electricity market simulation: multiagent system approach,,"This paper suggests a multiagent system (MAS) approach for market simulation. This is achieved through analysis, modeling, implementation and simulation of artificial markets populated by software agents that represent economic self interested agents. Software agents are the constructs of a complex system, an artificial market that model a real existing market or an outline of a market design. The interest in simulating a market is multiple: exploiting existing market rules, searching for market design flaws and loopholes, and supporting decision making during a market mechanism design process. The main aim of the suggested approach is to analyze the behavior that emerges from the interaction of self interested agents acting in an artificial market. AEMAS (Artificial Economy MultiAgent System), a multiagent system architecture inspired by the Market Oriented Programming (MOP) approach is defined. In different economical sectors, e.g. energy markets, there is no consensus about which structures lead to social welfare maximization outcomes. An approach to find adequate architectures allows different market structure instances to be created and simulated, to ease the design and analysis of alternative structures. These alternatives can then be compared and potential design flaws eventually risen by simulation identified. Taking the electricity market as an example, two instances of the proposed architecture are presented, corresponding to the centralized dispatch arrangement common to non restructured markets, and the auction based pool, common to restructured markets. Copyright 2008 ACM.3438Al-Agtash, S., Evolutionary negotiation strategies in emerging electricity markets (2004) Lecture Notes in Artificial Intelligence, 3070, pp. 1099-1104. , ICAISC 2004Batten, D.F., (2000) Discovering Artificial Economics: How Agents Learn and Economies Evolve, , Westview Press, Boulder, ColoradoBower, J., Bunn, D.W., Experimental analysis of the efficiency of uniform-price versus discriminatory auctions in the England and Wales electricity market (2001) Journal of Economic Dynamics and Control, 25 (3-4), pp. 561-592. , MarBunn, D.W., Oliveira, F.S., Agent-based simulation - An application to the New Electricity Trading Arrangements of England and Wales (2001) IEEE Transactions on Evolutionary Computing, 5 (5), pp. 493-503. , OctF. S. Carvalho and C. D. N. Vinhal. Temporal difference methods applied to thermoelectric energy markets: A distributed multi-agents approach. In XV Congresso Brasileiro de Automática (CBA 2004), Gramado, RS, Sept.21-24 2004(2000) FIPA 2000, , http://www.fipa.org/repository/fipa2000.html, Foundation for Intelligent Physical AgentsGenoud, C., Regulation as a game: The role of independent regulatory agencies in the regulatory process (2003) CARR Risk and Regulation Research Student Conference, , London, UK, Sept, London School of Economics and Political ScienceM. Griss and R. Letsinger. Games at work - Agent mediated e-commerce simulation. In Autonomous Agents 2000, Barcelona, Spain, June 2000. HP Laboratories Technical Report HPL-2000-52Harp, S.A., Brignone, S., Wollenberg, B.F., Samad, T., SEPIA: A simulator for electric power industry agents (2000) IEEE Control Systems Magazine, 20 (4), pp. 53-69. , AugLima, W., Freitas, E.N.A., A multi agent based simulator for Brazilian wholesale electricity energy market (2006) Lecture Notes in Computer Science, 4140, pp. 68-77. , X Ibero-American Artificial Intelligence Conference, XVIII Brazilian Artificial Intelligence Symposium, IBERAMIA '2006/SBIA '2006, of, Ribeirão Preto, SP, Oct, SpringerMonclar, F.-R., Quatrain, R., Simulation of electricity markets: A multi-agent approach (2001) International Conference on Intelligent System Application to Power Systems, pp. 207-212. , Budapest, Hungary, June, IEEE Power Engineering SocietyPraça, I., Ramos, C., Vale, Z., Cordeiro, M., MASCEM: A multiagent system that simulates competitive electricity markets (2003) IEEE Intelligent Systems, 18 (6), pp. 54-60. , Nov-DecSimon, H.A., From substantive to procedural rationality (1979) Philosophy and Economic Theory, pp. 65-86. , F. Hahn and M. Hollis, editors, Oxford University PressSycara, K., Decker, K., Williamson, M., Matchmaking and brokering (1996) Proceedings of the Second International Conference on Multi-Agent SystemsTesfatsion, L., Agent-based computational economics: Growing economies from the bottom up (2002) Artificial Life, 8 (1), pp. 55-82Handbook of Computational Economics: Agent-Based Computational Economics (2006) Handbooks in Economics, 2. , L. Tesfatsion and K. L. Judd, editors, of, North HollandVeit, D., Matchmaking in electronic markets: An agent-based approach towards matchmaking in electronic negotiations (2003) Lecture Notes in Artificial Intelligence, 2882. , J. G. Carbonell and J. Siekmann, editors, SpringerWalter, I., (2006) Sistemas Multiagentes em Mercados de Energia Elétrica, , PhD thesis, Faculdade de Engenharia Elétrica e de Computação, Universidade Estadual de Campinas, DecWalter, I., Gomide, F., Simulação de mercados de energia elétrica: Abordagem multi-agentes (2005) VII SBAI Simpósio Brasileiro de Automação Inteligente, , O. R. Saavedra et al, editors, São Luís, MA, SeptI. Walter and F. Gomide. Design of coordination strategies in multiagent systems via genetic fuzzy systems. Soft Computing, 10(10):903-915, Aug. 2006. Special Issue: New Trends in the Design of Fuzzy SystemsWalter, I., Gomide, F., Genetic fuzzy systems to evolve coordination strategies in multiagent systems (2007) International Journal of Intelligent Systems, 22 (9), pp. 971-991. , Special Issue on Genetic Fuzzy SystemsWellman, M.P., A market-oriented programming environment and its application to distributed multicommodity flow problems (1993) Journal of Artificial Intelligence Research, 1 (1), pp. 1-23. , AugWooldridge, M., Jennings, N.R., Intelligent agents: Theory and practice (1995) The Knowledge Engineering Review, 10 (2), pp. 115-15",multimedia,648,not included
10.1016/j.sbspro.2015.01.1180,to_check,core,The Authors. Published by Elsevier Ltd.,2015-02-12 00:00:00,core,marketing decision support using artificial intelligence and knowledge modeling: application to tourist destination management ,,"AbstractKnowledge-based information systems are advanced tools in the hands of the marketer, enabling him to take evidence-based decisions in complex situations. In this paper, advanced data analysis, neural networks and knowledge representation technologies are brought together towards an intelligent information system for tourist destination marketing. In previous work, knowledge engineering methods were proposed for the extraction and modeling from market survey data of factors, associations, clusters and hidden patterns that explain a market phenomenon or customer behavior. The feasibility of managing these findings in a Knowledge-Base, as reusable, sharable and machine understandable knowledge was shown using preliminary results from primary surveys on the tourism of Thessaloniki. In the current work, we present the continuation of these developments, including: (a) the final results of the survey on the tourism of Thessaloniki, (b) a refined Knowledge Base filled with real and validated content derived from the analysis of the full-scale survey data, (c) the extension of the methods with an artificial neural network classifier and (d) the deployment of an inference engine and a query mechanism in order to exercise the knowledge content for decision support. Pilot trials showed that the intelligent system was able to assist users who are not experts in analysis to solve typical destination marketing problems",multimedia,649,not included
10.1109/cec.2003.1299606,to_check,core,IEEE Computer Society,2015-11-26 00:00:00,core,implementation of an immuno-genetic network on a real khepera ii robot,,"The design of autonomous navigation systems for mobile robots, with simultaneous objectives to be satisfied such as garbage collection with integrity maintenance, requires refined coordination mechanisms to deal with modules of elementary behaviour. This paper shows the implementation on a real Khepera II robot of an immuno-genetic network for autonomous navigation that combines an evolutionary algorithm with a continuous immune network model. The proposed immuno-genetic system has the immune network implementing a dynamic process of decision-making, and the evolutionary algorithm defining the network structure. To be able to evaluate the controllers (immune networks) on the evolutionary process, a virtual environment was used for computer simulation, based on the characteristics of the navigation problem. The immune networks obtained by evolution were then analyzed and tested on new situations, presenting coordination capability in simple and more complex tasks. Some preliminary experiments on a real Khepera II robot demonstrate the feasibility of the evolved immune networks. © 2003 IEEE.1420426Dasgupta, D., (1999) Artificial Immune Systems and Their Applications, , Springer-VerlagDorigo, M., Colombetti, M., (1997) Robot Shaping: An Experiment in Behavior Engineering (Intelligent Robotics and Autonomous Agents), , MIT PressDe Castro, L.N., Timmis, J.I., (2002) Artificial Immune Systems: A New Computational Intelligence Approach, , Springer-Verlag: LondonFarmer, J.D., Packard, N.H., Perelson, A.S., The immune system, adaptation and machine learning (1986) Physica, 22 D, pp. 187-204Goldberg, D.E., (1989) Genetic Algorithms in Search Optimization, and Machine Learning, , Addison-Wesley, IncHolland, J.H., (1992) Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence, pp. MI91. , The MIT Press. Ann ArborIshiguro, A., Kondo, T., Watanabe, Y., Shirai, Y., Uchikawa, H., Immunoid: A robot with a decentralized consensus-making mechanism based on the immune system (1996) ICMAS Workshop on Inimumfy: Based Systems, pp. 82-92. , DecemberJeme, N.K., Towards a network theory of the immune system (1974) Ann. Immunol. (Int. Pasteur), 125 C, pp. 373-389Jeme, N.K., Idiotypic Networks and other preconceived ideas (1984) Immunological Rev, 79, pp. 5-24(2003), http//www.k-team.com, URL:, provides additional information about the Khepera II robotsMichelan, R., Von Zuben, F.J., Decentralized control system for autonomous navigation based on an evolved artificial immune network (2002) Proc. of the 2002 Congress on Evolutionary Computation (CEC2002), 2, pp. 1021-1026. , Workshop on Artificial Immune Systems in the 2002 WCCI'2002, Honolulu, Hawaii, May 12-17Nolfi, S., Floreano, D., (2000) Evolutionary Robotics: The Biology, Intelligence, and Technology of Self-organizing Machines, , The MIT PressVargas, P.A., De Castro, L.N., Von Zuben, F.J., Artificial immune systems as complex adaptive systems (2002) Proceedings of the First International Conference on Artificial Immune Systems-ICARIS-2002, pp. 115-123Vargas, P.A., De Castro, L.N., Von Zuben, F.J., Mapping artificial immune systems into learning classifier systems (2003) 1WLCS-2002. to Appear in Lecture Notes in Artificial Intelligence (LNAI), , Springer-VerlagVargas, P.A., De Castro, L.N., Michelan, R., Von Zuben, F.J., An immune learning classifier network for autonomous navigation (2003) Proceedings of the Second International Conference on Artificial Immune Systems (ICARIS'2003), 2787, pp. 69-80. , Timmis. J., Bentley, P. and Hart. E, eds., LNCS, Edinburgh, UKWatanabe, Y., Ishiguro, A., Uchikawa, H., Decentralized behaviour arbitration mechanism for autonomous mobile robot using immune network (1999) Artificial Immune Systems and Their Applications, , D. Dasgupta Editor, Springe",multimedia,650,not included
10.1016/j.procs.2014.11.102,to_check,core,How Gamification Applies for Educational Purpose Specially with College Algebra ,2014-12-31 00:00:00,core,https://core.ac.uk/download/pdf/82329949.pdf,The Authors. Published by Elsevier B.V.,"AbstractGaming environments have been used to teach mathematical topics such as addition and division in a fun manner**http://www.thefreedictionary.com/fun.. However, when it comes to college level mathematical concepts such as the use of the quadratic formula, there are very few software that explain these concepts in a fun way. In this paper, we present a first step towards using video game elements and Artificial Intelligence Tutoring system techniques to teach mathematical concepts such as factoring and the quadratic formula. These concepts are explained in a way that helps learners make a connection between the mathematical concepts and their real life experience. These methods of learning are supported by several studies (Bonwell & Eison, 1991; Donovan & Bransford, 2004; Scarlatos, 2006). We use gamification techniques during the training and test phases to help students learn the mathematical concepts. We then compare the performance of students who used our system (MathDungeon) with that of students who used the most popular math tutoring programs used in US colleges: Assessment and Learning, K-12, Higher Education (ALEKS). The number of students who used MathDungeon and scored above the median score on the test of math performance was greater than the number of students who used ALEKS and scored higher than the median score",multimedia,651,not included
10.3844/ajassp.2014.764.768,to_check,core,VIRTUAL MINING MODEL FOR CLASSIFYING TEXT USING UNSUPERVISED LEARNING,2014-01-01 00:00:00,core,https://core.ac.uk/download/pdf/25869752.pdf,Science Publications,"In real world data mining is emerging in various era, one of its most outstanding performance is held in various research such as Big data, multimedia mining, text mining etc. Each of the researcher proves their contribution with tremendous improvements in their proposal by means of mathematical representation. Empowering each problem with solutions are classified into mathematical and implementation models. The mathematical model relates to the straight forward rules and formulas that are related to the problem definition of particular field of domain. Whereas the implementation model derives some sort of knowledge from the real time decision making behaviour such as artificial intelligence and swarm intelligence and has a complex set of rules compared with the mathematical model. The implementation model mines and derives knowledge model from the collection of dataset and attributes. This knowledge is applied to the concerned problem definition. The objective of our work is to efficiently mine knowledge from the unstructured text documents. In order to mine textual documents, text mining is applied. The text mining is the sub-domain in data mining. In text mining, the proposed Virtual Mining Model (VMM) is defined for effective text clustering. This VMM involves the learning of conceptual terms; these terms are grouped in Significant Term List (STL). VMM model is appropriate combination of layer 1 arch with ABI (Analysis of Bilateral Intelligence). The frequent update of conceptual terms in the STL is more important for effective clustering. The result is shown, Artifial neural network based unsupervised learning algorithm is used for learning texual pattern in the Virtual Mining Model. For learning of such terminologies, this paper proposed Artificial Neural Network based learning algorithm",multimedia,652,not included
https://core.ac.uk/download/213617647.pdf,to_check,core,,2012-06-01 00:00:00,core,conception d'une plateforme d'animats destinée à l'étude d'aglgorithmes d'apprentissage appliqués à la survie en environnement réel,,"RÉSUMÉ
Le présent mémoire résume les travaux de maîtrise réalisés dans l’optique de proposer une plateforme de déploiement et d’analyse d’algorithmes d’apprentissage appliqués aux animats. Les animats désignent des agents minimalement équipés aussi bien sur le plan mécanique qu’en termes de puissance de calcul. Néanmoins, la théorie gravitant autour de l’étude de ces entités vise à mettre en avant des méthodes d’apprentissages permettant à ceux-ci d’évoluer à partir de ce qu’ils auront appris de leur expérience dans l’environnement. À l’image des êtres vivants, les différences de perception du milieu environnant devraient permettre de voir naître des particularités et des comportements différents pour chacun d’entre eux. Par ailleurs, ces animats devraient avoir la faculté de pouvoir communiquer entre eux dans le but de permettre, s’ils le désirent, de pouvoir partager de l’information et de se servir d’une connaissance et d’un apprentissage communs pour assouvir ce qui devrait être leur principal objectif : survivre dans leur environnement. Le domaine de recherche des animats comprend aussi bien l’étude des animats simulés que celle des animats réels. Alors que le premier permet un déploiement facilité par l’abstraction rendue possible par les langages de haut niveau, la nécessité de devoir simuler un environnement avec l’ensemble de ses singularités induit une erreur de modélisation qui n’existe pas lorsque les animats sont physiquement réalisés. Par conséquent, si une telle plateforme était disponible, il serait alors seulement nécessaire de se concentrer sur les algorithmes d’apprentissage plutôt que sur des problèmes de modélisation. Par ailleurs, bien souvent, des algorithmes puissants lors des simulations se retrouvent inadaptés pour des problèmes réels de par le manque de fidélité entre l’environnement simulé et réel. Notamment, qu’adviendrait-il si un capteur devenait défectueux? Si une situation inconnue était rencontrée? Si le bruit ou la précision des capteurs avait mal été modélisés?Dans ce contexte, nous proposons d’étudier et de concevoir ce qui sera la base d’une plateforme de développement et d’analyse d’algorithmes d’apprentissage. À la différence des plateformes existantes, la principale originalité de cette plateforme réside dans la prise en compte des problèmes énergétiques. En effet, pour pouvoir survivre l’animat devrait être capable d’estimer et de prévoir ses dépenses énergétiques présentes et futures et de les prendre en considération dans le choix des tâches à effectuer.----------ABSTRACT This master’s thesis summarizes the achievements realized towards proposing a platform for deployment and analysis of machine learning algorithms applied to animats. Animats are agents minimally equipped on the mechanical plan as well as in terms of computing power. Nevertheless, the theory describing the study of these entities aims at discovering learning methods allowing them to evolve using what they learnt from their experience in the environment. Just like human beings, differences in perception of the surrounding environment should allow peculiarities and several behaviors for each of them. Besides, these animats should be able to communicate between them so as to allow, if needed, to share information and to use common knowledge and learning in order to succeed in what should be their main objective: survive in their environment. 
The research field of animats includes the study of simulated and real animats. While the former allows a facilitated deployment due to the abstraction made possible by high-level languages, the necessity to emulate an environment with all peculiarities can lead to modeling errors that may be avoided when animats are physically built. Consequently, if such a platform was available, it would only be necessary to focus on the learning algorithms rather than on the modelling problems. Besides, very often, powerful algorithms tested on simulation may prove themselves unsuitable for real problems in real environments. For instance, it is hard to predict what would happen if a sensor became defective, if an unexpected situation was met, or if the noise or the precision of the sensors was not properly modeled.In this context, we suggest studying and designing what will be the base of a platform for development and analysis of learning algorithms. Unlike the existing platforms, the main originality of this platform lies in the consideration of energy constraints. Indeed, to enhance its survivability, the animat should be able to estimate and to plan its present and future energy consumption and to consider it in the choices made. Besides, the objective was to design an animat sufficiently equipped to realize simple tasks but relatively primitive to make it necessary to take advantage of algorithmic parades, like for example the pooling of information. Therefore, the computing power embedded in the animat was purposely reduced to force the relocation of processing that proves too complex. On the other hand, a reduced size and a moderate price are necessary features to enable deployment of a colony of animats",multimedia,653,not included
https://core.ac.uk/download/pdf/54849353.pdf,to_check,core,CU Scholar,2012-01-01 00:00:00,core,discuss: toward a domain independent representation of dialogue,,"While many studies have demonstrated that conversational tutoring systems have a positive effect on learning, the amount of manual effort required to author, design, and tune dialogue behaviors remains a major barrier to widespread deployment and adoption of these systems. Such dialogue systems must not only understand student speech, but must also endeavor to keep students engaged while scaffolding them through the curriculum. Crafting robust, natural tutoring interactions typically involves writing tightly scripted behaviors for a wide variety of student responses and scenarios.
Combining statistical machine learning with corpus-based methods in natural language processing presents a possible path to reducing this effort. Advances in reinforcement learning have been applied toward dialogue systems to learn optimal behaviors for a given task. However, these learned dialogue policies are tightly coupled to the specific dialogue system implementation. For content-rich applications such as intelligent tutoring systems, there is an immediate need to learn tutoring strategies and dialogue behaviors that can be leveraged across a variety of materials, concepts and lessons. Further generalization will require an intermediate representation of dialogue that can abstract the conversation to its underlying action, function, and content.
This work introduces the Dialogue Schema Unifying Speech and Semantics (DISCUSS), an intermediate linguistic representation that captures the semantics and pragmatics of speech while also allowing for domain-independent modeling of tutorial dialogue. To better understand the benefits of the DISCUSS representation, a corpus of computer-mediated tutorial dialogues was manually tagged with DISCUSS labels. These data were then used for three different tasks: utterance classification, dialogue move selection, and learning gains prediction.
System performance in these tasks demonstrate the utility and viability of the DISCUSS representation for analyzing and automating dialogue interactions. Utterance classifiers achieve DISCUSS labeling performance on par with inter-annotator agreement levels. System performance in ranking and selecting follow-up questions illustrates the usefulness of DISCUSS-based features for modeling and identifying the factors behind human decision making when teaching. Correlating features of the dialogue with measured learning gains in students shows how DISCUSS-derived metrics provide a detailed account of real tutoring strategies and student behaviors. Together these results represent a step toward more domain-independent mechanisms for modeling dialogue",multimedia,654,not included
https://core.ac.uk/download/296274972.pdf,to_check,core,Italiano LinguaDue,2013-07-09 00:00:00,core,la lingua2 nel web. prospettive digitali per la didattica dell’italiano a stranieri,10.13130/2037-3597/3124,"L'articolo si compone di due parti. Nella prima si espongono alcuni punti focali del dibattito sul rapporto tra glottodidattica e tecnologie digitali (dai dispositivi mobili ai software per la comunicazione, fino agli attuali socialnetwork). In particolare l’attenzione è rivolta sul fatto che la lingua della comunicazione digitale costituisce una varietà linguistica dal confronto con la quale l’insegnamento di una L2 non può ormai prescindere, e che le tecnologie digitali della comunicazione, se correttamente adottate fuori e dentro la classe, consentono di ridurre il confine tra apprendimento guidato e acquisizione spontanea di una lingua seconda. Filo conduttore della riflessione è il concetto di “competenza comunicativa digitale”, che si aggiunge alle componenti della competenza linguistico-comunicativa indicate dal Quadro Comune Europeo di Riferimento per le Lingue. Nella seconda parte del lavoro, dal taglio più didattico, si presenta una serie di strumenti e di esperienze di insegnamento/apprendimento della L2 con l’uso delle tecnologie digitali. In particolare ci si sofferma sulle opportunità offerte dal Web 2.0, simulando la creazione di una piattaforma per l’insegnamento/apprendimento dell’italiano L2 completamente online. Non si tratta di una semplice piattaforma per l’e-learning (o d-learning), ma di una vera e propria rete per l’insegnamento, l’apprendimento, la comunicazione e la formazione che trasforma il modo in cui si usa, si apprende e si insegna una L2. L2 on the Web. Digital perspectives for teaching italian to foreignersThe article consists of two parts. In the first we illustrate some focal points in the debate on the relationship between language teaching and digital technologies (from mobile devices to communication software and current social networks). In particular, we focus on the fact that the language of digital communication is a linguistic variety which L2 teaching can no longer ignore, and that digital communication technologies, if utilized properly both inside and outside the classroom, reduce the boundary between guided learning and the spontaneous acquisition of a second language. The underlying theme is the concept of “digital communicative competence”, added to linguistic and communicative competences indicated by the Common European Framework of Reference for Languages. In the second, more didactic part, is a set of tools and experiences regarding L2 teaching/learning using digital technologies. In particular, we focus on the opportunities offered by the Web 2.0, simulating the creation of a platform for online L2 teaching/learning. This is not just a platform for e-learning (or d-learning), but a real network for teaching, learning, communication and training that transforms the way one learns and teaches a L2",multimedia,655,not included
https://core.ac.uk/download/230197532.pdf,to_check,core,İTÜDERGİSİ/d,2011-05-12 00:00:00,core,a low-power multilevel cmos classifier circuit,,"İnsanların g&uuml;nl&uuml;k yaşamında belirli bir sesi, g&ouml;r&uuml;nt&uuml;y&uuml; veya analog bir veriyi tanımak i&ccedil;in kullandıkları kuralları tanımlamak olduk&ccedil;a karmaşık bir dizi işlem gerektirmektedir ve hatta bu kuralları tanımlamak bazen m&uuml;mk&uuml;n olamamaktadır. Oysa pratikte karşılaşılan &ouml;r&uuml;nt&uuml; tanıma olaylarını, yazılım ve donanım tabanlı tanıma uygulamalarında belirli kriterlere oturtmak m&uuml;mk&uuml;nd&uuml;r. Sınıflandırma y&ouml;ntemleri ilk olarak &ouml;r&uuml;nt&uuml; sınıflandırma adı altında g&ouml;r&uuml;lmeye başlanmış ve ilk algoritmalarda basit yapılar ele alınmıştır; ilk ger&ccedil;eklenen yapıda en yakın komşu yakınsaması kullanılmıştır. Sınıflandırma işlemi, benzer &ouml;zellik taşıyan objelerin başka farklı &ouml;zellikte olanlardan ayırt edilmesi şeklinde tanımlanabilir ve otomatik hedef belirleme, yapay zek&acirc;, yapay sinir ağları, analog-sayısal d&ouml;n&uuml;şt&uuml;r&uuml;c&uuml;ler, kuantalama, tıbbi tanı, istatistik gibi &ccedil;eşitli alanlarda kullanılır. Dolaysıyla da, g&uuml;n&uuml;m&uuml;zde, gerek ger&ccedil;ek d&uuml;nyada gerekse sayısal d&uuml;nyada verilerin sınıflandırılması b&uuml;y&uuml;k &ouml;nem taşımaktadır. Bug&uuml;ne kadar sınıflandırma işlemi genellikle &ccedil;eşitli algoritmalar yardımıyla yazılımsal olarak yapılmaktaydı, oysa bir&ccedil;ok uygulamada, sınıflandırma işlemini daha hızlı ve ger&ccedil;ek zamanda yapmak gerektiğinden bu algoritmaların donanımsal olarak ger&ccedil;eklenmeleri &ccedil;ok daha yararlı olmaktadır. Ayrıca g&uuml;n&uuml;m&uuml;zde portatif cihazların da artmasından dolayı donanımsal olarak ger&ccedil;eklenecek cihazlarda da g&uuml;&ccedil; t&uuml;ketimi b&uuml;y&uuml;k &ouml;nem kazanmıştır. Dolayısıyla sınıflandırıcı devrelerin de bu ihtiya&ccedil;ları karşılayacak şekilde tasarlanması gerekmektedir. Bu makalede akım-modlu d&uuml;ş&uuml;k g&uuml;&ccedil;te &ccedil;alışan bir sınıflandırıcı devresi sunulmaktadır. &Ouml;nerilen sınıflandırıcı devresi, temel bir bloktan yararlanmaktadır; bu temel bloklar kullanılarak daha gelişmiş sınıflandırıcı yapılarının ger&ccedil;ekleştirilebileceği g&ouml;sterilmiştir. &Ouml;nerilen devrenin benzetimleri i&ccedil;in 0.35 &micro;m AMS CMOS teknoloji parametreleri kullanılmıştır. Ayrıca &ccedil;ekirdek devre adı verilen temel bloğun, tek boyutlu ve iki boyutlu sınıflandırıcı yapılarının benzetim sonu&ccedil;ları verilmiştir.&nbsp; &nbsp;Anahtar Kelimeler: Sınıflandırıcı devreler, CMOS, akım modlu, d&uuml;ş&uuml;k g&uuml;&ccedil;.In the everyday life of humans, to define the rules used to recognize a certain sound, image or an analog data necessitates a sequence of complex processes which sometimes becomes impossible to accomplish. However, to develop well defined software and hardware based criteria in the application of pattern recognition problems, is possible. The aim of classification can be defined as to assign an unknown object to a class containing similar objects (or to distinguish objects having the same properties from those not possessing). Classification is especially important in the real world applications or in the digital world. Basic classification methods using nearest neighbourhood algorithm have first been seen in early sixties under the subject tile"" pattern recognition.""  Classification is used in a huge variety of applications such as automatic target identification, artificial neural networks, artificial intelligence, template matching, pattern recognition, analog to digital converters, quantization, medical diagnosis, statistics etc. Therefore nowadays, be it in the real or digital world, data classification is becoming increasingly important. But until recently, major work on classification was on developing algorithms used in software packages whereas, in many applications it is becoming more and more important to classify data much faster and in real time, entailing the need for hardware realization of these algorithms. Software approaches are not practical for real time applications, the processing is computationally very expensive, consuming a lot of Central Processing Unit (CPU) time when implemented as software running on general purpose computers. So in literature hardware implementation of classifier topologies become necessary. Also in literature hardware realized classifiers which are designed to work in low power operation; moreover some of these hardware classifiers do not have custom tunability. So they can only be used for a specific application. The recent developments in electronics technology has created a perfect medium for the hardware realization of classifier structures which, in turn, will render many classifier application prospects feasible in real time. This paper targets the design and application to real world problems of tunable, low power new classifier circuits using CMOS technology. So, a low-power CMOS implementation of a multi-input data classifier with several output levels is presented. The proposed circuit operates in current-mode and can classify several types of analog vector data. An architecture is developed comprising a threshold circuit based on CMOS transistors operating in subthreshold region. To this purpose a one dimensional classifier, called core circuit is proposed. The core circuit also works as a one-dimensional classifier. As this circuit is designed to operate in current-mode the input and the output data is provided to the core circuit with currents. So by interconnecting several core circuits and adding the output currents a multi output classifier can be obtained. Also, com-bining several core circuits in groups in such a way that each group has identical input current (different from the others), a multi-dimensional, multi-level output classifier can be obtained. Also, numerous efforts in balancing the trade off between power consumption, area and speed have resulted in an acceptable performance. On the other hand, the rapid increasing use of battery operated portable equipment in application areas such as telecommunications and medical electronics increases the importance of low-power and small sized VLSI circuits' technologies. One solution to achieve low-power and acceptable performance is to operate the transistors in the subthreshold region. The CMOS transistors working in subthreshold region are suitable only for specific applications which need, not very high performance, but low power consumption.The primary aim of this paper is to develop a low power classifier circuit with n inputs and externally tunable decision regions with different output amplitude for each region. Due to the subthreshold operation of the transistors in the proposed core circuit, very low power consumption becomes possible. The proposed core circuit is constructed with two threshold and a subtractor circuit. The SPICE simulation of the threshold circuit, core circuit, one dimensional and two dimensional classifier circuits are given. Using 0.35 µm parameters of AMS CMOS technology, SPICE simulations are performed and a  low-power, custom tunable classifier circuit is  realized. Because of the parallel processing characteristic of the circuit, it is well suited for real-world applications. Keywords: Classifier circuits, CMOS, current-mode, low-power",multimedia,656,not included
10.1016/s0743-1066(98)10003-1,to_check,core,A framework to incorporate non-monotonic reasoning into constraint logic programming ,1998-12-31 00:00:00,core,https://core.ac.uk/download/pdf/81105317.pdf,'Elsevier BV',"AbstractImpressive work has been done in the last years concerning the meaning of negation and disjunction in logic programs, but most of this research concentrated on propositional programs only. While it suffices to consider the propositional case for investigating general properties and the overall behavior of a semantics, we feel that for real applications and for computational purposes an implementation should be able to handle first-order programs without grounding them. In this paper we present a theoretical framework by defining a calculus of program transformations that apply directly to rules with variables and function symbols. Our main results are that (a) this calculus is weakly confluent for arbitrary programs (i.e., it has the normal form property), (b) it is weakly terminating for Datalog ∨,¬ programs, (c) for finite ground programs it is equivalent to a weakly terminating calculus introduced by Brass and Dix (S.Brass, J.Dix, in: J.Dix, L.Pereira, T.Przymusinski (Eds.), Non-monotonic Extensions of Logic Programming, Springer Lecture Notes in Artificial Intelligence, Vol.927, Springer, Berlin, 1995, pp. 127–155), and (d) it approximates a generalization of Disjunctive Well-founded semantics (D-WFS) for arbitrary programs. We achieve this by transforming program rules into rules with equational constraints thereby using heavily methods and techniques from constraint logic programming (CLP). In particular, disconnection-methods play a crucial role. In principle, any constraint theory known from CLP can be exploited in the context of non-monotonic reasoning, not only equational constraints over the Herbrand domain. However, the respective constraint solver must be able to treat negative constraints of the considered constraint domain. In summary, this work yields the basis for a general combination of two paradigms: constraint logic programming and non-monotonic reasoning",multimedia,657,not included
10.1016/j.softx.2017.09.002,to_check,core,Elsevier,,core,confocalgn: a minimalistic confocal image generator,,"Validating image analysis pipelines and training machine-learning segmentation algorithms require images with known features. Synthetic images can be used for this purpose, with the advantage that large reference sets can be produced easily. It is however essential to obtain images that are as realistic as possible in terms of noise and resolution, which is challenging in the field of microscopy. We describe ConfocalGN, a user-friendly software that can generate synthetic microscopy stacks from a ground truth (i.e. the observed object) specified as a 3D bitmap or a list of fluorophore coordinates. This software can analyze a real microscope image stack to set the noise parameters and directly generate new images of the object with noise characteristics similar to that of the sample image. With a minimal input from the user and a modular architecture, ConfocalGN is easily integrated with existing image analysis solutions. Keywords: Synthetic image, Image analysis, Bioinformatic",multimedia,658,included
10.3233/978-1-61499-806-8-196,to_check,core,IOS Press,,core,multivariate regression with incremental learning of gaussian mixture models,,"La publicació definitiva d'aquest treball està disponible a IOS Press a través de http://dx.doi.org/10.3233/978-1-61499-806-8-196Within the machine learning framework, incremental learning of multivariate spaces is of special interest for on-line applications. In this work, the regression problem for multivariate systems is solved by implementing an efficient probabilistic incremental algorithm. It allows learning high-dimensional redundant non-linear maps by the cumulative acquisition of data from input-output systems. The proposed model is aimed at solving prediction and inference problems. The implementation introduced in this work allows learning from data batches without the need of keeping them in memory afterwards. The learning architecture is built using Incremental Gaussian Mixture Models. The Expectation-Maximization algorithm and general geometric properties of Gaussian distributions are used to train the models. Our current implementation can produce accurate results fitting models in real multivariate systems. Results are shown from testing the algorithm for both situations, one where the incremental learning is demonstrated and the second where the performance to solve the regression problem is evaluated on a toy example.Peer Reviewe",multimedia,659,not included
10.1109/fccm.2017.58,to_check,2017 IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM),IEEE,2017-05-02 00:00:00,ieeexplore,accelerating large-scale graph analytics with fpga and hmc,https://ieeexplore.ieee.org/document/7966655/,"Graph analytics that explores the relationship among interconnected entities is becoming increasingly important due to its broad applicability from machine learning to social science. However, one major challenge for graph processing systems is the irregular data access pattern of graph computation which can significantly degrade the performance. The algorithms, software, and hardware that have been tailored for mainstream parallel applications are, as a result, generally not effective for massive-scale sparse graphs from the real world due to their complexity and irregularity. To address the performance issues in large-scale graph analytics, we combine the emerging Hybrid Memory Cube (HMC) with a modern FPGA in order to achieve exceptional random access performance without any loss of flexibility or efficiency in computation. In particular, we develop collaborative software/hardware techniques to perform a level-synchronized breadth first search (BFS) on the FPGA-HMC platform. From the software perspective, we develop an architecture-aware graph clustering algorithm that fully exploits the platform's capability to improve data locality and memory access efficiency. For each input graph, this algorithm provides an efficient data layout that allows the FPGA to coalesce memory requests into the largest possible HMC payload requests so that the number of memory requests, which is the primary factor in runtime, can be minimized. From the hardware perspective, we further improve the FPGA-HMC graph processor architecture by adding a merging unit. The merging unit takes the best advantage of the increased data locality resulting from graph clustering. We evaluated the performance of our BFS implementation using the AC-510 development kit from Micron over a set of benchmarks from a wide range of applications. We observed that the combination of the clustering algorithm and the merging hardware achieved 2.8 × average performance improvement compared to the latest FPGA-HMC based graph processing system.",science,660,included
10.1109/bigdata.2014.7004374,to_check,2014 IEEE International Conference on Big Data (Big Data),IEEE,2014-10-30 00:00:00,ieeexplore,access-averse framework for computing low-rank matrix approximations,https://ieeexplore.ieee.org/document/7004374/,"Low-rank matrix approximations play important roles in many statistical, scientific, and engineering applications. To compute such approximations, different algorithms have been developed by researchers from a wide range of areas including theoretical computer science, numerical linear algebra, statistics, applied mathematics, data analysis, machine learning, and physical and biological sciences. In this paper, to combine these efforts, we present an “access-averse” framework which encapsulates some of the existing algorithms for computing a truncated singular value decomposition (SVD). This framework not only allows us to develop software whose performance can be tuned based on domain specific knowledge, but it also allows a user from one discipline to test an algorithm from another, or to combine the techniques from different algorithms. To demonstrate this potential, we implement the framework on multicore CPUs with multiple GPUs and compare the performance of two representative algorithms, blocked variants of matrix power and Lanczos methods. Our performance studies with large-scale graphs from real applications demonstrate that, when combined with communication-avoiding and thick-restarting techniques, the Lanczos method can be competitive with the power method, which is one of the most popular methods currently used for these applications. InIn addition, though we only focus on the truncated SVDs, the two computational kernels used in our studies, the sparse-matrix dense-matrix multiply and tall-skinny QR factorization, are fundamental building blocks for computing low-rank approximations with other objectives. Hence, our studies may have a greater impact beyond the truncated SVDs.",science,661,not included
10.1109/incos.2009.51,to_check,2009 International Conference on Intelligent Networking and Collaborative Systems,IEEE,2009-11-06 00:00:00,ieeexplore,an architecture for adaptive collaboration support guided by learning design,https://ieeexplore.ieee.org/document/5369352/,"A CSCL environment provides support to manage collaborative tasks. However, these systems do not usually provide the personalization features required to adapt the learning experience to the student needs, a drawback that can affect the collaboration objective and ultimately a successful learning. To alleviate this disadvantage we propose an architecture that provides adaptive collaboration support for a CSCL environment framed in an open and standards-based LMS. Our proposal combines adaptation rules defined in IMS Learning Design specification and dynamic support through recommendations via an accessible and adaptive guidance system. The implementation offers CSCL courses following a methodology called Collaborative Logical Framework. This system has been tested on a real world scenario at the Madrid Science Week 2009.",science,662,not included
10.1109/wccit.2013.6618739,to_check,2013 World Congress on Computer and Information Technology (WCCIT),IEEE,2013-06-24 00:00:00,ieeexplore,design and application of academic frontier-based approach in engineering courses,https://ieeexplore.ieee.org/document/6618739/,"The reform of teaching approach has been the focus of modern opening educational reform and research. The real science thrives on both revolutionary and frontier progress that the textbook can not include. Based on the analysis of the characteristics of several the engineering courses, we build the academic frontier-based approach (AFA) for the purpose of combination of the basic and academic frontier knowledge by the application of the theory of constructivist learning. AFA is a learner-centred instructional approach used to promote active and deep learning by involving learners in learning academic frontier topics in an open and collaborative environment. The design and implementation of the approach enrich the teaching mode and the content of the key curriculums in depth, have an effective role in expansion knowledge, fun learning, and dig the potential of the learners in the group and team.",science,663,not included
10.1109/smc-it.2009.40,to_check,2009 Third IEEE International Conference on Space Mission Challenges for Information Technology,IEEE,2009-07-23 00:00:00,ieeexplore,rapid prototyping of planning &amp; scheduling tools,https://ieeexplore.ieee.org/document/5226820/,"The Advanced Planning and Scheduling Initiative, or APSI, is an ESA programme to design and implement an Artificial Intelligence (AI) software infrastructure for planning and scheduling that can generically support different types and classes of space mission operations. The goal of the APSI is twofold: (1)~creating a software framework to improve the cost-effectiveness and flexibility of mission planning support tool development; (2)~bridging the gap between AI planning and scheduling technology and the world of space mission planning. A key aspect of the success of this project is the presence of a flexible timeline representation module that allows to exploit alternatives in the modeling of mission features. This paper shows an example of such a flexibility by using a real problem in the space realm - the HERSCHEL Science Long Term Planning process.",science,664,not included
10.1109/iccad45719.2019.8942103,to_check,2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),IEEE,2019-11-07 00:00:00,ieeexplore,tucker tensor decomposition on fpga,https://ieeexplore.ieee.org/document/8942103/,"Tensor computation has emerged as a powerful mathematical tool for solving high-dimensional and/or extreme-scale problems in science and engineering. The last decade has witnessed tremendous advancement of tensor computation and its applications in machine learning and big data. However, its hardware optimization on resource-constrained devices remains an (almost) unexplored field. This paper presents an hardware accelerator for a classical tensor computation framework, Tucker decomposition. We study three modules of this architecture: tensor-times-matrix (TTM), matrix singular value decomposition (SVD), and tensor permutation, and implemented them on Xilinx FPGA for prototyping. In order to further reduce the computing time, a warm-start algorithm for the Jacobi iterations in SVD is proposed. A fixed-point simulator is used to evaluate the performance of our design. Some synthetic data sets and a real MRI data set are used to validate the design and evaluate its performance. We compare our work with state-of-the-art software toolboxes running on both CPU and GPU, and our work shows 2.16 - 30.2× speedup on the cardiac MRI data set.",science,665,not included
10.1109/ijcnn.2008.4633828,to_check,2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),IEEE,2008-06-08 00:00:00,ieeexplore,wafer-scale integration of analog neural networks,https://ieeexplore.ieee.org/document/4633828/,"This paper introduces a novel design of an artificial neural network tailored for wafer-scale integration. The presented VLSI implementation includes continuous-time analog neurons with up to 16 k inputs. A novel interconnection and routing scheme allows the mapping of a multitude of network models derived from biology on the VLSI neural network while maintaining a high resource usage. A single 20 cm wafer contains about 60 million synapses. The implemented neurons are highly accelerated compared to biological real time. The power consumption of the dense interconnection network providing the necessary communication bandwidth is a critical aspect of the system integration. A novel asynchronous low-voltage signaling scheme is presented that makes the wafer-scale approach feasible by limiting the total power consumption while simultaneously providing a flexible, programmable network topology.",science,666,not included
10.1109/access.2020.2997921,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,a physics-based neural-network way to perform seismic full waveform inversion,https://ieeexplore.ieee.org/document/9102272/,"Seismic full waveform inversion is a common technique that is used in the investigation of subsurface geology. Its classic implementation involves forward modeling of seismic wavefield based on a certain type of wave equation, which reflects the physics nature of subsurface seismic wavefield propagation. However, obtaining a good inversion result using traditional seismic waveform inversion methods usually comes with a high computational cost. Recently, with the emerging popularity of deep learning techniques in various computer vision tasks, deep neural network (DNN) has demonstrated an impressive ability in dealing with complex nonlinear problems, including seismic velocity inversion. Now, extensive efforts have been made in developing a DNN architecture to tackle the problem of seismic velocity inversion, and promising results have been achieved. However, due to the dependence of a labeled dataset, i.e., the barely accessible true velocity model corresponding to real seismic data, the current supervised deep learning inversion framework may suffer from limitations on generalization. One possible solution to mitigate this issue is to impose the governing physics into this kind of purely data-driven method. Thus, following the procedures of traditional seismic full waveform inversion, we propose a seismic waveform inversion network, namely SWINet, based on wave-equation-based forward modeling network cells. By treating the single-shot observation data and its corresponding shot position as training data pairs, the inverted velocity model can be obtained as the trainable network parameters. Moreover, since the proposed seismic waveform inversion method is performed in a neural-network way, its implementation and inversion effect could benefit from some built-in tools in Pytorch, such as automatic differentiation, Adam optimizer and mini-batch strategy, etc. Numerical examples indicate that the SWINet method may possess great potential in resulting a good velocity inversion effect with relatively fast convergence and lower computation cost.",science,667,not included
10.1109/roman.2006.314387,to_check,ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication,IEEE,2006-09-08 00:00:00,ieeexplore,adaptive social skills for robots interacting with virtual characters in real worlds,https://ieeexplore.ieee.org/document/4107778/,"We propose the implementation of a new interaction type that allows the creation of adaptive social relationships between robots and virtual characters in a real world environment, using reinforcement learning. We present the implementation of a storytelling scenario, which results in an immersion experience for the robot. The robot is able to interact and learn dynamically from the virtual character",science,668,not included
10.1109/bigdata.2018.8621926,to_check,2018 IEEE International Conference on Big Data (Big Data),IEEE,2018-12-13 00:00:00,ieeexplore,harnessing the nature of spam in scalable online social spam detection,https://ieeexplore.ieee.org/document/8621926/,"Disinformation in social networks has been a worldwide problem. Social users are surrounded by a huge volume of malicious links, biased comments, fake reviews, or fraudulent advertisements, etc. Traditional spam detection approaches propose a variety of statistical feature-based models to filter out social spam from a historical dataset. However, they omit the real word situation of social data, that is, social spam is fast changing with new topics or events. Therefore, traditional approaches cannot effectively achieve online detection of the ""drifting"" social spam with a fixed statistic feature set. In this paper, we present Sifter, a system which can detect online social spam in a scalable manner without the labor-intensive feature engineering. The Sifter system is two-fold: (1) a decentralized DHT-based overlay deployment for harnessing the group characteristics of social spam activities within a specific topic/event; (2) a social spam processing with the support of Recurrent Neural Network (RNN) to get rid of the traditional manual feature engineering. Results show that Sifter achieves graceful spam detection performances with the minimal size of data and good balance in group management.",science,669,included
10.1109/osscom.2016.7863679,to_check,2016 2nd International Conference on Open Source Software Computing (OSSCOM),IEEE,2016-12-03 00:00:00,ieeexplore,on tackling social engineering web phishing attacks utilizing software defined networks (sdn) approach,https://ieeexplore.ieee.org/document/7863679/,"Web phishing attacks are one of the challenging security threats. Phishing depends on humans' behavior but not protocols and devices vulnerabilities. In this work, software defined networking (SDN) will be tailored to tackle phishing attacks. In SDN, network devices forward received packets to a central point `controller' that makes decision on behalf of them. This approach allows more control and management over network devices and protocol. In this work, we propose a neural network based phishing prevention algorithm (PPA) that is implemented utilizing Ryu, an open source, SDN controller. The PPA algorithm has been tested in a home network that is constructed with HP2920-24G switch. Moreover, a phished version of Facebook, Yahoo and Hotmail login pages have been written and hosted on three different free hosting domains. PPA has detected all of the phished versions and allowed the access to real version of these services.",science,670,included
10.1109/infocom.2017.8057066,to_check,IEEE INFOCOM 2017 - IEEE Conference on Computer Communications,IEEE,2017-05-04 00:00:00,ieeexplore,sybilscar: sybil detection in online social networks via local rule based propagation,https://ieeexplore.ieee.org/document/8057066/,"Detecting Sybils in online social networks (OSNs) is a fundamental security research problem as adversaries can leverage Sybils to perform various malicious activities. Structure-based methods have been shown to be promising at detecting Sybils. Existing structure-based methods can be classified into two categories: Random Walk (RW)-based methods and Loop Belief Propagation (LBP)-based methods. RW-based methods cannot leverage labeled Sybils and labeled benign users simultaneously, which limits their detection accuracy, and they are not robust to noisy labels. LBP-based methods are not scalable, and they cannot guarantee convergence. In this work, we propose SybilSCAR, a new structure-based method to perform Sybil detection in OSNs. SybilSCAR maintains the advantages of existing methods while overcoming their limitations. Specifically, SybilSCAR is Scalable, Convergent, Accurate, and Robust to label noises. We first propose a framework to unify RW-based and LBP-based methods. Under our framework, these methods can be viewed as iteratively applying a (different) local rule to every user, which propagates label information among a social graph. Second, we design a new local rule, which SybilSCAR iteratively applies to every user to detect Sybils. We compare SybilSCAR with a state-of-the-art RW-based method and a state-of-the-art LBP-based method, using both synthetic Sybils and large-scale social network datasets with real Sybils. Our results demonstrate that SybilSCAR is more accurate and more robust to label noise than the compared state-of-the-art RW-based method, and that SybilSCAR is orders of magnitude more scalable than the state-of-the-art LBP-based method and is guaranteed to converge. To facilitate research on Sybil detection, we have made our implementation of SybilSCAR publicly available on our webpages.",science,671,not included
10.1109/access.2018.2868427,to_check,IEEE Access,IEEE,2018-01-01 00:00:00,ieeexplore,empirical study on the evolution of developer social networks,https://ieeexplore.ieee.org/document/8454243/,"Software development is incredibly complex. Specifically, open-source software (OSS) development requires developers to collaborate with each other to conduct their work. Because software systems are evolving with time, collaboration among software developers may affect the quality of evolved software. The OSS developer teams collaborate in various tasks, including communications, coordination, and making various social collaboration in the OSS projects (e.g., bug/issue report, discussion, code revisions, and so on) without access restriction, and all these activities are used to generate an implicit developer social network (DSN). The DSN that is based on a bug tracking system is one of the most important DSNs that reflect the real collaboration between developers. As the software system evolves, the DSN evolves. This paper describes an empirical study of the evolution of DSNs on OSS projects collected from GitHub. Four perspectives were used: social network analysis, DSN as an ecosystem, community evolution patterns, and the core-periphery structure. The results demonstrated the DSNs over time have followed the power law degree distribution with +1% or more as an increasing rate to be more fitting overtime. DSNs over time are considered a small-world community. DSNs over time exhibits about 55% diversity with 75% of evenness between the developers to contribute in different OSS projects in the same environment. Moreover, DSNs over time have a few developers as core members and large developers as peripheral members. Finally, about 10% of developers changed their positions frequently over time.",science,672,not included
10.1049/cp:19990285,to_check,"Image Processing And Its Applications, 1999. Seventh International Conference on (Conf. Publ. No. 465)",IET,1999-07-15 00:00:00,ieeexplore,a neural vision based controller for a robot footballer,https://ieeexplore.ieee.org/document/791354/,"Robot football is growing in popularity both as a research topic and as a sporting event. The football setting provides rich interaction possibilities and a ready source of competition in an environment containing both predictable and non-deterministic elements. Successful players must be able to react quickly in real time, exhibit multiple competences and choose between several possibly conflicting goals. Opportunities exist to explore reflexive behaviour, strategic behaviour and even communication and social behaviour in team events. At the same time, artificial neural networks are increasingly being used in robot controllers to explore new biologically-inspired ideas relating to perception, memory and motor control. The research described in this paper attempts to combine these two areas of study to produce a framework for a neurally based and visually guided football-playing controller. A controller architecture is proposed in which a small set of high-level features in the robot's environment are extracted from raw image data by using a feedforward neural network. These feature signals, collectively termed the ""feature bus"", are then available for use by other controller modules. The feature bus signals are sufficiently general and high-level to be used with many different controller strategies, and their low dimensionality compared to the raw visual input makes the implementation of learning controllers more feasible.",science,673,not included
10.1109/ccwc47524.2020.9031269,to_check,2020 10th Annual Computing and Communication Workshop and Conference (CCWC),IEEE,2020-01-08 00:00:00,ieeexplore,an automated framework for real-time phishing url detection,https://ieeexplore.ieee.org/document/9031269/,"An increasing number of services, including banking and social networking, are being integrated with world wide web in recent years. The crux of this increasing dependence on the internet is the rise of different kinds of cyberattacks on unsuspecting users. One such attack is phishing, which aims at stealing user information via deceptive websites. The primary defense against phishing consists of maintaining a black list of the phishing URLs. However, a black list approach is reactive and cannot defend against new phishing websites. For this reason, a number of research have been done on using machine learning techniques to detect previously unseen phishing URLs. While they show promising results, any such implementation is yet to be seen. This is because 1) little work has been done on developing a complete end-to-end framework for phishing URL detection 2) it is prohibitively slow to detect phishing URLs using machine learning algorithms. In this work we address these two issues by formulating a robust framework for fast and automated detection of phishing URLs. We have validated our framework with a real dataset achieving 87% accuracy in a real-time setup.",science,674,not included
10.1109/iccsre.2019.8807586,to_check,2019 International Conference of Computer Science and Renewable Energies (ICCSRE),IEEE,2019-07-24 00:00:00,ieeexplore,arab sign language recognition with convolutional neural networks,https://ieeexplore.ieee.org/document/8807586/,"The implementation of an automatic recognition system for Arab sign language (ArSL) has a major social and humanitarian impact. With the growth of the deaf-dump community, such a system will help in integrating those people and enjoy a normal life. Like other languages, Arab sign language has many details and diverse characteristics that need a powerful tool to treat it. In this work, we propose a new system based on the convolutional neural networks, fed with a real dataset, this system will recognize automatically numbers and letters of Arab sign language. To validate our system, we have done a comparative study that shows the effectiveness and robustness of our proposed method compared to traditional approaches based on k-nearest neighbors (KNN) and support vector machines (SVM).",science,675,not included
10.1145/3341161.3343690,to_check,2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),IEEE,2019-08-30 00:00:00,ieeexplore,concept drift in bias and sensationalism detection: an experimental study,https://ieeexplore.ieee.org/document/9073558/,"Due to easy dissemination of news in social media and the Web, there has been an increasing rise of disinformation on important political issues like elections in recent years. Computational solutions for automatic bias and sensationalism detection for news articles can have tremendous impact if used in the right way. Because news is an ever-shifting domain, concept drift is an issue that must be dealt with in any real-world computational news classification system that relies on features and trained machine learning models. Yet, an empirical study of concept drift in such systems, especially popular systems released recently as open-source and used within organizations, has been lacking thus far. This short paper reports results on an empirical study specifically designed to assess concept drift, using an open-source, popular computational news classification system, on real news data crawled from the Web. We find that even a gap of two years (2017 vs. 2019) can lead to significant concept drift, a far narrower gap than observed in traditional machine learning domains, making deployment of pre-trained or openly available computational news classification models an ethically suspect issue.",science,676,not included
10.1109/emctech49634.2020.9261512,to_check,2020 International Conference on Engineering Management of Communication and Technology (EMCTECH),IEEE,2020-10-22 00:00:00,ieeexplore,digital socio-political communication and its transformation in the technological evolution of artificial intelligence and neural network algorithms,https://ieeexplore.ieee.org/document/9261512/,"The study aims to analyze the specifics of determining the subjects of digital social and political communication in the context of the development of artificial intelligence technologies and neural network algorithms. The work uses a case-study design. As a research methodology, the method of critical analysis of the digital communications practice in the socio-political sphere, as well as discourse analysis of modern scientific research in the field of the development of artificial intelligence and neural network algorithms, are used. It is concluded that the implementation of technological solutions based on artificial intelligence and neural network algorithms into the processes of socio-political communications creates a problem of defining the subject of communication acts in the socio-political sphere. Society may face such communication practices in which hybrid subjectness is realized. In the conditions of hybrid subjectness, both real subjects and programmed neural network algorithms acting as real subjects (but only imitating own subjectivity) interact in common communication space. The originality of the work lies in the formulation of the author's hypothesis about the emergence of the phenomenon of hybrid subjectness in the space of modern socio-political communications and its potential in the aspect of influencing the mass consciousness of citizens.",science,677,not included
10.1109/icitr51448.2020.9310890,to_check,2020 5th International Conference on Information Technology Research (ICITR),IEEE,2020-12-04 00:00:00,ieeexplore,hybrid approach and architecture to detect fake news on twitter in real-time using neural networks,https://ieeexplore.ieee.org/document/9310890/,"Fake news has been a key issue since the dawn of social media. Currently, we are at a stage where it is merely impossible to differentiate between real and fake news. This directly and indirectly affects people's decision patterns and makes us question the credibility of the news shared via social media platforms. Twitter is one of the leading social networks in the world by active users. There has been an exponential spread of fake news on Twitter in the recent past. In this paper, we will discuss the implementation of a browser extension which will identify fake news on Twitter using deep learning models with a focus on real-world applicability, architectural stability and scalability of such a solution. Experimental results show that the proposed browser extension has an accuracy of 86% accuracy in fake news detection. To the best of our knowledge, our work is the first of its kind to detect fake news on Twitter real-time using a hybrid approach and evaluate using real users.",science,678,included
10.1109/wetice.2019.00013,to_check,2019 IEEE 28th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE),IEEE,2019-06-14 00:00:00,ieeexplore,meaning extraction in a domotic assistant agent interacting by means of natural language,https://ieeexplore.ieee.org/document/8795403/,"This paper presents a software architecture to let users interact with their smart home devices, through the commonly used social network channels. The software, called FABULOS, is the result of the combination of components used to interact with devices and social networks, and a rule-based artificial intelligence, which implements the base logic for the automation. The core of the software architecture is the translation service, which has the objective of extracting the meaning of the sentences provided by text and voice messages by users in natural language, transforming them into proper device commands. In order to achieve this, the proposed solution relies on an approach which has the capability to interpret and extract the meaning of the intentions plus the entities involved, associated to the sentences sent by the users. A description of a realistic case-study, which shows an example of how the proposed software behaves in a real interaction with an user, is also included in the paper.",science,679,not included
10.1049/ic:19960648,to_check,IEE Colloquium on Intelligent Agents and Their Applications (Digest No: 1996/101),IET,1996-04-30 00:00:00,ieeexplore,what agents aren't: a discussion paper,https://ieeexplore.ieee.org/document/573230/,"A standard criticism of artificial intelligence is that it does not direct itself to real problems: and yet, we argue that AI research on intelligent agents, and in particular, work on agent architectures and agent oriented programming, is aimed at developing software systems that have precisely these attributes. Specifically, the term agent is used to refer to a system that is: autonomous in that it operates independently; reactive; pro-active in that it is capable of taking the initiative, and is not driven solely by events; and social in that it can communicate, cooperate, and negotiate with other agents in order to achieve its tasks. A particularly interesting tradition in intelligent agent research is that of viewing agents as rational systems. We argue that this notion of agents is of importance to software engineering researchers, who require tools to design and implement particularly complex computer systems.",science,680,not included
10.1109/access.2021.3124386,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,a multiple pheromone communication system for swarm intelligence,https://ieeexplore.ieee.org/document/9594791/,"Pheromones are chemical substances essential for communication among social insects. In the application of swarm intelligence to real micro mobile robots, the deployment of a single virtual pheromone has emerged recently as a powerful real-time method for indirect communication. However, these studies usually exploit only one kind of pheromones in their task, neglecting the crucial fact that in the world of real insects, multiple pheromones play important roles in shaping stigmergic behaviors such as foraging or nest building. To explore the multiple pheromones mechanism which enable robots to solve complex collective tasks efficiently, we introduce an artificial multiple pheromone system (ColCOS<inline-formula> <tex-math notation=""LaTeX"">$\Phi $ </tex-math></inline-formula>) to support swarm intelligence research by enabling multiple robots to deploy and react to multiple pheromones simultaneously. The proposed system ColCOS<inline-formula> <tex-math notation=""LaTeX"">$\Phi $ </tex-math></inline-formula> uses optical signals to emulate different evaporating chemical substances i.e. pheromones. These emulated pheromones are represented by trails displayed on a wide LCD display screen positioned horizontally, on which multiple miniature robots can move freely. The color sensors beneath the robots can detect and identify lingering “pheromones” on the screen. Meanwhile, the release of any pheromone from each robot is enabled by monitoring its positional information over time with an overhead camera. No other communication methods apart from virtual pheromones are employed in this system. Two case studies have been carried out which have verified the feasibility and effectiveness of the proposed system in achieving complex swarm tasks as empowered by multiple pheromones. This novel platform is a timely and powerful tool for research into swarm intelligence.",science,681,not included
10.1109/kcic.2018.8628512,to_check,2018 International Electronics Symposium on Knowledge Creation and Intelligent Computing (IES-KCIC),IEEE,2018-10-30 00:00:00,ieeexplore,estimating adaptive individual interests and needs based on online local variational inference for a logistic regression mixture model,https://ieeexplore.ieee.org/document/8628512/,"In real companies engaged in economic activities through transactions involving consumer items, such as retail, distribution, finance, and information materials, supplying an opportunity to customers to choose specialized items is an important factor that can improve customer satisfaction and convenience allowing their diverse and time-dependent needs to be met. However, capturing the specialized needs of customers accurately is a difficult task because their needs depend on time, context, situation, and meaning. Recently, physical computational environments have been developing rapidly, thereby allowing easy implementation to sense a customer's action and deal with it sequentially. In this paper, we propose a personalized method to predict individual interests and demands appropriately. In particular, the system learns the customers' situation, meaning, and action from their action history, and reflects a feedback of the result to predict the next action. To realize this method, we utilize the following two methodologies: the mathematical model of meaning (MMM), which is a semantic associative search technology; and the local variational inference (LVI), which is an approximation of the Bayesian inference. A numerical experiment shows that the proposed method performed better than a typical method.",science,682,not included
10.1109/igarss.2013.6723057,to_check,2013 IEEE International Geoscience and Remote Sensing Symposium - IGARSS,IEEE,2013-07-26 00:00:00,ieeexplore,parallel sparse unmixing of hyperspectral data,https://ieeexplore.ieee.org/document/6723057/,"In this paper, a new parallel method for sparse spectral unmixing of remotely sensed hyperspectral data on commodity graphics processing units (GPUs) is presented. A semi-supervised approach is adopted, which relies on the increasing availability of spectral libraries of materials measured on the ground instead of resorting to endmember extraction methods. This method is based on the spectral unmixing by splitting and augmented Lagrangian (SUNSAL) that estimates the material's abundance fractions. The parallel method is performed in a pixel-by-pixel fashion and its implementation properly exploits the GPU architecture at low level, thus taking full advantage of the computational power of GPUs. Experimental results obtained for simulated and real hyperspectral datasets reveal significant speedup factors, up to 164 times, with regards to optimized serial implementation.",science,683,not included
10.1007/s10140-020-01885-z,to_check,Emergency Radiology,Springer,2021-06-01 00:00:00,springer,automated processing of social media content for radiologists: applied deep learning to radiological content on twitter during covid-19 pandemic,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10140-020-01885-z,"Purpose The purpose of this study was to develop an automated process to analyze multimedia content on Twitter during the COVID-19 outbreak and classify content for radiological significance using deep learning (DL). Materials and methods Using Twitter search features, all tweets containing keywords from both “radiology” and “COVID-19” were collected for the period January 01, 2020 up to April 24, 2020. The resulting dataset comprised of 8354 tweets. Images were classified as (i) images with text (ii) radiological content (e.g., CT scan snapshots, X-ray images), and (iii) non-medical content like personal images or memes. We trained our deep learning model using Convolutional Neural Networks (CNN) on training dataset of 1040 labeled images drawn from all three classes. We then trained another DL classifier for segmenting images into categories based on human anatomy. All software used is open-source and adapted for this research. The diagnostic performance of the algorithm was assessed by comparing results on a test set of 1885 images. Results Our analysis shows that in COVID-19 related tweets on radiology, nearly 32% had textual images, another 24% had radiological content, and 44% were not of radiological significance. Our results indicated a 92% accuracy in classifying images originally labeled as chest X-ray or chest CT and a nearly 99% accurate classification of images containing medically relevant text. With larger training dataset and algorithmic tweaks, the accuracy can be further improved. Conclusion Applying DL on rich textual images and other metadata in tweets we can process and classify content for radiological significance in real time.",science,684,not included
10.1007/978-3-030-81321-5_15,to_check,Fashion Communication,Springer,2021-01-01 00:00:00,springer,"building a prosocial communication model in the fashion sector, based on sustainability and artificial intelligence, derived from covid-19",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-81321-5_15,"Considering communication in fashion from the perspective of sustainability and artificial intelligence allows us to resize its social focus, especially when we contextualize it in the era of COVID-19, which has stopped humanity in a loop, to rethink a new normal that accelerates processes, unthinkable in others historical moments. We see an undeniable trend, where a humanitarian sensitivity towards sustainability in fashion develops, towards the rational and real use of an intelligence in the fashion sector that is no longer artificial but advanced. This research responds to a question that belongs to the world of the social. How to communicate for a society that urgently needs to implement sustainability strategies in the fashion sector, based on artificial intelligence at the service of rescuing the human? In this sense, based on a projective documentary research and a series of in-depth interviews with fashion experts, it allowed us to initiate what aims to lay the foundations of a prosocial communication model in the fashion sector, based on the sustainability of the human being on the planet and, as a corollary, also based on artificial intelligence. The prosocial field is oriented to collaborate and help other people in a positive, productive and social way. Artificial intelligence in fashion, at the service of sustainability and communication, allows us to think with a humanistic approach to communication, where the focus is to understand the human being in a deep way, with an interrelation with artificial intelligence and the increase of sustainability in fashion in different areas of the value chain. This investigation concludes by showing that the implementation of the proposed model of this investigation is plausible.",science,685,not included
10.1007/s10817-015-9330-8,to_check,Journal of Automated Reasoning,Springer,2015-10-01 00:00:00,springer,mizar 40 for mizar 40,http://link.springer.com/openurl/pdf?id=doi:10.1007/s10817-015-9330-8,"As a present to Mizar on its 40th anniversary, we develop an AI/ATP system that in 30 seconds of real time on a 14-CPU machine automatically proves 40 % of the theorems in the latest official version of the Mizar Mathematical Library ( MML ). This is a considerable improvement over previous performance of large-theory AI/ATP methods measured on the whole MML . To achieve that, a large suite of AI/ATP methods is employed and further developed. We implement the most useful methods efficiently, to scale them to the 150000 formulas in MML . This reduces the training times over the corpus to 1–3 seconds, allowing a simple practical deployment of the methods in the online automated reasoning service for the Mizar users ( Miz AR $\mathbb {A}\mathbb {R}$ ).",science,686,not included
10.1007/s10817-014-9303-3,to_check,Journal of Automated Reasoning,Springer,2014-08-01 00:00:00,springer,learning-assisted automated reasoning with flyspeck,http://link.springer.com/openurl/pdf?id=doi:10.1007/s10817-014-9303-3,"The considerable mathematical knowledge encoded by the Flyspeck project is combined with external automated theorem provers (ATPs) and machine-learning premise selection methods trained on the Flyspeck proofs, producing an AI system capable of proving a wide range of mathematical conjectures automatically. The performance of this architecture is evaluated in a bootstrapping scenario emulating the development of Flyspeck from axioms to the last theorem, each time using only the previous theorems and proofs. It is shown that 39 % of the 14185 theorems could be proved in a push-button mode (without any high-level advice and user interaction) in 30 seconds of real time on a fourteen-CPU workstation. The necessary work involves: (i) an implementation of sound translations of the HOL Light logic to ATP formalisms: untyped first-order, polymorphic typed first-order, and typed higher-order, (ii) export of the dependency information from HOL Light and ATP proofs for the machine learners, and (iii) choice of suitable representations and methods for learning from previous proofs, and their integration as advisors with HOL Light. This work is described and discussed here, and an initial analysis of the body of proofs that were found fully automatically is provided.",science,687,not included
http://arxiv.org/abs/1805.03045v2,to_check,arxiv,arxiv,2018-05-08 00:00:00,arxiv,"a new method for unveiling open clusters in gaia: new nearby open
  clusters confirmed by dr2",http://arxiv.org/abs/1805.03045v2,"The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in
Astronomy. It includes precise astrometric data (positions, proper motions and
parallaxes) for more than $1.3$ billion sources, mostly stars. To analyse such
a vast amount of new data, the use of data mining techniques and machine
learning algorithms are mandatory. The search for Open Clusters, groups of
stars that were born and move together, located in the disk, is a great example
for the application of these techniques. Our aim is to develop a method to
automatically explore the data space, requiring minimal manual intervention. We
explore the performance of a density based clustering algorithm, DBSCAN, to
find clusters in the data together with a supervised learning method such as an
Artificial Neural Network (ANN) to automatically distinguish between real Open
Clusters and statistical clusters. The development and implementation of this
method to a $5$-Dimensional space ($l$, $b$, $\varpi$, $\mu_{\alpha^*}$,
$\mu_\delta$) to the Tycho-Gaia Astrometric Solution (TGAS) data, and a
posterior validation using Gaia DR2 data, lead to the proposal of a set of new
nearby Open Clusters. We have developed a method to find OCs in astrometric
data, designed to be applied to the full Gaia DR2 archive.",science,688,not included
http://arxiv.org/abs/2004.05953v1,to_check,arxiv,arxiv,2020-04-13 00:00:00,arxiv,"software-defined network for end-to-end networked science at the
  exascale",http://arxiv.org/abs/2004.05953v1,"Domain science applications and workflow processes are currently forced to
view the network as an opaque infrastructure into which they inject data and
hope that it emerges at the destination with an acceptable Quality of
Experience. There is little ability for applications to interact with the
network to exchange information, negotiate performance parameters, discover
expected performance metrics, or receive status/troubleshooting information in
real time. The work presented here is motivated by a vision for a new smart
network and smart application ecosystem that will provide a more deterministic
and interactive environment for domain science workflows. The Software-Defined
Network for End-to-end Networked Science at Exascale (SENSE) system includes a
model-based architecture, implementation, and deployment which enables
automated end-to-end network service instantiation across administrative
domains. An intent based interface allows applications to express their
high-level service requirements, an intelligent orchestrator and resource
control systems allow for custom tailoring of scalability and real-time
responsiveness based on individual application and infrastructure operator
requirements. This allows the science applications to manage the network as a
first-class schedulable resource as is the current practice for instruments,
compute, and storage systems. Deployment and experiments on production networks
and testbeds have validated SENSE functions and performance. Emulation based
testing verified the scalability needed to support research and education
infrastructures. Key contributions of this work include an architecture
definition, reference implementation, and deployment. This provides the basis
for further innovation of smart network services to accelerate scientific
discovery in the era of big data, cloud computing, machine learning and
artificial intelligence.",science,689,included
http://arxiv.org/abs/2010.07634v3,to_check,arxiv,arxiv,2020-10-15 00:00:00,arxiv,"towards reflectivity profile inversion through artificial neural
  networks",http://arxiv.org/abs/2010.07634v3,"The goal of Specular Neutron and X-ray Reflectometry is to infer materials
Scattering Length Density (SLD) profiles from experimental reflectivity curves.
This paper focuses on investigating an original approach to the ill-posed
non-invertible problem which involves the use of Artificial Neural Networks
(ANN). In particular, the numerical experiments described here deal with large
data sets of simulated reflectivity curves and SLD profiles, and aim to assess
the applicability of Data Science and Machine Learning technology to the
analysis of data generated at neutron scattering large scale facilities. It is
demonstrated that, under certain circumstances, properly trained Deep Neural
Networks are capable of correctly recovering plausible SLD profiles when
presented with never-seen-before simulated reflectivity curves. When the
necessary conditions are met, a proper implementation of the described approach
would offer two main advantages over traditional fitting methods when dealing
with real experiments, namely, 1. sample physical models are described under a
new paradigm: detailed layer-by-layer descriptions (SLDs, thicknesses,
roughnesses) are replaced by parameter free curves $\rho(z)$, allowing a-priori
assumptions to be fed in terms of the sample family to which a given sample
belongs (e.g. ""thin film"", ""lamellar structure"", etc.) 2. the time-to-solution
is shrunk by orders of magnitude, enabling faster batch analyses for large
datasets.",science,690,not included
http://arxiv.org/abs/2004.00104v1,to_check,arxiv,arxiv,2020-03-31 00:00:00,arxiv,"improvement of electronic governance and mobile governance in
  multilingual countries with digital etymology using sanskrit grammar",http://arxiv.org/abs/2004.00104v1,"With huge improvement of digital connectivity (Wifi,3G,4G) and digital
devices access to internet has reached in the remotest corners now a days.
Rural people can easily access web or apps from PDAs, laptops, smartphones etc.
This is an opportunity of the Government to reach to the citizen in large
number, get their feedback, associate them in policy decision with e governance
without deploying huge man, material or resourses. But the Government of
multilingual countries face a lot of problem in successful implementation of
Government to Citizen (G2C) and Citizen to Government (C2G) governance as the
rural people tend and prefer to interact in their native languages. Presenting
equal experience over web or app to different language group of speakers is a
real challenge. In this research we have sorted out the problems faced by Indo
Aryan speaking netizens which is in general also applicable to any language
family groups or subgroups. Then we have tried to give probable solutions using
Etymology. Etymology is used to correlate the words using their ROOT forms. In
5th century BC Panini wrote Astadhyayi where he depicted sutras or rules -- how
a word is changed according to person,tense,gender,number etc. Later this book
was followed in Western countries also to derive their grammar of comparatively
new languages. We have trained our system for automatic root extraction from
the surface level or morphed form of words using Panian Gramatical rules. We
have tested our system over 10000 bengali Verbs and extracted the root form
with 98% accuracy. We are now working to extend the program to successfully
lemmatize any words of any language and correlate them by applying those rule
sets in Artificial Neural Network.",science,691,not included
http://arxiv.org/abs/1903.05575v1,to_check,arxiv,arxiv,2019-03-13 00:00:00,arxiv,"on the efficacy and high-performance implementation of quaternion matrix
  multiplication",http://arxiv.org/abs/1903.05575v1,"Quaternion symmetry is ubiquitous in the physical sciences. As such, much
work has been afforded over the years to the development of efficient schemes
to exploit this symmetry using real and complex linear algebra. Recent years
have also seen many advances in the formal theoretical development of
explicitly quaternion linear algebra with promising applications in image
processing and machine learning. Despite these advances, there do not currently
exist optimized software implementations of quaternion linear algebra. The
leverage of optimized linear algebra software is crucial in the achievement of
high levels of performance on modern computing architectures, and thus provides
a central tool in the development of high-performance scientific software. In
this work, a case will be made for the efficacy of high-performance quaternion
linear algebra software for appropriate problems. In this pursuit, an optimized
software implementation of quaternion matrix multiplication will be presented
and will be shown to outperform a vendor tuned implementation for the analogous
complex matrix operation. The results of this work pave the path for further
development of high-performance quaternion linear algebra software which will
improve the performance of the next generation of applicable scientific
applications.",science,692,not included
http://arxiv.org/abs/2003.10548v1,to_check,arxiv,arxiv,2020-03-23 00:00:00,arxiv,spsurv: an r package for semi-parametric survival analysis,http://arxiv.org/abs/2003.10548v1,"Software development innovations and advances in computing have enabled more
complex and less costly computations in medical research (survival analysis),
engineering studies (reliability analysis), and social sciences event analysis
(historical analysis). As a result, many semi-parametric modeling efforts
emerged when it comes to time-to-event data analysis. In this context, this
work presents a flexible Bernstein polynomial (BP) based framework for survival
data modeling. This innovative approach is applied to existing families of
models such as proportional hazards (PH), proportional odds (PO), and
accelerated failure time (AFT) models to estimate unknown baseline functions.
Along with this contribution, this work also presents new automated routines in
R, taking advantage of algorithms available in Stan. The proposed computation
routines are tested and explored through simulation studies based on artificial
datasets. The tools implemented to fit the proposed statistical models are
combined and organized in an R package. Also, the BP based proportional hazards
(BPPH), proportional odds (BPPO), and accelerated failure time (BPAFT) models
are illustrated in real applications related to cancer trial data using maximum
likelihood (ML) estimation and Markov chain Monte Carlo (MCMC) methods.",science,693,not included
http://arxiv.org/abs/1409.7699v3,to_check,arxiv,arxiv,2014-09-26 00:00:00,arxiv,"the overlooked potential of generalized linear models in astronomy-ii:
  gamma regression and photometric redshifts",http://arxiv.org/abs/1409.7699v3,"Machine learning techniques offer a precious tool box for use within
astronomy to solve problems involving so-called big data. They provide a means
to make accurate predictions about a particular system without prior knowledge
of the underlying physical processes of the data. In this article, and the
companion papers of this series, we present the set of Generalized Linear
Models (GLMs) as a fast alternative method for tackling general astronomical
problems, including the ones related to the machine learning paradigm. To
demonstrate the applicability of GLMs to inherently positive and continuous
physical observables, we explore their use in estimating the photometric
redshifts of galaxies from their multi-wavelength photometry. Using the gamma
family with a log link function we predict redshifts from the PHoto-z Accuracy
Testing simulated catalogue and a subset of the Sloan Digital Sky Survey from
Data Release 10. We obtain fits that result in catastrophic outlier rates as
low as ~1% for simulated and ~2% for real data. Moreover, we can easily obtain
such levels of precision within a matter of seconds on a normal desktop
computer and with training sets that contain merely thousands of galaxies. Our
software is made publicly available as an user-friendly package developed in
Python, R and via an interactive web application
(https://cosmostatisticsinitiative.shinyapps.io/CosmoPhotoz). This software
allows users to apply a set of GLMs to their own photometric catalogues and
generates publication quality plots with minimum effort from the user. By
facilitating their ease of use to the astronomical community, this paper series
aims to make GLMs widely known and to encourage their implementation in future
large-scale projects, such as the Large Synoptic Survey Telescope.",science,694,not included
http://arxiv.org/abs/1908.04172v2,to_check,arxiv,arxiv,2019-08-12 00:00:00,arxiv,"ngraph-he2: a high-throughput framework for neural network inference on
  encrypted data",http://arxiv.org/abs/1908.04172v2,"In previous work, Boemer et al. introduced nGraph-HE, an extension to the
Intel nGraph deep learning (DL) compiler, that enables data scientists to
deploy models with popular frameworks such as TensorFlow and PyTorch with
minimal code changes. However, the class of supported models was limited to
relatively shallow networks with polynomial activations. Here, we introduce
nGraph-HE2, which extends nGraph-HE to enable privacy-preserving inference on
standard, pre-trained models using their native activation functions and number
fields (typically real numbers). The proposed framework leverages the CKKS
scheme, whose support for real numbers is friendly to data science, and a
client-aided model using a two-party approach to compute activation functions.
  We first present CKKS-specific optimizations, enabling a 3x-88x runtime
speedup for scalar encoding, and doubling the throughput through a novel use of
CKKS plaintext packing into complex numbers. Second, we optimize
ciphertext-plaintext addition and multiplication, yielding 2.6x-4.2x runtime
speedup. Third, we exploit two graph-level optimizations: lazy rescaling and
depth-aware encoding, which allow us to significantly improve performance.
  Together, these optimizations enable state-of-the-art throughput of 1,998
images/s on the CryptoNets network. Using the client-aided model, we also
present homomorphic evaluation of (to our knowledge) the largest network to
date, namely, pre-trained MobileNetV2 models on the ImageNet dataset, with
60.4\percent/82.7\percent\ top-1/top-5 accuracy and an amortized runtime of 381
ms/image.",science,695,not included
http://arxiv.org/abs/2001.09938v1,to_check,arxiv,arxiv,2019-10-22 00:00:00,arxiv,"autonomous discovery of battery electrolytes with robotic
  experimentation and machine-learning",http://arxiv.org/abs/2001.09938v1,"Innovations in batteries take years to formulate and commercialize, requiring
extensive experimentation during the design and optimization phases. We
approached the design and selection of a battery electrolyte through a
black-box optimization algorithm directly integrated into a robotic test-stand.
We report here the discovery of a novel battery electrolyte by this experiment
completely guided by the machine-learning software without human intervention.
Motivated by the recent trend toward super-concentrated aqueous electrolytes
for high-performance batteries, we utilize Dragonfly - a Bayesian
machine-learning software package - to search mixtures of commonly used lithium
and sodium salts for super-concentrated aqueous electrolytes with wide
electrochemical stability windows. Dragonfly autonomously managed the robotic
test-stand, recommending electrolyte designs to test and receiving experimental
feedback in real time. In 40 hours of continuous experimentation over a
four-dimensional design space with millions of potential candidates, Dragonfly
discovered a novel, mixed-anion aqueous sodium electrolyte with a wider
electrochemical stability window than state-of-the-art sodium electrolyte. A
human-guided design process may have missed this optimal electrolyte. This
result demonstrates the possibility of integrating robotics with
machine-learning to rapidly and autonomously discover novel battery materials.",science,696,included
http://arxiv.org/abs/2011.09780v1,to_check,arxiv,arxiv,2020-11-19 00:00:00,arxiv,kernel phase and coronagraphy with automatic differentiation,http://arxiv.org/abs/2011.09780v1,"The accumulation of aberrations along the optical path in a telescope
produces distortions and speckles in the resulting images, limiting the
performance of cameras at high angular resolution. It is important to achieve
the highest possible sensitivity to faint sources such as planets, using both
hardware and data analysis software. While analytic methods are efficient, real
systems are better-modelled numerically, but such models with many parameters
can be hard to understand, optimize and apply. Automatic differentiation
software developed for machine learning now makes calculating derivatives with
respect to aberrations straightforward for arbitrary optical systems. We apply
this powerful new tool to enhance high-angular-resolution astronomical imaging.
Self-calibrating observables such as the 'closure phase' or 'bispectrum' have
been widely used in optical and radio astronomy to mitigate optical aberrations
and achieve high-fidelity imagery. Kernel phases are a generalization of
closure phases in the limit of small phase errors. Using automatic
differentiation, we reproduce existing kernel phase theory within this
framework and demonstrate an extension to the Lyot coronagraph, finding
self-calibrating combinations of speckles which are resistant to phase noise,
but only in the very high-wavefront-quality regime. As an illustrative example,
we reanalyze Palomar adaptive optics observations of the binary alpha Ophiuchi,
finding consistency between the new pipeline and the existing standard. We
present a new Python package 'morphine' that incorporates these ideas, with an
interface similar to the popular package poppy, for optical simulation with
automatic differentiation. These methods may be useful for designing improved
astronomical optical systems by gradient descent.",science,697,not included
10.1016/j.conbuildmat.2021.124915,to_check,Construction and Building Materials,scopus,2021-11-01,sciencedirect,prediction models for low-temperature creep compliance of asphalt mixtures containing reclaimed asphalt pavement (rap),https://api.elsevier.com/content/abstract/scopus_id/85115205905,"The low-temperature creep compliances (D(t)) of asphalt mixture is one of the necessary parameters to predict the depth and amount of low-temperature cracks. Level 3 analysis in Mechanistic-Empirical Pavement Design Guide (MEPDG) software uses asphalt binder properties parameters and mixture volumetric properties to predict D(t) when the real laboratorial data is not available. However, some parameters in the model may not be routinely measured in Superpave system, which restricts the use of the prediction model. In addition, new additives and recycling materials such as reclaimed asphalt pavement (RAP) have been extensively used in recent years and shown to have significant effect on the low-temperature cracking resistance of asphalt mixture. However, the effects have not been considered in the existing D(t) prediction models. Hence, the objective of this study is to develop models with significantly high accuracy to predict the D(t) of asphalt mixtures containing RAP. A total of 1890 sets of data points were collected from three different research projects. A Pearson correlation analysis was carried out to select the input parameters which are most influential to D(t). Two prediction models (i.e., multiple linear regression and artificial neural network (ANN) models) were proposed. A comprehensive analysis on the prediction accuracy and reasonability of the proposed models was conducted. The results showed that the proposed models had much better prediction performance with high accuracy than the existing models. The comparisons between the proposed models with the existing models confirmed that it is necessary to take the new additives and recycling materials into account in developing D(t) prediction models.",science,698,not included
10.1016/j.asoc.2021.107792,to_check,Applied Soft Computing,scopus,2021-11-01,sciencedirect,sentiment classification using attention mechanism and bidirectional long short-term memory network,https://api.elsevier.com/content/abstract/scopus_id/85112745936,"We propose a sentiment classification method for large scale microblog text based on the attention mechanism and the bidirectional long short-term memory network (SC-ABiLSTM). We use an experimental study to compare our proposed method with baseline methods using real world large-scale microblog data. Comparing the accuracy of the baseline methods to the accuracy of our model, we demonstrate the efficacy of our proposed method. While sentiment classification of social media data has been extensively studied, the main novelty of our study is the implementation of the attention mechanism in a deep learning network for analyzing large scale social media data.",science,699,not included
10.1016/j.softx.2021.100773,to_check,SoftwareX,scopus,2021-07-01,sciencedirect,netcausality: a time-delayed neural network tool for causality detection and analysis,https://api.elsevier.com/content/abstract/scopus_id/85111857161,"The analysis of causality between systems is still an important research activity, which finds application in several fields of science. The software presented is a new tool for causality detection and analysis between time series. The proposed technique is based on time-delayed neural networks (TDNN). The tool is developed in MATLAB and it comprises three main functions. The first one returns the total causality between two or more systems of equations. The second tool is used to find the “time horizon”, id est the time delay at which the influence between the systems occurs. The last function is a causality feature detection to determine the time intervals, in which the mutual coupling is sufficiently strong to have a real influence on the target.",science,700,not included
10.1016/j.jmps.2020.104277,to_check,Journal of the Mechanics and Physics of Solids,scopus,2021-02-01,sciencedirect,thermodynamics-based artificial neural networks for constitutive modeling,https://api.elsevier.com/content/abstract/scopus_id/85097707935,"Machine Learning methods and, in particular, Artificial Neural Networks (ANNs) have demonstrated promising capabilities in material constitutive modeling. One of the main drawbacks of such approaches is the lack of a rigorous frame based on the laws of physics. This may render physically inconsistent the predictions of a trained network, which can be even dangerous for real applications.
                  Here we propose a new class of data-driven, physics-based, neural networks for constitutive modeling of strain rate independent processes at the material point level, which we define as Thermodynamics-based Artificial Neural Networks (TANNs). The two basic principles of thermodynamics are encoded in the network’s architecture by taking advantage of automatic differentiation to compute the numerical derivatives of a network with respect to its inputs. In this way, derivatives of the free-energy, the dissipation rate and their relation with the stress and internal state variables are hardwired in the architecture of TANNs. Consequently, our approach does not have to identify the underlying pattern of thermodynamic laws during training, reducing the need of large data-sets. Moreover the training is more efficient and robust, and the predictions more accurate. Finally and more important, the predictions remain thermodynamically consistent, even for unseen data. Based on these features, TANNs are a starting point for data-driven, physics-based constitutive modeling with neural networks.
                  We demonstrate the wide applicability of TANNs for modeling elasto-plastic materials, using both hyper- and hypo-plasticity models. Strain hardening and softening are also considered for the hyper-plastic scenario. Detailed comparisons show that the predictions of TANNs outperform those of standard ANNs. Finally, we demonstrate that the implementation of the laws of thermodynamics confers to TANNs high robustness in the presence of noise in the training data, compared to standard approaches.
                  TANNs’ architecture is general, enabling applications to materials with different or more complex behavior, without any modification.",science,701,not included
10.1016/j.prro.2020.07.003,to_check,Practical Radiation Oncology,scopus,2021-01-01,sciencedirect,time analysis of online adaptive magnetic resonance–guided radiation therapy workflow according to anatomical sites,https://api.elsevier.com/content/abstract/scopus_id/85090017005,"Purpose
                  To document time analysis of detailed workflow steps for the online adaptive magnetic resonance–guided radiation therapy treatments (MRgRT) with the ViewRay MRIdian system and to identify the barriers to and solutions for shorter treatment times.
               
                  Methods and Materials
                  A total of 154 patients were treated with the ViewRay MRIdian system between September 2018 and October 2019. The time process of MRgRT workflow steps of 962 fractions for 166 treatment sites was analyzed in terms of patient and online adaptive treatment (ART) characteristics.
               
                  Results
                  Overall, 774 of 962 fractions were treated with online ART, and 83.2% of adaptive fractions were completed in less than 60 minutes. Sixty-three percent, 50.3%, and 4.2% of fractions were completed in less than 50 minutes, 45 minutes, and 30 minutes, respectively. Eight-point-three percent and 3% of fractions were completed in more than 70 minutes and 80 minutes, respectively. The median time (tmed) for ART workflow steps were as follows: (1) setup tmed: 5.0 minutes, (2) low-resolution scanning tmed: 1 minute, (3) high-resolution scanning tmed: 3 minutes, (4) online contouring tmed: 9 minutes, (5) reoptimization with online quality assurance tmed: 5 minutes, (6) real targeting tmed: 3 minutes, (7) beam delivery with gating tmed: 17 minutes, and (8) net total treatment time tmed: 45 minutes. The shortest and longest tmean rates of net total treatment time were 41.59 minutes and 64.43 minutes for upper-lung-lobe-located thoracic tumors and ultracentrally located thoracic tumors, respectively.
               
                  Conclusions
                  To our knowledge, this is the first broad treatment-time analysis for online ART in the literature. Although treatment times are long due to human- and technology-related limitations, benefits offered by MRgRT might be clinically important. In the future, implementation of artificial intelligence segmentation, an increase in dose rate, and faster multileaf collimator and gantry speeds may lead to achieving shorter MRgRT treatments.",science,702,not included
10.1016/j.future.2020.04.018,to_check,Future Generation Computer Systems,scopus,2020-09-01,sciencedirect,software-defined network for end-to-end networked science at the exascale,https://api.elsevier.com/content/abstract/scopus_id/85083299223,"Domain science applications and workflow processes are currently forced to view the network as an opaque infrastructure into which they inject data and hope that it emerges at the destination with an acceptable Quality of Experience. There is little ability for applications to interact with the network to exchange information, negotiate performance parameters, discover expected performance metrics, or receive status/troubleshooting information in real time. The work presented here is motivated by a vision for a new smart network and smart application ecosystem that will provide a more deterministic and interactive environment for domain science workflows. The Software-Defined Network for End-to-end Networked Science at Exascale (SENSE) system includes a model-based architecture, implementation, and deployment which enables automated end-to-end network service instantiation across administrative domains. An intent based interface allows applications to express their high-level service requirements, an intelligent orchestrator and resource control systems allow for custom tailoring of scalability and real-time responsiveness based on individual application and infrastructure operator requirements. This allows the science applications to manage the network as a first-class schedulable resource as is the current practice for instruments, compute, and storage systems. Deployment and experiments on production networks and testbeds have validated SENSE functions and performance. Emulation based testing verified the scalability needed to support research and education infrastructures. Key contributions of this work include an architecture definition, reference implementation, and deployment. This provides the basis for further innovation of smart network services to accelerate scientific discovery in the era of big data, cloud computing, machine learning and artificial intelligence.",science,703,included
10.1016/j.aca.2020.04.007,to_check,Analytica Chimica Acta,scopus,2020-06-01,sciencedirect,dual-emission cdte/agins<inf>2</inf> photoluminescence probe coupled to neural network data processing for the simultaneous determination of folic acid and iron (ii),https://api.elsevier.com/content/abstract/scopus_id/85083073504,"This work focused on the combination of CdTe and AgInS2 quantum dots in a dual-emission nanoprobe for the simultaneous determination of folic acid and Fe(II) in pharmaceutical formulations. The surface chemistry of the used QDs was amended with suitable capping ligands to obtain appropriate reactivity in terms of selectivity and sensitivity towards the target analytes. The implementation of PL-based sensing schemes combining multiple QDs of different nature, excited at the same wavelength and emitting at different ones, allowed to obtain a specific analyte-response profile. The first-order fluorescence data obtained from the whole emission spectra of the CdTe/AgInS2 combined nanoprobe upon interaction with folic acid and Fe(II) were processed by using chemometric tools, namely partial least-squares (PLS) and artificial neural network (ANN). This enabled to circumvent the selectivity issues commonly associated with the use of QDs prone to indiscriminate interaction with multiple species, which impair reliable and accurate quantification in complex matrices samples.
                  ANN demonstrated to be the most efficient chemometric model for the simultaneous determination of both analytes in binary mixtures and pharmaceutical formulations due to the non-linear relationship between analyte concentration and fluorescence data that it could handle. The R2
                     P and SEP% obtained for both analytes quantification in pharmaceutical formulations through ANN modelling ranged from 0.92 to 0.99 and 5.7–9.1%, respectively. The obtained results revealed that the developed approach is able to quantify, with high reliability and accuracy, more than one analyte in complex mixtures and real samples with pharmaceutical interest.",science,704,not included
10.1016/j.cie.2019.06.040,to_check,Computers and Industrial Engineering,scopus,2019-09-01,sciencedirect,"bernard, an energy intelligent system for raising residential users awareness",https://api.elsevier.com/content/abstract/scopus_id/85067600850,"Energy efficiency is still a hot topic today. Coming roughly the 25% of the energy consumption in EU from the residential sector, very few cheap and simple tools to promote energy efficiency in home users have been developed. The purpose of this paper is to present Bernard, a concept proof designed for filling this gap. This aims that householders become aware of their energy habits and have useful information that help them to redirect their consumption pattern. To achieve these goals, Bernard offers, through a mobile application, the home energy consumption monitoring in real time, the energy price forecast for the next hour and the appliances which are switched on, among others. Furthermore, it is important to highlight that the system has been designed with the premises of being cheap, non-intrusive, reliable and easily scalable, in order that utilities can gradually deploy and provide it to their customers, gaining at the same time valuable information for decision making and improving its corporate social image. Therefore, the adopted solution is based on a real time streaming data architecture suitable for handling huge volumes of data and applying predictive techniques on a cloud-computing environment. The paper provides a detailed description of the system and experimental results evaluating the performance of the predictive modules built. As case study, REFIT and REDD datasets were used.",science,705,included
10.1016/j.procs.2019.01.012,to_check,Procedia Computer Science,scopus,2019-01-01,sciencedirect,combining supervised and unsupervised machine learning algorithms to predict the learners' learning styles,https://api.elsevier.com/content/abstract/scopus_id/85062675875,"The implementation of an efficient adaptive e-learning system requires the construction of an effective student model that represents the student’s characteristics, among those characteristics, there is the learning style that refers to the way in which a student prefers to learn. Knowing learning styles helps adaptive E-learning systems to improve the learning process by providing customized materials to students. In this work, we have proposed an approach to identify the learning style automatically based on the existing learners’ behaviors and using web usage mining techniques and machine learning algorithms. The web usage mining techniques were used to pre-process the log file extracted from the E-learning environment and capture the learners’ sequences. The captured learners’ sequences were given as an input to the K-modes clustering algorithm to group them into 16 learning style combinations based on the Felder and Silverman learning style model. Then the naive Bayes classifier was used to predict the learning style of a student in real time. To perform our approach, we used a real dataset extracted from an e-learning system’s log file, and in order to evaluate the performance of the used classifier, the confusion matrix method was used. The obtained results demonstrate that our approach yields excellent results.",science,706,not included
10.1016/j.heliyon.2018.e00972,to_check,Heliyon,scopus,2018-11-01,sciencedirect,modeling the output power of heterogeneous photovoltaic panels based on artificial neural networks using low cost microcontrollers,https://api.elsevier.com/content/abstract/scopus_id/85057181952,"Many implementations of artificial neural networks have been reported in scientific papers. However, few of these implementations allow the direct use of off-line trained networks. Moreover, no implementation reported the use of relatively small network adequate to run on low cost microcontroller. Hence, this work, which presents a small artificial neural network, which models the output power of heterogeneous photovoltaic panel. In addition, the work discuss the hardware implementation that allows such network to run on low cost microcontroller. The hardware implementation has the ability to model heterogeneous photovoltaic panel's output power with very high accuracy and fast response time. Feedforward back propagation has been used because of its high resolution and accurate activation function. Real-time measured parameters can be used as inputs for the developed system. The resulting hardware data is tested with data from real photovoltaic panels; to confirm that it can efficiently implement the models prepared off-line with Matlab. The comparison revealed the robustness of the proposed heterogeneous photovoltaic model system at different conditions. The proposed heterogeneous photovoltaic model system offer a proper and efficient tool that can be used in monitoring photovoltaic panels, such as the ones used in smart-house applications.",science,707,not included
10.1016/j.ifacol.2018.07.176,to_check,,scopus,2018-01-01,sciencedirect,artificial colloquist: treating social anxiety disorder using altera fpga,https://api.elsevier.com/content/abstract/scopus_id/85052906741,Purpose of this paper is to present the project that will use artificial intelligence and features of Altera FPGA board to imitate human. This problem was solved by using Cleverbot Google API and Altera FPGA board which has Linux system installed to run backend of our application. Frontend input and output peripherals will be used to read input from user and to present output to user. We present an implementation which has all the necessary features to be used for treating social anxiety disorder by simulating real human interaction by effectively employing such algorithms and system.,science,708,not included
10.1016/j.compedu.2017.09.007,to_check,Computers and Education,scopus,2018-01-01,sciencedirect,educational apps from the android google play for greek preschoolers: a systematic review,https://api.elsevier.com/content/abstract/scopus_id/85029678082,"In the seven years since the introduction of the tablet (Apple iPad) in 2010, the use of software for smart mobile devices has grown rapidly in popularity and has become a hotly debated issue in the field of education and child development. However, the rise in popularity of mobile applications (apps) mainly addressed to young children is not in line with a corresponding increase in their quality, as there is conflicting evidence about the real value and suitability of educational apps. The purpose of this study was to examine whether self-proclaimed educational apps for Greek preschoolers have been designed in accordance with developmentally appropriate standards to contribute to the social, emotional and cognitive development of children in formal and informal learning environments. The study results were discouraging. The majority of the apps aimed to teach children the basics about numbers and letters. Overall, they were drill-and-practice-style, based on a low level of thinking skills, thereby promoting rote learning, and were unable to contribute to a deeper conceptual understanding of certain concepts.",science,709,not included
10.1016/j.enconman.2017.05.006,to_check,Energy Conversion and Management,scopus,2017-10-15,sciencedirect,diagnostic information system dynamics in the evaluation of machine learning algorithms for the supervision of energy efficiency of district heating-supplied buildings,https://api.elsevier.com/content/abstract/scopus_id/85019049599,"Modern ways of exploring the diagnostic knowledge provided by data mining and machine learningraise some concern about the ways of evaluating the quality of output knowledge, usually represented by information systems. Especially in district heating, the stationarity of efficiency models, and thus the relevance of diagnostic classification system, cannot be ensured due to the impact of social, economic or technological changes, which are hard to identify or predict. Therefore, data mining and machine learning have become an attractive strategy for automatically and continuously absorbing such dynamics.
                  This paper presents a new method of evaluation and comparison of diagnostic information systems gathered algorithmically in district heating efficiency supervision based on exploring the evolution of information system and analyzing its dynamic features.
                  The process of data mining and knowledge discovery was applied to the data acquired from district heating substations’ energy meters to provide the automated discovery of diagnostic knowledge base necessary for the efficiency supervision of district heating-supplied buildings. The implemented algorithm consists of several steps of processing the billing data, including preparation, segmentation, aggregation and knowledge discovery stage, where classes of abstract modelsrepresenting energy efficiency constitute an information system representing diagnostic knowledge about the energy efficiency of buildings favorably operatingunder similar climate conditions and supplied from the same district heating network.
                  The authors analyzed the evolution of a series of information systems originating from the same knowledge discovery algorithm applied to a sequence of energy consumption-related data. Specifically, the rough sets theory was applied to describe the knowledge base and measure the uncertainty of machine learning predictions of currentclassification based on a past knowledge base. Fluctuations of diagnostic class membership were identified and provided for the differentiationbetween returning and novel fault detections, thus introducing the qualities of information system uncertainty and its sustainability. Theusability of the new method was demonstrated in the comparison of results for exemplary data mining algorithms implemented on real data from over one thousand buildings.",science,710,not included
10.1016/j.bios.2016.08.040,to_check,Biosensors and Bioelectronics,scopus,2017-01-15,sciencedirect,a contemporary approach for design and characterization of fiber-optic-cortisol sensor tailoring lmr and zno/ppy molecularly imprinted film,https://api.elsevier.com/content/abstract/scopus_id/84982300232,"A fiber optic salivary cortisol sensor using a contemporary approach of lossy mode resonance and molecular imprinting of nanocomposites of zinc oxide (ZnO) and polypyrrole (PPY) is structured and depicted for the concentration range of 0–10−6
                     g/ml of cortisol prepared in artificial saliva. Components of polymer preparation and the nanocomposite of polymer with ZnO are optimized for realizing the molecular imprinted layer of the sensor. Nanocomposite having 20% of ZnO in PPY is found to give highest sensitivity of the sensor. The sensor reports the best limit of detection ever reported with better stability, repeatability and response time. Lossy mode resonance based salivary cortisol sensor using nanocomposite molecular imprinted layer reported first time boosts the specificity of the sensor. The implementation of sensor over optical fiber adds up other advantages such as real time and online monitoring along with remote sensing abilities which makes the sensor usable for nonintrusive clinical applications.",science,711,not included
10.1016/j.engappai.2016.09.004,to_check,Engineering Applications of Artificial Intelligence,scopus,2016-11-01,sciencedirect,a feature selection method for author identification in interactive communications based on supervised learning and language typicality,https://api.elsevier.com/content/abstract/scopus_id/84988025909,"Authorship attribution, conceived as the identification of the origin of a text between different authors, has been a very active area of research in the scientific community mainly supported by advances in Natural Language Processing (NLP), machine learning and Computational Intelligence. This paradigm has been mostly addressed from a literary perspective, aiming at identifying the stylometric features and writeprints which unequivocally typify the writer patterns and allow their unique identification. On the other hand, the upsurge of social networking platforms and interactive messaging have undoubtedly made the anonymous expression of feelings, the sharing of experiences and social relationships much easier than in other traditional communication media. Unfortunately, the popularity of such communities and the virtual identification of their users deploy a rich substrate for cybercrimes against unsuspecting victims and other forms of illegal uses of social networks that call for the activity tracing of accounts. In the context of one-to-one communications this manuscript postulates the identification of the sender of a message as a useful approach to detect impersonation attacks in interactive communication scenarios. In particular this work proposes to select linguistic features extracted from messages via NLP techniques by means of a novel feature selection algorithm based on the dissociation between essential traits of the sender and receiver influences. The performance and computational efficiency of different supervised learning models when incorporating the proposed feature selection method is shown to be promising with real SMS data in terms of identification accuracy, and paves the way towards future research lines focused on applying the concept of language typicality in the discourse analysis field.",science,712,not included
10.1016/j.cose.2016.05.005,to_check,Computers and Security,scopus,2016-08-01,sciencedirect,íntegro: leveraging victim prediction for robust fake account detection in large scale osns,https://api.elsevier.com/content/abstract/scopus_id/84974733314,"Detecting fake accounts in online social networks (OSNs) protects both OSN operators and their users from various malicious activities. Most detection mechanisms attempt to classify user accounts as real (i.e., benign, honest) or fake (i.e., malicious, Sybil) by analyzing either user-level activities or graph-level structures. These mechanisms, however, are not robust against adversarial attacks in which fake accounts cloak their operation with patterns resembling real user behavior.
                  In this article, we show that victims – real accounts whose users have accepted friend requests sent by fakes – form a distinct classification category that is useful for designing robust detection mechanisms. In particular, we present Íntegro – a robust and scalable defense system that leverages victim classification to rank most real accounts higher than fakes, so that OSN operators can take actions against low-ranking fake accounts. Íntegro starts by identifying potential victims from user-level activities using supervised machine learning. After that, it annotates the graph by assigning lower weights to edges incident to potential victims. Finally, Íntegro ranks user accounts based on the landing probability of a short random walk that starts from a known real account. As this walk is unlikely to traverse low-weight edges in a few steps and land on fakes, Íntegro achieves the desired ranking.
                  We implemented Íntegro using widely-used, open-source distributed computing platforms, where it scaled nearly linearly. We evaluated Íntegro against SybilRank, which is the state-of-the-art in fake account detection, using real-world datasets and a large-scale deployment at Tuenti – the largest OSN in Spain with more than 15 million active users. We show that Íntegro significantly outperforms SybilRank in user ranking quality, with the only requirement that the employed victim classifier is better than random. Moreover, the deployment of Íntegro at Tuenti resulted in up to an order of magnitude higher precision in fake account detection, as compared to SybilRank.",science,713,not included
10.1016/j.cmpb.2016.04.005,to_check,Computer Methods and Programs in Biomedicine,scopus,2016-07-01,sciencedirect,a mapreduce approach to diminish imbalance parameters for big deoxyribonucleic acid dataset,https://api.elsevier.com/content/abstract/scopus_id/84964534094,"Background
                  In the age of information superhighway, big data play a significant role in information processing, extractions, retrieving and management. In computational biology, the continuous challenge is to manage the biological data. Data mining techniques are sometimes imperfect for new space and time requirements. Thus, it is critical to process massive amounts of data to retrieve knowledge. The existing software and automated tools to handle big data sets are not sufficient. As a result, an expandable mining technique that enfolds the large storage and processing capability of distributed or parallel processing platforms is essential.
               
                  Method
                  In this analysis, a contemporary distributed clustering methodology for imbalance data reduction using k-nearest neighbor (K-NN) classification approach has been introduced. The pivotal objective of this work is to illustrate real training data sets with reduced amount of elements or instances. These reduced amounts of data sets will ensure faster data classification and standard storage management with less sensitivity. However, general data reduction methods cannot manage very big data sets. To minimize these difficulties, a MapReduce-oriented framework is designed using various clusters of automated contents, comprising multiple algorithmic approaches.
               
                  Results
                  To test the proposed approach, a real DNA (deoxyribonucleic acid) dataset that consists of 90 million pairs has been used. The proposed model reduces the imbalance data sets from large-scale data sets without loss of its accuracy.
               
                  Conclusions
                  The obtained results depict that MapReduce based K-NN classifier provided accurate results for big data of DNA.",science,714,not included
10.1016/j.ijepes.2015.12.013,to_check,International Journal of Electrical Power and Energy Systems,scopus,2016-06-01,sciencedirect,demand response governed swarm intelligent grid scheduling framework for social welfare,https://api.elsevier.com/content/abstract/scopus_id/84952836614,"Peak load defines the generation, transmission and distribution capacity of interconnected power network. As load changes throughout the day and the year, electricity systems must be able to deliver the maximum load at all times, which will be hard trade for a practical power network. Smart grid technologies show strong potential to optimize asset utilization by shifting peak load to off peak times, thereby decoupling the electricity growth from peak load growth. Under Smart grid trade regulation, with continuous varying demand pattern, electricity price will be uneven as well. On this view point, in order to obtain a flatten demand, without affecting the welfare of the market participants, this paper presents an on-going effort to develop Demand Response (DR) governed swarm intelligence based stochastic peak load modeling methodology capable of restoring the market equilibrium during price and demand oscillations of the real-time smart power networks. This proposed DR based methodology allows generators and loads to interact in an automated fashion in real time, coordinating demand to flatten spikes and thereby minimizing erratic variations of price of electricity. For proper utilization of DR connectivity, a Curtailment Limiting Index (CLI) has been formulated, monitoring which in real time, for each of the Load Dispatch Centers (LDCs), the system operator can shape the electricity demand according to the available capacity of generation, transmission and distribution assets. The proposed methodology can also be highlighted for generating the most economical schedule for social welfare with standard operational status in terms of voltage profile, system loss and optimal load curtailment. The case study has been carried out in IEEE 30 bus scenario as well as on a practical 203 bus-265 line power network (Indian Eastern Grid) with both generator characteristics and price responsive demand characteristics or DR as inputs and illustrious Particle Swarm Optimization (PSO) technique has assisted the fusion of the proposed model and methodology. Encouraging simulation results suggest that, the effective deployment of this methodology may lead to an operating condition where an overall benefit of all the power market participants with standard operational status can be ensured and the misuse of electricity will be minimized.",science,715,not included
10.1016/j.procs.2016.05.474,to_check,Procedia Computer Science,scopus,2016-01-01,sciencedirect,wowmon: a machine learning-based profiler for self-adaptive instrumentation of scientific workflows,https://api.elsevier.com/content/abstract/scopus_id/84978472514,"Performance debugging using program profiling and tracing for scientific workflows can be extremely difficult for two reasons. 1) Existing performance tools lack the ability to automatically produce global performance data based on local information from coupled scientific applications of workflows, particularly at runtime. 2) Profiling/tracing with static instrumentation may incur high overhead and significantly slow down science-critical tasks. To gain more insights on workflows we introduce a lightweight workflow monitoring infrastructure, WOW-MON (WOrkfloW MONitor), which enables user's access not only to cross-application performance data such as end-to-end latency and execution time of individual workflow components at runtime, but also to customized performance events. To reduce profiling overhead, WOW-MON uses adaptive selection of performance metrics based on machine learning algorithms to guide profilers collecting only metrics that have most impact on performance of workflows. Through the study of real scientific workflows (e.g., LAMMPS) with the help of WOWMON, we found that the performance of the workflows can be significantly affected by both software and hardware factors, such as the policy of process mapping and in-situ buffer size. Moreover, we experimentally show that WOWMON can reduce data movement for profiling by up to 54% without missing the key metrics for performance debugging.",science,716,not included
10.1016/j.ascom.2015.01.002,to_check,Astronomy and Computing,scopus,2015-04-01,sciencedirect,the overlooked potential of generalized linear models in astronomy-ii: gamma regression and photometric redshifts,https://api.elsevier.com/content/abstract/scopus_id/84922370372,"Machine learning techniques offer a precious tool box for use within astronomy to solve problems involving so-called big data. They provide a means to make accurate predictions about a particular system without prior knowledge of the underlying physical processes of the data. In this article, and the companion papers of this series, we present the set of Generalized Linear Models (GLMs) as a fast alternative method for tackling general astronomical problems, including the ones related to the machine learning paradigm. To demonstrate the applicability of GLMs to inherently positive and continuous physical observables, we explore their use in estimating the photometric redshifts of galaxies from their multi-wavelength photometry. Using the gamma family with a log link function we predict redshifts from the PHoto-z Accuracy Testing simulated catalogue and a subset of the Sloan Digital Sky Survey from Data Release 10. We obtain fits that result in catastrophic outlier rates as low as ∼1% for simulated and ∼2% for real data. Moreover, we can easily obtain such levels of precision within a matter of seconds on a normal desktop computer and with training sets that contain merely thousands of galaxies. Our software is made publicly available as a user-friendly package developed in Python, R and via an interactive web application. This software allows users to apply a set of GLMs to their own photometric catalogues and generates publication quality plots with minimum effort. By facilitating their ease of use to the astronomical community, this paper series aims to make GLMs widely known and to encourage their implementation in future large-scale projects, such as the Large Synoptic Survey Telescope.",science,717,not included
10.1016/j.snb.2014.12.136,to_check,"Sensors and Actuators, B: Chemical",scopus,2015-01-01,sciencedirect,zebra gc: a mini gas chromatography system for trace-level determination of hazardous air pollutants,https://api.elsevier.com/content/abstract/scopus_id/84923321088,"A ready-to-deploy implementation of a microfabricated gas chromatography (μGC) system characterized for detecting hazardous air pollutants (HAPs) at parts-per-billion (ppb) concentrations in complex mixtures has been described. A microfabricated preconcentrator (μPC), MEMS separation column with on-chip thermal conductivity detector (μSC-TCD), flow controller unit, and all necessary flow and thermal management as well as user interface circuitry are integrated to realize a fully functional μGC system. The work reports extensive characterization of μPC and μSC-TCD for target analytes: benzene, toluene, tetrachloroethylene, chlorobenzene, ethylbenzene, and p-xylene. A Limit of Detection (LOD) of ∼1ng was achieved, which corresponds to a sampling time of 10min at a flow rate of 1mL/min for an analyte present at ∼25ppbv. An innovative method using flow-manipulation generated sharp injection plugs from the μPC even in the presence of a flow-sensitive detector like a μTCD. The μGC system is compared against conventional automated thermal desorption–gas chromatography–flame ionization detector (ATD–GC–FID) system for real gasoline samples in simulated car refueling scenario. The μGC system detected five peaks, including three of the target analytes and required ∼3 orders of magnitude lower sample volume as compared to the conventional system.",science,718,not included
10.1016/j.actaastro.2012.06.003,to_check,Acta Astronautica,scopus,2012-11-01,sciencedirect,"robotic mission to mars: hands-on, minds-on, web-based learning",https://api.elsevier.com/content/abstract/scopus_id/84865224510,"Problem-based learning has been demonstrated as an effective methodology for developing analytical skills and critical thinking. The use of scenario-based learning incorporates problem-based learning whilst encouraging students to collaborate with their colleagues and dynamically adapt to their environment. This increased interaction stimulates a deeper understanding and the generation of new knowledge. The Victorian Space Science Education Centre (VSSEC) uses scenario-based learning in its Mission to Mars, Mission to the Orbiting Space Laboratory and Primary Expedition to the M.A.R.S. Base programs. These programs utilize methodologies such as hands-on applications, immersive-learning, integrated technologies, critical thinking and mentoring to engage students in Science, Technology, Engineering and Mathematics (STEM) and highlight potential career paths in science and engineering. The immersive nature of the programs demands specialist environments such as a simulated Mars environment, Mission Control and Space Laboratory, thus restricting these programs to a physical location and limiting student access to the programs. To move beyond these limitations, VSSEC worked with its university partners to develop a web-based mission that delivered the benefits of scenario-based learning within a school environment. The Robotic Mission to Mars allows students to remotely control a real rover, developed by the Australian Centre for Field Robotics (ACFR), on the VSSEC Mars surface. After completing a pre-mission training program and site selection activity, students take on the roles of scientists and engineers in Mission Control to complete a mission and collect data for further analysis. Mission Control is established using software developed by the ACRI Games Technology Lab at La Trobe University using the principles of serious gaming. The software allows students to control the rover, monitor its systems and collect scientific data for analysis. This program encourages students to work scientifically and explores the interaction between scientists and engineers. This paper presents the development of the program, including the involvement of university students in the development of the rover, the software, and the collation of the scientific data. It also presents the results of the trial phase of this program including the impact on student engagement and learning outcomes.",science,719,not included
10.1016/j.neunet.2012.02.029,to_check,Neural Networks,scopus,2012-08-01,sciencedirect,real-time human-robot interaction underlying neurorobotic trust and intent recognition,https://api.elsevier.com/content/abstract/scopus_id/84861781307,"In the past three decades, the interest in trust has grown significantly due to its important role in our modern society. Everyday social experience involves “confidence” among people, which can be interpreted at the neurological level of a human brain. Recent studies suggest that oxytocin is a centrally-acting neurotransmitter important in the development and alteration of trust. Its administration in humans seems to increase trust and reduce fear, in part by directly inhibiting the amygdala. However, the cerebral microcircuitry underlying this mechanism is still unknown. We propose the first biologically realistic model for trust, simulating spiking neurons in the cortex in a real-time human–robot interaction simulation. At the physiological level, oxytocin cells were modeled with triple apical dendrites characteristic of their structure in the paraventricular nucleus of the hypothalamus. As trust was established in the simulation, this architecture had a direct inhibitory effect on the amygdala tonic firing, which resulted in a willingness to exchange an object from the trustor (virtual neurorobot) to the trustee (human actor). Our software and hardware enhancements allowed the simulation of almost 100,000 neurons in real time and the incorporation of a sophisticated Gabor mechanism as a visual filter. Our brain was functional and our robotic system was robust in that it trusted or distrusted a human actor based on movement imitation.",science,720,not included
10.1016/j.bmcl.2012.05.123,to_check,Bioorganic and Medicinal Chemistry Letters,scopus,2012-07-15,sciencedirect,cclab - a multi-objective genetic algorithm based combinatorial library design software and an application for histone deacetylase inhibitor design,https://api.elsevier.com/content/abstract/scopus_id/84863430722,"The introduction of the multi-objective optimization has dramatically changed the virtual combinatorial library design, which can consider many objectives simultaneously, such as synthesis cost and drug-likeness, thus may increase positive rates of biological active compounds. Here we described a software called CCLab (Combinatorial Chemistry Laboratory) for combinatorial library design based on the multi-objective genetic algorithm. Tests of the convergence ability and the ratio to re-take the building blocks in the reference library were conducted to assess the software in silico, and then it was applied to a real case of designing a 5×6 HDAC inhibitor library. Sixteen compounds in the resulted library were synthesized, and the histone deactetylase (HDAC) enzymatic assays proved that 14 compounds showed inhibitory ratios more than 50% against tested 3 HDAC enzymes at concentration of 20μg/mL, with IC50 values of 3 compounds comparable to SAHA. These results demonstrated that the CCLab software could enhance the hit rates of the designed library and would be beneficial for medicinal chemists to design focused library in drug development (the software can be downloaded at: http://202.127.30.184:8080/drugdesign.html).",science,721,not included
10.1016/j.eswa.2012.01.059,to_check,Expert Systems with Applications,scopus,2012-07-01,sciencedirect,analyzing the solutions of dea through information visualization and data mining techniques: smartdea framework,https://api.elsevier.com/content/abstract/scopus_id/84858339895,"Data envelopment analysis (DEA) has proven to be a useful tool for assessing efficiency or productivity of organizations, which is of vital practical importance in managerial decision making. DEA provides a significant amount of information from which analysts and managers derive insights and guidelines to promote their existing performances. Regarding to this fact, effective and methodologic analysis and interpretation of DEA results are very critical. The main objective of this study is then to develop a general decision support system (DSS) framework to analyze the results of basic DEA models. The paper formally shows how the results of DEA models should be structured so that these solutions can be examined and interpreted by analysts through information visualization and data mining techniques effectively. An innovative and convenient DEA solver, SmartDEA, is designed and developed in accordance with the proposed analysis framework. The developed software provides DEA results which are consistent with the framework and are ready-to-analyze with data mining tools, thanks to their specially designed table-based structures. The developed framework is tested and applied in a real world project for benchmarking the vendors of a leading Turkish automotive company. The results show the effectiveness and the efficacy of the proposed framework.",science,722,included
10.1016/b978-0-444-59520-1.50104-4,to_check,Computer Aided Chemical Engineering,scopus,2012-01-01,sciencedirect,intelligent automation platform for bioprocess development,https://api.elsevier.com/content/abstract/scopus_id/84862870640,"High throughput technology has been increasingly adapted for drug screen and bioprocess development, due to the small amount of processing materials and reagents required and parallel experiment execution. It allows a wide design space to be explored in order to discover novel bioprocess solutions. Currently, the high throughput experiments for bioprocess development are implemented in a sequential fashion in which liquid handling system will perform the web lab experiment to prepare the samples; standalone analysis devices detect the data such as protein concentration; and specific software is used to realise the data analysis for process design or further experimentation.
                  The aim of this paper is to show how the efficiency of the high throughput bioprocess development approach can be enhanced by creating an intelligent automation platform that systematically drives liquid handling system, analysis devices and data analysis to perform a closed-loop learning. The first generation prototype has been established which consists of three parts: automated devices, design algorithms and database. In order to prove the concept of prototype, both simulation and real experiments studies have been established. In this case study, the platform is used to investigate the solubility of lysozyme at various ion strengths and pH values. Tecan liquid handling system for experimentation as well as buffer preparation and a plate reader for uv absorption measurement to determine protein concentration were used as the automated devices. The simplex search algorithm and artificial neural network modelling were utilised as design algorithm to iteratively select the experiments to execute and determine the optimal design solution. An entity-relationship database with Tecan system configuration information and experimental data was established. The results demonstrate this integrated approach can implement experiments and data analysis automatically to provide specific bioprocess design solutions in a closed loop strategy at first time. It is a promising approach that may significant increase the level of lab automation to release the engineer from the labour intensive R&D activities and provides the base for sophisticated artificial intelligent learning in the future.",science,723,included
https://core.ac.uk/download/459213790.pdf,to_check,core,"[{'title': 'Journal of Artificial Intelligence Research', 'identifiers': ['issn:1076-9757', '1076-9757']}]",2020-01-01 00:00:00,core,'ai access foundation',To regulate or not: A social dynamics analysis of an idealised ai race,"PTDC/EEI-SII/5081/2014 PTDC/MAT/STA/3358/2014 UIDB/50021/2020 grant number 31257234 FLAG-ERA JCT 2016 FP2-154 UIDB/04516/2020Rapid technological advancements in Artificial Intelligence (AI), as well as the growing deployment of intelligent technologies in new application domains, have generated serious anxiety and a fear of missing out among different stake-holders, fostering a racing narrative. Whether real or not, the belief in such a race for domain supremacy through AI, can make it real simply from its consequences, as put forward by the Thomas theorem. These consequences may be negative, as racing for technological supremacy creates a complex ecology of choices that could push stake-holders to underestimate or even ignore ethical and safety procedures. As a consequence, different actors are urging to consider both the normative and social impact of these technological advancements, contemplating the use of the precautionary principle in AI innovation and research. Yet, given the breadth and depth of AI and its advances, it is difficult to assess which technology needs regulation and when. As there is no easy access to data describing this alleged AI race, theoretical models are necessary to understand its potential dynamics, allowing for the identification of when procedures need to be put in place to favour outcomes beneficial for all. We show that, next to the risks of setbacks and being reprimanded for unsafe behaviour, the time-scale in which domain supremacy can be achieved plays a crucial role. When this can be achieved in a short term, those who completely ignore the safety precautions are bound to win the race but at a cost to society, apparently requiring regulatory actions. Our analysis reveals that imposing regulations for all risk and timing conditions may not have the anticipated effect as only for specific conditions a dilemma arises between what is individually preferred and globally beneficial. Similar observations can be made for the long-term development case. Yet different from the short-term situation, conditions can be identified that require the promotion of risk-taking as opposed to compliance with safety regulations in order to improve social welfare. These results remain robust both when two or several actors are involved in the race and when collective rather than individual setbacks are produced by risk-taking behaviour. When defining codes of conduct and regulatory policies for applications of AI, a clear understanding of the time-scale of the race is thus required, as this may induce important non-trivial effects.publishersversionpublishe",science,724,not included
"[{'title': 'fuzzy sets and systems', 'identifiers': ['0165-0114', 'issn:0165-0114']}]",to_check,core,'Elsevier BV',2019-11-01 00:00:00,core,the fuzzy kalman filter: improving its implementation by reformulating uncertainty representation,10.1016/j.fss.2019.10.015,"The fuzzy Kalman filter (FKF), introduced some years ago, is revisited. In the initial version, trapezoidal possibility distributions functions instead of gaussian probability distributions were proposed, and trapezius were modeled by four representative points. Nevertheless, and although the algorithm works properly, an implementation problem occurs when propagating uncertainty through a non-linear function in multi-variable systems, something that was solved by linearization. In this work we propose an alternative method to represent uncertainty, still using trapezoidal distributions, which avoids the previous inconvenience and eases the Kalman filter steps computation. We reformulate the FKF algorithm, presenting a new theoretical approach as well as validation tests in both simulation and a real mobile robot.This work has been funded by Spanish Ministry of Science, Innovation and Universities (Artificial Intelligence Techniques and Assistance to Autonomous Navigation, reference DPI 2017-86915-C3-3-R). It has also received funding from the RoboCity2030-DIH-CM Madrid Robotics Digital Innovation Hub “Robótica aplicada a la mejora de la calidad de vida de los ciudadanos, fase IV”; S2018/NMT-4331), funded by “Programas de Actividades I+D en la Comunidad de Madrid” and cofunded by Structural Funds of the EU.Peer reviewe",science,725,not included
10.1051/0004-6361/201833390,to_check,core,"A new method for unveiling Open Clusters in Gaia: new nearby Open
  Clusters confirmed by DR2",2018-06-12 00:00:00,core,http://arxiv.org/abs/1805.03045,,"The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in
Astronomy. It includes precise astrometric data (positions, proper motions and
parallaxes) for more than $1.3$ billion sources, mostly stars. To analyse such
a vast amount of new data, the use of data mining techniques and machine
learning algorithms are mandatory. The search for Open Clusters, groups of
stars that were born and move together, located in the disk, is a great example
for the application of these techniques. Our aim is to develop a method to
automatically explore the data space, requiring minimal manual intervention. We
explore the performance of a density based clustering algorithm, DBSCAN, to
find clusters in the data together with a supervised learning method such as an
Artificial Neural Network (ANN) to automatically distinguish between real Open
Clusters and statistical clusters. The development and implementation of this
method to a $5$-Dimensional space ($l$, $b$, $\varpi$, $\mu_{\alpha^*}$,
$\mu_\delta$) to the Tycho-Gaia Astrometric Solution (TGAS) data, and a
posterior validation using Gaia DR2 data, lead to the proposal of a set of new
nearby Open Clusters. We have developed a method to find OCs in astrometric
data, designed to be applied to the full Gaia DR2 archive.Comment: 18 pages, accepted by Astronomy and Astrophysics (A&A) the 11th June,
  201",science,727,not included
10.1007/978-3-319-70338-1,to_check,core,"An overview and comparative analysis of Recurrent Neural Networks for
  Short Term Load Forecasting",2018-07-20 00:00:00,core,http://arxiv.org/abs/1705.04378,,"The key component in forecasting demand and consumption of resources in a
supply network is an accurate prediction of real-valued time series. Indeed,
both service interruptions and resource waste can be reduced with the
implementation of an effective forecasting system. Significant research has
thus been devoted to the design and development of methodologies for short term
load forecasting over the past decades. A class of mathematical models, called
Recurrent Neural Networks, are nowadays gaining renewed interest among
researchers and they are replacing many practical implementation of the
forecasting systems, previously based on static methods. Despite the undeniable
expressive power of these architectures, their recurrent nature complicates
their understanding and poses challenges in the training procedures. Recently,
new important families of recurrent architectures have emerged and their
applicability in the context of load forecasting has not been investigated
completely yet. In this paper we perform a comparative study on the problem of
Short-Term Load Forecast, by using different classes of state-of-the-art
Recurrent Neural Networks. We test the reviewed models first on controlled
synthetic tasks and then on different real datasets, covering important
practical cases of study. We provide a general overview of the most important
architectures and we define guidelines for configuring the recurrent networks
to predict real-valued time series.Comment: Springer Briefs in Computer Science (ISBN 978-3-319-70338-1), 201",science,728,not included
10.1016/s0278-2626(03)00037-x,to_check,core,'Elsevier BV',2014-01-01 00:00:00,core,rapid eye movement sleep dreaming is characterized by uncoupled eeg activity between frontal and perceptual cortical regions,,"The image processing nowadays is a field in development, many image filtering algorithms are tested every day; however, the main hurdles to overcome are the difficulty of implementation or the time response in a general purpose processors. When the amount of data is too big, a specific hardware accelerator is required because a software implementation or a generic processor is not fast enough to respond in real time. In this paper optimal hardware implementation is proposed for extracting edges and noise reduction of an image in real time. Furthermore, the hardware configuration is flexible with the ability to select between power and area optimization or speed and performance. The results of algorithms implementation are reported. "" Springer International Publishing Switzerland 2014."",,,,,,,,,""http://hdl.handle.net/20.500.12104/44077"",""http://www.scopus.com/inward/record.url?eid=2-s2.0-84921625630&partnerID=40&md5=4306e6c932b0395c7c861a9ca3104934"",,,,,,,,""Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"",,""8",science,732,not included
10.1109/icra.2019.8793510,to_check,2019 International Conference on Robotics and Automation (ICRA),IEEE,2019-05-24 00:00:00,ieeexplore,bonnet: an open-source training and deployment framework for semantic segmentation in robotics using cnns,https://ieeexplore.ieee.org/document/8793510/,"The ability to interpret a scene is an important capability for a robot that is supposed to interact with its environment. The knowledge of what is in front of the robot is, for example, relevant for navigation, manipulation, or planning. Semantic segmentation labels each pixel of an image with a class label and thus provides a detailed semantic annotation of the surroundings to the robot. Convolutional neural networks (CNNs) are popular methods for addressing this type of problem. The available software for training and the integration of CNNs for real robots, however, is quite fragmented and often difficult to use for non-experts, despite the availability of several high-quality open-source frameworks for neural network implementation and training. In this paper, we propose a tool called Bonnet, which addresses this fragmentation problem by building a higher abstraction that is specific for the semantic segmentation task. It provides a modular approach to simplify the training of a semantic segmentation CNN independently of the used dataset and the intended task. Furthermore, we also address the deployment on a real robotic platform. Thus, we do not propose a new CNN approach in this paper. Instead, we provide a stable and easy-to-use tool to make this technology more approachable in the context of autonomous systems. In this sense, we aim at closing a gap between computer vision research and its use in robotics research. We provide an open-source codebase for training and deployment. The training interface is implemented in Python using TensorFlow and the deployment interface provides C++ library that can be easily integrated in an existing robotics codebase, a ROS node, and two standalone applications for label prediction in images and videos.",robotics,733,included
10.1109/icirca48905.2020.9182995,to_check,2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA),IEEE,2020-07-17 00:00:00,ieeexplore,an approach for digital farming using mobile robot,https://ieeexplore.ieee.org/document/9182995/,"Farming is the backbone of the Indian economy and it has been unchartered territory for a technological solution. As of late developments in Artificial Intelligence technology combined with Robotics has paved the way for an option of digital farming. As a matter of fact, Indian farming has been facing various challenges that include abrupt change in climatic conditions, spoiling of yields, soil nutrient requirement, pests/weed control and so forth. Robotics and Artificial Intelligence (AI) along with the integration of various sensors ensures the possibility of better outcome. In this work the simulation of Mobile robot for the purpose of seed sowing along with its movement has been presented. The implementation comprises of the Motor schema for the navigation of robot and Gale Shapley (GS) algorithm for stable match of seed and yield combination. Such a robotic system combined with AI in real time will form excellent means of farming in terms of yield.",robotics,734,not included
10.1109/iros40897.2019.8967694,to_check,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),IEEE,2019-11-08 00:00:00,ieeexplore,benchmarking and workload analysis of robot dynamics algorithms,https://ieeexplore.ieee.org/document/8967694/,"Rigid body dynamics calculations are needed for many tasks in robotics, including online control. While there currently exist several competing software implementations that are sufficient for use in traditional control approaches, emerging sophisticated motion control techniques such as nonlinear model predictive control demand orders of magnitude more frequent dynamics calculations. Current software solutions are not fast enough to meet that demand for complex robots. The goal of this work is to examine the performance of current dynamics software libraries in detail. In this paper, we (i) survey current state-of-the-art software implementations of the key rigid body dynamics algorithms (RBDL, Pinocchio, Rigid-BodyDynamics.jl, and RobCoGen), (ii) establish a methodology for benchmarking these algorithms, and (iii) characterize their performance through real measurements taken on a modern hardware platform. With this analysis, we aim to provide direction for future improvements that will need to be made to enable emerging techniques for real-time robot motion control. To this end, we are also releasing our suite of benchmarks to enable others to help contribute to this important task.",robotics,735,not included
10.1109/cnna.2000.876849,to_check,Proceedings of the 2000 6th IEEE International Workshop on Cellular Neural Networks and their Applications (CNNA 2000) (Cat. No.00TH8509),IEEE,2000-05-25 00:00:00,ieeexplore,design of a dedicated cnn chip for autonomous robot navigation,https://ieeexplore.ieee.org/document/876849/,"Obstacle avoidance is the main issue in autonomous robotics. It requires a three-dimensional effective environment sensing in real time. Among the others, the stereo vision approach to environmental information extraction seems to be very appealing, even if it leads an extremely high computational cost. However, a high performance implementation of this algorithm on a cellular neural network is able to overcome these difficulties. In the paper, the design of a CNN chip well suited for this algorithm is presented. This chip, performing a real time processing of the stereo vision data, will improve the cruising speed of a robotic platform.",robotics,736,not included
10.1109/saci.2007.375494,to_check,2007 4th International Symposium on Applied Computational Intelligence and Informatics,IEEE,2007-05-18 00:00:00,ieeexplore,fpga parallel implementation of cmac type neural network with on chip learning,https://ieeexplore.ieee.org/document/4262496/,"The hardware implementation of neural networks is a new step in the evolution and use of neural networks in practical applications. The CMAC cerebellar model articulation controller is intended especially for hardware implementation, and this type of network is used successfully in the areas of robotics and control, where the real time capabilities of the network are of particular importance. The implementation of neural networks on FPGA's has several benefits, with emphasis on parallelism and the real time capabilities. This paper discusses the hardware implementation of the CMAC type neural network, the architecture and parameters and the functional modules of the hardware implemented neuro-processor.",robotics,737,included
10.23919/acc.1992.4792313,to_check,1992 American Control Conference,IEEE,1992-06-26 00:00:00,ieeexplore,learning for skill acquisition and refinement: toward exploring everyday physics,https://ieeexplore.ieee.org/document/4792313/,"The present talk claims that ""robotics"" is not a test bed for AI but should involve a research frontier, which attempts to account for intelligibility of everyday physics underlying human activities such as perception, remembrance, planning, practices, and skill. In addition to traditional AI and neuro-network approaches, more of new domains that can account for any aspect of human intellectual behaviors must be exploited, and also more of new tools that actualize real implementation of intelligence in machines need to be devised. To aim at going on an expedition in this direction, this talk introduces one new domain and another new tool. The former is practice-based learning for skill refinement and the latter is a design tool of signal-based structured information base for skill acquirement.",robotics,738,not included
10.1109/iros.1991.174419,to_check,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,IEEE,1991-11-05 00:00:00,ieeexplore,learning for skill refinement,https://ieeexplore.ieee.org/document/174419/,"It is claimed that 'robotics' is not a test bed for AI but should involve a research frontier relating to the physics underlying human activities such as perception, remembering, planning, practice, and skill. In addition to traditional AI and neural network approaches, other domains that can account for any aspect of human intellectual behavior must be exploited, and tools that actualize real implementation of intelligence in machines need to be devised. A practice-based learning domain for skill refinement and a design tool for a signal-based structured information base for skill acquisition are presented.&lt;<ETX>&gt;</ETX>",robotics,739,not included
10.1109/optim.2008.4602496,to_check,2008 11th International Conference on Optimization of Electrical and Electronic Equipment,IEEE,2008-05-24 00:00:00,ieeexplore,neural control based on rbf network implemented on fpga,https://ieeexplore.ieee.org/document/4602496/,"The RBF radial basis function network is intended especially for hardware implementation and this type of network is used successfully in the areas of robotics and control, where the real time capabilities of the network are of particular importance. The implementation of neural networks on FPGA has several benefits, with emphasis on parallelism and the real time capabilities. This paper discusses the hardware implementation of the RBF type neural network, the architecture and parameters and the functional modules of the hardware implemented neuro-processor.",robotics,740,not included
10.1109/icra.2016.7487351,to_check,2016 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2016-05-21 00:00:00,ieeexplore,object discovery and grasp detection with a shared convolutional neural network,https://ieeexplore.ieee.org/document/7487351/,"Grasp an object from a stack of objects in real-time is still a challenge in robotics. This requires the robot to have the ability of both fast object discovery and grasp detection: a target object should be picked out from the stack first and then a proper grasp configuration is applied to grasp the object. In this paper, we propose a shared convolutional neural network (CNN) which can simultaneously implement these two tasks in real-time. The processing speed of the model is about 100 frames per second on a GPU which largely satisfies the requirement. Meanwhile, we also establish a labeled RGBD dataset which contains scenes of stacked objects for robotic grasping. At last, we demonstrate the implementation of our shared CNN model on a real robotic platform and show that the robot can accurately discover a target object from the stack and successfully grasp it.",robotics,741,included
10.1109/ijcnn.2016.7727359,to_check,2016 International Joint Conference on Neural Networks (IJCNN),IEEE,2016-07-29 00:00:00,ieeexplore,self-repairing mobile robotic car using astrocyte-neuron networks,https://ieeexplore.ieee.org/document/7727359/,"A self-repairing robot utilising a spiking astrocyte-neuron network is presented in this paper. It uses the output spike frequency of neurons to control the motor speed and robot activation. A software model of the astrocyte-neuron network previously demonstrated self-detection of faults and its self-repairing capability. In this paper the application demonstrator of mobile robotics is employed to evaluate the fault-tolerant capabilities of the astrocyte-neuron network when implemented in a hardware-based robotic car system. Results demonstrated that when 20% or less synapses associated with a neuron are faulty, the robot car can maintain system performance and complete the task of forward motion correctly. If 80% synapses are faulty, the system performance shows a marginal degradation, however this degradation is much smaller than that of conventional fault-tolerant techniques under the same levels of faults. This is the first time that astrocyte cells merged within spiking neurons demonstrates a self-repairing capabilities in the hardware system for a real application.",robotics,742,not included
10.1109/isc2.2016.7580798,to_check,2016 IEEE International Smart Cities Conference (ISC2),IEEE,2016-09-15 00:00:00,ieeexplore,smartseal: a ros based home automation framework for heterogeneous devices interconnection in smart buildings,https://ieeexplore.ieee.org/document/7580798/,"With this paper we present the SmartSEAL inter-connection system developed for the nationally founded SEAL project. SEAL is a research project aimed at developing Home Automation (HA) solutions for building energy management, user customization and improved safety of its inhabitants. One of the main problems of HA systems is the wide range of communication standards that commercial devices use. Usually this forces the designer to choose devices from a few brands, limiting the scope of the system and its capabilities. In this context, SmartSEAL is a framework that aims to integrate heterogeneous devices, such as sensors and actuators from different vendors, providing networking features, protocols and interfaces that are easy to implement and dynamically configurable. The core of our system is a Robotics middleware called Robot Operating System (ROS). We adapted the ROS features to the HA problem, designing the network and protocol architectures for this particular needs. These software infrastructure allows for complex HA functions that could be realized only levering the services provided by different devices. The system has been tested in our laboratory and installed in two real environments, Palazzo Fogazzaro in Schio and “Le Case” childhood school in Malo. Since one of the aim of the SEAL project is the personalization of the building environment according to the user needs, and the learning of their patterns of behaviour, in the final part of this work we also describe the ongoing design and experiments to provide a Machine Learning based re-identification module implemented with Convolutional Neural Networks (CNNs). The description of the adaptation module complements the description of the SmartSEAL system and helps in understanding how to develop complex HA services through it.",robotics,743,included
10.1109/fie.2008.4720346,to_check,2008 38th Annual Frontiers in Education Conference,IEEE,2008-10-25 00:00:00,ieeexplore,"teaching concepts in fuzzy logic using low cost robots, pdas, and custom software",https://ieeexplore.ieee.org/document/4720346/,"Fuzzy logic is a topic traditionally taught in artificial intelligence, machine learning, and robotics courses. Students receive the necessary mathematical and theoretical foundation in lecture format. The final learning experience may require that students create and code their own fuzzy logic application that solves a real world problem. This can be an issue when the target is a bioengineering course that introduces classical control theory, fuzzy logic, neural networks, genetic algorithms and genetic programming through the use of a low cost robot, personal digital assistant (PDA) handheld computer, and custom PDA software. In this course, the concepts and theories discussed in lecture are reinforced and extended in a corresponding laboratory through the use of wireless robots and PDAs. Fuzzy logic libraries and software modules for laptops and desktop computers are readily available, however, when it comes to handheld computers no such libraries exist. Students are able to spend more time experimenting with different fuzzy logic controllers when a custom fuzzy logic library and PDA graphical user interface are utilized. In this paper we introduce and discuss a unique low cost wireless robot, a custom fuzzy logic library, a custom fuzzy logic GUI for the PDA, and the implementation results for the fuzzy logic section in a newly created bioengineering course. Diagnostic and summative assessment in the form of a pre-test and post-test was administered for each section of the course, however, only the results for the fuzzy logic section will be provided.",robotics,744,not included
10.1109/eeei.2014.7005895,to_check,2014 IEEE 28th Convention of Electrical & Electronics Engineers in Israel (IEEEI),IEEE,2014-12-05 00:00:00,ieeexplore,verification of safety for autonomous unmanned ground vehicles,https://ieeexplore.ieee.org/document/7005895/,"The existing tools for hardware and software reliability and safety engineering do not supply sufficient solutions regarding AI (Artificial Intelligent) adaptive and learning algorithms, which are being used in autonomous robotics and massively rely on designer experience and include methods such as Heuristic, Rules based decision, Fuzzy Logic, Neural Networks, and Genetic Algorithms, Bayes Networks, etc. Since it is obvious that only this kind of algorithms can deal with the complexity and the uncertainty of the real world environment, suitable safety validation methodology is required. In this paper we present the limitation of the existing reliability and safety engineering tools in dealing with autonomous systems and propose a novel methodology based on statistical testing in simulated environment.",robotics,745,not included
10.1109/access.2020.3038605,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,a gentle introduction to reinforcement learning and its application in different fields,https://ieeexplore.ieee.org/document/9261348/,"Due to the recent progress in Deep Neural Networks, Reinforcement Learning (RL) has become one of the most important and useful technology. It is a learning method where a software agent interacts with an unknown environment, selects actions, and progressively discovers the environment dynamics. RL has been effectively applied in many important areas of real life. This article intends to provide an in-depth introduction of the Markov Decision Process, RL and its algorithms. Moreover, we present a literature review of the application of RL to a variety of fields, including robotics and autonomous control, communication and networking, natural language processing, games and self-organized system, scheduling management and configuration of resources, and computer vision.",robotics,746,not included
10.1109/tsmc.2020.2967936,to_check,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",IEEE,2021-12-01 00:00:00,ieeexplore,deep q-learning with q-matrix transfer learning for novel fire evacuation environment,https://ieeexplore.ieee.org/document/8989970/,"Deep reinforcement learning (RL) is achieving significant success in various applications like control, robotics, games, resource management, and scheduling. However, the important problem of emergency evacuation, which clearly could benefit from RL, has been largely unaddressed. Indeed, emergency evacuation is a complex task that is difficult to solve with RL. An emergency situation is highly dynamic, with a lot of changing variables and complex constraints that make it challenging to solve. Also, there is no standard benchmark environment available that can be used to train RL agents for evacuation. A realistic environment can be complex to design. In this article, we propose the first fire evacuation environment to train RL agents for evacuation planning. The environment is modeled as a graph capturing the building structure. It consists of realistic features like fire spread, uncertainty, and bottlenecks. The implementation of our environment is in the OpenAI gym format, to facilitate future research. We also propose a new RL approach that entails pretraining the network weights of a DQN-based agent [DQN/Double-DQN (DDQN)/Dueling-DQN] to incorporate information on the shortest path to the exit. We achieved this by using tabular <inline-formula> <tex-math notation=""LaTeX"">$Q$ </tex-math></inline-formula>-learning to learn the shortest path on the building model’s graph. This information is transferred to the network by deliberately overfitting it on the <inline-formula> <tex-math notation=""LaTeX"">$Q$ </tex-math></inline-formula>-matrix. Then, the pretrained DQN model is trained on the fire evacuation environment to generate the optimal evacuation path under time varying conditions due to fire spread, bottlenecks, and uncertainty. We perform comparisons of the proposed approach with state-of-the-art RL algorithms like DQN, DDQN, Dueling-DQN, PPO, VPG, state-action-reward-state-action (SARSA), actor–critic method, and ACKTR. The results show that our method is able to outperform state-of-the-art models by a huge margin including the original DQN-based models. Finally, our model is tested on a large and complex real building consisting of 91 rooms, with the possibility to move to any other room, hence giving 8281 actions. In order to reduce the action space, we propose a strategy that involves one step simulation. That is, an action importance vector is added to the final output of the pretrained DQN and acts like an attention mechanism. Using this strategy, the action space is reduced by 90.1%. In this manner, the model is able to deal with large action spaces. Hence, our model achieves near optimal performance on the real world emergency environment.",robotics,747,included
10.1109/lra.2017.2665694,to_check,IEEE Robotics and Automation Letters,IEEE,2017-04-01 00:00:00,ieeexplore,shakey 2016—how much does it take to redo shakey the robot?,https://ieeexplore.ieee.org/document/7847341/,"Shakey the robot was one of the first autonomous robots that showed impressive capabilities of navigation and mobile manipulation. Since then, robotics research has made great progress, showing more and more capable robotic systems for a large variety of application domains and tasks. In this letter, we look back on decades of research by rebuilding Shakey with modern robotics technology in the open-source Shakey 2016 system. Hereby, we demonstrate the impact of research by showing that ideas from the original Shakey are still alive in state-of-the-art systems, while robotics in general has improved to deliver more robust and more capable software and hardware. Our Shakey 2016 system has been implemented on real robots and leverages mostly open-source software. We experimentally evaluate the system in real-world scenarios on a PR2 robot and a Turtlebot-based robot and particularly investigate the development effort. The experiments documented in this letter demonstrate that results from robotics research are readily available for building complex robots such as Shakey within a short amount of time and little effort.",robotics,748,included
10.1109/aiiot52608.2021.9454183,to_check,2021 IEEE World AI IoT Congress (AIIoT),IEEE,2021-05-13 00:00:00,ieeexplore,image classification with knowledge-based systems on the edge for real-time danger avoidance in robots,https://ieeexplore.ieee.org/document/9454183/,"Mobile robots are increasingly common in society and are increasingly being used for complex and high-stakes tasks such as search and rescue. The growing requirements for these robots demonstrate a need for systems which can review and react in real time to environmental hazards, which will allow robots to handle environments that are both dynamic and dangerous. We propose and test a system which allows mobile robots to reclassify environmental objects during operation in conjunction with an edge system. We train an image classification model with 99 percent accuracy and deploy it in conjunction with an edge server and JSON-based ruleset to allow robots to react to and avoid hazards.",robotics,749,included
10.1109/smartworld.2018.00106,to_check,"2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",IEEE,2018-10-12 00:00:00,ieeexplore,real-time data processing architecture for multi-robots based on differential federated learning,https://ieeexplore.ieee.org/document/8560084/,"The emergency of ubiquitous intelligence in various things has become the ultimate cornerstone in building a smart interconnection of the physical world and the human world, which also caters to the idea of Internet of Things (IoT). Nowadays, robots as a new type of ubiquitous IoT devices have gained much attention. With the increasing number of distributed multi-robots, such smart environment generates unprecedented amounts of data. Robotic applications are faced with challenges of such big data: the serious real-time assurance and data privacy. Therefore, in order to obtain the big data values via knowledge sharing under the premise of ensuring the real-time data processing and data privacy, we propose a real-time data processing architecture for multi-robots based on the differential federated learning, called RT-robots architecture. A global shared model with differential privacy protection is trained on the cloud iteratively and distributed to multiple edge robots in each round, and the robotic tasks are processed locally in real time. Our implementation and experiments demonstrate that our architecture can be applied on multiple robotic recognition tasks, balance the trade-off between the performance and privacy.",robotics,750,included
10.1109/smicnd.2005.1558827,to_check,"CAS 2005 Proceedings. 2005 International Semiconductor Conference, 2005.",IEEE,2005-10-05 00:00:00,ieeexplore,virtual environment for robots interfaces design and testing,https://ieeexplore.ieee.org/document/1558827/,"This paper refers to the implementation of a virtual environment for the robot interfaces testing. This software environment is very useful because, comparing to the experiments with real robots, it allow the testing and evaluation of different types of interfaces and different working environments with diverse configurations. A very important facility of this interactive software environment is the fact that the designers of the robots sensors and interfaces are able to work in parallel to design test, optimize and realize different control devices for the robot",robotics,751,not included
10.1109/tamd.2010.2086453,to_check,IEEE Transactions on Autonomous Mental Development,IEEE,2010-12-01 00:00:00,ieeexplore,multilevel darwinist brain (mdb): artificial evolution in a cognitive architecture for real robots,https://ieeexplore.ieee.org/document/5599851/,"The multilevel Darwinist brain (MDB) is a cognitive architecture that follows an evolutionary approach to provide autonomous robots with lifelong adaptation. It has been tested in real robot on-line learning scenarios obtaining successful results that reinforce the evolutionary principles that constitute the main original contribution of the MDB. This preliminary work has lead to a series of improvements in the computational implementation of the architecture so as to achieve realistic operation in real time, which was the biggest problem of the approach due to the high computational cost induced by the evolutionary algorithms that make up the MDB core. The current implementation of the architecture is able to provide an autonomous robot with real time learning capabilities and the capability for continuously adapting to changing circumstances in its world, both internal and external, with minimal intervention of the designer. This paper aims at providing an overview or the architecture and its operation and defining what is required in the path towards a real cognitive robot following a developmental strategy. The design, implementation and basic operation of the MDB cognitive architecture are presented through some successful real robot learning examples to illustrate the validity of this evolutionary approach.",robotics,752,included
10.1109/acsos-c51401.2020.00067,to_check,2020 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C),IEEE,2020-08-21 00:00:00,ieeexplore,a deep domain-specific model framework for self-reproducing robotic control systems,https://ieeexplore.ieee.org/document/9196470/,"As robots play more critical roles in diverse and complex scenarios in the real world, monomorphic robots are limited to repeating and rather simple tasks. How to achieve a robust, flexible, and scalable multi-robot system becomes essential research. Model-driven software development (MDSD) provides a sturdy methodology for robotic programming using multilevel domain-specific languages (DSLs). These DSLs lay a solid foundation for the design, integration, and extensibility of robotic applications. In this paper, we propose a deep domain-specific model framework for the self-reproducing robotic control system to escort reliable, versatile tasks of heterogeneous robots.",robotics,753,not included
10.1109/rose52553.2021.00011,to_check,2021 IEEE/ACM 3rd International Workshop on Robotics Software Engineering (RoSE),IEEE,2021-06-02 00:00:00,ieeexplore,a modeling tool for reconfigurable skills in ros,https://ieeexplore.ieee.org/document/9474550/,"Known attempts to build autonomous robots rely on complex control architectures, often implemented with the Robot Operating System platform (ROS). The implementation of adaptable architectures is very often ad hoc, quickly gets cumbersome and expensive. Reusable solutions that support complex, runtime reasoning for robot adaptation have been seen in the adoption of ontologies. While the usage of ontologies significantly increases system reuse and maintainability, it requires additional effort from the application developers to translate requirements into formal rules that can be used by an ontological reasoner. In this paper, we present a design tool that facilitates the specification of reconfigurable robot skills. Based on the specified skills, we generate corresponding runtime models for self-adaptation that can be directly deployed to a running robot that uses a reasoning approach based on ontologies. We demonstrate the applicability of the tool in a real robot performing a patrolling mission at a university campus.",robotics,754,included
10.1109/cig.2011.6032027,to_check,2011 IEEE Conference on Computational Intelligence and Games (CIG'11),IEEE,2011-09-03 00:00:00,ieeexplore,a neuronal global workspace for human-like control of a computer game character,https://ieeexplore.ieee.org/document/6032027/,"This paper describes a system that uses a global workspace architecture implemented in spiking neurons to control an avatar within the Unreal Tournament 2004 (UT2004) computer game. This system is designed to display human-like behaviour within UT2004, which provides a good environment for comparing human and embodied AI behaviour without the cost and difficulty of full humanoid robots. Using a biologically-inspired approach, the architecture is loosely based on theories about the high level control circuits in the brain, and it is the first neural implementation of a global workspace that is embodied in a dynamic real time environment. At its current stage of development the system can navigate through UT2004 and shoot opponents. We are currently completing the implementation and testing in preparation for the human-like bot competition at CIG 2011 in September.",robotics,755,not included
10.1109/iccitechn.2016.7860248,to_check,2016 19th International Conference on Computer and Information Technology (ICCIT),IEEE,2016-12-20 00:00:00,ieeexplore,a support vector machine approach for real time vision based human robot interaction,https://ieeexplore.ieee.org/document/7860248/,"Today humanoid robots are being exhibited to redact various task as a personal assistant of a human. To be an assistant, a robot needs to interact with human as a human. For this reason robot needs to understand the human gender, facial expression, facial gesture in real time. Ribo - A humanoid robot build in RoboSUST lab which has the ability to communicate in Bangla with the people speaking in Bengali. In this article the authors show the implementation of theoretical knowledge of the recognition of real time facial expression, detection of human gender and yes / no from facial gesture in Ribo. Real time facial expression and gender detection can be performed using Support Vector Machine (SVM). A prepared dataset containing the facial landmarks leveled as five different expression: sad, angry, smile, surprise and normal, is given to SVM to construct a classifier. For the prediction of any expression, facial images are taken in real time and provided the facial landmarks data to SVM. Local Binary Pattern(LBP) algorithm is used for extracting features from face images. These features leveled as male and female are responsible to build the classifier. The face gesture for detecting `yes/no' is performed by tracking the movement of face in a certain time. After those implementations the principal results will make a framework that will be used in Ribo to recognize human facial expression, facial gesture movement and detect human gender.",robotics,756,not included
10.1109/ijcnn.2015.7280807,to_check,2015 International Joint Conference on Neural Networks (IJCNN),IEEE,2015-07-17 00:00:00,ieeexplore,applying the canonical distributed embodied evolution algorithm in a collective indoor navigation task,https://ieeexplore.ieee.org/document/7280807/,"The automatic design of control systems for multi-robot teams that operate in real time is not affordable with traditional evolutionary algorithms mainly due to the huge computational requirements they imply. Embodied Evolution (EE) is an evolutionary paradigm that aims to address this problem through the embodiment of the individuals that make up the population in the physical robots. The interest for this type of evolutionary approach has been increasing steadily, leading to different algorithms and variations adapted to solve very specific practical cases. In a previous work, the authors started the implementation of a standard canonical EE algorithm that captures the more general principles of this paradigm and that can be applied to any distributed optimization problem. This canonical algorithm has been characterized already over a set of theoretical fitness landscapes corresponding to representative examples of the basic casuistry found in collective tasks. The current paper goes one step ahead in this research line, and the canonical algorithm is applied here in a collective navigation task in which a fleet of Micro Aerial Vehicles (MAVs) has to gather red rocks in an indoor scenario. The objective is to confirm that the characterization conclusions are generalizable to a practical case and to show that the canonical algorithm can be configured to operate as a specific algorithm easily.",robotics,757,included
10.1109/ams.2017.22,to_check,2017 Asia Modelling Symposium (AMS),IEEE,2017-12-06 00:00:00,ieeexplore,autonomous rover navigation using gps based path planning,https://ieeexplore.ieee.org/document/8424312/,"Nowadays, with the constant evolution of Artificial Intelligence and Machine Learning, robots are getting more perceptive than ever. For this quality they are being used in varying circumstances which humans cannot control. Rovers are special robots, capable of traversing through areas that are too difficult for humans. Even though it is a robust bot, lack of proper intelligence and automation are its basic shortcomings. As the main purpose of a rover is to traverse through areas of extreme difficulties, therefore an intelligent path generation and following system is highly required. Our research work aimed at developing an algorithm for autonomous path generation using GPS (Global Positioning System) based coordinate system and implementation of this algorithm in real life terrain, which in our case is MDRS, Utah, USA. Our prime focus was the development of a robust but easy to implement system. After developing such system, we have been able to successfully traverse our rover through that difficult terrain. It uses GPS coordinates of target points that will be fed into the rover from a control station. The rover capturing its own GPS signal generates a path between the current location and the destination location on its own. It then finds the deviation in its current course of direction and position. And eventually it uses Proportional Integral Derivative control loop feedback mechanism (PID control algorithm) for compensating the error or deviation and thus following that path and reach destination. A low cost on board computer (Raspberry Pi in our case) handles all the calculations during the process and drives the rover fulfilling its task using an microcontroller (Arduino).",robotics,758,included
10.1109/ijcnn.2008.4633875,to_check,2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),IEEE,2008-06-08 00:00:00,ieeexplore,bio-inspired stochastic chance-constrained multi-robot task allocation using wsn,https://ieeexplore.ieee.org/document/4633875/,"The multi-robot task allocation (MRTA) especially in unknown complex environment is one of the fundamental problems, a mostly important object in research of multi-robot. The MRTA problem is initially formulated as a chance-constrained optimization problem. Monte Carlo simulation is used to verify the accuracy of the solution provided by the algorithm. Ant colony optimization (ACO) algorithm based on bionic swarm intelligence was used. A hybrid intelligent algorithm combined Monte Carlo simulation and neural network is used for solving stochastic chance constrained models of MRTA. A practical implementation with real WSN and real mobile robots were carried out. In environment the successful implementation of tasks without collision validates the efficiency, stability and accuracy of the proposed algorithm. The convergence curve shows that as iterative generation grows, the utility increases and finally reaches a stable and optimal value. Results show that using sensor information fusion can greatly improve the efficiency. The algorithm is proved better than tradition algorithms without WSN for MRTA in real time.",robotics,759,included
10.1109/icra.2016.7487617,to_check,2016 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2016-05-21 00:00:00,ieeexplore,decentralized multi-agent exploration with online-learning of gaussian processes,https://ieeexplore.ieee.org/document/7487617/,"Exploration is a crucial problem in safety of life applications, such as search and rescue missions. Gaussian processes constitute an interesting underlying data model that leverages the spatial correlations of the process to be explored to reduce the required sampling of data. Furthermore, multi-agent approaches offer well known advantages for exploration. Previous decentralized multi-agent exploration algorithms that use Gaussian processes as underlying data model, have only been validated through simulations. However, the implementation of an exploration algorithm brings difficulties that were not tackle yet. In this work, we propose an exploration algorithm that deals with the following challenges: (i) which information to transmit to achieve multi-agent coordination; (ii) how to implement a light-weight collision avoidance; (iii) how to learn the data's model without prior information. We validate our algorithm with two experiments employing real robots. First, we explore the magnetic field intensity with a ground-based robot. Second, two quadcopters equipped with an ultrasound sensor explore a terrain profile. We show that our algorithm outperforms a meander and a random trajectory, as well as we are able to learn the data's model online while exploring.",robotics,760,included
10.1109/ijcnn.2017.7965938,to_check,2017 International Joint Conference on Neural Networks (IJCNN),IEEE,2017-05-19 00:00:00,ieeexplore,modeling direction selective visual neural network with on and off pathways for extracting motion cues from cluttered background,https://ieeexplore.ieee.org/document/7965938/,"The nature endows animals robust vision systems for extracting and recognizing different motion cues, detecting predators, chasing preys/mates in dynamic and cluttered environments. Direction selective neurons (DSNs), with preference to certain orientation visual stimulus, have been found in both vertebrates and invertebrates for decades. In this paper, with respect to recent biological research progress in motion-detecting circuitry, we propose a novel way to model DSNs for recognizing movements on four cardinal directions. It is based on an architecture of ON and OFF visual pathways underlies a theory of splitting motion signals into parallel channels, encoding brightness increments and decrements separately. To enhance the edge selectivity and speed response to moving objects, we put forth a bio-plausible spatial-temporal network structure with multiple connections of same polarity ON/OFF cells. Each pair-wised combination is filtered with dynamic delay depending on sampling distance. The proposed vision system was challenged against image streams from both synthetic and cluttered real physical scenarios. The results demonstrated three major contributions: first, the neural network fulfilled the characteristics of a postulated physiological map of conveying visual information through different neuropile layers; second, the DSNs model can extract useful directional motion cues from cluttered background robustly and timely, which hits at potential of quick implementation in vision-based micro mobile robots; moreover, it also represents better speed response compared to a state-of-the-art elementary motion detector.",robotics,761,not included
10.1109/aims.2014.23,to_check,"2014 2nd International Conference on Artificial Intelligence, Modelling and Simulation",IEEE,2014-11-20 00:00:00,ieeexplore,online tool for benchmarking of simulated intervention autonomous underwater vehicles: evaluating position controllers in changing underwater currents,https://ieeexplore.ieee.org/document/7102468/,"Benchmarking is nowadays an issue on robotic research platforms, due to the fact that it is not easy to reproduce previous experiments and knowing in detail in which real conditions other algorithms have been performed. Having a web-based tool to configure and execute benchmarks opens the door to new opportunities as the design of virtual tele-laboratories that permit the implementation of new algorithms using specific and detailed constraints. This is fundamental for designing benchmarks that allow the experiments to be made in a more scientific manner, taking into account that these experiments should be able to be reproduced again by other people under the same circumstances. In the context of underwater interventions with semi-autonomous robots, the situation gets even more interesting, specially those performed on real sea scenarios, which are expensive, and difficult to perform and reproduce. This paper presents the recent advances in the online configuration tool for benchmarking, a tool that is continuously being improved in our laboratory. Our last contribution focuses on evaluating position controllers for changing underwater currents and the possibility for the user to upload its own controllers to the benchmarking tool to get online performance results.",robotics,762,not included
10.1109/cdc.2006.377499,to_check,Proceedings of the 45th IEEE Conference on Decision and Control,IEEE,2006-12-15 00:00:00,ieeexplore,path generation using matrix representations of previous robot state data,https://ieeexplore.ieee.org/document/4178112/,"Humans learn by repetition and using past experiences. It is possible for robots to act in a similar fashion. By representing past path traversal experiences with matrices, a new path can be generated without relying on calculations of complex dynamics or control laws. This paper presents one approach for allowing robots to use past experience to generate new paths and control actions. This approach relies on using several matrices to associate each new input value with previous robot states. An example is provided and analyzed which shows a successful simulated implementation of this approach. In addition a real world test of the approach was conducted which demonstrates that the implementation not only generates new paths, but does so fast enough to be feasible for real time systems",robotics,763,included
10.1109/sii.2010.5708353,to_check,2010 IEEE/SICE International Symposium on System Integration,IEEE,2010-12-22 00:00:00,ieeexplore,realization and analysis of giant-swing motion using q-learning,https://ieeexplore.ieee.org/document/5708353/,"Many research papers have reported studies on sports robots that realize giant-swing motion. However, almost all these robots were controlled using trajectory planning methods, and few robots realized giant-swing motion by learning. Consequently, in this study, we attempted to construct a humanoid robot that realizes giant-swing motion by Q-learning, a reinforcement learning technique. The significant aspect of our study is that few robotic models were constructed beforehand; the robot learns giant-swing motion only by interaction with the environment during simulations. Our implementation faced several problems such as imperfect perception of the velocity state and robot posture issues caused by using only the arm angle. However, our real robot realized giant-swing motion by averaging the Q value and by using rewards - the absolute angle of the foot angle and the angular velocity of the arm angle-in the simulated learning data; the sampling time was 250 ms. Furthermore, the feasibility of generalization of learning for realizing selective motion in the forward and backward rotational directions was investigated; it was revealed that the generalization of learning is feasible as long as it does not interfere with the robot's motions.",robotics,764,not included
10.1109/iros.2018.8593518,to_check,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),IEEE,2018-10-05 00:00:00,ieeexplore,simultaneous end-user programming of goals and actions for robotic shelf organization,https://ieeexplore.ieee.org/document/8593518/,"Arrangement of items on shelves in stores or warehouses is a tedious, repetitive task that can be feasible for robots to perform. The diversity of products that are available in stores and the different setups and preferences of each store makes pre-programming a robot for this task extremely challenging. Instead, our work argues for enabling end-users to customize the robot to their specific objects and setup at deployment time by programming it themselves. To that end, this paper contributes (i) a task representation for shelf arrangements based on a large dataset of grocery store shelf images, (ii) a method for inferring goal configurations from user inputs including demonstrations and direct parameter specifications, and (iii) a system implementation of the proposed approach that allows simultaneously learning task goals and actions. We evaluate our goal inference approach with ten different teaching strategies that combine alternative user inputs in different ways on the large dataset of grocery configurations, as well as with real human teachers through an online user study (N=32). We evaluate our full system implemented on a Fetch mobile manipulator on eight benchmark tasks that demonstrate end-to-end programming and execution of shelf arrangement tasks.",robotics,765,not included
10.1109/3477.499796,to_check,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",IEEE,1996-06-01 00:00:00,ieeexplore,hidden state and reinforcement learning with instance-based state identification,https://ieeexplore.ieee.org/document/499796/,"Real robots with real sensors are not omniscient. When a robot's next course of action depends on information that is hidden from the sensors because of problems such as occlusion, restricted range, bounded field of view and limited attention, we say the robot suffers from the hidden state problem. State identification techniques use history information to uncover hidden state. Some previous approaches to encoding history include: finite state machines, recurrent neural networks and genetic programming with indexed memory. A chief disadvantage of all these techniques is their long training time. This paper presents instance-based state identification, a new approach to reinforcement learning with state identification that learns with much fewer training steps. Noting that learning with history and learning in continuous spaces both share the property that they begin without knowing the granularity of the state space, the approach applies instance-based (or ""memory-based"") learning to history sequences-instead of recording instances in a continuous geometrical space, we record instances in action-percept-reward sequence space. The first implementation of this approach, called Nearest Sequence Memory, learns with an order of magnitude fewer steps than several previous approaches.",robotics,766,not included
10.1109/tase.2014.2377791,to_check,IEEE Transactions on Automation Science and Engineering,IEEE,2015-04-01 00:00:00,ieeexplore,roboearth semantic mapping: a cloud enabled knowledge-based approach,https://ieeexplore.ieee.org/document/7015601/,"The vision of the RoboEarth project is to design a knowledge-based system to provide web and cloud services that can transform a simple robot into an intelligent one. In this work, we describe the RoboEarth semantic mapping system. The semantic map is composed of: 1) an ontology to code the concepts and relations in maps and objects and 2) a SLAM map providing the scene geometry and the object locations with respect to the robot. We propose to ground the terminological knowledge in the robot perceptions by means of the SLAM map of objects. RoboEarth boosts mapping by providing: 1) a subdatabase of object models relevant for the task at hand, obtained by semantic reasoning, which improves recognition by reducing computation and the false positive rate; 2) the sharing of semantic maps between robots; and 3) software as a service to externalize in the cloud the more intensive mapping computations, while meeting the mandatory hard real time constraints of the robot. To demonstrate the RoboEarth cloud mapping system, we investigate two action recipes that embody semantic map building in a simple mobile robot. The first recipe enables semantic map building for a novel environment while exploiting available prior information about the environment. The second recipe searches for a novel object, with the efficiency boosted thanks to the reasoning on a semantically annotated map. Our experimental results demonstrate that, by using RoboEarth cloud services, a simple robot can reliably and efficiently build the semantic maps needed to perform its quotidian tasks. In addition, we show the synergetic relation of the SLAM map of objects that grounds the terminological knowledge coded in the ontology.",robotics,767,included
10.1109/robio.2017.8324803,to_check,2017 IEEE International Conference on Robotics and Biomimetics (ROBIO),IEEE,2017-12-08 00:00:00,ieeexplore,classification-lock tracking approach applied on person following robot,https://ieeexplore.ieee.org/document/8324803/,"The task of following a person in the real complex environment by camera still keeps at risk even the visual tracking technologies have been well studied in the last decade. Currently, most approaches only utilize single-shot initialization in the first frame and update their tracking models according to the result of the last frame. However, it leads to an uncorrected target selection once the inner appearance changes, i.e., a feature-rich object is moved out of the human. In this paper, we reveal a classification-lock tracking framework and apply our approach on a mobile platform. A pairwise cluster tracker is used to locate the person. A positive &amp; negative classifier is utilized to verify the tracker's result and update tracking model. In addition, a pre-trained CPU optimized neural network is employed to lock the tracking result to only be human. In the experiment, we deploy the common challenges of visual tracking both on the static scene and a real-following task. Furthermore, our approach is compared with other state-of-art approaches on common datasets. Results prove the tracking quality of our approach in both the static and the dynamic scenes. Our approach achieves the best average score on the common dataset.",robotics,768,not included
10.1109/ictc49870.2020.9289214,to_check,2020 International Conference on Information and Communication Technology Convergence (ICTC),IEEE,2020-10-23 00:00:00,ieeexplore,learning control policy with previous experiences from robot simulator,https://ieeexplore.ieee.org/document/9289214/,"Advances in deep reinforcement learning enabled cost-efficient training of control policy of physical robot actions from robot simulators. Learning control policy in a simulated environment is cost-efficient over learning in a real environment. Reward engineering is one of the key components to train efficient control policy. For tasks with long horizons such as navigation and manipulation, a sparse reward is providing limited information. The robot simulator for a physical engine of physical robot manipulation has made it easy for researchers in the field of deep reinforcement learning to simulate complicated robot manipulation environments. In this paper, A robot manipulation simulator and a deep RL framework are utilized for implement a training control policy by utilizing previous experiences. For implementation, Recent innovation Hindsight Experience Replay (HER) algorithms with previous experiences to calculate dense rewards from a sparse reward is leveraged . Proposed implementation showed an approach to investigate the reward engineering method to formulate dense reward in robot manipulator tasks.",robotics,769,not included
10.1109/robot.2004.1308781,to_check,"IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004",IEEE,2004-05-01 00:00:00,ieeexplore,software approach for the autonomous inspection robot makro,https://ieeexplore.ieee.org/document/1308781/,"The sewer inspection robot MAKRO is an autonomous multi-segment robot with worm-like shape driven by wheels. It is currently under development in the project MAKRO-PLUS. The robot has to navigate autonomously within sewer systems. Its first tasks is to take water probes, analyze them onboard, and measure positions of manholes and pipes to detect pollution loaded sewage and to improve current maps of sewer systems. One of the challenging problems is the control software, which should enable the robot to navigate in the sewer system and perform the inspection tasks autonomously, while always taking care of its own safety. Tests in our test environment and in a real sewer system show promising results. This paper focuses on the software approach. To manage the complexity a layered architecture has been chosen, each layer defining a different level of abstraction. After determining the abstraction levels, we use different methods for implementation. For the highest abstraction level a standard AI-planning algorithm is used. For the next level, finite state automata has been chosen. For ""simple"" task implementation we use a modular C++ based method (MCA2), which is also used on the lowest software level.",robotics,770,included
10.1109/tsmcc.2004.840063,to_check,"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",IEEE,2005-11-01 00:00:00,ieeexplore,"""sticky hands"": learning and generalization for cooperative physical interactions with a humanoid robot",https://ieeexplore.ieee.org/document/1522534/,"""Sticky Hands"" is a physical game for two people involving gentle contact with the hands. The aim is to develop relaxed and elegant motion together, achieve physical sensitivity-improving reactions, and experience an interaction at an intimate yet comfortable level for spiritual development and physical relaxation. We developed a control system for a humanoid robot allowing it to play Sticky Hands with a human partner. We present a real implementation including a physical system, robot control, and a motion learning algorithm based on a generalizable intelligent system capable itself of generalizing observed trajectories' translation, orientation, scale and velocity to new data, operating with scalable speed and storage efficiency bounds, and coping with contact trajectories that evolve over time. Our robot control is capable of physical cooperation in a force domain, using minimal sensor input. We analyze robot-human interaction and relate characteristics of our motion learning algorithm with recorded motion profiles. We discuss our results in the context of realistic motion generation and present a theoretical discussion of stylistic and affective motion generation based on, and motivating cross-disciplinary research in computer graphics, human motion production and motion perception.",robotics,771,not included
10.1109/tcyb.2013.2275291,to_check,IEEE Transactions on Cybernetics,IEEE,2013-10-01 00:00:00,ieeexplore,real-time multiple human perception with color-depth cameras on a mobile robot,https://ieeexplore.ieee.org/document/6583249/,"The ability to perceive humans is an essential requirement for safe and efficient human-robot interaction. In real-world applications, the need for a robot to interact in real time with multiple humans in a dynamic, 3-D environment presents a significant challenge. The recent availability of commercial color-depth cameras allow for the creation of a system that makes use of the depth dimension, thus enabling a robot to observe its environment and perceive in the 3-D space. Here we present a system for 3-D multiple human perception in real time from a moving robot equipped with a color-depth camera and a consumer-grade computer. Our approach reduces computation time to achieve real-time performance through a unique combination of new ideas and established techniques. We remove the ground and ceiling planes from the 3-D point cloud input to separate candidate point clusters. We introduce the novel information concept, depth of interest, which we use to identify candidates for detection, and that avoids the computationally expensive scanning-window methods of other approaches. We utilize a cascade of detectors to distinguish humans from objects, in which we make intelligent reuse of intermediary features in successive detectors to improve computation. Because of the high computational cost of some methods, we represent our candidate tracking algorithm with a decision directed acyclic graph, which allows us to use the most computationally intense techniques only where necessary. We detail the successful implementation of our novel approach on a mobile robot and examine its performance in scenarios with real-world challenges, including occlusion, robot motion, nonupright humans, humans leaving and reentering the field of view (i.e., the reidentification challenge), human-object and human-human interaction. We conclude with the observation that the incorporation of the depth information, together with the use of modern techniques in new ways, we are able to create an accurate system for real-time 3-D perception of humans by a mobile robot.",robotics,772,included
10.1109/ijcnn48605.2020.9207332,to_check,2020 International Joint Conference on Neural Networks (IJCNN),IEEE,2020-07-24 00:00:00,ieeexplore,deep reinforcement learning control of hand-eye coordination with a software retina,https://ieeexplore.ieee.org/document/9207332/,"Deep Reinforcement Learning (DRL) has gained much attention for solving robotic hand-eye coordination tasks from raw pixel values. Despite promising results, training agents using images is hardware intensive often requiring millions of training steps to converge incurring long training times and increased risk of wear and tear on the robot. To speed up training, images are often cropped and downscaled resulting in a smaller field of view and loss of valuable high-frequency data. In this paper, we propose training the vision system using supervised learning prior to training robotic actuation using Deep Deterministic Policy Gradient (DDPG). The vision system uses a software retina, based on the mammalian retino-cortical transform, to preprocess full-size images to compress image data while preserving the full field of view and high-frequency visual information around the fixation point prior to processing by a Deep Convolutional Neural Network (DCNN) to extract visual state information. Using the vision system to preprocess the environment improves the agent's sample complexity and network update speed leading to significantly faster training with reduced image data loss. Our method is used to train a DRL system to control a real Baxter robot's arm, processing full-size images captured by an in-wrist camera to locate an object on a table and centre the camera over it by actuating the robot arm.",robotics,773,included
10.23919/iccas50221.2020.9268370,to_check,"2020 20th International Conference on Control, Automation and Systems (ICCAS)",IEEE,2020-10-16 00:00:00,ieeexplore,deep reinforcement learning-based ros-controlled rc car for autonomous path exploration in the unknown environment,https://ieeexplore.ieee.org/document/9268370/,"Nowadays, Deep reinforcement learning has become the front runner to solve problems in the field of robot navigation and avoidance. This paper presents a LiDAR-equipped RC car trained in the GAZEBO environment using the deep reinforcement learning method. This paper uses reshaped LiDAR data as the data input of the neural architecture of the training network. This paper also presents a unique way to convert the LiDAR data into a 2D grid map for the input of training neural architecture. It also presents the test result from the training network in different GAZEBO environment. It also shows the development of hardware and software systems of embedded RC car. The hardware system includes-Jetson AGX Xavier, teensyduino and Hokuyo LiDAR; the software system includes-ROS and Arduino C. Finally, this paper presents the test result in the real world using the model generated from training simulation.",robotics,774,included
10.1109/ascc.2017.8287420,to_check,2017 11th Asian Control Conference (ASCC),IEEE,2017-12-20 00:00:00,ieeexplore,deep learning for picking point detection in dense cluster,https://ieeexplore.ieee.org/document/8287420/,"This paper considers the problem of picking objects in cluster. This requires the robot to reliably detect the picking point for the known or unseen objects under the environment with occlusion, disorder and a variety of objects. We present a novel pipeline to detect picking point based on deep convolutional neural network (CNN). A two-dimensional picking configuration is proposed, thus an extensive data augmentation strategy is enabled and a labeled dataset is established quickly and easily. At last, we demonstrate the implementation of our method on a real robot and show that our method can accurately detect picking point of unseen objects and achieve a pick success of 91% in cluster bin-picking scenario.",robotics,775,not included
10.1109/isic.1994.367848,to_check,Proceedings of 1994 9th IEEE International Symposium on Intelligent Control,IEEE,1994-08-18 00:00:00,ieeexplore,fuzzy neural network implementation of self tuning pid control systems,https://ieeexplore.ieee.org/document/367848/,"The fuzzy cognitive map (FCM) is a powerful universal method for representation of knowledge in various domains. The fuzzy inference engine can be implemented in the form of a network of FCMs. FCM implementation of the inference engine provides a suitable mechanism for expert control systems and information engineers to embed acquired human expertise, which is often imprecise, vague, or incomplete. The exploitation of an online learning algorithm empowers the fuzzy inference engine with the ability to modify its incomplete or possibly inconsistent knowledge base resulting in continuous improvement of the embedded knowledge. The fact that learning is an inherent feature of neural networks has inspired several researchers with the idea of using neural networks to implement fuzzy inference engines capable of learning. This paper presents a method for neural network FCM implementation of the fuzzy inference engine using the fuzzy columnar neural network architecture (FCNA). In this method the available human expertise is mapped first into an initial set of weights for the neurons. A new learning algorithm is then used to enhance the embedded knowledge in the neural network as a result of real time experience. The fuzzy inference engine (the neural network FCM) is used in computer simulations to control the speed of an underwater autonomous mobile robot. Results and computer simulation experiments are presented along with an evaluation of the new approach.&lt;<ETX>&gt;</ETX>",robotics,776,not included
10.1109/iros.1994.407570,to_check,Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS'94),IEEE,1994-09-16 00:00:00,ieeexplore,generation of optimal configuration for a redundant manipulator with a trained neural network,https://ieeexplore.ieee.org/document/407570/,"Redundant manipulators have more degrees of freedom than what is absolutely necessary for performing a task. The extra degrees of freedom can be used for avoiding obstacles or to optimize certain performance indices like manipulability or task compatibility. Maximizing manipulability keeps the manipulator away from singularities and provides more velocity transmission ratios in all directions. Optimizing task compatibility improves the force/velocity transmission ratios in the specified directions. However, the real time implementation of various optimizing algorithms is difficult because of the need of large computing time. In the present work, robot configurations for an optimum performance index are computed throughout the workspace. These configurations are then used to train a layered feed forward neural network (FFNN). During operation of the robot, the trained neural net outputs optimal configurations in real-time. The neural net captures the gross behaviour of the training data rather than memorizing the individual data, as in a lookup table. Thus its output is smooth and ideally suited for control purposes. We have simulated this approach on a 3-DOF redundant planar manipulator and the results are discussed in this paper.&lt;<ETX>&gt;</ETX>",robotics,777,not included
10.1109/ssst.1998.660084,to_check,Proceedings of Thirtieth Southeastern Symposium on System Theory,IEEE,1998-03-10 00:00:00,ieeexplore,implementation of a navigational neural network on a parallel dsp board,https://ieeexplore.ieee.org/document/660084/,"This work presents a neural network architecture that is motivated by the learning and memory characteristics of a part of the brain known as hippocampus, which is important in navigational behavior in humans and animals. Neural networks perform nonlinear transformations on data to yield suitable classification or control actions. In our case, the navigation network takes the distance information as data and maps it to control actions by the mobile robot. Navigation is a very important engineering problem for unknown or hazardous environments to ensure the safety of equipment and human life. Hardware implementation can benefit applications in real time where speed is the major concern. Our objective is to implement such a navigational neural network in parallel so that real time performance can be achieved by using a parallel DSP board system. Supplementary studies are also being carried out on the IBM SP2 supercomputer to understand the design and scaling properties of the parallel algorithm.",robotics,778,not included
10.1109/icmtma.2018.00075,to_check,2018 10th International Conference on Measuring Technology and Mechatronics Automation (ICMTMA),IEEE,2018-02-11 00:00:00,ieeexplore,implementing multi-dof trajectory tracking control system for robotic arm experimental platform,https://ieeexplore.ieee.org/document/8337386/,"To implement the control system of a multi-DOF robotic manipulator (Dobot), the robot dynamics, trajectory planning algorithm and motion control strategy are studied for designing the trajectory tracking control system. In this paper, the hardware and software of Dobot magician control system are designed. The hardware mainly includes STM32 controller. The software part mainly builds the host computer display interface, completes the protocol communication between the robot manipulator and the PC, so as to realize the trajectory tracking control of the robot manipulator and implement the track-following in real time. The experimental results show that the control system can accurately track the trajectory of robotic manipulator with a certain degree of real-time and stability.",robotics,779,not included
10.1109/lars-sbr.2016.49,to_check,2016 XIII Latin American Robotics Symposium and IV Brazilian Robotics Symposium (LARS/SBR),IEEE,2016-10-12 00:00:00,ieeexplore,integration of people detection and simultaneous localization and mapping systems for an autonomous robotic platform,https://ieeexplore.ieee.org/document/7783535/,"This paper presents the implementation of a people detection system for a robotic platform able to perform Simultaneous Localization and Mapping (SLAM), allowing the exploration and navigation of the robot considering people detection interaction. The robotic platform consists of a Pioneer 3DX robot equipped with an RGB-D camera, a Sick Lms200 sensor laser and a computer using the robot operating system ROS. The idea is to integrate the people detection system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. Furthermore, this paper presents an evaluation of two different approaches for the people detection system. The first one uses a manual feature extraction technique, and the other one is based on deep learning methods. The manual feature extraction method in the first approach is based on HOG (Histogram of Oriented Gradients) detectors. The accuracy of the techniques was evaluated using two different libraries. The PCL library (Point Cloud Library) implemented in C ++ and the VLFeat MatLab library with two HOG variants, the original one, and the DPM (Deformable Part Model) variant. The second approaches are based on a Deep Convolutional Neural Network (CNN), and it was implemented using the MatLab MatConvNet library. Tests were made objecting the evaluation of losses and false positives in the people's detection process in both approaches. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment.",robotics,780,included
10.1109/iros40897.2019.8968004,to_check,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),IEEE,2019-11-08 00:00:00,ieeexplore,long range neural navigation policies for the real world,https://ieeexplore.ieee.org/document/8968004/,"Learned Neural Network based policies have shown promising results for robot navigation. However, most of these approaches fall short of being used on a real robot due to the extensive simulated training they require. These simulations lack the visuals and dynamics of the real world, which makes it infeasible to deploy on a real robot. We present a novel Neural Net based policy, NavNet, which allows for easy deployment on a real robot. It consists of two sub policies - a high level policy which can understand real images and perform long range planning expressed in high level commands; a low level policy that can translate the long range plan into low level commands on a specific platform in a safe and robust manner. For every new deployment, the high level policy is trained on an easily obtainable scan of the environment modeling its visuals and layout. We detail the design of such an environment and how one can use it for training a final navigation policy. Further, we demonstrate a learned low-level policy. We deploy the model in a large office building and test it extensively, achieving 0.80 success rate over long navigation runs and outperforming SLAM-based models in the same settings.",robotics,781,included
10.1109/icra40945.2020.9196540,to_check,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,meta reinforcement learning for sim-to-real domain adaptation,https://ieeexplore.ieee.org/document/9196540/,"Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.",robotics,782,included
10.1109/cbs46900.2019.9114416,to_check,2019 IEEE International Conference on Cyborg and Bionic Systems (CBS),IEEE,2019-09-20 00:00:00,ieeexplore,"motion prediction of virtual patterns, human hand motions, and a simplified hand manipulation task with hierarchical temporal memory",https://ieeexplore.ieee.org/document/9114416/,"In this paper we utilize Numenta's Hierarchical Temporal Memory implementation NuPIC for online visual motion pattern prediction and test its performance on virtual animations as well as real world human motion data. For evaluation we run a series of progressively more complex experiments testing specific capabilities: Prediction of fixed-time noise-free motion animations, prediction of protocol-directed tasks with real-world camera captured human motion data, and lastly prediction of repetitive tasks performed without a strict protocol. Results show that the presented setup is able to predict time sequenced images as well as highly variable human motions increasingly well over several iterations. Limits are faced for non sequential variable hand motion execution: Here, predictions are made but do not improve in quality over time. The network runs online in real time and can be transferred to different tasks without expert knowledge. These characteristics qualify the setup for human robot interaction scenarios without the need for verified prediction accuracy.",robotics,783,not included
10.1109/ijcnn.1999.832713,to_check,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),IEEE,1999-07-16 00:00:00,ieeexplore,nonlinear system adaptive trajectory tracking by dynamic neural control,https://ieeexplore.ieee.org/document/832713/,"In this article, new nonlinear control techniques based on dynamic neural networks are presented. The authors discuss the implementation of a modified identification algorithm using dynamic neural networks as well as a control law, based on the neural identifier, which eliminates modeling error effects via sliding mode techniques. Simulation and real time results are presented for systems like an inverted pendulum and a full actuated robot manipulator.",robotics,784,not included
10.1109/med48518.2020.9183337,to_check,2020 28th Mediterranean Conference on Control and Automation (MED),IEEE,2020-09-18 00:00:00,ieeexplore,unsupervised learning for subterranean junction recognition based on 2d point cloud,https://ieeexplore.ieee.org/document/9183337/,"This article proposes a novel unsupervised learning framework for detecting the number of tunnel junctions in subterranean environments based on acquired 2D point clouds. The implementation of the framework provides valuable information for high level mission planners to navigate an aerial platform in unknown areas or robot homing missions. The framework utilizes spectral clustering, which is capable of uncovering hidden structures from connected data points lying on non-linear manifolds. The spectral clustering algorithm computes a spectral embedding of the original 2D point cloud by utilizing the eigen decomposition of a matrix that is derived from the pairwise similarities of these points. We validate the developed framework using multiple data-sets, collected from multiple realistic simulations, as well as from real flights in underground environments, demonstrating the performance and merits of the proposed methodology.",robotics,785,not included
10.1109/iros.2013.6696581,to_check,2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,IEEE,2013-11-07 00:00:00,ieeexplore,unsupervised learning of predictive parts for cross-object grasp transfer,https://ieeexplore.ieee.org/document/6696581/,"We present a principled solution to the problem of transferring grasps across objects. Our approach identifies, through autonomous exploration, the size and shape of object parts that consistently predict the applicability of a grasp across multiple objects. The robot can then use these parts to plan grasps onto novel objects. By contrast to most recent methods, we aim to solve the part-learning problem without the help of a human teacher. The robot collects training data autonomously by exploring different grasps on its own. The core principle of our approach is an intensive encoding of low-level sensorimotor uncertainty with probabilistic models, which allows the robot to generalize the noisy autonomously-generated grasps. Object shape, which is our main cue for predicting grasps, is encoded with surface densities, that model the spatial distribution of points that belong to an object's surface. Grasp parameters are modeled with grasp densities, that correspond to the spatial distribution of object-relative gripper poses that lead to a grasp. The size and shape of grasp-predicting parts are identified by sampling the cross-object correlation of local shape and grasp parameters. We approximate sampling and integrals via Monte Carlo methods to make our computer implementation tractable. We demonstrate the applicability of our method in simulation. A proof of concept on a real robot is also provided.",robotics,786,not included
10.1007/978-3-030-77070-9_10,to_check,Artificial Intelligence for a Sustainable Industry 4.0,Springer,2021-01-01 00:00:00,springer,smart and intelligent chatbot assistance for future industry 4.0,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-77070-9_10,"Chatbot is an implementation of artificial intelligence (AI) technology that is used to interact with human beings and make them feel like they are talking to the real person, and the chatbot helps them to solve their queries. A chatbot can provide 24 × 7 customer support so that the customer may have a good service experience by any organization. Chatbot helps to resolve the queries and respond to the questions of users. The user is providing the input to the chatbot first, and then, the same input will be processed further; this input can be in the form of text or voice. Therefore, on the basis of the given input and after processing it, the chatbot application will generate the response to the user, and the same response will be the best answer found by the chat application. This response can be in any format like text or a voice output. In this chapter, various approaches of chatbots and how they interact with users are discussed. The proposed approach is also defined using Dialogflow, and it can be accessible through mobile phones, laptops, and portable devices. Chatbots such as Facebook chatbot, WeChat chatbot, Hike chatbot called Natasha, etc. are available in the marker and will respond on the basis of their local databases (DBs). In the proposed method, the focus will be on the scalability, user interactivity, and flexibility of the system, which can be provided by adding both local and Web databases due to which our system will be more fast and accurate. Chatbot uses unification of emerging technologies like machine learning and artificial intelligence. The motive of this chapter is to improve the chatbot system to support and scale businesses and industry domain and maintain relations with customers.",robotics,787,included
10.1007/s11063-015-9426-5,to_check,Neural Processing Letters,Springer,2016-04-01 00:00:00,springer,introducing synaptic delays in the neat algorithm to improve modelling in cognitive robotics,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11063-015-9426-5,"This paper describes and tests an approach to improve the temporal processing capabilities of the neuroevolution of augmenting topologies (NEAT) algorithm. This algorithm is quite popular within the robotics community for the production of trained neural networks without having to determine a priori their size and topology. The main drawback of the traditional NEAT algorithm is that, even though it can implement recurrent synaptic connections, which allow it to perform some time related processing tasks, its capabilities are rather limited, especially when dealing with precise time dependent phenomena. NEAT’s ability to capture the underlying dynamics that correspond to complex time series still has a lot of room for improvement. To address this issue, the paper describes a new implementation of the NEAT algorithm that is able to generate artificial neural networks (ANNs) with trainable time delayed synapses in addition to its previous capacities. We show that this approach, called $$\uptau $$ τ -NEAT improves the behavior of the neural networks obtained when dealing with complex time related processes. Several examples are presented, both dealing with the generation of ANNs that are able to produce complex theoretical signals such as chaotic signals or real data series, as in the case of the monthly number of international airline passengers or monthly $$\hbox {CO}_{2}$$ CO 2 concentrations. In these examples, $$\uptau $$ τ -NEAT clearly improves over the traditional NEAT algorithm in these tasks. A final example of the integration of this approach within a robot cognitive mechanism is also presented, showing the clear improvements it could provide in the modeling required for many cognitive processes.",robotics,788,not included
10.1007/s10846-011-9627-8,to_check,Journal of Intelligent & Robotic Systems,Springer,2012-04-01 00:00:00,springer,odometry-based viterbi localization with artificial neural networks and laser range finders for mobile robots,http://link.springer.com/openurl/pdf?id=doi:10.1007/s10846-011-9627-8,"This paper proposes an approach that solves the Robot Localization problem by using a conditional state-transition Hidden Markov Model (HMM). Through the use of Self Organized Maps (SOMs) a Tolerant Observation Model (TOM) is built, while odometer-dependent transition probabilities are used for building an Odometer-Dependent Motion Model (ODMM). By using the Viterbi Algorithm and establishing a trigger value when evaluating the state-transition updates, the presented approach can easily take care of Position Tracking (PT), Global Localization (GL) and Robot Kidnapping (RK) with an ease of implementation difficult to achieve in most of the state-of-the-art localization algorithms. Also, an optimization is presented to allow the algorithm to run in standard microprocessors in real time, without the need of huge probability gridmaps.",robotics,789,not included
10.1007/s10846-010-9462-3,to_check,Journal of Intelligent & Robotic Systems,Springer,2011-06-01 00:00:00,springer,adaptive impedance control for upper-limb rehabilitation robot using evolutionary dynamic recurrent fuzzy neural network,http://link.springer.com/openurl/pdf?id=doi:10.1007/s10846-010-9462-3,"Control system implementation is one of the major difficulties in rehabilitation robot design. A newly developed adaptive impedance controller based on evolutionary dynamic fuzzy neural network (EDRFNN) is presented, where the desired impedance between robot and impaired limb can be regulated in real time according to the impaired limb’s physical recovery condition. Firstly, the impaired limb’s damping and stiffness parameters for evaluating its physical recovery condition are online estimated by using a slide average least squares (SALS)identification algorithm. Then, hybrid learning algorithms for EDRFNN impedance controller are proposed, which comprise genetic algorithm (GA), hybrid evolutionary programming (HEP) and dynamic back-propagation (BP) learning algorithm. GA and HEP are used to off-line optimize DRFNN parameters so as to get suboptimal impedance control parameters. Dynamic BP learning algorithm is further online fine-tuned based on the error gradient descent method. Moreover, the convergence of a closed loop system is proven using the discrete-type Lyapunov function to guarantee the global convergence of tracking error. Finally, simulation results show that the proposed controller provides good dynamic control performance and robustness with regard to the change of the impaired limb’s physical condition.",robotics,790,not included
10.1007/bf03037589,to_check,New Generation Computing,Springer,2000-06-01 00:00:00,springer,interactive learning and management of visual information via human-like software robot,http://link.springer.com/openurl/pdf?id=doi:10.1007/BF03037589,"To achieve smooth real-world interaction between people and computers, we developed a system that displays a three-dimensional computer-graphic human-like image from the waist up (anthropomorphic software robot: hereinafter “robot”) on the display, that interactively sees and hears, and that has fine and detailed control functions such as facial expressions, line of sight, and pointing at targets with its finger. The robot visually searches and identifies persons and objects in real space that it has learned in advance (registered space, which was our office in this case), manages the history information of the places and times it found objects and/or persons, and tells the user, indicating their three-dimensional positions with line of sight and its finger. It interactively learns new objects and persons with line of with their names and owners. By using this function, the robot can engage in simple dialogue (do a task) with the user.",robotics,791,not included
10.1016/j.robot.2021.103891,to_check,Robotics and Autonomous Systems,scopus,2021-12-01,sciencedirect,hybrid autonomous controller for bipedal robot balance with deep reinforcement learning and pattern generators[formula presented],https://api.elsevier.com/content/abstract/scopus_id/85116222985,"Recovering after an abrupt push is essential for bipedal robots in real-world applications within environments where humans must collaborate closely with robots. There are several balancing algorithms for bipedal robots in the literature, however most of them either rely on hard coding or power-hungry algorithms. We propose a hybrid autonomous controller that hierarchically combines two separate, efficient systems, to address this problem. The lower-level system is a reliable, high-speed, full state controller that was hardcoded on a microcontroller to be power efficient. The higher-level system is a low-speed reinforcement learning controller implemented on a low-power onboard computer. While one controller offers speed, the other provides trainability and adaptability. An efficient control is then formed without sacrificing adaptability to new dynamic environments. Additionally, as the higher-level system is trained via deep reinforcement learning, the robot could learn after deployment, which is ideal for real-world applications. The system’s performance is validated with a real robot recovering after a random push in less than 5 s, with minimal steps from its initial positions. The training was conducted using simulated data.",robotics,792,included
10.1016/j.ress.2019.106700,to_check,Reliability Engineering and System Safety,scopus,2020-03-01,sciencedirect,optimizing inspection routes in pipeline networks,https://api.elsevier.com/content/abstract/scopus_id/85073997788,"Maintaining an aging network is a challenge for many water utilities due to limited budgets and uncertainty surrounding the physical condition of buried pipeline assets. The deployment of robotic inspections provides high quality data, but these platforms have limited use due to cost and operational constraints. To facilitate cost-efficient inspections, operators need to identify high-risk assets while accounting for the effectiveness of the tools at hand. This paper addresses inspection planning with the goal of finding an optimal route while considering tool limitations. An exact integer programming formulation is presented where only three factors are used to characterize tool constraints. Two classes of solution methods are explored: 1) tree based searches, and 2) integer programming. This paper demonstrates how each method can be used to identify optimal paths within a real water distribution system. Empirical trials suggest that tree-based search methods are the most efficient when the path limit is short, but do not scale well when the path length increases. In contrast, integer-programming methods are more effective for longer path lengths but have scalability issues for large network sizes. Data preprocessing, where the input network size is reduced, can provide large computation time reductions while returning near-optimal solutions.",robotics,793,included
10.1016/j.eswa.2017.11.011,to_check,Expert Systems with Applications,scopus,2018-06-15,sciencedirect,towards a common implementation of reinforcement learning for multiple robotic tasks,https://api.elsevier.com/content/abstract/scopus_id/85035079318,"Mobile robots are increasingly being employed for performing complex tasks in dynamic environments. Those tasks can be either explicitly programmed by an engineer or learned by means of some automatic learning method, which improves the adaptability of the robot and reduces the effort of setting it up. In this sense, reinforcement learning (RL) methods are recognized as a promising tool for a machine to learn autonomously how to do tasks that are specified in a relatively simple manner. However, the dependency between these methods and the particular task to learn is a well-known problem that has strongly restricted practical implementations in robotics so far. Breaking this barrier would have a significant impact on these and other intelligent systems; in particular, having a core method that requires little tuning effort for being applicable to diverse tasks would boost their autonomy in learning and self-adaptation capabilities. In this paper we present such a practical core implementation of RL, which enables the learning process for multiple robotic tasks with minimal per-task tuning or none. Based on value iteration methods, we introduce a novel approach for action selection, called Q-biased softmax regression (QBIASSR), that takes advantage of the structure of the state space by attending the physical variables involved (e.g., distances to obstacles, robot pose, etc.), thus experienced sets of states accelerate the decision-making process of unexplored or rarely-explored states. Intensive experiments with both real and simulated robots, carried out with the software framework also introduced here, show that our implementation is able to learn different robotic tasks without tuning the learning method. They also suggest that the combination of true online SARSA(λ) (TOSL) with QBIASSR can outperform the existing RL core algorithms in low-dimensional robotic tasks. All of these are promising results towards the possibility of learning much more complex tasks autonomously by a robotic agent.",robotics,794,included
10.1016/j.robot.2018.02.010,to_check,Robotics and Autonomous Systems,scopus,2018-06-01,sciencedirect,visual attention and object naming in humanoid robots using a bio-inspired spiking neural network,https://api.elsevier.com/content/abstract/scopus_id/85044145526,"Recent advances in behavioural and computational neuroscience, cognitive robotics, and in the hardware implementation of large-scale neural networks, provide the opportunity for an accelerated understanding of brain functions and for the design of interactive robotic systems based on brain-inspired control systems. This is especially the case in the domain of action and language learning, given the significant scientific and technological developments in this field. In this work we describe how a neuroanatomically grounded spiking neural network for visual attention has been extended with a word learning capability and integrated with the iCub humanoid robot to demonstrate attention-led object naming. Experiments were carried out with both a simulated and a real iCub robot platform with successful results. The iCub robot is capable of associating a label to an object with a ‘preferred’ orientation when visual and word stimuli are presented concurrently in the scene, as well as attending to said object, thus naming it. After learning is complete, the name of the object can be recalled successfully when only the visual input is present, even when the object has been moved from its original position or when other objects are present as distractors.",robotics,795,included
10.1016/j.physa.2017.11.155,to_check,Physica A: Statistical Mechanics and its Applications,scopus,2018-03-15,sciencedirect,efficient digital implementation of a conductance-based globus pallidus neuron and the dynamics analysis,https://api.elsevier.com/content/abstract/scopus_id/85042234061,"Balance between biological plausibility of dynamical activities and computational efficiency is one of challenging problems in computational neuroscience and neural system engineering. This paper proposes a set of efficient methods for the hardware realization of the conductance-based neuron model with relevant dynamics, targeting reproducing the biological behaviors with low-cost implementation on digital programmable platform, which can be applied in wide range of conductance-based neuron models. Modified GP neuron models for efficient hardware implementation are presented to reproduce reliable pallidal dynamics, which decode the information of basal ganglia and regulate the movement disorder related voluntary activities. Implementation results on a field-programmable gate array (FPGA) demonstrate that the proposed techniques and models can reduce the resource cost significantly and reproduce the biological dynamics accurately. Besides, the biological behaviors with weak network coupling are explored on the proposed platform, and theoretical analysis is also made for the investigation of biological characteristics of the structured pallidal oscillator and network. The implementation techniques provide an essential step towards the large-scale neural network to explore the dynamical mechanisms in real time. Furthermore, the proposed methodology enables the FPGA-based system a powerful platform for the investigation on neurodegenerative diseases and real-time control of bio-inspired neuro-robotics.",robotics,796,not included
10.1016/j.cogsys.2017.08.002,to_check,Cognitive Systems Research,scopus,2018-01-01,sciencedirect,a computational cognitive framework of spatial memory in brains and robots,https://api.elsevier.com/content/abstract/scopus_id/85034106426,"Computational cognitive models of spatial memory often neglect difficulties posed by the real world, such as sensory noise, uncertainty, and high spatial complexity. On the other hand, robotics is unconcerned with understanding biological cognition. Here, we describe a computational framework for robotic architectures aiming to function in realistic environments, as well as to be cognitively plausible.
                  We motivate and describe several mechanisms towards achieving this despite the sensory noise and spatial complexity inherent in the physical world. We tackle error accumulation during path integration by means of Bayesian localization, and loop closing with sequential gradient descent. Finally, we outline a method for structuring spatial representations using metric learning and clustering. Crucially, unlike the algorithms of traditional robotics, we show that these mechanisms can be implemented in neuronal or cognitive models.
                  We briefly outline a concrete implementation of the proposed framework as part of the LIDA cognitive architecture, and argue that this kind of probabilistic framework is well-suited for use in cognitive robotic architectures aiming to combine spatial functionality and psychological plausibility.",robotics,797,included
10.1016/j.eswa.2017.03.002,to_check,Expert Systems with Applications,scopus,2017-09-01,sciencedirect,incremental q-learning strategy for adaptive pid control of mobile robots,https://api.elsevier.com/content/abstract/scopus_id/85015894497,"Expert and intelligent systems are being developed to control many technological systems including mobile robots. However, the PID (Proportional-Integral-Derivative) controller is a fast low-level control strategy widely used in many control engineering tasks. Classic control theory has contributed with different tuning methods to obtain the gains of PID controllers for specific operation conditions. Nevertheless, when the system is not fully known and the operative conditions are variable and not previously known, classical techniques are not entirely suitable for the PID tuning. To overcome these drawbacks many adaptive approaches have been arisen, mainly from the field of artificial intelligent. In this work, we propose an incremental Q-learning strategy for adaptive PID control. In order to improve the learning efficiency we define a temporal memory into the learning process. While the memory remains invariant, a non-uniform specialization process is carried out generating new limited subspaces of learning. An implementation on a real mobile robot demonstrates the applicability of the proposed approach for a real-time simultaneous tuning of multiples adaptive PID controllers for a real system operating under variable conditions in a real environment.",robotics,798,included
10.1016/j.neunet.2016.08.003,to_check,Neural Networks,scopus,2016-11-01,sciencedirect,implementation of imitation learning using natural learner central pattern generator neural networks,https://api.elsevier.com/content/abstract/scopus_id/84984998255,"In this paper a new design of neural networks is introduced, which is able to generate oscillatory patterns. The fundamental building block of the neural network is O-neurons that can generate an oscillation in its transfer functions. Since the natural policy gradient learning has been used in training a central pattern generator paradigm, it is called Natural Learner CPG Neural Networks (NLCPGNN). O-neurons are connected and coupled to each other in order to shape a network and their unknown parameters are found by a natural policy gradient learning algorithm. The main contribution of this paper is design of this learning algorithm which is able to simultaneously search for the weights and topology of the network. This system is capable to obtain any complex motion and rhythmic trajectory via first layer and learn rhythmic trajectories in the second layer and converge towards all these movements. Moreover this two layers system is able to provide various features of a learner model for instance resistance against perturbations, modulation of trajectories amplitude and frequency. Simulation of the learning system in the robot simulator (WEBOTS) that is linked with MATLAB software has been done. Implementation on a real NAO robot demonstrates that the robot has learned desired motion with high accuracy. These results show proposed system produces high convergence rate and low test errors.",robotics,799,included
10.1016/j.jneumeth.2015.06.015,to_check,Journal of Neuroscience Methods,scopus,2015-09-01,sciencedirect,watching from a distance: a robotically controlled laser and real-time subject tracking software for the study of conditioned predator/prey-like interactions,https://api.elsevier.com/content/abstract/scopus_id/84936742109,"Background
                  The physical distance between predator and prey is a primary determinant of behavior, yet few paradigms exist to study this reliably in rodents.
               
                  New method
                  The utility of a robotically controlled laser for use in a predator–prey-like (PPL) paradigm was explored for use in rats. This involved the construction of a robotic two-dimensional gimbal to dynamically position a laser beam in a behavioral test chamber. Custom software was used to control the trajectory and final laser position in response to user input on a console. The software also detected the location of the laser beam and the rodent continuously so that the dynamics of the distance between them could be analyzed. When the animal or laser beam came within a fixed distance the animal would either be rewarded with electrical brain stimulation or shocked subcutaneously.
               
                  Results
                  Animals that received rewarding electrical brain stimulation could learn to chase the laser beam, while animals that received aversive subcutaneous shock learned to actively avoid the laser beam in the PPL paradigm. Mathematical computations are presented which describe the dynamic interaction of the laser and rodent.
               
                  Comparison with existing methods
                  The robotic laser offers a neutral stimulus to train rodents in an open field and is the first device to be versatile enough to assess distance between predator and prey in real time.
               
                  Conclusions
                  With ongoing behavioral testing this tool will permit the neurobiological investigation of predator/prey-like relationships in rodents, and may have future implications for prosthetic limb development through brain–machine interfaces.",robotics,800,not included
10.1016/j.neucom.2015.04.066,to_check,Neurocomputing,scopus,2015-01-01,sciencedirect,a general cpg network and its implementation on the microcontroller,https://api.elsevier.com/content/abstract/scopus_id/84952631900,"Over the last few years, it has been confirmed by the biologists that Central Pattern Generator (CPG) is the key mechanism of generating adaptive and versatile locomotion in animals. This gives a new inspiration of locomotion control for robots. Although the design of CPG controller using coupled oscillators has been proposed previously, it cannot comprehensively reproduce different rhythmic motion along with smooth transitions to mimic the versatility of animal locomotion. To tackle this problem, we propose a general CPG model emphasizing on its stability analysis, smooth transition and implementation architecture. Global exponential stability of the model is derived by using strict mathematical analysis. Transitions between different oscillating forms are also smooth, and the implementation architecture has low computational cost, thus suitable for microcontrollers. Moreover, all control parameters not only have clear relationships with the physical outputs, but also can be modified online. Both virtual and real robotic fish are developed to verify the effectiveness of our CPG controller together with the proposed implementation architecture, through the experiments of four locomotion gaits and three transitions among them.",robotics,801,not included
10.1016/j.engappai.2013.12.003,to_check,Engineering Applications of Artificial Intelligence,scopus,2014-02-01,sciencedirect,hardware opposition-based pso applied to mobile robot controllers,https://api.elsevier.com/content/abstract/scopus_id/84892858663,"Adaptation of mobile robot controllers commonly requires the computation of optimal points of operation. Specifically, for miniature mobile robots with serious computational limitations, that are typical of embedded systems, one of the main challenges is the adaptation of efficient computational methods in order to find solutions of complex optimization problems, which demand large execution times. This drawback compels the design of high-performance parallel optimization algorithms which must run over embedded system platforms. This paper describes how adequate hardware implementations of the Particle Swarm Optimization (PSO) algorithm can be useful for real time adaptation of mobile robot controllers. For achieving this, a new architecture is proposed, which is based on an FPGA implementation of the opposition-based learning (OBL) approach applied to the PSO (for short HPOPSO), and which explores the intrinsic parallelism of this algorithm in order to adjust the weights of a neural robot controller in real time according to desired behaviors. The proposed HPOPSO was applied to the learning-from-demonstration problem in which a teacher performs executions of the desired behavior. Effectiveness of the proposed architecture was demonstrated by numerical simulations and the feasibility of the adaptive behavior of the neural robot controller was confirmed for two obstacle avoidance case studies that were preserved when one or more failures on the distance sensors occur. The HPOPSO, which uses the OBL technique, improves the quality of the solutions in comparison with the standard PSO. Comparisons of the adaptation time between hardware and software approaches have demonstrated the suitability of the FPGA implementation of the proposed HPOPSO for attending specific requirements of embedded system applications.",robotics,802,not included
10.1016/j.robot.2014.08.002,to_check,Robotics and Autonomous Systems,scopus,2014-01-01,sciencedirect,integrated neural and robotic simulations. simulation of cerebellar neurobiological substrate for an object-oriented dynamic model abstraction process,https://api.elsevier.com/content/abstract/scopus_id/84908424049,"Experimental studies of the Central Nervous System (CNS) at multiple organization levels aim at understanding how information is represented and processed by the brain’s neurobiological substrate. The information processed within different neural subsystems is neurocomputed using distributed and dynamic patterns of neural activity. These emerging patterns can be hardly understood by merely taking into account individual cell activities. Studying how these patterns are elicited in the CNS under specific behavioral tasks has become a groundbreaking research topic in system neuroscience. This methodology of synthetic behavioral experimentation is also motivated by the concept of embodied neuroscience, according to which the primary goal of the CNS is to solve/facilitate the body–environment interaction.
                  With the aim to bridge the gap between system neuroscience and biological control, this paper presents how the CNS neural structures can be connected/integrated within a body agent; in particular, an efficient neural simulator based on EDLUT (Ros et al., 2006) has been integrated within a simulated robotic environment to facilitate the implementation of object manipulating closed loop experiments (action–perception loop). This kind of experiment allows the study of the neural abstraction process of dynamic models that occurs within our neural structures when manipulating objects.
                  The neural simulator, communication interfaces, and a robot platform have been efficiently integrated enabling real time simulations. The cerebellum is thought to play a crucial role in human-body interaction with a primary function related to motor control which makes it the perfect candidate to start building an embodied nervous system as illustrated in the simulations performed in this work.",robotics,803,not included
10.1016/j.neunet.2013.01.019,to_check,Neural Networks,scopus,2013-11-01,sciencedirect,realtime cerebellum: a large-scale spiking network model of the cerebellum that runs in realtime using a graphics processing unit,https://api.elsevier.com/content/abstract/scopus_id/84884150994,"The cerebellum plays an essential role in adaptive motor control. Once we are able to build a cerebellar model that runs in realtime, which means that a computer simulation of 1 s in the simulated world completes within 1 s in the real world, the cerebellar model could be used as a realtime adaptive neural controller for physical hardware such as humanoid robots. In this paper, we introduce “Realtime Cerebellum (RC)”, a new implementation of our large-scale spiking network model of the cerebellum, which was originally built to study cerebellar mechanisms for simultaneous gain and timing control and acted as a general-purpose supervised learning machine of spatiotemporal information known as reservoir computing, on a graphics processing unit (GPU). Owing to the massive parallel computing capability of a GPU, RC runs in realtime, while reproducing qualitatively the same simulation results of the Pavlovian delay eyeblink conditioning with the previous version. RC is adopted as a realtime adaptive controller of a humanoid robot, which is instructed to learn a proper timing to swing a bat to hit a flying ball online. These results suggest that RC provides a means to apply the computational power of the cerebellum as a versatile supervised learning machine towards engineering applications.",robotics,804,not included
10.1016/j.neunet.2013.04.005,to_check,Neural Networks,scopus,2013-09-01,sciencedirect,fpga implementation of a configurable neuromorphic cpg-based locomotion controller,https://api.elsevier.com/content/abstract/scopus_id/84880792738,"Neuromorphic engineering is a discipline devoted to the design and development of computational hardware that mimics the characteristics and capabilities of neuro-biological systems. In recent years, neuromorphic hardware systems have been implemented using a hybrid approach incorporating digital hardware so as to provide flexibility and scalability at the cost of power efficiency and some biological realism. This paper proposes an FPGA-based neuromorphic-like embedded system on a chip to generate locomotion patterns of periodic rhythmic movements inspired by Central Pattern Generators (CPGs). The proposed implementation follows a top-down approach where modularity and hierarchy are two desirable features. The locomotion controller is based on CPG models to produce rhythmic locomotion patterns or gaits for legged robots such as quadrupeds and hexapods. The architecture is configurable and scalable for robots with either different morphologies or different degrees of freedom (DOFs). Experiments performed on a real robot are presented and discussed. The obtained results demonstrate that the CPG-based controller provides the necessary flexibility to generate different rhythmic patterns at run-time suitable for adaptable locomotion.",robotics,805,included
10.1016/j.procs.2013.05.187,to_check,Procedia Computer Science,scopus,2013-01-01,sciencedirect,comparing support vector machines and artificial neural networks in the recognition of steering angle for driving of mobile robots through paths in plantations,https://api.elsevier.com/content/abstract/scopus_id/84896966222,"The use of mobile robots turns out to be interesting in activities where the action of human specialist is difficult or dangerous. Mobile robots are often used for the exploration in areas of difficult access, such as rescue operations and space missions, to avoid human experts exposition to risky situations. Mobile robots are also used in agriculture for planting tasks as well as for keeping the application of pesticides within minimal amounts to mitigate environmental pollution. In this paper we present the development of a system to control the navigation of an autonomous mobile robot through tracks in plantations. Track images are used to control robot direction by pre-processing them to extract image features. Such features are then submitted to a support vector machine and an artificial neural network in order to find out the most appropriate route. A comparison of the two approaches was performed to ascertain the one presenting the best outcome. The overall goal of the project to which this work is connected is to develop a real time robot control system to be embedded into a hardware platform. In this paper we report the software implementation of a support vector machine and of an artificial neural network, which so far presented respectively around 93% and 90% accuracy in predicting the appropriate route.",robotics,806,not included
10.1016/j.eswa.2011.04.207,to_check,Expert Systems with Applications,scopus,2011-10-01,sciencedirect,intelligent control based on wavelet decomposition and neural network for predicting of human trajectories with a novel vision-based robotic,https://api.elsevier.com/content/abstract/scopus_id/79959924555,"In this paper, an intelligent novel vision-based robotic tracking model is developed to predict the performance of human trajectories with a novel vision-based robotic tracking system. The developed model is based on wavelet packet decomposition, entropy and neural network. We represent an implementation of a novel vision-based robotic tracking system based on wavelet decomposition and artificial neural (WD-ANN) which can track desired human trajectory pattern in real environments. The input–output data set of the novel vision-based robotic tracking system were first stored and than these data sets were used to predict the robotic tracking based on WD-ANN. In simulations, performance measures were obtained to compare the predicted and human–robot trajectories like actual values for model validation. In statistical analysis, the RMS value is 0.0729 and the R
                     2 value is 99.76% for the WD-ANN model. This study shows that the values predicted with the WD-ANN can be used to predict human trajectory by vision-based robotic tracking system quite accurately. All simulations have shown that the proposed method is more effective and controls the systems quite successful.",robotics,807,not included
10.1016/j.neucom.2008.06.019,to_check,Neurocomputing,scopus,2009-01-01,sciencedirect,discretized iso-learning neural network for obstacle avoidance in reactive robot controllers,https://api.elsevier.com/content/abstract/scopus_id/58149470368,"Isotropic sequence order learning (ISO-learning) and its variations, input correlation only learning (ICO-learning) and ISO three-factor learning (ISO3-learning) are unsupervised neural algorithms to learn temporal differences. As robotic software operates mainly in discrete time domain, a discretization of ISO-learning is needed to apply classical conditioning to reactive robot controllers.
                  Discretization of ISO-learning is achieved by modifications to original rules: weights sign restriction, to adequate ISO-learning devices outputs to the usually predefined kinds of connections (excitatory/inhibitory) used in neural networks, and decay term in learning rate for weights stabilization. Discrete ISO-learning devices are included into neural networks used to learn simple obstacle avoidance in the reactive control of two real robots.",robotics,808,included
10.1016/j.robot.2008.08.010,to_check,Robotics and Autonomous Systems,scopus,2008-11-30,sciencedirect,logic-based robot control in highly dynamic domains,https://api.elsevier.com/content/abstract/scopus_id/53849128142,"In this paper, we present the robot programming and planning language Readylog, a Golog dialect, which was developed to support the decision making of robots acting in dynamic real-time domains, such as robotic soccer. The formal framework of Readylog, which is based on the situation calculus, features imperative control structures such as loops and procedures, allows for decision-theoretic planning, and accounts for a continuously changing world. We developed high-level controllers in Readylog for our soccer robots in RoboCup’s Middle-size league, but also for service robots and for autonomous agents in interactive computer games. For a successful deployment of Readylog on a real robot it is also important to account for the control problem as a whole, integrating the low-level control of the robot (such as localization, navigation, and object recognition) with the logic-based high-level control. In doing so, our approach can be seen as a step towards bridging the gap between the fields of robotics and knowledge representation.",robotics,809,not included
10.1016/s0921-8890(02)00232-4,to_check,Robotics and Autonomous Systems,scopus,2002-08-31,sciencedirect,behavior generation for a mobile robot based on the adaptive fitness function,https://api.elsevier.com/content/abstract/scopus_id/0037206240,"We have to prepare the evaluation (fitness) function to evaluate the performance of the robot when we apply the machine learning techniques to the robot application. In many cases, the fitness function is composed of several aspects. Simple implementation to cope with the multiple fitness functions is a weighted summation. This paper presents an adaptive fitness function for the evolutionary computation to obtain the purposive behaviors through changing the weights for the fitness function. As an example task, a basic behavior in a simplified soccer game (shooting a ball into the opponent goal) is selected to show the validity of the adaptive fitness function. Simulation results and real experiments are shown, and a discussion is given.",robotics,810,not included
10.1016/s0925-2312(01)00500-8,to_check,Neurocomputing,scopus,2001-06-01,sciencedirect,a biologically inspired visual system for an autonomous robot,https://api.elsevier.com/content/abstract/scopus_id/17544395043,"We have implemented an artificial visual system that takes advantage of known properties of biological systems to achieve segmentation and recognition of simple images. The use of biologically plausible mechanisms makes the system inherit a series of features that are present in biological systems, such as flexibility, robustness and adaptability. The implementation of the model on an autonomous robot has proved its reliability and robustness in real environments and shows the relevance of this kind of approach.",robotics,811,included
10.15587/1729-4061.2020.213989,to_check,core,'Private Company Technology Center',2020-10-31 00:00:00,core,побудова моделі мережевої взаємодії складових мультиагентної системи мобільних роботів,https://core.ac.uk/download/337304742.pdf,"The results reported here represent the first stage in the development of a full-featured laboratory system aimed at studying machine learning algorithms. The relevance of the current work is predetermined by the lack of network small-size mobile robots and appropriate control software that would make it possible to conduct field experiments in real time. This paper reports the selection of network data transmission technology for managing mobile robots in real time. Based on the chosen data transmission protocol, a complete stack of technologies of the network model of a multi-agent system of mobile robots has been proposed. This has made it possible to build a network model of the system that visualizes and investigates machine learning algorithms. In accordance with the requirements set by the OSI network model for constructing such systems, the model includes the following levels:1) the lower level of data collection and controlling elements – mobile robots;2) the top level of the model includes a user interface server and a business logic support server.Based on the built diagram of the protocol stack and the network model, the software and hardware implementation of the obtained results has been carried out. This paper employed the JavaScript library React with a SPA technology (Single Page Application), a Virtual DOM technology (Document Object Model), stored in the device's RAM and synchronized with the actual DOM. That has made it possible to simplify the process of control over the clients and reduce network traffic.The model provides the opportunity to:1) manage the prototypes of robot clients in real time;2) reduce the use of network traffic, compared to other data transmission technologies;3) reduce the load on the CPU processors of robots and servers; 4) virtually simulate an experiment;5) investigate the implementation of machine learning algorithmsПредставленные результаты работы являются первым этапом разработки полнофункциональной лабораторной системы исследования алгоритмов машинного обучения. Актуальность работы обусловлена отсутствием сетевых малогабаритных мобильных роботов и соответствующего управляющего программного обеспечения, что позволило бы проводить натурные эксперименты в реальном времени. В работе осуществлен подбор сетевой технологии передачи данных для управления мобильными роботами в реальном времени. На основе выбранного протокола передачи данных предложен полный стек технологий сетевой модели мультиагентной системы мобильных роботов. Это позволило построить сетевую модель системы визуализации и исследования алгоритмов машинного обучения. В соответствии с требованиями сетевой модели OSI по построению подобных систем, модель включает в себя следующие уровни:1. Нижний уровень сбора данных и исполнительных механизмов – мобильные работы;2. Верхний уровень модели состоит из сервера пользовательского интерфейса и сервера поддержки бизнес-логики.Основываясь на построенной диаграмме стека протоколов и сетевой модели, осуществлена программно-аппаратная реализация полученных результатов. В работе использованы JavaScript библиотека React с технологией SPA (Single Page Application), технология Virtual DOM (Document Object Model), хранящаяся в оперативной памяти устройства и синхронизируемая с реальным DOM. Это позволило упростить процесс управления клиентами и уменьшить сетевой трафик.Модель позволяет:1) управлять прототипами роботов-клиентов в реальном времени;2) уменшить использование сетевого трафика, в сравнении с другими технологиями передачи данных;3) уменьшить нагрузку на центральные процессоры роботов и серверов;4) выполнять виртуальную симуляцию эксперимента;5) исследовать выполнение алгоритмов машинного обученияПредставлені результати роботи є першим етапом розробки повнофункціональної лабораторної системи дослідження алгоритмів машинного навчання. Актуальність роботи зумовлена відсутністю мережевих малогабаритних мобільних роботів та відповідного керуючого програмного забезпечення, що дозволило б проводити натурні експерименти в реальному часі. В роботі здійснено підбір мережевої технології передачі даних для керування мобільними роботами в реальному часі. На основі обраного протоколу передачі даних запропоновано повний стек технологій мережевої моделі мультиагентної системи мобільних роботів. Це дозволило побудувати мережеву модель системи візуалізації та дослідження алгоритмів машинного навчання. Відповідно до вимог мережевої моделі OSI щодо побудови подібних систем, модель включає в себе наступні рівні:1) нижній рівень збору даних та виконавчих механізмів – мобільні роботи;2) верхній рівень моделі – складається з серверу користувацького інтерфейсу та серверу підтримки бізнес-логіки.Базуючись на побудованих діаграмі стеку протоколів та мережевій моделі здійснена програмно-апаратна реалізація отриманих результатів. У роботі використано JavaScript бібліотека React з технологією SPA (Single Page Application), технологію Virtual DOM (Document Object Model), що зберігається в оперативній пам’яті пристрою і синхронізується з реальним DOM. Це дозволило спростити процес керування клієнтами та зменшити мережевий трафік.Модель надає можливість: 1) керувати прототипами роботів-клієнтів в реальному часі;2) зменшити використання мережевого трафіку, в порівнянні з іншими технологіями передачі даних;3) зменшити навантаження на центральні процесори роботів та серверів;4) виконувати віртуальну симуляцію експерименту;5) досліджувати виконання алгоритмів машинного навчання",robotics,812,not included
proceedings of machine learning research,to_check,core,,2018-01-01 00:00:00,core,learning deployable navigation policies at kilometer scale from a single traversal,,"Model-free reinforcement learning has recently been shown to be effective at learning navigation policies from complex image input. However, these algorithms tend to require large amounts of interaction with the environment, which can be prohibitively costly to obtain on robots in the real world. We present an approach for efficiently learning goal-directed navigation policies on a mobile robot, from only a single coverage traversal of recorded data. The navigation agent learns an effective policy over a diverse action space in a large heterogeneous environment consisting of more than 2km of travel, through buildings and outdoor regions that collectively exhibit large variations in visual appearance, self-similarity, and connectivity. We compare pretrained visual encoders that enable precomputation of visual embeddings to achieve a throughput of tens of thousands of transitions per second at training time on a commodity desktop computer, allowing agents to learn from millions of trajectories of experience in a matter of hours. We propose multi- ple forms of computationally efficient stochastic augmentation to enable the learned policy to generalise beyond these precomputed embeddings, and demonstrate successful deployment of the learned policy on the real robot without fine tuning, despite environmental appearance differences at test time. The dataset and code required to reproduce these results and apply the technique to other datasets and robots is made publicly available at rl-navigation.github.io/deployable ",robotics,813,included
georgia institute of technology,to_check,core,,2019-05-22 00:00:00,core,balancing generality and specialization for machine learning in the post-isa era,https://core.ac.uk/download/304994832.pdf,"A growing number of commercial and enterprise systems are increasingly relying on compute-intensive machine learning algorithms. While the demand for these apaplications is growing, the performance benefits from general-purpose platforms is diminishing. This challenge has coincided with the explosion of data where the rate of data generation has reached an overwhelming level that is beyond the capabilities of current computing systems. Therefore, applications such as machine learning and robotics can benefit from hardware acceleration. Traditionally, to accelerate a set of workloads, we pro- file the code optimized for CPUs and offload the hot functions on hardware compute units designed specially for that particular function, hence providing higher performance and energy efficiency. Instead in this work, we take a revolutionary approach where we delve into the algorithmic properties of applications to define domain-generic hardware acceleration solutions. We leverage the property that a wide range of machine learning algorithms can be modeled as stochastic optimization problems. Using this insight we devise compute stacks for hardware acceleration that are built independent of the CPU. These stacks expose a high-level mathematical programming interface and automatically generate accelerators for users who have limited knowledge about hardware design, but can benefit from large performance and efficiency gains for their programs. 
Keeping these ambitious goals in mind, our work (1) strikes a balance between generality and specialization by breaking the long-held traditional abstraction of the Instruction Set Architecture (ISA) in favor of a more algorithm-centric approach; (2) develops hard- ware acceleration frameworks by co-designing a language, compiler, runtime system, and hardware to provide high performance and efficiency, in addition to flexibility and programmability; (3) segregates algorithmic specification from implementation to shield the programmer from continual hardware/software modifications while allowing them to benefit from the emerging heterogeneity of modern compute platforms; and (4) develops real cross-stack prototypes to evaluate these innovative solutions in a real-world setting and make them open-source to maximize community engagement and industry impact. Our work TABLA (http://act-lab.org/artifacts/tabla/) is public, and defines the very first open-source hardware platform for machine learning and artificial intelligence.Ph.D",robotics,814,included
'edp sciences',to_check,core,10.1051/matecconf/201925501003,2019-01-01 00:00:00,core,automatic image annotation for small and ad hoc intelligent applications using raspberry pi,,"The cutting-edge technology Machine Learning (ML) is successfully applied for Business Intelligence. Among the various pre-processing steps of ML, Automatic Image Annotation (also known as automatic image tagging or linguistic indexing) is the process in which a computer system automatically assigns metadata in the form of captioning or keywords to a digital image. Automatic Image Annotation (AIA) methods (which have appeared during the last several years) make a large use of many ML approaches. Clustering and classification methods are most frequently applied to annotate images. In addition, these proposed solutions require a high computational infrastructure. However, certain real-time applications (small and ad-hoc intelligent applications) for example, autonomous small robots, gadgets, drone etc. have limited computational processing capacity. These small and ad-hoc applications demand a more dynamic and portable way to automatically annotate data and then perform ML tasks (Classification, clustering etc.) in real time using limited computational power and hardware resources. Through a comprehensive literature study we found that most image pre-processing algorithms and ML tasks are computationally intensive, and it can be challenging to run them on an embedded platform with acceptable frame rates. However, Raspberry Pi is sufficient for AIA and ML tasks that are relevant to small and ad-hoc intelligent applications. In addition, few critical intelligent applications (which require high computational resources, for example, Deep Learning using huge dataset) are only feasible to run on more powerful hardware resources. In this study, we present the framework of “Automatic Image Annotation for Small and Ad-hoc Intelligent Application using Raspberry Pi” and propose the low-cost infrastructures (single node and multi node using Raspberry Pi) and software module (for Raspberry Pi) to perform AIA and ML tasks in real time for small and ad-hoc intelligent applications. The integration of both AIA and ML tasks in a single software module (with in Raspberry Pi) is challenging. This study will helpful towards the improvement in various practical applications areas relevant to small intelligent autonomous systems",robotics,815,included
10.1109/icra.2016.7487170,to_check,core,'Institute of Electrical and Electronics Engineers (IEEE)',2017-08-16 00:00:00,core,safe controller optimization for quadrotors with gaussian processes,http://arxiv.org/abs/1509.01066,"One of the most fundamental problems when designing controllers for dynamic
systems is the tuning of the controller parameters. Typically, a model of the
system is used to obtain an initial controller, but ultimately the controller
parameters must be tuned manually on the real system to achieve the best
performance. To avoid this manual tuning step, methods from machine learning,
such as Bayesian optimization, have been used. However, as these methods
evaluate different controller parameters on the real system, safety-critical
system failures may happen. In this paper, we overcome this problem by
applying, for the first time, a recently developed safe optimization algorithm,
SafeOpt, to the problem of automatic controller parameter tuning. Given an
initial, low-performance controller, SafeOpt automatically optimizes the
parameters of a control law while guaranteeing safety. It models the underlying
performance measure as a Gaussian process and only explores new controller
parameters whose performance lies above a safe performance threshold with high
probability. Experimental results on a quadrotor vehicle indicate that the
proposed method enables fast, automatic, and safe optimization of controller
parameters without human intervention.Comment: IEEE International Conference on Robotics and Automation, 2016. 6
  pages, 4 figures. A video of the experiments can be found at
  http://tiny.cc/icra16_video . A Python implementation of the algorithm is
  available at https://github.com/befelix/SafeOp",robotics,816,not included
10.12681/eadd/40861,to_check,core,'National Documentation Centre (EKT)',2014-01-01 00:00:00,core,χωροχρονικά συναφείς σημασιολογικοί χάρτες με κινούμενα ρομπότ,,"Along the last decades persistent research endeavors in the areas of robotics andartificial intelligence revealed the great challenge of robot navigation, an areathat combines robot mobility with perception of the environment. Moreover,in modern human societies it is of great importance to build up machines thatcan be operated by non specialists or even by technologically illiterate people,such as youngsters or elderly. Therefore, the mobile robots to be released intothe market in the near future should possess, among others, the potential ofproducing meaningful internal perceptual representations of their own environment,capacitating them to cope a range of real-life situations and tasks. Tomake matters even more challenging, when it comes for mapping and navigation,robots should comprehend human concepts about places and objects,to skillfully deploy in human frequented environments. In response to thischallenge, intense research efforts, to build cognitive robots apt to competentlyperceive and understand their surroundings and to cooperate with humans,take place. With this goal in view, semantic mapping with mobile robots can constituteto a holistic solution in response to the aforementioned challenges. Thesemantic mapping is an augmented representation of the robot’s environmentthat -supplementary to the geometrical knowledge- encapsulates characteristicscompatible with human understanding. It provides several algorithmic opportunitiesfor innovative development of applications that will eventually lead tothe human robot interaction. The main objective of the PhD dissertation in handis the construction of accurate and consistent semantic maps facilitating amplerobot deployment in domestic environments.The motivation behind this PhD dissertation has been the observation thatthe plethora of the existing mapping and navigation algorithms are not ableto provide a sufficient representation of the environment in terms of humans’concepts. This is due to the fact that the mapping methods developed sofar focus on the construction of geometrical maps. Although some of thesesolutions proved to be capable of driving robots into specific target positions,they lack of high level cognition attributes, which would allow them to bring the human-robot interaction one step beyond. Aiming to remove this barrier, thisdissertation is oriented towards the direction of developing semantic mappingalgorithms for high level robot navigation. Therefore, this doctoral researchidentified the basic components of the semantic mapping and developed aninnovative solution for each one of them. Due to the fact that the semanticmapping requires an integrated system that comprises several subordinatemodules, a wide range of a algorithms that serve different tasks had to bedesigned and implemented. Within the context of this thesis several algorithmicmodules have been developed including a competent localization algorithm,novel simultaneously localization and mapping strategies, breakthrough place andobject recognition tactics, as well as the integration of all these methods undera time supervised framework able to produce consistent semantic maps. Dueto the fact that each subordinate module comprises an innovative solutionto the respective field, the resulting semantic mapping system constitutesa state of the art solution in the area of conceptual mapping with mobilerobots. The introduced algorithms exploit solely visual and depth sensors,while by combining basic tools from three different scientific areas such asrobotics, computer vision and machine learning the final objective is accomplished.Overall, the main contribution of this thesis to the advancement the stateof the art is the introduction of a stacked map hierarchy of four differenttype of maps, namely a metric, a topological, a labeled sparse topological andan augmented navigation one. Each of these representations accomplishes aunique purpose: (i) the metric is the physical (lower) layer; (ii) the topologicalone contains abstract geometrical information of the environment, i.e. pointclouds registered in a graph of nodes; (iii) the labeled sparse topological oneestablishes the spatiotemporal coherence by associating the respective nodes inthe topometric maps via place labels and geometrical transformations, enablingbidirectional exchange of information among the conceptual and metric mapsand, last, (iv) the augmented navigation map inheres the significance of thedetected places as well as their connectivity relationships, expressed in termsof their transition probability.The thesis in hand has been developed in a hierarchical fashion and can bedivided into four main chapters. The first one comprises a literature survey ofthe existing semantic mapping methods in which an explicit analysis of the sofar developed methods is sought. The insights of the semantic mapping arereviewed, the distinct components encompassing, to give a categorization of therelated literature, are studied the possible applications in mobile robotics areexamined and, lastly, the methods and databases available for benchmarkingare referred. Furthermore, a quality-based taxonomy of the existing semanticmapping methods highlights the dominant attributes such methods retain.More precisely, according to the scale, to which each method is expanded, the metric map could be either a single scene or a progressively created map.Another important attribute a typical semantic mapping method possessesis the existence of the respective topological map, that is an abstraction ofthe explored environment in terms of a graph. The nodes of such a graph areorganized in a geometrical manner, so as to simultaneously preserve conceptualknowledge about the explored places. Moreover, the modalities (single ormultiple visual cues) utilized to reason about the observed scene constitute anelement apt to distinguish the abundance of different methods. An additionalfeature in many recent semantic mapping techniques is the temporal coherencesuch a map reveals, which renders it useful for high-level activities, viz. taskplanning or human robot interaction.The second chapter refers to the description of the developed technologicalbackground required to build a consistent semantic map. Therefore, a majorcontribution of the first part is the development of an innovative visual odometryalgorithm able to operate in real time. This algorithm receives as input successivestereo pair of images from a stereoscopic camera mounted on a mobilerobot. It involves the detection of the salient landmarks between successiveimages. A depth estimation of these features is then obtained and a novelnon-iterative outlier detection and discarding methodology able to remove boththe mismatches between the features and the inserted errors due to the 3Dreconstruction procedure. A hierarchical motion estimation technique, whichproduces robust estimations for the movement of the robot is then adopted,thus providing refinements to the robot’s global position and orientation. Anadditional 3D reconstruction algorithm that operates on stereo images hasbeen developed providing accurate reconstruction of the area observed bythe robot. Moreover, the localization algorithm comprises the cornerstone forthe development of a simultaneously localization and mapping system suitablefor the 3D geometrical mapping of the explored environment. This 3D metricmapping system is based solely on an RGB-D sensor, where in course of robot’slocomotion 3D point clouds are merged with respect to the visual odometry.The resulted 3D map is refined by exploiting a random sample consensus planedetection algorithm accompanied by an iterative closest point registration step,among the dominant planes of the consecutive time instances, resulting thusin a very consistent geometrical 3D map. All the aforementioned developedalgorithms have been evaluated on a custom made robot platform bearing twostereoscopic cameras with different baselines and a RGB-D sensor.The third chapter encloses all the semantic mapping methods based onvisual cues that have been developed within this PhD dissertation. The firstone examines the overall traversability of the observed scene taking into considerationthe robot’s embodiment. This knowledge constitutes a cornerstone forthe autonomous robot navigation. The developed system utilizes an algorithm to retrieve specific characteristics of the environment using a stereo cameraand to produce a disparity map of the scene. Then, the v-disparity image iscalculated based on the disparity map. The v-disparity is then exploited by afeature extraction procedure to provide the system with respective conceptualvectors, which are used to train support vector machines in order to assess theoverall traversability of the scene. The traversable classified scenes are furtherprocessed and the likelihood distribution of the collision risk assessment, whenthe robot moves towards any direction, is calculated. The second part involvesthe description of a novel object recognition algorithm based on hierarchical temporalmemory networks. This constitutes a supervised learning method usedto recognize objects in different orientations. It introduces specific alternativerules for the design of each building block of a hierarchical temporal memorynetwork. These rules expand both the spatial and the temporal module ofthe network. Various type of input layers have been tested such as logppolarand saliency detection ones in order to find the solution that fits better in applicationsthat concern cluttered environments. The third part of this chaptercomprises the description of an innovative place classification algorithm. Withinthis work, in course of robot’s locomotion, salient visual features are detectedand they shape a bag-of-features problem, quantized by a neural gas to codethe spatial information for each scene. Each input image is transformed intoan appearance based histogram representation that abstracts the place in avery compatible and consistent manner. The learning procedure is performedby support vector machines able to accurately recognize multiple dissimilarplaces. The innovative appearance based algorithm produces semantic inferencessuitable for labeling unexplored environments. In the rest of this chaptera long range semantic mapping framework is described, where geometricalmapping, with place and object recognition is combined in order to construct ahuman oriented semasiological map. This framework features geometrical andsemasiological attributes capable to reveal relationships between objects andplaces in a real-life environment. The geometrical component consists of a 3Dmap, onto which a topological map is deployed, representing the explored areaas a graph of nodes. The semasiological part is realized by putting together aplace recognition algorithm and an object recognition one. The categorizationof the different places relies on the resolution of appearance-based consistencyhistograms, while for the recognition of objects in the scene we make use of thehierarchical temporal memory network boosted by a saliency attentional model.These semantical attributes are then deposited on each node of the topologicalmap, in order to augment it with the belief distributions regarding the visitedplaces, thus resulting in a 3D object map embodying geometrical and semanticalproperties.Finally, in the fourth chapter of this thesis a novel semantic mapping method suitable for robot navigation in a human compatible manner is illustrated. Themain objective of this chapter is to take into account the time proximity of therobot acquired frames within the semantic map. The goal of this method is; (i)to introduce a semasiological mapping method for robot exploration and (ii) tomake use of the constructed semantic map as a means to provide a hierarchicalnavigation solution. The semantic map is formed during the robot’s course,relying on the memorization of abstract place representations. It integrates thespace quantization, the time proximity of the acquired frames and the spatialcoherence in terms of a novel labeled sparse topological map. A time evolvingaugmented navigation graph is shaped determining the semantic topology ofthe explored environment and the physical connectivity among the recognizedplaces expressed by the inter-place transition probability. Concerning therobot navigation part, a human-robot interaction methodology is illustratedcapable of competently addressing go-to commands. As a product of thismodule, an augmented navigation graph is formed, which handles the high levelrobot navigation, forwarding the presumable sequence of places the robotshould traverse to reach its target location. Accordingly, for the low levellocal navigation the topometric data of the semantic map are made use of.Additionally, to assist the human robot interaction a graphical user interfacehas been developed providing an overall supervision of the mapping andnavigation procedure.The last chapter of this dissertation concludes the doctoral research conductedhereby. It discusses the achievements of this work, while it also highlightsthe open issues and the questions revealed during the developmentof the several algorithmic solutions. Additionally, some future trends of thesemantic mapping are outlined establishing the potential descendants of thisdissertation",robotics,817,not included
10.3233/thc-140783,to_check,core,'IOS Press',2014-01-01 00:00:00,core,cardiopulmonary performance testing using a robotics-assisted tilt table: feasibility assessment in able-bodied subjects,,"BACKGROUND:

Robotics-assisted tilt table technology was introduced for early rehabilitation of neurological patients. It provides cyclical stepping movement and physiological loading of the legs. The aim of the present study was to assess the feasibility of this type of device for peak cardiopulmonary performance testing using able-bodied subjects.

METHODS:

A robotics-assisted tilt table was augmented with force sensors in the thigh cuffs and a work rate estimation algorithm. A custom visual feedback system was employed to guide the subjects' work rate and to provide real time feedback of actual work rate. Feasibility assessment focused on: (i) implementation (technical feasibility), and (ii) responsiveness (was there a measurable, high-level cardiopulmonary reaction?). For responsiveness testing, each subject carried out an incremental exercise test to the limit of functional capacity with a work rate increment of 5 W/min in female subjects and 8 W/min in males.

RESULTS:

11 able-bodied subjects were included (9 male, 2 female; age 29.6 ± 7.1 years: mean ± SD). Resting oxygen uptake (O_{2}) was 4.6 ± 0.7 mL/min/kg and O_{2}peak was 32.4 ± 5.1 mL/min/kg; this mean O_{2}peak was 81.1% of the predicted peak value for cycle ergometry. Peak heart rate (HRpeak) was 177.5 ± 9.7 beats/min; all subjects reached at least 85% of their predicted HRpeak value. Respiratory exchange ratio (RER) at O_{2}peak was 1.02 ± 0.07. Peak work rate) was 61.3 ± 15.1 W. All subjects reported a Borg CR10 value for exertion and leg fatigue of 7 or more.

CONCLUSIONS:

The robotics-assisted tilt table is deemed feasible for peak cardiopulmonary performance testing: the approach was found to be technically implementable and substantial cardiopulmonary responses were observed. Further testing in neurologically-impaired subjects is warranted",robotics,818,included
10.1007/978-3-319-01168-4,to_check,core,'Springer Science and Business Media LLC',2013-01-01 00:00:00,core,texplore: temporal difference reinforcement learning for robots and time-constrained domains,,"This book presents and develops new reinforcement learning methods that enable fast and robust learning on robots in real-time. Robots have the potential to solve many problems in society, because of their ability to work in dangerous places doing necessary jobs that no one wants or is able to do. One barrier to their widespread deployment is that they are mainly limited to tasks where it is possible to hand-program behaviors for every situation that may be encountered. For robots to meet their potential, they need methods that enable them to learn and adapt to novel situations that they were not programmed for. Reinforcement learning (RL) is a paradigm for learning sequential decision making processes and could solve the problems of learning and adaptation on robots. This book identifies four key challenges that must be addressed for an RL algorithm to be practical for robotic control tasks. These RL for Robotics Challenges are: 1) it must learn in very few samples; 2) it must learn in domains with continuous state features; 3) it must handle sensor and/or actuator delays; and 4) it should continually select actions in real time. This book focuses on addressing all four of these challenges. In particular, this book is focused on time-constrained domains where the first challenge is critically important. In these domains, the agent’s lifetime is not long enough for it to explore the domains thoroughly, and it must learn in very few samples",robotics,819,not included
hindawi limited,to_check,core,,2010-01-01 00:00:00,core,bootstrap learning and visual processing management on mobile robots,10.1155/2010/765876,"A central goal of robotics and AI is to enable a team of robots to operate autonomously in the real world and collaborate with humans over an extended period of time. Though developments in sensor technology have resulted in the deployment of robots in specific applications the ability to accurately sense and interact with the environment is still missing. Key challenges to the widespread deployment of robots include the ability to learn models of environmental features based on sensory inputs, bootstrap off of the learned models to detect and adapt to environmental changes, and autonomously tailor the sensory processing to the task at hand. This paper summarizes a comprehensive effort towards such bootstrap learning, adaptation, and processing management using visual input. We describe probabilistic algorithms that enable a mobile robot to autonomously plan its actions to learn models of color distributions and illuminations. The learned models are used to detect and adapt to illumination changes. Furthermore, we describe a probabilistic sequential decision-making approach that autonomously tailors the visual processing to the task at hand. All algorithms are fully implemented and tested on robot platforms in dynamic environments",robotics,820,not included
trace: tennessee research and creative exchange,to_check,core,https://core.ac.uk/download/268809713.pdf,2011-12-01 00:00:00,core,cytongrasp: cyton alpha controller via graspit! simulation,,"This thesis addresses an expansion of the control programs for the Cyton Alpha 7D 1G arm. The original control system made use of configurable software which exploited the arm’s seven degrees of freedom and kinematic redundancy to control the arm based on desired behaviors that were configured off-line. The inclusions of the GraspIt! grasp planning simulator and toolkit enables the Cyton Alpha to be used in more proactive on-line grasping problems, as well as, presenting many additional tools for on-line learning applications. In short, GraspIt! expands what is possible with the Cyton Alpha to include many machine learning tools and opportunities for future research. Noteworthy features of GraspIt!:
• A 3D user interface allowing the user to see and interact virtual objects, obstacles, and robots, in addition to a 3D representation of the Cyton Alpha
• A collision detection and contact determination system within simulation • On-line grasp analysis routines
• Visualization methods for determining the weak points within a grasp, as well as, creating projections of grasp quality and ability to resist dynamic forces.
• Computation of numerical grasp quality metrics and visualization methods for proposed grasps
• Dynamics engine
• Support for lower-dimensional hand posture subspaces
• Interaction with sensors (Flock of Birds tracker) and hardware (Pioneer robot) within simulation
• GraspIt! can generate huge databases of labeled grasp data, which can be used for data-driven grasp-planning algorithms and has built in support for the Columbia Grasp Database.
By making use of the GraspIt! simulator, it is possible to test algorithms for grasp manipulation, grasp planning, or grasp synthesis more quickly and with greater repeatability than would be possible on the real robot. Contributions of this system include:
1. A joint based 3D rendering of the Cyton Alpha 7D 1G arm
2. Simulated bodies for several objects in the DI Lab
3. Support for multiple representations of joint data within three-dimensional space
• Euler Angles
• Quaternions
• Denavit-Hartenberg Parameters
4. Framework for future work in grasp-planning, grasp synthesis, cooperative grasping tasks, and transfer learning applications with the Cyton Alpha arm",robotics,821,not included
10.1007/bfb0095437,to_check,core,,1998-01-01 00:00:00,core,golex - bridging the gap between logic (golog) and a real robot,,". The control of mobile robots acting autonomously in the real world is one of the long-term goals of the field of artificial intelligence. So far the field lacks methods bridging the gap between the sophisticated symbolic techniques to represent and reason about action and more and more reliable low-level robot control and navigation systems. In this paper we present GOLEX, an execution and monitoring system for the logic-based action language GOLOG and the complex and distributed RHINO control software which operates on RWI B21 and B14 mobile robots. GOLEX provides the following features: it maps abstract primitive actions into low-level commands of the robot control system, thus allowing the user to concentrate on the application rather than the inner workings of the robot; it monitors the execution of the primitive GOLOG actions, making it possible to detect simple execution failures and timeouts; and it includes means to deal with sensing and user input and to continue the operati..",robotics,822,included
10.1007/3-540-46084-5_26,to_check,core,'Springer Science and Business Media LLC',2002-01-01 00:00:00,core,a direction sensitive network based on a biophysical neurone model,,"Iske B, Löffler A, Rückert U. A Direction Sensitive Network Based on a Biophysical Neurone Model. In: Dorronsoro JR, ed. Artificial Neural Networks- ICANN 2002. Lecture notes in computer science. Vol 2415. Madrid, Spain: Springer-Verlag;  2002: 153-159.To our understanding, modelling the dynamics of brain functions on cell level is essential to develop both a deeper understanding and classification of the experimental data as well as a guideline for further research. This paper now presents the implementation and training of a direction sensitive network on the basis of a biophisical neurone model including synaptic excitation, dendritic propagation and action-potential generation. The underlying model not only describes the functional aspects of neural signal processing, but also provides insight into their underlying energy consumption. Moreover, the training data set has been recorded by means of a real robotics system, thus bridging the gap to technical applications",robotics,823,not included
,to_check,core,,2019-01-01 00:00:00,core,implementation of electrical and simulation for autonomous driving,,"The project calls for the application of engineering and programming knowledge to develop and implement an autonomous vehicle, the Nanyang Venture 11 (NV-11) which will be taking part in the Shell Eco-marathon Asia 2019. Throughout the project, different task had been allocated to collaborate as a team to build the NV-11. A 3-phase Brushless DC (BLDC) electric motor controller that was designed by the previous student had a few flaws which were the instantaneous current surge that cause the components on the circuitry board to be damaged. The implementation of the Soft Start code improved the performance and prevent it from any sudden current spikes occurring. The software kit by Renesas Electronics Corporation was used to program the microprocessor board to determine the clockwise and anti-clockwise rotation of the vehicle wheels. The NVIDIA Jetson TX2 is a supercomputer which is an embedded Artificial Intelligence computing device that acts as the brain on the autonomous vehicle NV-11. It operates using Ubuntu 16.04 Operating System and has the compatibility of using the Robot Operating System (ROS) as a platform that provides a set of software libraries and tools to build an application. A virtual environment was designed as a simulation to test the functionality of the autonomous vehicle using the data from the exterior sensors like the Lidar sensors, Zed camera, and Radar sensors. Using TensorFlow which is an open-source software library for machine learning to gather data for obstacles avoidance and detection. The Tinkerforge IMU Brick 2.0 is a device which is equipped with a 3-axis accelerometer, magnetometer (compass) and gyroscope. It provides the quaternions data which is a mathematical notation for representing orientations and rotations of the vehicle in three dimensions. It was implemented as part of the simulation to determine the exact location in the real world, comparing with the virtual world.Bachelor of Engineering (Electrical and Electronic Engineering",autonomous vehicle,824,not included
,to_check,core,Lunds universitet/Institutionen för elektro- och informationsteknik,2020-01-01 00:00:00,core,implementation of a deep learning inference accelerator on the fpga.,,"Today, Artificial Intelligence is one of the most important technologies, ubiquitous in our daily lives. Deep Neural Networks (DNN's) have come up as state of art for various machine intelligence applications such as object detection, image classification, face recognition and performs myriad of activities with exceptional prediction accuracy. AI in this contemporary world is moving towards embedded platforms for inference on the edge. This is essential to avoid latency, enhance data security and realize real-time performance. However, these DNN algorithms are computational and memory intensive. Consequently, exploiting immense energy, compute resources and memory-bandwidth making it difficult to be deployed in embedded devices. To solve this problem and realize an on-device AI acceleration, dedicated energy-efficient hardware accelerators are paramount. This thesis involves the implementation of such a dedicated deep learning accelerator on the FPGA. The NVIDIA's Deep Learning Accelerator (NVDLA), is encompassed in this research to explore SoC designs for integrated inference acceleration. NVDLA, an open-source architecture, standardizes deep learning inference acceleration on hardware. It optimizes inference acceleration all across the full stack from application through hardware to achieve energy efficiency synergy with the demanding throughput requirements. Therefore, the following thesis probes into the NVDLA framework to perceive the consistent workflow across the whole hardware-software programming hierarchies. Besides, the hardware design parameters, optimization features and system configurations of the NVDLA systems are analyzed for efficient implementations. Also, a comparative study of the diverse NVDLA SoC implementations (nv\_small and nv\_medium) with respect to performance metrics such as power, area, and throughput are discussed. Our approach engages prototyping of Nvidia’s Deep Learning Accelerator on a Zynq Ultrascale+ ZCU104 FPGA to examine its system functionality. The Hardware design of the system is carried out using Xilinx's Vivado Design Suite 2018.3 in Verilog. While the on-device software runs Linux kernel 4.14 on Zynq MPSoC. Thus, the software ecosystem is built with PetaLinux tools from Xilinx. The entire system architecture is validated using the pre-built regression tests that verify individual CNN layers. Besides these NVDLA hardware design also runs pre-compiled AlexNet as a benchmark for performance evaluation and comparisonToday, Artificial Intelligence is at the edge. This edge or endpoint device is becoming more sophisticated with the evolution of Internet of Things (IoT) and 5G. For instance, these devices are employed in different applications such as autonomous cars, drones, and other IoT gadgets. At present, a self-driving car is a data center on wheels, a drone is a data center on wings as well as robots are data centers with arms and legs. All these mechanisms collect vast real-world information that demands to be processed in real-time. Here in these applications, there is no time to send data to the cloud for processing and wait for action. As the decision making needs to be instantaneous. There is a shift in transforming the processing to the edge devices. The edge acceleration brings computation and data storage closer to the device. With the evolution of specialized hardware’s providing increased computational capabilities, the AI models are processed on the edge. As a result, the overall system latency gets reduced, the bandwidth costs for data transfers are lowered and the data processing is done locally enhances privacy concerns. For example, autonomous cars require a spontaneous reaction (in seconds) to avoid potential hazards on the road. Consider the situation where a self-driving car is collecting real word information like images, videos, in this case, assume it’s sensing for a stop sign. If the system sends the specific image information to the cloud for processing and waits for a decision to stop. By that response time, the autonomous vehicle could have already blown through the stop sign running over several people. Therefore, it is paramount to process the data in real-time which could be accomplished using dedicated hardware for processing locally. This thesis primarily explores those hardware architectures for efficient processing of AI algorithms and their corresponding software execution environment setup. The particular thesis was carried out as a joint collaboration between Ericsson and Lund University. Here Nvidia’s Deep Learning Accelerator architecture is engaged as a target to comprehend the complete system incorporating a hardware-software co-design. The particular architecture is an essential characteristic of NVIDIA’s Xavier Drive chip which is utilized in their autonomous drive platforms. This thesis is addressed to a variety of audiences who are passionate about Deep Learning, Computer Architecture, and System-on-Chip Design. The thesis illustrates a comprehensive implementation of an AI accelerator to envision AI processing on the edge",autonomous vehicle,825,included
,to_check,core,,2021-06-25 00:00:00,core,feature attributions and counterfactual explanations can be manipulated,http://arxiv.org/abs/2106.12563,"As machine learning models are increasingly used in critical decision-making
settings (e.g., healthcare, finance), there has been a growing emphasis on
developing methods to explain model predictions. Such \textit{explanations} are
used to understand and establish trust in models and are vital components in
machine learning pipelines. Though explanations are a critical piece in these
systems, there is little understanding about how they are vulnerable to
manipulation by adversaries. In this paper, we discuss how two broad classes of
explanations are vulnerable to manipulation. We demonstrate how adversaries can
design biased models that manipulate model agnostic feature attribution methods
(e.g., LIME \& SHAP) and counterfactual explanations that hill-climb during the
counterfactual search (e.g., Wachter's Algorithm \& DiCE) into
\textit{concealing} the model's biases. These vulnerabilities allow an
adversary to deploy a biased model, yet explanations will not reveal this bias,
thereby deceiving stakeholders into trusting the model. We evaluate the
manipulations on real world data sets, including COMPAS and Communities \&
Crime, and find explanations can be manipulated in practice.Comment: arXiv admin note: text overlap with arXiv:2106.0266",health,826,not included
,to_check,core,'Springer Fachmedien Wiesbaden GmbH',2021-01-01 00:00:00,core,network traffic analysis using machine learning: an unsupervised approach to understand and slice your network,,"International audienceRecent development in smart devices has lead us to an explosion in data generation and heterogeneity, which requires new network solutions for better analysing and understanding traffic. These solutions should be intelligent and scalable in order to handle the huge amount of data automatically. With the progress of high-performance computing (HPC), it becomes feasible easily to deploy machine learning (ML) to solve complex problems and its efficiency has been validated in several domains (e.g., healthcare or computer vision). At the same time, network slicing (NS) has drawn significant attention from both industry and academia as it is essential to address the diversity of service requirements. Therefore, the adoption of ML within NS management is an interesting issue. In this paper, we have focused on analyzing network data with the objective of defining network slices according to traffic flow behaviors. For dimensionality reduction, the feature selection has been applied to select the most relevant features (15 out of 87 features) from a real dataset of more than 3 million instances. Then, a K-Means clustering is applied to better understand and distinguish behaviors of traffic. The results demonstrated a good correlation among instances in the same cluster generated by the unsupervised learning. This solution can be further integrated in a real environment using network function virtualization",health,827,not included
,to_check,core,,2021-03-05 00:00:00,core,reducing cybersickness in 360-degree virtual reality,http://arxiv.org/abs/2103.03898,"Despite the technological advancements in Virtual Reality (VR), users are
constantly combating feelings of nausea and disorientation, the so called
cybersickness. Triggered by a sensory conflict between the visual and
vestibular systems, cybersickness symptoms cause discomfort and hinder the
immersive VR experience. Here we investigated cybersickness in 360-degree VR.
In 360-degrees VR experiences, movement in the real world is not reflected in
the virtual world, and therefore self-motion information is not corroborated by
matching visual and vestibular cues, which may potentially induce
cybersickness. We have evaluated whether an Artificial Intelligence (AI)
software designed to supplement the VR experience with artificial
6-degree-of-freedom motion may reduce sensory conflict, and therefore
cybersickness. Explicit (questionnaires) and implicit (physiological responses)
measurements were used to measure cybersickness symptoms during and after VR
exposure. Our results confirmed a reduction in feelings of nausea during the AI
supplemented 6-degree-of-freedom motion VR. Through improving the congruency
between visual and vestibular cues, users can experience more engaging,
immersive and safe virtual reality, which is critical for the application of VR
in educational, medical, cultural and entertainment settings.Comment: 18 pages, 1 figure; Software available at
  https://www.kagenova.com/products/copernic360",health,828,not included
,to_check,core,,2020-12-01 00:00:00,core,"escholarship, university of california",,"ObjectiveThe coronavirus disease 2019 pandemic has inspired new innovations in diagnosing, treating, and dispositioning patients during high census conditions with constrained resources. Our objective is to describe first experiences of physician interaction with a novel artificial intelligence (AI) algorithm designed to enhance physician abilities to identify ground-glass opacities and consolidation on chest radiographs.MethodsDuring the first wave of the pandemic, we deployed a previously developed and validated deep-learning AI algorithm for assisted interpretation of chest radiographs for use by physicians at an academic health system in Southern California. The algorithm overlays radiographs with ""heat"" maps that indicate pneumonia probability alongside standard chest radiographs at the point of care. Physicians were surveyed in real time regarding ease of use and impact on clinical decisionmaking.ResultsOf the 5125 total visits and 1960 chest radiographs obtained in the emergency department (ED) during the study period, 1855 were analyzed by the algorithm. Among these, emergency physicians were surveyed for their experiences on 202 radiographs. Overall, 86% either strongly agreed or somewhat agreed that the intervention was easy to use in their workflow. Of the respondents, 20% reported that the algorithm impacted clinical decisionmaking.ConclusionsTo our knowledge, this is the first published literature evaluating the impact of medical imaging AI on clinical decisionmaking in the emergency department setting. Urgent deployment of a previously validated AI algorithm clinically was easy to use and was found to have an impact on clinical decision making during the predicted surge period of a global pandemic",health,829,not included
,to_check,core,,2021-01-01 00:00:00,core,πανεπιστήμιο θεσσαλίας,,"This doctoral dissertation explores intelligent systems and services for image and video analysis. In view of scientific challenges for developing innovative solutions with a broad social impact, it investigates applications in biomedicine and computer-assisted navigation of visually impaired individuals. In this context, it focuses on machine learning, particularly the investigation of methods to improve the efficiency and the effectiveness of deep artificial neural network architectures, such as the Convolutional Neural Networks (CNNs).In Convolutional Neural Networks (CNNs) the input data can contain uncertainties, such as noise, color and geometric ubiquities, that is naturally propagated from the input layer to the convolution layers of the network affecting the quality of the extracted features. To cope with this problem, a novel pooling operation based on (type-1) fuzzy sets is proposed, named Fuzzy Pooling, which can be used as a drop-in replacement of the current, crisp, pooling layers of CNN architectures. Several experiments using publicly available datasets show that the proposed approach can enhance the classification performance of a CNN.Aiming to improve the effectiveness of CNNs, especially in the context of medical image analysis, a novel architecture named Look Behind Fully Convolutional Neural Network (LB-FCN) is proposed. The architecture is capable of extracting multi-scale image features by using blocks of parallel convolutional layers with different filter sizes. These blocks are connected by look-behind connections, so that the features they produce are combined with features extracted from behind layers, thus preserving the respective information. Furthermore, it has a smaller number of free parameters than conventional CNN architectures, which makes it suitable for training with smaller datasets. This is particularly useful in medical image analysis, since data availability is usually limited, due to ethicolegal constraints. Experiments on publicly available gastrointestinal image datasets show higher classification performance compared to state-of-the-art machine and deep learning methodologies. The architecture is capable of generalizing well even when the training dataset is different than the one on which it is tested. To investigate that, a novel cross-dataset experimental study was performed on various publicly available gastrointestinal tract image datasets, containing images from different modalities, including Wireless Capsule Endoscopy (WCE) and flexible endoscopy. The number of training samples in CNN training is directly linked to their generalization performance. When the training samples are limited, such as in the case of medical images, the generalization performance is negatively affected. A typical approach to mediate this problem is to use data augmentation techniques, which image rotation and translation. While effective, this technique still requires a substantial amount of training samples to be available. To battle this problem, in the context of inflammatory conditions detection in WCE images, a novel approach is presented that uses Generative Adversarial Networks (GANs) to generate artificial images. More specifically the study trained two GANs, one to generate healthy small bowel images and another, images with inflammatory conditions. The images are then used to train a CNN architecture and validate its performance in real images. The results from this study show that the substitution of real with artificially generated endoscopic images for CNN training can be a viable option.While CNNs have a remarkable performance in computer vision problems, usually, they are computationally expensive. This limits their usage in high-end expensive devices with multiple graphical processing units (GPUs). To mediate the problem, a typical approach is to reduce the number of floating-point operations (FLOPs) required for inference, at the expense of generalization performance. In this context, a novel LB-FCN inspired CNN architecture was proposed, named LB-FCN light. The architecture features a relatively low number of free parameters and FLOPs, while managing to maintain high generalization performance. The performance of the network is validated in the problem of staircase detection in indoor and outdoor environments, with application on assisted navigation of visually impaired individuals. The results from the experimental evaluation of LB-FCN light indicate its advantageous performance over the relevant state-of-the-art architectures.The development of easy-to-use machine learning (ML) application frameworks has enabled the development of advanced artificial intelligence (AI) applications with only a few lines of self-explanatory code. However, the deployment of ML algorithms as a service for remote high throughput ML task execution, involving complex data-processing pipelines can still be challenging, especially with respect to production ML use cases. To cope with this issue, a novel system architecture is presented, which enables Algorithm-agnostic, Scalable ML (ASML) task execution for high throughput applications. It aims to provide an answer to the research question of how to design and implement an abstraction framework, suitable for the deployment of end-to-end ML pipelines in a generic and standard way. The architecture manages horizontal scaling, task scheduling, reporting, monitoring and execution of multi-client ML tasks using modular, extensible components that abstract the execution details of the underlying algorithms. Applications of ASML are investigated for the analysis of image streams in the context of medical image analysis and assisted navigation of visually impaired individuals. The results of the experiments performed demonstrate its capacity for parallel, mission critical, task execution. Assistive navigation systems require the development, assessment, and optimization of different algorithms for obstacle detection, recognition, and avoidance, as well as path planning. This is a painstaking and costly process that requires repetitive measurements under stable conditions, which is usually difficult to achieve. To this end, a novel digital twin framework for the simulation and evaluation of assistive navigation systems is presented. The framework can replicate relevant real-life situations, enabling the evaluation and optimization of algorithms through adjustable and cost-effective simulations. The utility and the effectiveness of the framework are demonstrated with an indicative simulation study in the context of a camera-based wearable system for the navigation of visually impaired individuals in an outdoor cultural space.The work presented in this dissertation includes methods with both theoretical and practical impact, that can be used as the basis for further research, and the applications presented can be used as paradigms for applications on different domains, such as telemedicine, robotics, and intelligent transportation systems.Η παρούσα διδακτορική διατριβή διερευνά πρωτότυπα έξυπνα συστήματα και υπηρεσίες ανάλυσης εικόνας και βίντεο. Λαμβάνοντας υπόψη τις επιστημονικές προκλήσεις για την ανάπτυξη καινοτόμων λύσεων με ευρύ κοινωνικό αντίκτυπο, διερευνά εφαρμογές στη βιοϊατρική και την καθοδήγηση ατόμων με προβλήματα όρασης. Σε αυτό το πλαίσιο, επικεντρώνεται στη μηχανική μάθηση, εστιάζοντας στη διερεύνηση μεθόδων για τη βελτίωση της αποδοτικότητας και αποτελεσματικότητας των αρχιτεκτονικών βαθέων τεχνητών νευρικών δικτύων, όπως τα Συνελικτικά Νευρωνικά Δίκτυα (Convolutional Neural Networks, CNN).Τα δεδομένα εισόδου των CNN μπορούν να περιέχουν αβεβαιότητες, όπως θόρυβος, χρώμα και γεωμετρική απροσδιοριστία, που μεταδίδονται από το επίπεδο εισόδου στα συνελικτικά επίπεδα του δικτύου επηρεάζοντας την ποιότητα των εξαγόμενων χαρακτηριστικών. Προκειμένου να αντιμετωπιστεί αυτό το πρόβλημα, προτείνεται μια νέα λειτουργία συγκέντρωσης (pooling) βασισμένη σε ασαφή σύνολα (τύπου-1), με όνομα Fuzzy Pooling, η οποία μπορεί να χρησιμοποιηθεί για την αντικατάσταση των υπαρχόντων επιπέδων pooling των CNN αρχιτεκτονικών. Πειράματα σε δημοσίως διαθέσιμα δεδομένα έδειξαν ότι η χρήση της  προτεινόμενη προσέγγισης μπορεί να χρησιμοποιηθεί για την βελτίωση της απόδοσης ταξινόμησης των CNN.Με στόχο τη βελτίωση της αποτελεσματικότητας των CNN, και ειδικότερα στο πλαίσιο της ανάλυσης ιατρικών εικόνων, προτάθηκε μια νέα αρχιτεκτονική CNN που ονομάζεται Look Behind Fully Convolutional Neural Network (LB-FCN). Η αρχιτεκτονική είναι ικανή να εξαγάγει χαρακτηριστικά πολλαπλών κλιμάκων χρησιμοποιώντας σύνολα (μπλοκ) παράλληλων συνελικτικών στρωμάτων με διαφορετικά μεγέθη φίλτρου. Τα σύνολα αυτά, συνδέονται με οπίσθιες συνδέσεις, με στόχο τον συνδυασμό των παραγόμενων χαρακτηριστικών με τα χαρακτηριστικά  εισόδου, διατηρώντας έτσι τις αντίστοιχες πληροφορίες. Επιπλέον, η αρχιτεκτονική έχει μικρότερο πλήθος ελεύθερων παραμέτρων σε σχέση με συμβατικές αρχιτεκτονικές CNN, γεγονός που επιτρέπει την εκπαίδευσή της με μικρό πλήθος δεδομένων εκπαίδευσης. Αυτό είναι ιδιαίτερα χρήσιμο στην ανάλυση ιατρικών εικόνας, δεδομένου ότι η διαθεσιμότητα δεδομένων εκπαίδευσης είναι συνήθως περιορισμένη, λόγω βιοηθικών και νομικών περιορισμών. Πειράματα σε δημοσίως διαθέσιμα δεδομένα εικόνων του γαστρεντερικού συστήματος, παρουσιάζουν υψηλή απόδοση ταξινόμησης σε σύγκριση με άλλες σύγχρονες προσεγγίσεις. Η αρχιτεκτονική είναι ικανή να γενικεύει καλά ακόμη και όταν το δεδομένα εκπαίδευσης προέρχονται από διαφορετικά σύνολα δεδομένων από αυτά στα οποίο δοκιμάζεται. Σε αυτό το πλαίσιο, πραγματοποιήθηκε πειραματική μελέτη σε πληθώρα δημοσίων διαθέσιμων συνόλων δεδομένων γαστρεντερικού συστήματος, απαρτιζόμενα από εικόνες που έχουν ληφθεί κάνοντας χρήση διαφορετικών ιατρικών οργάνων, όπως ενδοσκοπικής κάψουλας και εύκαμπτου ενδοσκοπίου. Η δυνατότητα γενίκευσης των CNN συνδέεται άμεσα με το διαθέσιμο πλήθος δειγμάτων εκπαίδευσης. Όταν τα δείγματα εκπαίδευσης είναι περιορισμένα, όπως στην περίπτωση ιατρικών εικόνων, η δυνατότητα γενίκευσης επηρεάζεται αρνητικά. Μια τυπική προσέγγιση για τον περιορισμό αυτού του προβλήματος είναι η χρήση τεχνικών επαύξησης δεδομένων, τροποποιώντας τα υπάρχοντα δεδομένα. Αν και η τεχνική αυτή είναι αποτελεσματική και πάλι απαιτείται σημαντικό πλήθος δεδομένων εκπαίδευσης. Για την καταπολέμηση αυτού του προβλήματος, στο πλαίσιο της ανίχνευσης φλεγμονών σε εικόνες που προέρχονται από ενδοσκοπική κάψουλα, παρουσιάζεται μια προσέγγιση που χρησιμοποιεί Παραγωγικά Αντιπαραθετικά Δίκτυα (Generative Adversarial Networks, GAN) για τη δημιουργία συνθετικών εικόνων. Πιο συγκεκριμένα, η μελέτη βασίζεται στην εκπαίδευση δύο GAN, ένα για να την παραγωγή υγιών εικόνων του λεπτού εντέρου και ένα άλλο, για την παραγωγή εικόνων με φλεγμονές. Οι παραγόμενες εικόνες στη συνέχεια χρησιμοποιούνται για την εκπαίδευση ενός CNN με στόχο την αξιολόγηση της αποδοτικότητάς του σε πραγματικές εικόνες. Τα αποτελέσματα αυτής της μελέτης δείχνουν ότι η αντικατάσταση πραγματικών με τεχνητά παραγόμενων ενδοσκοπικών εικόνων για εκπαίδευση στο CNN μπορεί να είναι μια βιώσιμη επιλογή.Η αξιοσημείωτη απόδοση των CNN στον τομέα της υπολογιστικής όρασης, συνήθως, συνοδεύεται από αυξημένο υπολογιστικό κόστος. Αυτό περιορίζει τη χρήση τους σε συσκευές υψηλών υπολογιστικών προδιαγραφών εξοπλισμένες με πολλαπλές κάρτες γραφικών. Για την αντιμετώπιση αυτού του προβλήματος, μια τυπική προσέγγιση είναι η μείωση των απαιτούμενων αριθμητικών πράξεων, σε βάρος της απόδοσης γενίκευσης. Σε αυτό το πλαίσιο, προτάθηκε μια νέα αρχιτεκτονική CNN, εμπνευσμένη από την LB-FCN, με όνομα LB-FCN light. Η αρχιτεκτονική διαθέτει χαμηλό αριθμό ελεύθερων παραμέτρων και πράξεων, ενώ παράλληλα διατηρεί υψηλή απόδοση γενίκευσης. Η απόδοση του δικτύου διερευνήθηκε στο πρόβλημα της ανίχνευσης σκαλών σε εσωτερικούς και εξωτερικούς χώρους, με εφαρμογές στην υποβοηθούμενη πλοήγηση ατόμων με προβλήματα όρασης. Τα αποτελέσματα από την πειραματική αξιολόγηση του LB-FCN light δείχνουν πως απόδοσή του είναι υψηλότερη σε σύγκριση με άλλες, σύγχρονες αρχιτεκτονικές CNNs. Η ανάπτυξη εύχρηστων πλαισίων εφαρμογών μηχανικής μάθησης, δίνει την δυνατότητα ανάπτυξης προηγμένων εφαρμογών τεχνητής νοημοσύνης με μόνο λίγες γραμμές κώδικα. Ωστόσο, η εγκατάσταση αλγορίθμων μηχανικής μάθησης σε απομακρυσμένο περιβάλλον υψηλής απόδοσης, που περιλαμβάνει περίπλοκα επίπεδα επεξεργασίας δεδομένων, εξακολουθεί να είναι δύσκολη, ειδικά όταν τα περιβάλλοντα αυτά προορίζονται για χρήση από επιχειρήσεις. Για την αντιμετώπιση αυτού του προβλήματος, παρουσιάζεται μια νέα αρχιτεκτονική συστήματος, η οποία επιτρέπει την εκτέλεση εργασιών μηχανικής μάθησης για εφαρμογές υψηλής απόδοσης, με όνομα Algorithm-agnostic, Scalable Machine Learning (ASML). Στόχος της αρχιτεκτονικής είναι να δώσει μια απάντηση στο ερευνητικό πρόβλημα της σχεδίας και ανάπτυξης πλαισίου εφαρμογής, κατάλληλο για την ανάπτυξη διεργασιών μηχανικής μάθησης με γενικό και τυποποιημένο τρόπο, ανεξάρτητο του αλγορίθμου μηχανικής μάθησης. Η αρχιτεκτονική διαχειρίζεται την οριζόντια κλιμάκωση, τον προγραμματισμό εργασιών, την αναφορά, την παρακολούθηση και την εκτέλεση εργασιών μηχανικής μάθησης, με δυνατότητα χρήσης από πολλαπλούς χρήστες, χρησιμοποιώντας ανεξάρτητα και επεκτάσιμα στοιχεία που αποκρύπτουν τις λεπτομέρειες εκτέλεσης των υποκείμενων αλγορίθμων. Η δυνατότητες της αρχιτεκτονικής διερευνήθηκαν σε εφαρμογές ανάλυσης ροών εικόνων από ιατρικά δεδομένα και στα πλαίσια της υποβοηθούμενης πλοήγηση ατόμων με προβλήματα όρασης. Τα αποτελέσματα των πειραμάτων που πραγματοποιήθηκαν δείχνουν ότι η αρχιτεκτονική είναι κατάλληλη για παράλληλη χρήση και σε κρίσιμα συστήματα.Τα συστήματα υποβοηθούμενης πλοήγησης απαιτούν την ανάπτυξη, αξιολόγηση και βελτιστοποίηση διαφορετικών αλγορίθμων για την ανίχνευση εμποδίων, την αναγνώριση και την αποφυγή τους, καθώς και τον σχεδιασμό διαδρομών. Η διαδικασία αυτή είναι ιδιαιτέρως επίπονη και δαπανηρή και απαιτεί επαναλαμβανόμενες μετρήσεις υπό σταθερές συνθήκες, κάτι που συνήθως είναι δύσκολο να επιτευχθεί. Για το σκοπό αυτό, παρουσιάζεται ένα πρωτότυπο πλαίσιο εφαρμογής για την προσομοίωση και την αξιολόγηση συστημάτων υποβοήθησης πλοήγησης. Το πλαίσιο αυτό μπορεί να αναπαράγει πραγματικές καταστάσεις, επιτρέποντας την αξιολόγηση και βελτιστοποίηση αλγορίθμων μέσω ρυθμιζόμενων και οικονομικά αποδοτικών προσομοιώσεων. Η χρησιμότητα και η αποτελεσματικότητα του πλαισίου αποδεικνύονται με μια ενδεικτική μελέτη προσομοίωσης στο πλαίσιο ενός φορητού συστήματος που βασίζεται σε κάμερα για την πλοήγηση ατόμων με προβλήματα όρασης σε έναν υπαίθριο χώρο πολιτιστικού ενδιαφέροντος.Το έργο που παρουσιάστηκε στην παρούσα διατριβή περιλαμβάνει μεθόδους με θεωρητικό και πρακτικό αντίκτυπο, οι οποίες μπορούν να χρησιμοποιηθούν ως βάση για περαιτέρω έρευνα. Οι εφαρμογές που παρουσιάζονται μπορούν να χρησιμοποιηθούν ως πρότυπα για εφαρμογές σε διαφορετικούς τομείς, όπως τηλεϊατρική, ρομποτική και έξυπνα συστήματα μετακίνησης",health,830,not included
,to_check,core,,2020-01-01 00:00:00,core,αριστοτέλειο πανεπιστήμιο θεσσαλονίκης (απθ),,"Ageing consists one of the biggest societal challenges worldwide. Older adults’ desire of staying healthy, staying at their own home consists an important motive for the development of assistive technologies that will realize it through remote monitoring technologies. Proliferation of low-cost, off-the-shelf IoT devices has led to the implementation of the so called smart homes projects, which are solutions that are mixing the older adults’ physical spaces with monitoring and computing capabilities, allowing the collection of high-frequency daily life data. Methods: This stydy consist of two parts. An experimental one that takes place in a lab space and a real life application of sensor monitoring technology. In particular, an ecologically valid space was created within the Thessaloniki Active & Healthy Ageing Living, where fifteen (15) older adults participated in the pilot testing of the technology. Participants visited the ecologically valid space for almost two weeks, covering eight (8) 1 hour-sessions in total. There, they followed a protocol of typical daily life activities, where several unobtrusive scenario of monitoring were interweaved. No restrictions were imposed with respect to the execution of tasks, thus making sure that the data collection follows an ecologically valid paradigm. Subsequently, senor data collected from 10 participants were statistically correlated to clinical assessment tools. Then, a series of focus groups openly discussing about technology and obtrusiveness were carried out with thirteen participants. Moving to the second part of the study, some adjustments with respect to the technology had to be considered according to the Living Lab study findings. Favorable technology was installed to 5 older womens’ homes for more than a year time resulting to hundreds of thousands data points to be collected. Different modeling techniques were employed, such as statistical modeling, and machine learning, in order to explore clinical added value of digital biomarkers. As an application scenario, the study of modelling emotional disturbances and in particular the one of geriatric depression was carried out. In addition, a longitudinal study about sensor monitoring unobtrusiveness was conducted to examine end-user acceptance. A follow-up interview was carried out with each one of the participants. Finally, we explore the possibility of mapping the results of the data-driven modelling approaches with expert-driven models and knowledge representation schemata, such as the Fuzzy Cognitive Maps (FCMs) in a way that is transparent to the clinicians. Results: With respect to the first part of the study quite a few statistical significant correlations between sensor data and clinical tools were found, e.g. mobility and affective characteristics linked to emotional health and quality of life. With respect to the results of the focus groups these were transcribed and analysed with qualitative methods in order to extract the most significant themes and to categorize them to one of the obtrusiveness framework axes. For the second part o fthe study, a generalized linear mixed prediction model of PHQ-9 was developed utilizing information about TV usage patterns. Random Forests achieved mean accuracy score >80% when given to classify between healthy and depressive cases, while another RF classifier achieved an AUC>90% when having to deal with all sorts of depressive synptoms severity (from mild to severe). The initial FCM model also achieved a high classification rate up to 96% given some synthetic cases provided by experts. Finally, older adults’ longitudinal attitudes revealed a negative stance towards the use of the mirror camera and the smart watch, as well as the Kinect device. The answers to the questionnaire of the lady that left the study at month 12, were compared against the mean value of the rest four particpants to check for any particular reasons of her decision. Significant differences from the mean value of the rest four were found for the Kinect device, the smart watch and the mirror camera. Conclusions: Clinical value of digital biomarkers has been been revealed in many publications, yet their application in longitudinal studies is absent. A great challenge nowadays remains the robust operation of such technologies under real life circumstances, without any restrictions (in the wild) as well as their acceptance from older people, and their doctors, if at any time in the future we wish to integrate such data in the clinical practice.Η γήρανση του πληθυσμού αποτελεί μία από τις μεγαλύτερες κοινωνικές προκλήσεις σε ολόκληρο τον πλανήτη. Η επιθυμία των ηλικιωμένων για καλή γήρανση, παραμένοντας στο σπίτι τους αποτελεί σημαντικό παράγοντα για την ανάπτυξη υποστηρικτικών τεχνολογιών που θα προσφέρουν αυτήν την δυνατότητα μέσω τεχνολογιών απομακρυσμένης παρακολούθησης. Η εξάπλωση φθηνών έξυπνων συσκευών άμεσα διαθέσιμων στο εμπόριο και με την δυνατότητα σύνδεσης στο λεγόμενο Διαδίκτυο των Πραγμάτων (Internet of Things - IoT) έχει στρέψει την ερευνητική κοινότητα στην ανάπτυξη των λεγόμενων έξυπνων σπιτιών. Τα έξυπνα σπίτια αποτελούν λύσεις που συνδυάζουν τεχνολογίες αισθητήρων ενσωματώνοντάς τες στον φυσικό περιβάλλοντα χώρο των σπιτιών των ηλικιωμένων, επιτρέποντας την συνεχή και λεπτομερή παρακολούθηση των καθημερινών δραστηριότητων τους. Μέθοδοι: Η μελέτη μας αποτελείται από δύο σκέλη. Ένα πειραματικό σε εργαστηριακό χώρο και μια εφαρμογή της τεχνολογίας σε πραγματικό περιβάλλον. Συγκεκριμένα, δημιουργήθηκε ένας οικολογικά έγκυρος χώρος στα πλαίσια του Ζωντανού Εργαστηρίου Ενεργού και Υγιούς Γήρανσης, όπου δέκα πέντε (15) ηλικιωμένοι συμμετέχοντες έλαβαν μέρος στην πιλοτική δοκιμή της τεχνολογίας. Οι συμμετέχοντες επισκέπτονταν τον οικολογικά έγκυρο χώρο για περίπου 2 εβδομάδες (8 συνολικά συνεδρίες) όπου ακολουθούσαν ένα πρωτόκολλο δραστηριοτήτων, που περιελάμβαναν ορισμένα διακριτικά σενάρια παρακολούθησης, χωρίς αυστηρούς περιορισμούς επιτρέποντας την συλλογή ρεαλιστικών συμπεριφορικών δεδομένων από το δίκτυο αισθητήρων. Στην συνέχεια τα δεδομένα που συλλλέχθηκαν από 10 συμμετέχοντες αναλύθηκαν στατιστικά με τα κλινικά τεστ αξιολόγησης της υγείας των ηλικιωμένων ώστε να βρεθούν τυχόν συσχετίσεις και δείκτες υγείας. Μετά το πέρας αυτών των πιλοτικών δοκιμών πραγματοποιήθηκε μια σειρά από ομάδες εστιασμένης συζήτησης όπου πραγματοποιήθηκε κουβέντα γύρω από την παρεμβατικότητα της τεχνολογίας με την υιοθέτηση ενός θεωρητικού πλαισίου ορισμού από την βιβλιογραφία. Στο δεύτερο μέρος της μελέτης, μελετήθηκαν τα αποτελέσματα τόσο από την ποιοτική ανάλυση των ομάδων εστίασης, όσο και από τα μελή της ερευνητικής ομάδας και πραγματοποιήθηκε η εγκατάσταση ενός ελαφρώς τροποποιημένου τεχνολογικού συστήματος σε 5 σπίτια ηλικιωμένων για ένα διάστημα πλέον του ενός έτους, συλλέγοντας με αυτόν τον τρόπο εκατοντάδες χιλιάδες ψηφιακά στιγμιότυπα της καθημερινότητας των ηλικιωμένων. Στην συνέχεια επιχειρήθηκε η ανάπτυξη τόσο στατιστικών μοντέλων, όσο και μοντέλων μηχανικής μάθησης για την διερεύνηση της κλινικής αξίας των ψηφιακών βιοδεικτών. Σαν μελέτη εφαρμογής αυτών των μοντέλων ήταν η πρόβλεψη συναισθηματικών διαταραχών καθώς και της καταθλιπτικής συμπτωματολογίας. Παράλληλα με αυτές τις μελέτες, διενεργήθηκε και μια μακροπρόθεσμη (Longitudinal) μελέτη της αποδοχής των αισθητήρων από τους ηλικιωμένους με έμφαση και πάλι στον βαθμό παρεμβατικότητας. Η μελέτη διενεργήθηκε με την μορφή προσωπικών συνεντεύξεων με κάθε μία από τις πέντε συμμετέχουσες. Τέλος, με την πρόταση και δημιουργία ενός εμπειρικού μοντέλου (expert model) υποστήριξης της διάγνωσης της γηριατρικής κατάθλιψης (Fuzzy Cognitive Maps), αποκρυσταλλώνοντας την γνώση των ειδικών σε ένα σχήμα αναπαράστασης γνώσης προσιτό σε αυτούς, επιχειρείται η αντιστοίχηση της νέας γνώσης που παράγεται από τους αισθητήρες με την υπάρχουσα κλινική γνώση. Αποτελέσματα: Όσον αφορά το πρώτο σκέλος της διατριβής βρέθηκαν αρκετές στατιστικά σημαντικές συσχετίσεις ανάμεσα στις επιμέρους κατηγορίες ψηφιακών δεικτών, όπως κινητικοί, συναισθηματικοί και φυσιολογικοί και των επιμέρους κατηγοριών κλινικών αξιολογήσεων της υγείας των ηλικιωμένων. Όσον αφορά τα αποτελέσματα από τις ομάδες εστιασμένης συζήτησης αυτά αναλύθηκαν με ποιοτικές μεθόδους για να εξαχθούν τα πιο βασικά θέματα που προέκυψαν από τις συζητήσεις και να κατηγοριοποιηθούν σε κάθε ένα από τους άξονες του πλαισίου παρεμβατικότητας (obtrusiveness framework). Για το δεύτερο σκέλος της διατριβής, δημιουργήθηκε ένα γραμμικό μοντέλο πρόβλεψης του PHQ-9 σκορ από τα μοτίβα λειτουργίας της τηλεόρασης. Εκεί βρέθηκαν συσχετίσεις με κάποια από τα συμπτώματα της κατάθλιψης. Το μοντέλο μηχανικής μάθησης και συγκεκριμένα, Τυχαία Δάση (Random Forests, RF) πέτυχαν με ακρίβεια άνω του 80% να διακρίνουν μεταξύ καταθλιπτικών και υγιών καθημερινών στιγμυοτύπων συμπεριφοράς, ενώ το μοντέλο ταξινόμησης καθημερινών προτύπων συμπεριφοράς σε ήπια, μέτρια και σοβαρά καταθλιπτικά περιστατικά πέτυχε εμβαδόν επιφάνειας ROC >90%. Επίσης, η αρχική αξιολόγηση του μοντέλου υποστήριξης της διάγνωσης της κατάθλιψης, με βάση το σχήμα αναπαράστασης γνώσης FCM είχε μέση ακρίβεια περίπου 96%. Τέλος, η σύγκριση των μοτίβων αντίληψης των ηλικιωμένων όσον αφορά την παρεμβατικότητα των τεχνολογιών που είχαν εγκατεστημένες στο σπίτι τους, απεκάλυψε την αρνητική τους στάση απέναντι στο έξυπνο ρολόι και τον καθρέφτη-κάμερα. Επίσης, καθώς μία από τις 5 κυρίες απεχώρησε οικιοθελώς από την μελέτη, συγκρίθηκαν οι απαντήσεις της με τον μέσο όρο των υπολοίπων ηλικιωμένων γυναικών. ώστε να αποκαλυφθούν οι αιτίες της αποχώρησης από την μελέτη. Βασικοί λόγοι αποδείχθηκαν το έξυπνο ρολόι, ο καθρέφτης με την ενσωματωμένη κάμερα αλλά και η συσκευή Kinect λόγω και της οποίας αναγκάστηκε να αλλάξει την καθημερινή ρουτίνα της. Συμπεράσματα: Η κλινική αξία των ψηφιακών βιοδεικτών έχει αναδειχθεί σε πλήθος δημοσιεύσεων, όμως η χρήση και αξιολόγησή τους σε μακροπρόθεσμες μελέτες απουσιάζει. Μεγάλη πρόκληση αποτελεί στις μέρες μας η εύρωστη λειτουργία τέτοιων τεχνολογιών υπό πραγματικές συνθήκες χωρίς πριορισμούς (in the wild) αλλά και η αποδοχή τους τόσο από τους ηλικιωμένους, όσο και από τους γιατρούς αν κάποια στιγμή στο μέλλον θελήσουμε να χρησιμοποιήσουμε τέτοιου είδους δεδομένα στην κλινική πράξη",health,831,not included
,to_check,core,,2020-01-01 00:00:00,core,'now publishers',,"Climate econometrics is a new sub-discipline that has grown rapidly over the last few years. As greenhouse gas emissions like carbon dioxide (CO2), nitrous oxide (N2O) and methane (CH4) are a major cause of climate change, and are generated by human activity, it is not surprising that the tool set designed to empirically investigate economic outcomes should be applicable to studying many empirical aspects of climate change.

Economic and climate time series exhibit many commonalities. Both data are subject to non-stationarities in the form of evolving stochastic trends and sudden distributional shifts. Consequently, the well-developed machinery for modeling economic time series can be fruitfully applied to climate data. In both disciplines, we have imperfect and incomplete knowledge of the processes actually generating the data. As we don’t know that data generating process (DGP), we must search for what we hope is a close approximation to it. The data modeling approach adopted at Climate Econometrics (http://www.climateeconometrics.org/) is based on a model selection methodology that has excellent properties for locating an unknown DGP nested within a large set of possible explanations, including dynamics, outliers, shifts, and non-linearities. The software we use is a variant of machine learning which implements multi-path block searches commencing from very general specifications to discover a well-specified and undominated model of the processes under analysis. To do so requires implementing indicator saturation estimators designed to match the problem faced, such as impulse indicators for outliers, step indicators for location shifts, trend indicators for trend breaks, multiplicative indicators for parameter changes, and indicators specifically designed for more complex phenomena that have a common reaction ‘shape’ like the impacts of volcanic eruptions on temperature reconstructions. We also use combinations of these, inevitably entailing settings with more candidate variables than observations.

Having described these econometric tools, we take a brief excursion into climate science to provide the background to the later applications. By noting the Earth’s available atmosphere and water resources, we establish that humanity really can alter the climate, and is doing so in myriad ways. Then we relate past climate changes to the ‘great extinctions’ seen in the geological record. Following the Industrial Revolution in the mid-18th century, building on earlier advances in scientific, technological and medical knowledge, real income levels per capita have risen dramatically globally, many killer diseases have been tamed, and human longevity has approximately doubled. However, such beneficial developments have led to a global explosion in anthropogenic emissions of greenhouse gases. These are also subject to many relatively sudden shifts from major wars, crises, resource discoveries, technology and policy interventions. Consequently, stochastic trends, large shifts and numerous outliers must all be handled in practice to develop viable empirical models of climate phenomena. Additional advantages of our econometric methods for doing so are detecting the impacts of important policy interventions as well as improved forecasts. The econometric approach we outline can handle all these jointly, which is essential to accurately characterize non-stationary observational data. Few approaches in either climate or economic modeling consider all such effects jointly, but a failure to do so leads to mis-specified models and hence incorrect theory evaluation and policy analyses. We discuss the hazards of modeling wide-sense non-stationary data (namely data not just with stochastic trends but also distributional shifts), which also serves to describe our notation.

The application of the methods is illustrated by two detailed modeling exercises. The first investigates the causal role of CO2 in Ice Ages, where a simultaneous-equations system is developed to characterize land ice volume, temperature and atmospheric CO2 levels as non-linear functions of measures of the Earth’s orbital path round the Sun. The second turns to analyze the United Kingdom’s highly non-stationary annual CO2 emissions over the last 150 years, walking through all the key modeling stages. As the first country into the Industrial Revolution, the UK is one of the first countries out, with per capita annual CO2 emissions now below 1860’s levels when our data series begin, a reduction achieved with little aggregate cost. However, very large decreases in all greenhouse gas emissions are still required to meet the UK’s 2050 target set by its Climate Change Act in 2008 of an 80% reduction from 1970 levels, since reduced to a net zero target by that date, as required globally to stabilize temperatures. The rapidly decreasing costs of renewable energy technologies offer hope of further rapid emission reductions in that area, illustrated by a dynamic scenario analysis",health,832,not included
,to_check,core,,2020-08-28 00:00:00,core,'victoria university of wellington library',,"Robots are entering our daily lives from self-driving cars to health-care robots. Historically, pre-programmed robots were vulnerable to changing conditions in daily life, primarily because of a lack of ability to generate novel, non-preset flexible solutions. Thus there is a need for robotics to incorporate adaptation, which is a trait of higher order natural species. This adaptation allows higher-order natural species to change their behaviours and internal mechanisms based on experience with often dynamic environment. The ability to adapt emerged through evolutionary processes.
Evolutionary Robotics is an approach to create autonomous robots that are capable of automatically generating artificial behaviors and morphologies to achieve adaptation. Evolutionary robotics has the potential to automatically synthesize controllers for real autonomous robots and generate solutions to complete tasks in the uncertain real-world. Compared to the inflexibility of pre-programmed robots, evolutionary robots are able to learn flexible solutions to given tasks through evolutionary methods.

Cognitive robotics, a branch of artificial cognitive systems research, is such an attempt to create autonomous robots by applying bio-inspired methods. As the robot interacts with environment, an underlying cognitive system can learn its own solutions toward task completion. This learning-solution-from-interaction approach, also termed as a Reinforcement Learning (RL) approach, is widely applied in cognitive robotics to learn the solutions automatically. Ideally, the solutions can emerge in the cognitive system through the trial-and-error process of the RL approach without introducing human bias.

This thesis aims to develop an evolutionary cognitive architecture (system) for a robot that can learn adaptive solutions to complete tasks. Inspired by emotion theories, this work proposes Affective Computing Multilayer Cognitive Architecture (ACMCA), a universal cognitive architecture, which is able to learn diverse solutions. Extending from previous work, ACMCA has a five-layer structure, where each layer aims to achieve different components of the solutions. The position of this thesis is that introducing a novel emotion inspired multilayer architecture that produces task solutions through subsumption operations and underlying appropriate machine learning algorithms will allow a robot to complete admissible tasks.

ACMCA’s five layers are: primary reinforcer layer, secondary reinforcer layer, core affect state layer, strategy layer, and behaviour layer. This five-layer decomposition also meets the traditional decomposition of a mobile control system into functional modules (e.g. perception, modelling, planning, task execution, and motor control). Each layer contains computing nodes as functional modules that process various Stimuli, Actions, and their consequential Outcomes of the cognitive system. In this work, 17 computing nodes and their connections in ACMCA represent the solutions that a mobile robot has learned to complete navigation tasks in complex scenarios.

Inspired by the Constructive Theory 1 and the robotic subsumption system, this work proposes a contingency-based subsumption approach to construct ACMCA. This contingency is termed Stimuli-Action-Outcome Contingency (SAOC), which is extended from the Action-Outcome (AO) contingency of Construction Theory. SAOCs are represented by “if-then” rules, termed SAOC rules, which encapsulate Stimuli, Actions, and their consequential Outcomes, providing clear symbolic interpretations. That is, the symbolic meaning of a SAOC rule can be interpreted as: if the input stimulus is perceived, the output action will be advocated as a cognitive response, expecting the outcome of the action with an estimation of relevance. As low-level computing nodes encapsulate Stimulus, Actions, and Outcomes, high-level computing nodes can subsume these low-level ones through the form of SAOC rules. Therefore, the proposed ACMCA can be constructed by subsumption layers of Stimuli-Action-Outcome Contingency (SAOC) rules.

This work applies machine learning techniques to facilitate ACMCA’s real-world robotic implementation. This work selects Accuracy-based Learning Classifier Systems (XCS) algorithms as the underlying machine learning techniques that are deployed at computing nodes for the contingency-based subsumption operations. The mitosis approach of XCS and the XCS with a Combined Reward method (XCSCR) are two novel variants of XCS algorithm. They are proposed to amend two challenges that occur when the standard XCS approaches are applied for robotic applications. The mitosis approach introduces an accuracy pressure into the algorithm’s evolutionary process, improving the algorithms’ performance in robotic applications where noisy interferences exist. The XCSCR enables the policy to emerge earlier and more frequently than the existing benchmark approaches in multistep problems. Therefore, a robot with the XCSCR can handle a multistep scenario more effectively than those with the benchmarked algorithms.

This work conducts five experiments to test the capability of ACMCA and its underlying algorithms in learning solutions for robotic navigation tasks. The five experiments are conducted as follows: reflex-learning, IR-tuning, deliberation-establishing, emotion model, and combined reward assignment. As the results of the experiments, three different affective patterns have emerged in the first three experiments, an emotion model has emerged in the fourth experiments, and the fifth experiment explores ACMCA’s potential implementation in the life-long learning scenario.

These results demonstrate that ACMCA, a novel emotion inspired multilayer architecture, can produce task solutions through contingency-based subsumption operations and underlying appropriate machine learning algorithms, allowing a robot to complete admissible tasks through evolutionary processes. The contingency-based subsumption operations can establish three contingencies and one emotion model between the subsumed components by multiple RL agents which deploy the proposed mitosis approach of XCS algorithms. These three emotion patterns and emotion model can consistently improve the robot’s navigation performance with interpretable explanations. These two variants of XCS algorithms can amend shortfalls of the standard XCS approach in real-world robotic implementations. It has been demonstrated that the diverse solutions learned by ACMCA improve the navigation performance of the robot in terms of higher flexibility, reduction in continuous collisions and shorter navigation time consumption",health,833,not included
,to_check,core,https://core.ac.uk/download/297207699.pdf,2018-08-20 00:00:00,core,'portal de periodicos ufpb',,"Chronic Kidney Disease (CKD) is a public health problem that affects nearly 10% of the global adult population, with a mortality rate of 15% per year. This disease is classified in five progressive stages (1>2>3>4>5), and it is shocking to know that 70% of the patients that start Kidney Replacement Therapy (KRT), only in the last stage, do not know they were previously infected by it. It is clear that those patients in the early stages of CKD are underdiagnosed and deprived of health policies to optimize diagnosis and to warn them of the need for KRT and of the catastrophic cardiovascular outcomes that are the main cause of death in this segment. The barriers identified for kidney care were the factors related to the knowledge and attitude of physicians and patients, besides the geography. In this context, there is a need to develop well-designed strategies to guide decision-making and enhance the care for patients with CKD. In this, work, we aimed to develop an Artificial Intelligence tool called ontology, with a view to optimizing the early diagnosis of Chronic Kidney Disease in Primary Health Care. For this purpose, after acquiring knowledge about the disease domain, we drew up the global guideline for the assistance of the patient with kidney disease in the form of a Rule-Based System that was subsequently implemented manually following the model 101 for construction of ontologies in the Protégé software, an ontology editor, having as its reasoning element the Hermit, for the necessary inferences. Accordingly, the constructed ontology called ONTODRC is characterized as a Clinical Decision Support System that has been validated computationally and in real HULW cases. After validating ONTODRC, we applied it in a sample of 185 primary care physicians in the town of João Pessoa, in two moments. In the first one, we applied a questionnaire to assess the pre and post knowledge on ONTODRC; and, in the second, we measured the perception of ease and usefulness of the tool with the technology acceptance model (TAM). In order to assess the effect of the intervention in the Knowledge assessment, we used the McNemar’s Test; and, to check the reliability of the TAM constructs, we used the Cronbach’s Alpha indicator (C.A.). In the results, we noted that the constructed ontology has an approximate capacity to respond to 90% of the surveyed requirements and also has the capacity to bring knowledge to the users. In turn, these users have considered the tool as useful and easy in their daily lives. We can conclude that the ONTODRC has the capacity to optimize the early diagnosis of CKD, in order to provide human, economic and environmental benefits.NenhumaA Doença Renal Crônica (DRC) é um problema de saúde pública, que acomete cerca de 10% da população adulta mundial, com mortalidade de 15% ao ano. Classificada em cinco estágios (1>2>3>4>5) progressivos, sendo chocante saber que 70% dos pacientes que entram em Terapia Renal Substitutiva (TRS), apenas, no último estágio, desconhecem ser portador da doença, previamente. Fica claro que, aqueles pacientes nos estágios iniciais da DRC estão subdiagnosticados e carentes de políticas de saúde para otimização do diagnóstico, para prevenção da necessidade de TRS e dos desfechos cardiovasculares catastróficos que são a principal causa de óbito neste segmento. As barreiras identificadas para o cuidado renal foram fatores relacionados ao conhecimento e atitude de médicos e pacientes e a geografia. Neste contexto, há necessidade de desenvolver estratégias bem desenhadas para orientar a tomada de decisão e melhorar a prestação de cuidados aos pacientes com DRC. Neste trabalho, objetivou-se desenvolver uma ferramenta da Inteligência Artificial denominada ontologia, a fim de otimizar o diagnóstico precoce da Doença Renal Crônica, na Atenção Primária à Saúde. Para tanto, após aquisição do conhecimento sobre o domínio da doença foi modelada a diretriz mundial para a assistência do nefropata na forma de um Sistema Baseado em Regras que a seguir foi implementada manualmente seguindo o modelo 101 para construção de ontologias no software Protégé, um editor de ontologias, tendo como raciocinador o Hermit para as inferências necessárias. Sendo assim, a ontologia construída, denominada ONTODRC caracteriza-se como um Sistema de Apoio à Decisão Clínica que foi validado computacionalmente e em casos reais do HULW. Após a validação a ONTODRC foi aplicada numa amostra de 185 médicos da atenção primária do município de João Pessoa em dois momentos. No primeiro foi aplicado questionário para avaliar o conhecimento pré e pós ONTODRC e no segundo foi medido a percepção de facilidade e utilidade da ferramenta com o modelo de aceitação de tecnologia (TAM). Para avaliar o efeito da intervenção na Avaliação do conhecimento foi utilizado o Teste de McNemar e para verificar a fidedignidade dos constructos da TAM foi utilizado o indicador Alfa de Conbrach (A.C.). Nos resultados obteve-se que a ontologia construída possui capacidade aproximada de responder a 90 % dos requisitos levantados e tem capacidade de levar conhecimento aos usuários. E esses consideraram a ferramenta útil e fácil no seu dia a dia. Conclui-se que a ONTODRC tem a capacidade de otimizar o diagnóstico precoce da DRC propiciar ganhos humanos, econômicos e ambientais",health,834,not included
,to_check,core,,2018-07-24 00:00:00,core,http://arxiv.org/abs/1807.08942,,"Incremental Learning is well known machine learning approach wherein the
weights of the learned model are dynamically and gradually updated to
generalize on new unseen data without forgetting the existing knowledge.
Incremental learning proves to be time as well as resource-efficient solution
for deployment of deep learning algorithms in real world as the model can
automatically and dynamically adapt to new data as and when annotated data
becomes available. The development and deployment of Computer Aided Diagnosis
(CAD) tools in medical domain is another scenario, where incremental learning
becomes very crucial as collection and annotation of a comprehensive dataset
spanning over multiple pathologies and imaging machines might take years.
However, not much has so far been explored in this direction. In the current
work, we propose a robust and efficient method for incremental learning in
medical imaging domain. Our approach makes use of Hard Example Mining technique
(which is commonly used as a solution to heavy class imbalance) to
automatically select a subset of dataset to fine-tune the existing network
weights such that it adapts to new data while retaining existing knowledge. We
develop our approach for incremental learning of our already under test model
for detecting dental caries. Further, we apply our approach to one publicly
available dataset and demonstrate that our approach reaches the accuracy of
training on entire dataset at once, while availing the benefits of incremental
learning scenario",health,835,not included
,to_check,core,,2018-12-05 00:00:00,core,https://core.ac.uk/download/215314199.pdf,,"To meet the accuracy, latency and energy efficiency requirements during real-time collection and analysis of health data, a distributed edge computing environment is the answer, combined with 5G speeds and modern computing techniques. Using the state-of-the-art machine learning based classification techniques plays a crucial role in creating the optimal healthcare system on the edge. This thesis first provides a background on the current and emerging edge computing classification techniques for healthcare applications, specifically for electrocardiogram (ECG) beat classification. We then present key findings from an extensive survey of over hundred studies on the topic while taxonomizing the literature with respect to key architectural differences, application areas and requirements.  Leveraging the insights drawn from the extensive analysis of the pertinent literature we select a set of most promising machine learning based classification techniques for ECG beats, based on their suitability for implementation on a small edge device called a Raspberry Pi. After implementing these classification techniques on a Raspberry Pi based platform we perform a comparison of the performance of these classification techniques with respect to three key performance indicators (KPI) of interest for health care applications namely accuracy, energy efficiency, and latency.
ECG measures the electrical activity of the heart and help healthcare professionals to evaluate heart conditions of a patient, sometimes diagnosing life-threatening conditions. The features of ECG signals are pre-processed and fed into the classification algorithms to detect and classify abnormal beat types.  ECG classification requires low complexity but still high level of performance in terms of aforementioned three KPIs. The classification algorithms chosen, namely Naïve Bayes, Multilayer Perceptron (MLP), and distilled deep neural network (DNN) are all energy efficient methods hence suitable for implementation for small edge devices. The comparative multi-faceted evaluation presented in this thesis is a new contribution to research that exists on edge based classification as it offers comparison of selected classification algorithms in terms three KPIs instead of one while using real edge device based implementation. The performance of analyzed machine learning classification techniques is ranked according to each KPI. Benefiting from the results of the comparative analysis presented in this thesis a particular classification algorithm can be selected for optimal deployment in given scenario in healthcare system depending on the specific requirements of the given scenario. Edge computing paves the way for a new generation of health devices that can offer a higher quality of life for users if low-latency, low-energy, and high- performance requirements are addressed",health,836,not included
,to_check,core,,2017-01-01 00:00:00,core,delft university of technology,,"Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduces operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for datacenters with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Reinforcement learning, based in this work on Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learningbased portfolio scheduler can perform better (5–20%) and cost less (20–35%) than the considered alternatives.Distributed System",health,837,not included
,to_check,core,,2013-07-23 00:00:00,core,mobile code distributed systems a new development,,"Abstract: The paper presents an introduction in the Mobile Agents Systems and describes how this technology can be used in wireless applications. Also it is shown the possibility of securing wireless applications that use mobile agents and distributed computing. Wireless networks are a relatively new technology in the LAN market. With the weak encryption and security defined in the IEEE standards, wireless LANs, when improperly deployed or administered, can provide a significant risk to those economic sectors. These sectors include health-care, government, and banking in particular. Increasingly diverse heterogeneous wireless infrastructures in combination with more narrowly defined roles of parties participating in the delivery of applications to mobile users pose new challenges for support for delivering these applications. Key-Words: mobile agents, artificial intelligence, intelligent agents, wireless applications. 1. Mobile and intelligent agents Mobile software agents are a new concept used in distributed systems and this concept is based on human agents idea – real estate agent, travel agent. Figure 1 presents a new vision about intelligent agents",health,838,not included
,to_check,core,,2013-01-01 00:00:00,core,research of gold labeled immunoassay technology based on the image sensor,,"免疫测定是基础研究和临床检测中广泛使用的一种技术，遍及医学检测的各个领域。随着免疫测定技术的不断发展，许多新技术取代了传统的检测方法，金标免疫层析检测法，亦称金标免疫层析试纸条检测便是其中之一。金标免疫层析试纸条检测具有诸多优点，包括检测速度快、操作简便、无污染、试剂稳定等，已受到临床检验界的极大关注，并发展成为临床检验的前沿领域。      POCT (Point of Care Testing)以其方便、快速、高效而受到了人们的青睐，近年在国内外都得到了快速发展。随着中国医改的实施以及防病健康知识的提高，今后POCT在健康保健、疾病预防监控中的需求量更多，发挥的作用更大。      但是，目前国内外金标免疫层析试纸条(简称：金标试纸条)定量检测的POCT仪器的性能并不理想，并且国外进口仪器价格较高，使该类型仪器在国内的应用受到了限制。在已经出现的金标试纸条检测POCT仪器中，其测量方式主要分为扫描测量和成像测量。对于采用扫描测量方式对金标试纸条进行检测的系统，其机械结构复杂，测量时间相对较长，且易受外界干扰。成像测量较扫描测量，具有快速、稳定等优点。但是对于采用成像测量方式的检测仪器，我们尚未看到对于照明和图像质量进行相关研究的检测仪器，而恰恰这两个方面会直接影响到仪器的检测性能。因此，研究并发展具有中国自主知识产权的高性能金标试纸条定量检测的POCT仪器的具有重要的意义与价值。      围绕着研制出一种具有高灵敏度、高重复性、快速、稳定、便携、功耗低、成本低、结构简单等优点的成像型金标试纸条定量检测POCT仪器的目标，本论文的主要内容如下：      (1) 概述了POCT的发展及应用、金标免疫层析试纸条POCT定量检测仪器的发展现状、图像传感器以及DSP技术的发展状况，为本论文工作的开展奠定了必要的基础。      (2) 深入研究了金标试纸条定量检测理论，分析了纳米金颗粒的光学特性以及胶体金免疫标记技术的基本原理。之后，着重研究了金标试纸条结构，以及与本课题有关的双抗夹心法金标免疫层析反应原理。      (3) 开展了基于图像传感器的成像型金标试纸条检测POCT仪器(简称：金标条阅读仪)的整体设计。本论文从光学模块、控制系统模块和图像数据算法模块这三个模块开展金标条阅读仪的设计。同时，根据成像型仪器的特点，提出了提升仪器性能的关键技术点，即优化设计金标试纸条上照明均匀性和图像传感器成像参数，以此来提高金标条阅读仪的检测灵敏度。最后，详细介绍了金标条阅读仪设计的主要组成：OV663图像传感器、TMS320C6747 DSP芯片和嵌入式实时操作系统DSP/BIOS。      (4) 开展了金标条阅读仪的详细设计，具体内容包括金标条阅读仪照明均匀性设计，OV6630图像传感器成像参数优化设置，仪器内部图像硬件设备操做方法和电路连接，C6747 DSP芯片内部相关模块的软件设计，定量检测的图像数据处理算法设计，以及上述软件设计的操作流程。针对仪器的成像控制参数优化设计，提出了评价图像质量的两个评价参数。在上述详细设计的基础上，研制了一台金标条阅读仪样机，并介绍了该样机的工作过程。      (5) 开展了金标条阅读仪性能评价的实验。使用所研制的金标条阅读仪对系列浓度的心肌肌钙蛋白I(cTnI)金标试纸条进行检测实验研究。仪器对高浓度cTnI金标试纸条(64 ng/ml)测量结果的变异系数仅为0.134%，对低浓度cTnI金标试纸条(0.25 ng/ml)测量结果的变异系数仅为2.790%；检测灵敏度优于0.25 ng/ml；并采用4-PL曲线对实验测量的数据进行拟合，拟合相关系数R^2优于0.99；完成一次检测时间约为1秒。实验数据表明，该金标条阅读仪具有重复性好、灵敏度高、动态范围大、响应特性好、检测速度快等优点。Immunoassy is a sensitive detection technique in the field of clinical diagnostics, especially in clinical detection and medical studies. With the development of immunoassy, traditional detection methods in clinical diagnostics have been replaced by many new techniques. One of the new techniques is the gold immunochromato- graphic assay (GICA). With the advantages of high efficiency of detection, easy of operation, pllution free and reagent stable, GICA has gained great attention in clinical diagnostics.       Because of its characteristics of convenient, fast and efficient, POCT (Point of Care Testing) has attracted much research interest and been rapid developed at home and abroad in recent years. With the implementation of China''s medical reform and the improvement of knowledge of disease prevention and health, POCT will play an increasingly important role in health care, disease prevention and monitoring in the future.       However, the performance of POCT instrument for colloidal gold lateral flow (LF) strip quantitative detection is low, and the price of imported POCT instrument is expensive. There are mainly two forms of measurements, i.e. scanning measurement and imaging measurement. For the scanning measurement, the measurement reproducibility would be affected by instrument vibration and other disturbing factors, and the measurement time is relatively long. Additionally, the scanning mechanism also complicates the instrument’s structure. As for the imaging measurement, the LF strip picture is captured by image sensor. And image processing methods are used for quantitative analysis of the LF strip. So the imaging measurement is quick, stable, and has simple mechanical structure, etc. However, the image quality of the LF strip would directly affect the detection performance of the instrument. For example, illumination uniformity on the LF strip and the control parameters of image sensor has great impact on the image quality, and thus affect the measurement results. But the reported imaging measurement-based instruments are rarely concerned about the design of illumination uniformity and the control parameters.This situiation indicates that the research and development of high performance POCT instrument for colloidal gold LF strip quantitative detection with Chinese independent intellectual property rights is of great significance and value.       Aiming at researching and designing POCT instrument based on image sensor for colloidal gold LF strip quantitative detection with properties of sensitive, reproducible, fast, stable, portable, low power consumption, low cost, simple structure, etc., the main research work in this paper is as follows:       (1) An overview of the development and application of POCT and of the development of DSP technology, as well as of the development of POCT instrument for GICA quantitative detection is introduced.       (2) The GICA test strip quantitative detection theory, the optical properties of gold nanoparticles and the basic principles of immunogold labeling technique are analysed. Then deep researches on the colloidal gold LF strip structure and on the principle of the double-antibody sandwich GICA reaction have been done.       (3) The overall composition of the colloidal gold LF strip reader based on image sensor is researched and designed. The basic design modules of colloidal gold LF strip reader are introduced in three modules, i.e. optical module, control module and image algorithm module. Meanwhile, a new way to improve the performance of colloidal gold LF strip reader by the design of illumination uniformity and the optimization of image sensor’s image control parameters is proposed. Then, the basic design resources are introduced, including OV6630 image sensor, TMS320C6747 DSP chip and the embed real time operation system DSP/BIOS.       (4) The colloidal gold LF strip reader is researched and designed, including illumination uniformity design, image control parameters settings design of OV6630, internal hardwares operating methods and circuit connections, the software operation of C6747 DSP chip internal modules and the image data processing algorithms of quantitative detection. Two evaluation parameters are defined to evaluate the image for the optimization of image control parameters. At last, a prototype of colloidal gold LF strip reader has been developed on the basis of the detailed design, and the working process of this reader is introduced.       (5) Colloidal gold LF strips of Cardiac troponin I (cTnI) are tested by the colloidal gold LF strip reader. The coefficients of variation of 20 times repetitive experiments with 0.25 ng/ml and 64 ng/ml are respectively 2.790% and 0.134%. The functional sensitivity is better than 0.25 ng/ml. The test results are fitted by 4-PL curve model, whose R^2 is more than 0.99, and the single measurement time is less than 1 sencond. It proofs that the design of colloidal gold LF strip reader is of properties of high sensitivity, wide dynamic range, good response property and reproducibility",health,839,not included
,to_check,core,Sociedade Brasileira de Coloproctologia,2014-01-01 00:00:00,core,prototype of a computer system for managing data and video colonoscopy exams,,"OBJECTIVE: Develop a prototype using computer resources to optimize the management process of clinical information and video colonoscopy exams. MATERIALS AND METHODS: Through meetings with medical and computer experts, the following requirements were defined: management of information about medical professionals, patients and exams; video and image captured by video colonoscopes during the exam, and the availability of these videos and images on the Web for further analysis. The technologies used were Java, Flex, JBoss, Red5, JBoss SEAM, MySQL and Flamingo. RESULTS AND DISCUSSION: The prototype contributed to the area of colonocospy by providing resources to maintain the patients' history, tests and images from video colonoscopies. The web-based application allows greater flexibility to physicians and specialists. The resources for remote analysis of data and tests can help doctors and patients in the examination and diagnosis. CONCLUSION: The implemented prototype has contributed to improve colonoscopy-related processes. Future activities include the prototype deployment in the Service of Coloproctology and the utilization of this model to allow real-time monitoring of these exams and knowledge extraction from such structured database using artificial intelligence.OBJETIVO: Desenvolver um protótipo por meio de recursos computacionais para a otimização de processos de gerenciamento de informações clínicas e de exames de videocolonoscopia. MATERIAIS E MÉTODOS: Por meio de reuniões com especialistas médicos e computacionais, definiram-se os seguintes requisitos: gestão de informações sobre profissionais médicos, pacientes e exames complementares; aquisição dos vídeos e captura de imagens a partir do videocolonoscópio durante a realização desse exame, e a disponibilidade por meio da Web para análise posterior dessas imagens. As tecnologias aplicadas foram: Java, Flex, JBOSS, Red5, JBOSS SEAM, MySQL e Flamingo. RESULTADOS E DISCUSSÃO: O protótipo contribuiu para a área de colonocospia disponibilizando recursos para manutenção de histórico de pacientes, exames e imagens. O acesso à aplicação, por meio de browser, permite maior flexibilidade aos médicos e especialistas. Os recursos para análise remota de dados e exames podem auxiliar médicos e pacientes na realização de exames e diagnósticos. CONCLUSÃO: O protótipo implementado contribuiu para melhoria de processos relacionados a exames de videocolonoscopia. Trabalhos futuros incluem implantação do protótipo no serviço de coloproctologia, bem como a extensão do modelo para o acompanhamento dos exames em tempo real e extração de conhecimento dessa base de dados estruturada por meio de inteligência artificial",health,840,not included
,to_check,core,Software and System Health Management for Autonomous Robotics Missions,2012-09-04 00:00:00,core,10.1184/r1/6710654.v1,,"Advanced autonomous robotics space missions rely heavily on the flawless interaction of complex hardware, multiple sensors, and a mission-critical software system.  This software system consists of an operating system, device drivers, controllers, and executives; recently highly complex AI-based autonomy software have also been introduced. Prior to launch, this software has to undergo rigorous verification and validation (V&V).  Nevertheless, dormant software bugs, failing sensors, unexpected hardware-software interactions, and unanticipated environmental conditions—likely on a space exploration mission—can cause major software faults that can endanger the entire mission.

Our Integrated Software Health Management (ISWHM) system continuously monitors the hardware sensors and the software in real-time. The ISWHM uses Bayesian networks, compiled to arithmetic circuits, to model software and hardware interactions. Advanced reasoning algorithms using arithmetic circuits not only enable the ISWHM to handle large, hierarchical models that are necessary in the realm of complex autonomous systems, but also enable efficient execution on small embedded processors. The latter capability is of extreme importance for small (mobile) autonomous units with limited computational power and low telemetry bandwidth.  In this paper, we discuss the requirements of ISWHM.  As our initial demonstration platform, we use a primitive Lego rover. A Lego 
Mindstorms microcontroller is used to implement a highly simplified autonomous rover driving system, running on the OSEK real-time operating system. We demonstrate that our ISWHM, running on this small embedded microcontroller, can perform fault detection as well as on-board reasoning for advanced diagnosis and root-cause detection in real time",health,842,included
,to_check,core,A biophysically-based neuromorphic model of spike rate- and timing-dependent plasticity,2011-05-01 00:00:00,core,10.1073/pnas.1106161108,"[{'title': 'Proceedings of the National Academy of Sciences', 'identifiers': ['0027-8424', 'issn:1091-6490', 'issn:0027-8424', '1091-6490']}]","Current advances in neuromorphic engineering have made it possible to emulate complex neuronal ion channel and intracellular ionic dynamics in real time using highly compact and power-efficient complementary metal-oxide-semiconductor (CMOS) analog very-large-scale-integrated circuit technology. Recently, there has been growing interest in the neuromorphic emulation of the spike-timing-dependent plasticity (STDP) Hebbian learning rule by phenomenological modeling using CMOS, memristor or other analog devices. Here, we propose a CMOS circuit implementation of a biophysically grounded neuromorphic (iono-neuromorphic) model of synaptic plasticity that is capable of capturing both the spike rate-dependent plasticity (SRDP, of the Bienenstock-Cooper-Munro or BCM type) and STDP rules. The iono-neuromorphic model reproduces bidirectional synaptic changes with NMDA receptor-dependent and intracellular calcium-mediated long-term potentiation or long-term depression assuming retrograde endocannabinoid signaling as a second coincidence detector. Changes in excitatory or inhibitory synaptic weights are registered and stored in a nonvolatile and compact digital format analogous to the discrete insertion and removal of AMPA or GABA receptor channels. The versatile Hebbian synapse device is applicable to a variety of neuroprosthesis, brain-machine interface, neurorobotics, neuromimetic computation, machine learning, and neural-inspired adaptive control problems.National Institutes of Health (U.S.) (Grant Number EB005460)National Institutes of Health (U.S.) (Grant Number RR028241)National Institutes of Health (U.S.) (Grant Number HL067966",health,843,not included
,to_check,core,Prototype of a computer system for managing data and video colonoscopy exams,2012-03-01 00:00:00,core,10.1590/s2237-93632012000100007,"[{'title': None, 'identifiers': ['issn:2237-9363', '2237-9363']}]","OBJECTIVE: Develop a prototype using computer resources to optimize the management process of clinical information and video colonoscopy exams. MATERIALS AND METHODS: Through meetings with medical and computer experts, the following requirements were defined: management of information about medical professionals, patients and exams; video and image captured by video colonoscopes during the exam, and the availability of these videos and images on the Web for further analysis. The technologies used were Java, Flex, JBoss, Red5, JBoss SEAM, MySQL and Flamingo. RESULTS AND DISCUSSION: The prototype contributed to the area of colonocospy by providing resources to maintain the patients' history, tests and images from video colonoscopies. The web-based application allows greater flexibility to physicians and specialists. The resources for remote analysis of data and tests can help doctors and patients in the examination and diagnosis. CONCLUSION: The implemented prototype has contributed to improve colonoscopy-related processes. Future activities include the prototype deployment in the Service of Coloproctology and the utilization of this model to allow real-time monitoring of these exams and knowledge extraction from such structured database using artificial intelligence.<br>OBJETIVO: Desenvolver um protótipo por meio de recursos computacionais para a otimização de processos de gerenciamento de informações clínicas e de exames de videocolonoscopia. MATERIAIS E MÉTODOS: Por meio de reuniões com especialistas médicos e computacionais, definiram-se os seguintes requisitos: gestão de informações sobre profissionais médicos, pacientes e exames complementares; aquisição dos vídeos e captura de imagens a partir do videocolonoscópio durante a realização desse exame, e a disponibilidade por meio da Web para análise posterior dessas imagens. As tecnologias aplicadas foram: Java, Flex, JBOSS, Red5, JBOSS SEAM, MySQL e Flamingo. RESULTADOS E DISCUSSÃO: O protótipo contribuiu para a área de colonocospia disponibilizando recursos para manutenção de histórico de pacientes, exames e imagens. O acesso à aplicação, por meio de browser, permite maior flexibilidade aos médicos e especialistas. Os recursos para análise remota de dados e exames podem auxiliar médicos e pacientes na realização de exames e diagnósticos. CONCLUSÃO: O protótipo implementado contribuiu para melhoria de processos relacionados a exames de videocolonoscopia. Trabalhos futuros incluem implantação do protótipo no serviço de coloproctologia, bem como a extensão do modelo para o acompanhamento dos exames em tempo real e extração de conhecimento dessa base de dados estruturada por meio de inteligência artificial",health,844,not included
,to_check,core,https://core.ac.uk/download/299857058.pdf,2009-07-01 00:00:00,core,universitas telkom,,"ABSTRAKSI: Studi populasi yang dilakukan Organisasi Kesehatan Dunia (WHO) tahun 2000 menyebutkan, Indonesia berada pada posisi keempat negara dengan penderita diabetes terbesar, dengan jumlah penderita sekitar 8,4 juta orang pada tahun 2000. Sampai saat ini, para spesialis diabetes dan organisasi yang bergerak di bidang ini tengah gencar melakukan sosialisasi penanganan diabetes baik secara langsung maupun lewat dunia maya. Penyebab diabetes secara umum dipengaruhi oleh gaya hidup dan kualitas kesehatan yang kurang seimbang dalam kehidupan. Seiring dengan perkembangan teknologi dan pertimbangan aktivitas seseorang yang berbeda, maka sosialisasi dan terapi diabetes secara digital layak untuk dipertimbangkan. Dalam artian, bahwa penggunaan program komputer akan mempunyai keefektifan di dalam membantu sosialisasi penanganan diabetes. Program aplikasi deteksi dini penyakit diabetes yang diimplementasikan dalam tugas akhir ini merupakan salah satu program komputer sebagai alat bantu sekunder dalam bidang kedokteran, yang mendeteksi kemungkinan seseorang terkena penyakit diabetes. Proses kerja program adalah melakukan pengumpulan data dari user kemudian memprosesnya sesuai dengan urutan rules (aturan). Program ini diimplementasikan dengan bahasa pemrograman Ms. Visual Basic dan database Ms. Access menggunakan metode forward chaining sistem pakar dalam lingkup Artificial Intelligence (kecerdasan buatan). Sebagai keluarannya, sistem akan memberikan hasil apakah seseorang menderita penyakit diabetes berikut tipe diabetesnya, diagnosa, dan rekomendasi atau dalam keadaan normal. Secara teknis, program ini berupa prototype, yang berarti perlu pengembangan lebih lanjut untuk menyempurnakannya. Hasil diagnosa disesuaikan dengan standar penanganan kedokteran. Demikian pula dengan rekomendasi yang diberikan. Program ini diujicobakan ke responden dengan masukan yang berbeda. Dalam uji responden, didapatkan hasil bahwa keluaran hasil diagnosa program memiliki akurasi sekitar 96,67 %. Hal ini membuktikan bahwa algoritma forward chaining yang diterapkan di software ini mampu menghasilkan keputusan akhir yang hampir setara dengan diagnosa seorang dokter.Kata Kunci : diabetes, sistem pakar, diagnosa, rekomendasi, forward chainingABSTRACT: Population study held by World Health Organization in 2000 stated that Indonesia is in 4th rank country which has larger diabetes sufferer, with about 8.4 millions people. Many diabetes expertians and organization work on this field still do some socializations in the real world or cyber. Diabetes is generally caused by life style and imbalancing of health quality in human life. As technology growing rapidly and different personal activities, digitally diabetes socialization and therapy should be considered. Means that, computer program using will be more effective for assisting diabetes threatment. The diabetes predetecting program implemented on this final project is one of computer program which is used to be the secondary tool on medical field, it detects someone’s possibility of diabetes risk. Program works by gathering data from user then process it based on the rules base. This program was implemented using Ms. Visual Basic programming language and Ms. Access database, with forward chaining method on Artficial Intelligence scope. As the output, system will give result whether someone suffer from diabetes or not, with its diabetes type, diagnose, and recommendation. Technically, this program is still in its prototype, means that it needs further development. Diagnose result will be synchronized with standard medical threatment. So as the given recommendation. The program was tested to respondences with different input. In respondence testing, found that the program diagnose result has an accuracy value about 96,67 %. This means that forward chaining algorithm implemented on the software could result final decision which almost same as the doctor diagnose.Keyword: diabetes, expert system, diagnose, recommendation, forward chainin",health,845,not included
,to_check,core,,2010-10-16 00:00:00,core,university of verona,,"Magnetic resonance imaging (MRI) is increasingly being used in medical settings because of its ability to produce, non-invasively, high quality images of the inside of the human body. Since its introduction in early 70’s, more and more complex acquisition techniques have been proposed, raising MRI to be exploited in a wide spectrum of applications. Innovative MRI modalities, such as diffu- sion and functional imaging, require complex analysis techniques and advanced algorithms in order to extract useful information from the acquired data. The aim of the present work has been to develop and optimize state-of-the- art techniques to be applied in the analysis of MRI data both in experimental and clinical settings. During my doctoral program I have been actively involved in several research projects, each time facing many different issues. In this dissertation, however, I will report the results obtained in three most appealing projects I partecipated to. These projects were devoted (i) to the implementation of an innovative experimental protocol for functional MRI in laboratory animals, (ii) to the development of new methods for the analysis of Dynamic Contrast Enhanced MRI data in experimental tumour models and (iii) to the analysis of diffusion MRI data in stroke patients. Particular emphasis will be given to the technical aspects regarding the algorithms and processing methods used in the analysis of data. Apart from a brief introduction on magnetic resonance imaging principles, the dissertation is organised in three parts, each one of them covering a separate topic dealt with during my doctoral studies. Each chapter is self-contained, in that it gives an introduction about the issue faced in the study, describes the methods and the techniques employed, and concludes with a discussion about the results obtained. Chapter 3 illustrates the methodology proposed by our group for an innovative method of fMRI (Activation-Induced Manganese Enhanced MRI, AIM-MRI) in rats. The method is based on in-vivo, absolute quantification of Mn concentration in different brain regions performed by fast T1 mapping and coregistration to a rat brain atlas. This strategy allows to quantify the accumulation of Mn in different regions of the rat brain, which is strictly related to the ac- tivation status of each area. This mechanism is similar to what happens in classical functional MRI studies but, in addition, it might be used for functional experiments performed in awake animals. In chapter 4 we cover the topic of tissues classification from MRI images in some experimental tumour models and present some advanced image-processing techniques we introduced for the analysis of Dynamic Contrast-Enhanced MRI (DCEMRI) data. In particular, we proposed to estimate a set of characteristic features from DCEMRI time profiles which well describe their shape, and then to extract peculiar behaviours which discriminate each tissue inside the tumour by combining cluster analysis and machine learning techniques. We have tested several approaches, each time comparing them with the state-of-the-art techniques of analysis for this kind of data. Finally, the proposed approach has been validated in a real case application in order to assess the efficacy of an anti-cancer therapy. Chapter 5 addresses the study on post-stroke plasticity carried out in collab- oration with the Signal Processing Laboratory of the Swiss Federal Institute of Technology in Lausanne (EPFL). Brain structural connectivity was monitored with Diffusion Spectrum Imaging (DSI), i.e. a high angular resolution diffusion MRI technique, during the functional recovery from the stroke, at hyperacute, acute and chronic stages after the lesion onset. The reproducibility of extracted fibre tracts has been extensively studied on healthy subjects, and the parameter set of fiber-tracking algorithm giving the best results was then used in patient analysis. The extracted fibre bundles between each pair of cortical regions were characterized by means of several connectivity measures",health,846,not included
,to_check,core,'Pisa University Press',2051-09-28 00:00:00,core,sviluppo di un modello di ottimizzazione di un crm nell'ambito della cantieristica navale,,"Questo lavoro di tesi è il risultato di un periodo di stage svolto presso Navicelli S.p.A di Pisa, finalizzato allo sviluppo di un modello di ottimizzazione di un sistema gestionale CRM esistente di supporto al settore della cantieristica nautica. L’attività si colloca nell’ambito del progetto Mistral, che prevede la realizzazione di una piattaforma tecnologica collaborativa web based ed open source rivolta alla Nautica da Diporto dell'Alto Mediterraneo. L'obiettivo è quello di offrire ai cantieri una soluzione software che permetta loro di migliorare il processo di gestione delle relazioni con i loro clienti reali e potenziali. Con la collaborazione del cantiere Società Navale Pisa il lavoro svolto si è articolato principalmente in tre fasi: analisi della filiera nautica e rappresentazione dei processi riguardanti la gestione di una commessa navale semi-custom; analisi dei bisogni informativi del cantiere e studio delle problematiche legate allo strumento di gestione reso disponibile da Navicelli S.p.A nei confronti delle aziende sul canale dei Navicelli; traduzione dei bisogni individuati in requisiti funzionali attraverso lo sviluppo di un modello concettuale di reingegnerizzazione del sistema creato utilizzando lo standard internazionale UML. Le specifiche elaborate con UML saranno consegnate ad una softwarehouse per l'implementazione delle modifiche attuate al CRM.
This thesis is the result of an internship at the Navicelli S.p.A company in Pisa, aimed at developing an optimization model of a CRM (Customer Relationship Management) management system which exists in order to support the shipbuilding industry. The activity is part of the Mistral project that involves the realization of a web based and open source collaborative technological platform directed towards Recreational Boating of the North Mediterranean. The goal is to offer the shipyards a software solution allowing them to improve the relationship management process with their real and potential customers. In collaboration with the Pisa Naval Society the work performed was mainly divided into three phases: shipyard's organizational structure analysis and processes representation regarding the management of a semi-custom naval job order, individuation of the shipyard's informative needs and study of the problematics tied to the management tool made available by Navicelli S.p.A for the companies on the Navicelli canal; translation of the needs identified as functional requirements through the development of a conceptual model of system reengineering created by utilizing the international standard UML (Unified Modeling Language). The specifications elaborated through UML will be delivered to a softwarehouse for the implementation of the modifications effectuated to the CRM",industry,847,not included
,to_check,core,HAL CCSD,2021-07-11 00:00:00,core,some insights about the use of machine learning for solving vrp,,"International audienceHybrid optimisation methods using machine learning tools are a hot topic in combinatorial optimization.The AI promise is to learn from past solutions or in real time what are the markers of good solutions and then to guide the resolution of the problem.However, for many hard problems such as VRP, a wide range of powerful solvers have already proven their efficiency whereas the most recent successes in machine learning (e.g deep learning) need a huge amount of training data before reaching a satisfactory performance level.In this paper, we study the efficiency of a features-guided multiple-neighborhood search (FG-MNS) for realistic instances of HFVRP.The solver is based on two steps.In the learning step, a powerful solver (RADOS) is used to generate a set of solutions, which are characterized by a set of features described in previous work (Lucas et al., 2019, Lucas et. al, 2020).Then, a decision tree is built on this training set to determine where are the promising areas in the features spaces.In a second step, called exploitation step, the solver uses some rules extracted from the decision tree to guide the solution toward a promising area.We present a wide range of experimentations with different variants of the FG-MNS solver (offline, online).While the results are promising for a better understanding of what makes a good solution, the real benefits of machine learning in an industrial implementation are questionable and discussed in this work",industry,848,not included
,to_check,core,'Pisa University Press',2023-05-11 00:00:00,core,informatizzazione dei sistemi di gestione della sicurezza alimentare: valutazione ed applicabilità in un sito di produzione pasti per catering aereo.,,"IL mio elaborato pone le sue basi sull'innovazione tecnologica, che viene definita come quell'insieme di attività che le imprese e le istituzioni adottano al fine di introdurre nuovi prodotti e servizi, per renderli disponibili e fruibili ai consumatori. L’innovazione tecnologica ha influenzato e condizionato le varie epoche, difatti, nel 1784 , con la prima rivoluzione industriale, si è assistito all'invenzione del telaio e del motore a vapore, per poi arrivare al 1890 dove l’invenzione dell’energia elettrica è stata sicuramente una delle scoperte più importanti, fino al 1970 in cui si è assistito al sopravvento dell’automazione e quindi alla costruzione dei primi computer. Arriviamo poi ai primi anni del 2000, dove in occasione della fiera di Hannover, per descrivere l’avvento della quarta rivoluzione industriale, fu coniato il termine “industria 4.0”, ad evidenziare il passaggio da un sistema basato unicamente sulle interazioni fisiche tra uomo e macchina ad uno in cui l’interazione tra dati e macchine costituisce la maggior parte delle operazioni produttive. Nel settore food, sia le ricerche che le innovazioni tecnologiche sono state condotte non solo da enti di ricerca ed università ma anche dalle start-up, al fine di ottenere miglioramenti tecnologici e prodotti finora sconosciuti.
Il termine start-up (dall'inglese “inizio”), indica l’avvio di una nuova impresa con tutto ciò che ne comporta sia dal punto di vista strutturale che organizzativo e quest’elaborato infatti, si basa sull'utilizzo del software SCM (Safeaty Content Management), nato dallo Spin-Off della Università di Pisa, ed applicato ad un servizio di catering aereo al fine di integrarsi nella gestione delle fasi che caratterizzano l’azienda stessa evidenziandone i vantaggi e le potenzialità.
La missione principale del software è quella di aiutare le aziende alimentari a redigere un piano di autocontrollo basato sul principio dell’ HACCP, senza incappare in errori o dimenticanze, il tutto a portata di un click. Il sistema HACCP sta alla base della piramide della sicurezza alimentare, ma prima ancora incontriamo il regolamento 178/02 e il pacchetto igiene. Il regolamento 178/02, costituisce la fonte primaria del moderno diritto alimentare europeo e racchiude in un unico testo i diritti ed i doveri di tutti gli operatori del settore alimentare, quindi sia quelli attribuiti agli OSA che quelli designati alle Autorità Competenti che effettuano i controlli in materia. Stabilisce inoltre i principi e i requisiti generali della legislazione alimentare segnando il passaggio da una moltitudine di normative nazionali, in attuazione delle direttive comunitarie ad un insieme razionale di regole comuni testualmente applicate nell'intero Mercato Unico. Con l’avvento di questa normativa poi, si è attribuita una responsabilità primaria del raggiungimento degli obiettivi, agli operatori del settore alimentare; il compito di verifica invece, alle autorità di controllo. Si arriva poi al pacchetto igiene, un insieme di quattro testi legislativi, pubblicati per la prima volta nel 2004 ma entrati in vigore il 1° gennaio del 2006, e nominati: Regolamento 852/04, Regolamento 853/04, Regolamento 854/04 e regolamento 882/04 (gli ultimi due regolamenti sono stati abrogati e sostituiti dal Regolamento 625/2017).
I primi due regolamenti, sono indirizzati alle responsabilità che gli OSA devono ottemperare per il raggiungimento degli obiettivi di sicurezza; gli ultimi, invece, sono destinati alle autorità di controllo. Arriviamo così all’HACCP, acronimo di “Hazard Analysis and Critical Control Points”, ovvero “Analisi del pericolo e punti critici di controllo” che costituisce un valido sistema, organizzato e razionale oltre che condiviso a livello europeo, per l’applicazione dell’autocontrollo degli operatori che operano nelle industrie che trattano, manipolano o distribuiscono alimenti.
L’applicazione del sistema HACCP nasce proprio dall'esigenza di garantire la salubrità delle produzioni alimentari cambiando approccio, cioè passando da un controllo a valle del processo produttivo, che esisteva in passato ed effettuava il controllo sul prodotto finito, ad un controllo del processo produttivo in ogni sua fase, identificando quindi tutti i rischi che possono influire sulla sicurezza degli alimenti e attuando tutte le misure preventive necessarie a tenerli sotto controllo. Per cui, da un controllo di prodotto di tipo tradizionale, si è passati ad un controllo di processo, cioè il controllo di tipo moderno. In Italia è stato introdotto nel 1997, dal D.lgs 155 del ’97. Attualmente è in vigore poiché contemplato dal Regolamento CE 852 del 2004, il quale ha abrogato il precedente D.lgs 155 del ’97. Lo sviluppo del metodo HACCP prevede che debbano essere attuati e rispettati i cosiddetti “programmi prerequisito” o PRPs, cioè delle condizioni di base ed attività necessarie a mantenere un ambiente in condizioni consone per la preparazione di prodotti sani e sicuri per il consumo umano.
L’efficacia di questo metodo però si completa pienamente solo con l’ottemperanza a 5 fasi preliminari (costituzione dell’HACCP team, descrizione del prodotto, descrizione dei possibili usi, realizzazione di un diagramma di flusso, verifica in campo) e 7 principi (analisi dei pericoli, individuazione dei CCP, definizione dei limiti critici, definizione dei monitoraggi, individuazione delle azioni correttive, verifica dell’efficacia del sistema e verifica della documentazione).
Il software utilizzato per questa tesi, permette di redigere un piano di gestione ed autocontrollo aziendale, basato proprio sui dettami dell’HACCP ma il tutto a portata di un click. Nel caso specifico, lo abbiamo utilizzato per un’azienda londinese, la Niche free-from kitchen, che opera nel settore del catering aereo. Dopo una prima analisi, abbiamo riscontrato che il software, semplifica le operazioni di gestione: difatti la visione d’insieme delle produzioni aiuta l’operatore a tenere sotto controllo in tempo reale eventuali non conformità per una risposta immediata; la dematerializzazione dei documenti porta benefici in termini di precisione del lavoro, risparmio economico e riduzione di ingombro volumetrico in ufficio nonché un impatto ambientale inferiore; per ultimo, ma non meno importante, essendo la gestione totalmente digitalizzata, è senz'altro d’aiuto in un momento come quello che stiamo vivendo, in cui lo smart working ha assunto un’importanza fondamentale.
My thesis its foundations on technological innovation, which is described as the set of activities that businesses and institutions adopt at the end of new products and services, to make them available and usable to consumers. Technological innovation has influenced and conditioned the various eras, in fact, in 1784, with the first industrial revolution, we witnessed the invention of the frame and the steam engine, and then went on to 1890 where the electrical invention was certainly one of the most important discoveries, until 1970 in which he witnessed the upper hand of automation and therefore the construction of the first computers. We come then to the early 2000s, where on the occasion of the Hannover fair, to describe the advent of the fourth industrial revolution, the term ""industry 4.0"" was coined, to highlight the transition from a system based on physical interactions between man and machine to one in which the interaction between the data and the machines carry out most of the production operations. In the food sector, both research and technological innovations have been conducted not only by research entities and universities but also by start-ups, in order to obtain technological improvements and unknown products.
The term start-up (from English ""start""), indicates an intervention by a new company with all that is necessary both from a structural and organizational point of view and this elaborate, in fact, is based on the use of the SCM software (Safeaty Content Management), born from the Spin-Off of the University of Pisa, and applied to an air catering service in order to integrate itself into the management of the phases that carried out the same highlighting its advantages and potential.
The main mission of the software is to help food companies to draw up a self-control plan based on the HACCP principle, without running into errors or forgetfulness, all just a click away. The HACCP system is at the base of the food safety pyramid, but before that we meet regulation 178/02 and the hygiene package. Regulation 178/02, constitutes the primary source of modern European food law and contains in a single text the rights and duties of all operators in the food sector, therefore both those attributed to the FBOs and those designated to the Competent Authorities that carry out the controls in the field. It also establishes the general principles and requirements of food law, marking the transition from a multitude of national regulations, in implementation of the EU directives to a rational set of common rules verbatim applied in the entire Single Market. With the advent of this legislation, then, primary responsibility for achieving the objectives has been attributed to operators in the food sector; the task of verification, however, to the supervisory authorities. Then comes the hygiene package, a set of four legislative texts, published for the first time in 2004 but which entered into force on 1st January 2006, and named: Regulation 852/04, Regulation 853/04, Regulation 854/04 and regulation 882/04 (the last two regulations have been repealed and replaced by regulation 625/2017).
The first two regulations are addressed to the responsibilities that the FBOs must comply with in order to achieve the safety objectives; the latter, however, are intended for supervisory authorities. Thus we come to HACCP, an acronym for ""Hazard Analysis and Critical Control Points"", or ""Hazard Analysis and Critical Control Points"" which constitutes a valid system, organized and rational as well as shared at European level, for the application of the self-control of operators operating in the industries that treat, manipulate or distribute food.
The application of the HACCP system arises precisely from the need to guarantee the healthiness of food production by changing the approach, that is, passing from a control downstream of the production process, which existed in the past and carried out the control on the finished product, to a control of the production process at each stage, therefore identifying all the risks that can affect the safety of food and implementing all the preventive measures necessary to keep them under control. So, from a traditional product control, we have passed to a process control, that is the modern control. In Italy it was introduced in 1997, by Legislative Decree 155 of 1997. It is currently in force since it is covered by EC Regulation 852 of 2004, which repealed the previous Legislative Decree 155 of 1997. The development of the HACCP method requires that the so-called ""prerequisite programs"" or PRPs, that is the basic conditions and activities necessary to maintain an environment in a suitable condition for the preparation of healthy and safe products for human consumption, must be implemented and respected.
The effectiveness of this method, however, is only fully completed with compliance with 5 preliminary phases (establishment of the HACCP team, description of the product, description of possible uses, creation of a flowchart, field verification) and 7 principles ( hazard analysis, identification of CCPs, definition of critical limits, definition of monitoring, identification of corrective actions, verification of the effectiveness of the system and verification of documentation).
The software used for this thesis allows you to draw up a business management and self-control plan, based precisely on the dictates of HACCP but all within a click. In the specific case, we used it for a London-based company, the Niche free-from kitchen, which operates in the airline catering sector.
After a first analysis, we found that the software simplifies management operations: in fact, the overall view of the productions helps the operator to monitor any non-compliance in real time for an immediate response; the dematerialisation of documents brings benefits in terms of precision of work, economic savings and reduction of volumetric dimensions in the office as well as a lower environmental impact; last, but not least, being fully digitalized management, it is certainly helpful at a time like the one we are experiencing, in which smart working has taken on a fundamental importance",industry,849,not included
,to_check,core,,2020-02-20 00:00:00,core,двнз «приазовський державний технічний університет»,ДОСЛІДЖЕННЯ ТА МОДЕЛЮВАННЯ ІГРОВОГО ПРОЦЕСУ НА БАЗІ МЕТОДІВ МАШИННОГО НАВЧАННЯ,"Methods of machine intelligence with training contribute their specifics to the creation and commissioning of a gaming system.One of the main problems is the need to anticipate the entire set of input situations and possible answers at the time of design and the impossibility of expanding their list withoutretraining. This leads to a narrowing of the possibility of their use in real gaming systems.  The object of research is the processes of creating and training game agents based on the evolutionary approaches of artificial intelligence. The purpose of the work is the development and justification of a formal model of a game agent based on machine learning methods, software implementation of the game process using neural networks and evolutionary optimization methods for multiple generations of game agent populations. To achieve this goal, the theoretical base, the existing research and development in the industry; a game scenario was designed, the main agents were identified, their main capabilities and expected behavior; training of game agents by various types of neural networks; development testing, quality assessment of behavior and decision-making by artificial intelligence using neural networks were performed; a comparative analysis of various types of neural networks, the proposed recommendations for their use for given conditions. To display the artificial neural network, the template components of the layer, neuron, and bridge were used. The created software module will allow game agents built using various neural networks to compete with each other (and not with a person, as in the normal game mode), and will reveal more prepared ones.  The scientific novelty of the study lies in the fact that a model of a game agent is formalized, based on machine learning methods. The results obtained in this work can be used in the development of video games built on the basis of artificial intelligence of game agents based on machine learning methods, as well as in other scientific studies.Методы машинного интеллекта с обучением вносят свою специфику к созданию и наладке игровой системы.Одна из главных проблем — это необходимость предвидения всего набора входных ситуаций и возможных ответов на моменте проектирования и невозможность расширения их списка без переобучения. Это приводит к сужению возможности их использования в реальных игровых системах.Объектом исследования являются процессы создания и обучения игровых агентов на основе эволюционных подходов искусственного интеллекта. Цель работы - разработка и обоснование формальной модели игрового агента, основанного на методах машинного обучения, программная реализация игрового процесса сприменением нейронных сетей и эволюционных методов оптимизации при множественных поколениях популяций игровых агентов.Для осуществления поставленной цели исследована теоретическая база, существующие исследования и разработки в отрасли; спроектирован игровой сценарий, определены основные агенты, их основные возможности и ожидаемое поведение;реализовано обучение игровых агентов различными типами нейронных сетей; выполнено тестирование разработок, оценка качества поведения и принятия решений искусственныминтеллектом с использованием нейронных сетей; проведен сравнительный анализ различных типов нейронных сетей, предлагаемых рекомендаций по их использованию для заданныхусловий. Для отображения искусственной нейронной сети использованы шаблонные компоненты слоя, нейрона и моста.Созданный программный модуль позволит игровым агентам, построенным с использованием различных нейронных сетей, соперничать друг с другом (а не с человеком,как в обычном режиме игры), позволит выявить более подготовленного из них. Научная новизна исследования заключается в том, что формализована модель игрового агента, в основе которой методы машинного обучения. Полученные в работе результаты могут быть использованы при разработке видеоигр, построенных на базе искусственного интеллекта игровых агентов, в основекоторых методы машинного обучения, а также в рамках других научных исследований.Методи машинного інтелекту з навчанням привносять свою специфіку до створення і налагодження ігрової системи.  Одна з головних проблем - це необхідність передбачення усього набору вхідних ситуацій і можливих відповідей на моменті проектування і неможливість розширення їх списку без перенавчання. Це призводить до звуження можливості їх використання у реальних ігрових системах. Об'єктом дослідження є процеси створення і навчання ігрових агентів на основі еволюційних підходів штучного інтелекту. Мета роботи - розробка і обґрунтування формальної моделі ігрового агенту, заснованої на методах машинного навчання, програмна реалізація ігрового процесу з застосуванням нейронних мереж та еволюційних методів оптимізації при множинних генераціях популяцій ігрових агентів. Для здійснення поставленої мети досліджена теоретична база, існуюча дослідження та розробки в галузі; спроектовано ігровий сценарій, визначені основні агенти, їх основні можливості та очікувана поведінка; реалізоване навчання ігрових агентів різними типами нейронних мереж; виконане тестування розробок, оцінка якості поведінки й прийняття рішень штучним інтелектом з використанням нейронних мереж; проведений порівняльний аналіз різних типів нейронних мереж, запропонування рекомендацій щодо їх використання для заданих умов. Для відображення штучної нейронної мережі використані шаблоні компоненти слою, нейрону і мосту. Створений програмний модуль дозволить ігровим агентам, побудованим з використанням різних нейронних мереж, суперничати один з іншим (а не з людиною, як у звичайному режимі гри), дозволить виявити більш підготовленого з них. Наукова новизна одержаних результатів полягає у тому, що формалізовано модель ігрового агенту, в основі якої методи машинного навчання. Отримані в роботі результати можуть бути використані при розробці відеоігор, побудованих на базі штучного інтелекту ігрових агентів, що гуртуються на методах машинного навчання, а також в рамках інших наукових досліджень",industry,850,not included
,to_check,core,10.1007/978-1-4842-4932-1,2020-01-01 00:00:00,core,springer nature,Programming Persistent Memory,"Beginning and experienced programmers will use this comprehensive guide to persistent memory programming. You will understand how persistent memory brings together several new software/hardware requirements, and offers great promise for better performance and faster application startup times—a huge leap forward in byte-addressable capacity compared with current DRAM offerings. This revolutionary new technology gives applications significant performance and capacity improvements over existing technologies. It requires a new way of thinking and developing, which makes this highly disruptive to the IT/computing industry. The full spectrum of industry sectors that will benefit from this technology include, but are not limited to, in-memory and traditional databases, AI, analytics, HPC, virtualization, and big data. Programming Persistent Memory describes the technology and why it is exciting the industry. It covers the operating system and hardware requirements as well as how to create development environments using emulated or real persistent memory hardware. The book explains fundamental concepts; provides an introduction to persistent memory programming APIs for C, C++, JavaScript, and other languages; discusses RMDA with persistent memory; reviews security features; and presents many examples. Source code and examples that you can run on your own systems are included. What You’ll Learn Understand what persistent memory is, what it does, and the value it brings to the industry Become familiar with the operating system and hardware requirements to use persistent memory Know the fundamentals of persistent memory programming: why it is different from current programming methods, and what developers need to keep in mind when programming for persistence Look at persistent memory application development by example using the Persistent Memory Development Kit (PMDK) Design and optimize data structures for persistent memory Study how real-world applications are modified to leverage persistent memory Utilize the tools available for persistent memory programming, application performance profiling, and debugging Who This Book Is For C, C++, Java, and Python developers, but will also be useful to software, cloud, and hardware architects across a broad spectrum of sectors, including cloud service providers, independent software vendors, high performance compute, artificial intelligence, data analytics, big data, etc",industry,852,not included
,to_check,core,,2019-01-01 00:00:00,core,cool dh,Report on improved use of individual metering concept,"In a low temperature district heating (LTDH) system the importance of well performing network and customer installations are essential due to reduced operation margins. The use of low-value waste heat in the production sets limitations on the supply temperature. New piping materials sets limitations on the supply temperature and the pressure level. All in all, this means an increased focus on ensuring that DH installations arewell performing. This report is about ‘individual metering concepts’. The LTDH-system is monitored by direct connection to the heat meters at all the consumers. The data from the heat meters can be used as a basis for describing, predicting and analysing the system's performance for better control of production and operation, for better conditions for troubleshooting and maintenance of the grid and the customer installations as well as for better customer service. The aim of this report is to:  give an overview over the current regulations for metering and billing of heat in Sweden and Denmark give an overview of desirable functionalities for DH meters and metering systems to provide input to applications that can be developed with the help of meter readings and which can be useful for the district heating industry describe how DH meter readings can be used for condition monitoring of service pipes and develop a method for this Regulations for metering and billing of DH follows EU regulations which says that all DH customers must be charged according to actual consumption. The overview of the current regulations for metering and billing in Denmark and Sweden shows that the two neighbouring countries have somewhat different approach to the individual metering concept. In Denmark there is a long tradition of individual metering in multifamily buildings and the tenants heat consumption is measured. In Sweden there is no such tradition and the individual metering concept generally is not in focus. Instead, the property owner is the customer. Denmark follows the Energy Efficiency Directive from 2012 (supplemented 2018) that requires individual metering and charging. The Swedish regulations on metering and billing are based on the exception in the EU regulation concerning individual metering and charging saying that individual metering must be economically justifiable. The Swedish National Board of Housing has shown that implementation of the EU regulations is not economically justifiable for the Swedish case ‐ neither for existing buildings or in new buildings. There is a general attitude that increased frequency and higher resolution of meter readings automatically results in increased possibilities for new and improved analyses of customer performance. At present there are several researchers and research groups that are using meter readings from DH customers and machine learning with the aim to improve the energy performance of both the DH customer installations as well as the DH network and production operation. From ongoing projects four areas for applications have been identified: Fault detection, Load prediction, Production planning and Operational optimization. For these applications hourly meter readings are sufficient, at least in combination with historical data and weather conditions. For improved fault detection algorithms and moving towards fault diagnosis it would be beneficial to access secondary meter readings such as temperatures in the secondary side heating‐ and domestic hot water system as well as indoor temperatures. When moving on towards fault diagnosis, the frequency may also be in focus. A temporary shift to meter readings with higher frequency may be necessary for diagnosis. All in all, it might not be the frequency of the meter readings that should be increased, it might be more beneficial to include more meter parameters from the customer side of the heat exchanger. By integration of more meter parameters, the focus may shift from a matter of frequency to the broad concept of Internet of things (IoT). One scope within this project was to evaluate the feasibility for usage of meter readings from customer installations to detect increased heat losses in service pipes due to moisture. A theoretical study was carried out, but it did not turn into a success. Results showed that even though the heat losses in the service pipe increases due to moisture content, the impact on the measured parameters, that is temperature and flow, are low and would be hard to detect. A higher resolution or increased frequency of meter readings would not improve the feasibility to use meter readings for monitoring increased heat losses in service pipes since if feasible, it would require stable heat load at the customer installation and stable and known temperature conditions in the DH network. Through studies performed within this work and by other researchers it is clear that DH utilities, both in Denmark and Sweden, has a desire to improve customer performance and to reduce the DH return temperature. A key factor for success within this field is good customer relations and access to the customer’s DH substation. This may be a driving force for DH‐utilities to offer service agreement and ICT‐platforms for greater customer engagement. Customers seems to be more willing to take actions to improve their installations if they understand why it is important. Recommendations: The following recommendations are given based on the overall picture provided by the report's different compilations and studies:  The meter should be able to measure the energy for every hour and be able to convert to measure with higher frequencies (minutes). Some types of measurement data analyses may require a higher resolution than hourly measurement, which is why the possibility of higher resolution should be provided, without that being a default setting.  It should be possible to remotely upgrading the meters and to remotely change the meter frequency. This will provide that new functionalities can be introduced in a cost‐efficient way and that expensive field visits can be avoided.  Access to secondary meter readings. The following measurements were seen as the most important: Indoor temperatures, secondary temperatures measured on branches for the heating system in the building, and measurement on the district heating differential pressure. Measuring these parameters would be beneficial for developing algorithms for fault detection and for improving system performance for the total district heating system, as well as for as being able to guarantee the quality of energy supplies at the customer substations.  There should be a digital interface that the end customers can use to access their energy consumption locally. The communication solution should also support a future standard for communication with devices in the home. In order to be of real use for the customers, this should be delivered with some kind of analysis tools that can help the customers relate their consumption or data to reference data (for example historical data, norms or set point values) and that can provide the customer with extended analysis that make sense to the customer",industry,853,not included
,to_check,core,,2020-01-01 00:00:00,core,'dks center',Теорія та практика забезпечення кіберстійкості банків,"У статті доведено важливість формування концепції забезпечення кіберстійкості банків на сучасному етапі розвитку цифрової економіки країни, зважаючи на негативний фінансовий та нефінансовий вплив кібератак на банківську систему та економіку країни в цілому. Автором на основі узагальнення досліджень з цієї тематики уточнено зміст поняття “кіберстійкість банку” та визначено його сутнісні характеристики за якісним та кількісним підходами. В статті проведено дослідження теоретичних підходів до забезпечення кіберстійкості банків та на цій основі розроблено модель механізму забезпечення кіберстійкості, адекватну сучасному стану та умовам, в яких функціонують банки України. За результатами дослідження визначено, що ефективне функціонування механізму забезпечення кіберстійкості потребує відповідного організаційного забезпечення, зокрема створення Центра кіберстійкості банку.В статье доказана важность формирования концепции обеспечения киберустойчивости банков на современном этапе развития цифровой экономики страны, несмотря на отрицательное финансовое и нефинансовое влияние кибератак на банковскую систему и экономику страны в целом. Автором на основе обобщения исследований по этой тематике уточнено содержание понятия ""киберустойчивость банка"" и определены его сущностные характеристики согласно качественному и количественному подходам. В статье проведено исследование теоретических подходов к обеспечению киберустойчивости банков и на этой основе разработана модель механизма обеспечения киберустойчивости, адекватная условиям, в которых функционируют банки Украины. В результате исследования установлено, что эффективное функционирование механизма обеспечения киберустойчивости требует соответствующего организационного обеспечения, в частности создания Центра киберустойчивости банка.The article proves the importance of forming the concept of ensuring the cyber resilience of banks at the present stage of development of the country's digital economy in the transition to the sixth technological mode and the associated use of industry 4.0 technologies, such as artificial intelligence, «cloud» and «foggy» computing, IoT / IIoT, Big Data, Blockchain, VR / AR. This leads to a significant complication of the cyber threat landscape, an increase in the number of cyberattacks with a significant increase in the negative financial and non-financial consequences that cyberattacks have on the banking system and the economy as a whole. The author, based on the generalization of research on this topic, clarified the content of the concept of ""cyber resilience of a bank"". Its essential characteristics were determined in the context of qualitative and quantitative approaches. The article studies theoretical approaches to ensuring the cyber resilience of banks, which made it possible to develop a conceptual model of the mechanism for ensuring cyber resilience, adequate to the current state and conditions in which the banks of Ukraine operate. The developed mechanism for ensuring cyber resilience allows for: 1) the formalization of the landscape of real and potential cyber threats; 2) ensures the consistency of mechanisms and tools for countering them, adapting and/or recovering from cyber incidents; 3) allows not only to adequately respond to existing cyber threats but also to identify negative factors that can lead to the emergence and implementation of new cyber threats and cyber-attacks. According to the results of the study, it was found that the effective functioning of the mechanism for ensuring the cyber resilience of the bank requires appropriate organizational support. For this, the author substantiated the need to create a Bank Cyber Resilience Center. It should include representatives from the departments responsible for banking business continuity, cybersecurity, cyber risk management and IT quality. This will allow obtaining a synergistic effect by creating a single interconnected process-based model, including metrics of the bank's cyber resilience level and KPIs, as well as tools for monitoring, controlling and resisting external and internal cyber threats, adaptation and/or recovery after them",industry,854,not included
,to_check,core,https://core.ac.uk/download/pdf/287192453.pdf,2020-02-10 00:00:00,core,"ekonomski fakultet, univerzitet u istočnom sarajevu",FINANCE AND ARTIFICIAL INTELLIGENCE: THE FIFTH INDUSTRIAL REVOLUTION AND ITS IMPACT ON THE FINANCIAL SECTOR,"A true artificial intelligence (AI) system is something that ""learns"" from the data it stores,  in order to perform tasks and solve problems that typically require human intelligence - either with the help of a human expert or independently. The area of AI is an interdisciplinary field, which has been designated as a strategic area in the European Union (EU) approach and a key driver of economic development that can bring solutions to many social challenges and problems. Due to its nature and its tendency to be digitally advanced and smarter with analytics, the financial sector is one of the early adopters of AI and expects multiple benefits from its application, that is, the ability to provide better service in the shortest time possible and at a lower cost. AI in the financial sector is based on an understanding of the business needs of financial organizations, institutions and markets and the ability to connect with technological capabilities. They are powerful tools that completely transform this sector. The basic idea of this paper is to consider where the real value of AI in the financial sector is, i.e. what are the practical aspects and business implications of AI in the financial sector globally. It is common knowledge that evolving technologies have always had a strong impact on the sectors in which they are applied because they give them the opportunity to improve existing manufacturing processes, services, customer experiences, operate more efficiently, achieve cost savings, etc. The aim of this paper is to identify areas of application of AI in the financial sector, and to explore leading AI applications that are changing the financial ecosystem, transforming the financial sector and that have the potential to significantly improve many of its functions. The paper further highlights other implications of AI implementation in the financial sector such as employment - job creation and termination of existing AI-influenced employment, the scope and potential of application in developing countries, the problem of regulation and use in the best interests of man, and the importance of properly managing specific AI risks",industry,855,not included
,to_check,core,,2019-01-01 00:00:00,core,"faculty of educational studies, universiti putra malaysia",Why coding? Why now? From coding to computational thinking through computational mathematics problem based learning (CM-PBL),"4th Industrial revolution are spreading around the world, embedding the technology into societies. In the digital revolution, high technological tools and resources are regularly being developed. There has been increased attention on learning coding in education field, in order to nurture sufficient number of young generation to fill in 50 percent of jobs opportunity in science, technology, engineering, and mathematics (STEM) which are computing-related. It is increasingly clear that our new generation need to think critically to solve the ill-structured problems, uncertain and complex real world problems. Computational thinking is beneficial in providing automated or semi-automated solutions in problems solving in combination with critical thinking. Therefore, computational thinking is becoming an important implication in science and mathematics as well as in almost every other fields. This fact is reflected in the recent implementation of computer coding in the Malaysia’s school curriculum to nurture 21st century skills among students. Computational thinking is one of the conceptual foundations which required combination of data and ideas to solve problems effectively and efficiently. Computational thinking is essential to computing and information science (i.e., algorithmically, with or without the assistance of computers) to solve problems with solutions that are reusable in different contexts. Promoting different kind of skills and abilities among students is important to solve the complexity problem in real world. Thus, it is a challenging task for many instructors to create a learning environment who lack of feasible resources availability and research-based information to redesign their teaching pedagogies. This conceptual framework is aimed at two goals: (1) development of deep learning, connected computational thinking through the mathematical curriculum instructional model which able to enhance students’ problem solving abilities and (2) implementation of the designed model to assist teacher in education who has practicing difficulty in elementary classrooms after adopting teacher enactment of problem-based curriculum resources. To achieve these goals, computational mathematics problem based learning (CM-PBL) instructional strategy is developed to promote an active learning environment for classroom management involving problem solving. Rather than emphasizing student learning passively through listening, watching, practicing exercises and imitating isolated skills, the CM-PBL learning framework allowing students to simulate and build their own computational models to support their self-learning and understanding of mathematic concepts. It is also highlights ways that students can evolve from technical skills in using technology to soft engineering skills by using coding knowledge",industry,856,not included
,to_check,core,"Navigating the landscape for real-time localisation and mapping for robotics, virtual and augmented reality",2018-06-29 00:00:00,core,"[{'title': 'proceedings of the ieee', 'identifiers': ['0018-9219', 'issn:0018-9219']}]",'Institute of Electrical and Electronics Engineers (IEEE)',"Visual understanding of 3-D environments in real time, at low power, is a huge computational challenge. Often referred to as simultaneous localization and mapping (SLAM), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, and virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are: 1) tools and methodology for systematic quantitative evaluation of SLAM algorithms; 2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives; 3) end-to-end simulation tools to enable optimization of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches; and 4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context",industry,857,not included
,to_check,core,"A Systematic Review of Smart Real Estate Technology: Drivers of, and Barriers to, the Use of Digital Disruptive Technologies and Online Platforms",2018-09-01 00:00:00,core,"[{'title': 'sustainability', 'identifiers': ['2071-1050', 'issn:2071-1050']}]",'MDPI AG',"Real estate needs to improve its adoption of disruptive technologies to move from traditional to smart real estate (SRE). This study reviews the adoption of disruptive technologies in real estate. It covers the applications of nine such technologies, hereby referred to as the Big9. These are: drones, the internet of things (IoT), clouds, software as a service (SaaS), big data, 3D scanning, wearable technologies, virtual and augmented realities (VR and AR), and artificial intelligence (AI) and robotics. The Big9 are examined in terms of their application to real estate and how they can furnish consumers with the kind of information that can avert regrets. The review is based on 213 published articles. The compiled results show the state of each technology&rsquo;s practice and usage in real estate. This review also surveys dissemination mechanisms, including smartphone technology, websites and social media-based online platforms, as well as the core components of SRE: sustainability, innovative technology and user centredness. It identifies four key real estate stakeholders&mdash;consumers, agents and associations, government and regulatory authorities, and complementary industries&mdash;and their needs, such as buying or selling property, profits, taxes, business and/or other factors. Interactions between these stakeholders are highlighted, and the specific needs that various technologies address are tabulated in the form of a what, who and how analysis to highlight the impact that the technologies have on key stakeholders. Finally, stakeholder needs as identified in the previous steps are matched theoretically with six extensions of the traditionally accepted technology adoption model (TAM), paving the way for a smoother transition to technology-based benefits for consumers. The findings pertinent to the Big9 technologies in the form of opportunities, potential losses and exploitation levels (OPLEL) analyses highlight the potential utilisation of each technology for addressing consumers&rsquo; needs and minimizing their regrets. Additionally, the tabulated findings in the form of what, how and who links the Big9 technologies to core consumers&rsquo; needs and provides a list of resources needed to ensure proper information dissemination to the stakeholders. Such high-quality information can bridge the gap between real estate consumers and other stakeholders and raise the state of the industry to a level where its consumers have fewer or no regrets. The study, being the first to explore real estate technologies, is limited by the number of research publications on the SRE technologies that has been compensated through incorporation of online reports",industry,858,not included
,to_check,core,Avionics Graphics Hardware Performance Prediction with Machine Learning,2019-01-01 00:00:00,core,"[{'title': 'scientific programming', 'identifiers': ['1058-9244', 'issn:1058-9244', '1875-919x', 'issn:1875-919x']}]",'Hindawi Limited',"Within the strongly regulated avionic engineering field, conventional graphical desktop hardware and software application programming interface (API) cannot be used because they do not conform to the avionic certification standards. We observe the need for better avionic graphical hardware, but system engineers lack system design tools related to graphical hardware. The endorsement of an optimal hardware architecture by estimating the performance of a graphical software, when a stable rendering engine does not yet exist, represents a major challenge. As proven by previous hardware emulation tools, there is also a potential for development cost reduction, by enabling developers to have a first estimation of the performance of its graphical engine early in the development cycle. In this paper, we propose to replace expensive development platforms by predictive software running on a desktop computer. More precisely, we present a system design tool that helps predict the rendering performance of graphical hardware based on the OpenGL Safety Critical API. First, we create nonparametric models of the underlying hardware, with machine learning, by analyzing the instantaneous frames per second (FPS) of the rendering of a synthetic 3D scene and by drawing multiple times with various characteristics that are typically found in synthetic vision applications. The number of characteristic combinations used during this supervised training phase is a subset of all possible combinations, but performance predictions can be arbitrarily extrapolated. To validate our models, we render an industrial scene with characteristic combinations not used during the training phase and we compare the predictions to those real values. We find a median prediction error of less than 4 FPS",industry,859,not included
,to_check,core,,2018-01-01 00:00:00,core,'academy of traumatology',Comparison of Delayless Digital Filtering Algorithms and Their Application to Multi-Sensor Signal Processing,"In the phase of industry digitalization, data are collected from many sensors and signal processing techniques play a crucial role. Data preprocessing is a fundamental step in the analysis of measurements, and a first step before applying machine learning. To reduce the influence of distortions from signals, selective digital filtering is applied to minimize or remove unwanted components. Standard software and hardware digital filtering algorithms introduce a delay, which has to be compensated for to avoid destroying signal associations. The delay from filtering becomes more crucial when the analysis involves measurements from multiple sensors, therefore in this paper we provide an overview and comparison of existing digital filtering methods with an application based on real-life marine examples. In addition, the design of special-purpose filters is a complex process and for preprocessing data from many sources, the application of digital filtering in the time domain can have a high numerical cost. For this reason we describe discrete Fourier transformation digital filtering as a tool for efficient sensor data preprocessing, which does not introduce a time delay and has low numerical cost. The discrete Fourier transformation digital filtering has a simpler implementation and does not require expert-level filter design knowledge, which is beneficial for practitioners from various disciplines. Finally, we exemplify and show the application of the methods on real signals from marine systems",industry,860,not included
,to_check,core,,2018-08-09 00:00:00,core,university of southern california. libraries,Managing functional coupling sequences to reduce complexity and increase modularity in conceptual design,"2018-08-10According to the Axiomatic Design Theory (ADT), a design concept that can satisfy the upstream objectives under downstream constraints with the minimal relative complexity can lead to the most ideal design. As stated by Suh’s Complexity Theory, the relative complexity of a design concept is caused by couplings between functional requirements (FRs) and design parameters (DPs), and can be reduced by strategically ordering the execution (i.e., implementation) sequence of DPs. However, it is generally very difficult in the current design practice to obtain this “execution sequence” with existing methods due to their inherent limitations and/or many real-world restrictions. Meanwhile, many practical methods, such as the modular design approach, have been widely used in industries to produce real-world design results that don’t necessarily conform with the principles required by those ideal design theories. As a result, from the perspectives of design theories, most real-world designs are “not ideal” (i.e., having some relative complexities due to FR-DP couplings) and therefore can (and should) be improved by better sequencing their DPs. This is the motivation under which the Design Coupling Sequence (DCS) method was developed in this thesis research. The DCS method can assist designers to automatically obtain the “execution sequences,” in the forms of functional sets, that can yield the minimal relative complexity, hence making a practical design concept most ideal (i.e., as close to the ideal concept with the minimal relative complexity as possible) while taking into practical considerations (such as increasing the modularity to lower the production costs) in real-world conceptual designs. ❧ The DCS method defines the ‘precedence’ between ‘functional sets’ to manage coupled design concepts to support the modular approach during conceptual design. It identifies the ‘precedence’ by the level of functional coupling to determine the proper sequencing order to minimize the overall complexity. Two types of functional sets are defined in DCS as 1) the complete independently set U: the collection of all the functionally dependent DPs in the system so that the set is independent to other U sets, and 2) the indivisible coupled set C: the collection of coupled concepts that can’t be decoupled by sequencing, so it prescribes the designer to consider the group of DPs together as a set to match existing modules in the database. To handle the real complexity of design concepts which require redesign, the DCS algorithm helps to determine the proper execution sequence. To minimize the imaginary complexity, which occurs when design concepts “appear” to be functionally coupled due to a lack of understanding of the system structure, the DCS method provides a formula to reveal the number of acceptable execution sequences that can lead to the simplest design implementation. Compared with existing methods, the DCS method is applicable for any design cases with known design matrices, including the square, rectangular, zero-at-diagonal, large, and/or numerical matrices. In short, for all practical design cases, DCS can organize functional-coupled design concepts as “functional sets” with execution sequences of DPs that lead to the minimal complexity of this design concept. ❧ The foundation, hypothesis, algorithm and its usability of the DCS method are validated by four case studies in this research. The faucet design case demonstrates how to apply the DCS method and shows the differences between ADT and DCS results. The case of coffee maker design shows how the DCS method manages the functional sets based on the design matrices reengineered from existing design concepts. The vehicle tire design case demonstrates how different DCS strategies within the Innovative Design Thinking (IDT framework during the conceptual design stage can work in a real-world product development situation. The IDT framework prescribes four consecutive steps: (1) following the top-down process to ideate new design concepts that satisfy the principles/axioms suggested by the design theory to reach a certain layer of details, (2) following the bottom-up process to identify some existing design modules from available engineering database (or catalogs) that can satisfy the functional requirements at this detail layer, (3) constructing the design matrix that shows the couplings between FRs and DPs at this detail layer, and (4) apply the DCS algorithm to determine the execution sequence of DPs based on the above design matrix. This will yield a new design concept with an execution sequence that is most creative (because it satisfies the design principles at the top layers) and most practical (because it utilizes the existing modules at the bottom layers). Finally, a case of collision avoidance planning presents one of the possible extensions of the DCS algorithm. ❧ The results of this research have significant impacts on both design theory and design practice. Theoretically, the approach in this research 1) guides designers to improve concepts not only organizing design matrix but also extract additional coupling information to increase modularity and 2) is a more generalized approach than the previous methods that can be applied to any design cases with design matrix. Practically, the research 1) demonstrates the usability of the DCS algorithm within an executive program to generate the DCS functional sets automatically for large design system and 2) allows the principle of functional dependency and the practice of modular design to be considered simultaneously as much as possible during the conceptual design stage. It is a fundamental contribution that demonstrates how the ideal principles (or axioms) of design theories can be used together strategically with practical design methods (or considerations) in industry practices to generate real-world design results that are both most practical and creative. For future research, there would be three aspects- DCS algorithm, DCS sets, and DCS strategies. Number one, DCS algorithm would be further revised to extend to software design or machine learning with functional sets in terms of a component diagram. Number two, with DCS sets, the three-dimensional design matrix could be studied further. Number three, DCS strategies would be investigated further for applying on detailed design cases. ❧ As a recap, the research prescribes a functional coupling managing algorithm with functional sets for suggesting acceptable execution sequences in conceptual design. It not only helps designers with complexity reduction but also bridges the ideal design theory to practical modules. The designer can create better designs that are most creative and yet practical by using the DCS strategies",industry,861,not included
,to_check,core,10.1177/0142331218799148,2018-01-01 00:00:00,core,sage publications,Comparison of Delayless Digital Filtering Algorithms and Their Application to Multi-Sensor Signal Processing,"In the phase of industry digitalization, data are collected from many sensors and signal processing techniques play a crucial role. Data preprocessing is a fundamental step in the analysis of measurements, and a first step before applying machine learning. To reduce the influence of distortions from signals, selective digital filtering is applied to minimize or remove unwanted components. Standard software and hardware digital filtering algorithms introduce a delay, which has to be compensated for to avoid destroying signal associations. The delay from filtering becomes more crucial when the analysis involves measurements from multiple sensors, therefore in this paper we provide an overview and comparison of existing digital filtering methods with an application based on real-life marine examples. In addition, the design of special-purpose filters is a complex process and for preprocessing data from many sources, the application of digital filtering in the time domain can have a high numerical cost. For this reason we describe discrete Fourier transformation digital filtering as a tool for efficient sensor data preprocessing, which does not introduce a time delay and has low numerical cost. The discrete Fourier transformation digital filtering has a simpler implementation and does not require expert-level filter design knowledge, which is beneficial for practitioners from various disciplines. Finally, we exemplify and show the application of the methods on real signals from marine systems.acceptedVersion© 2018. This is the authors' accepted and refereed manuscript to the article. The final authenticated version is available online at: https://doi.org/10.1177%2F014233121879914",industry,862,not included
,to_check,core,,2018-03-12 00:00:00,core,ufcg,Use of soft sensors to estimate impurities in high purity distillation columns.,"Neste trabalho é proposta uma metodologia para construção de sensores virtuais

implementados em software, com objetivo de estimar e prever o comportamento de impurezas na corrente de base de uma coluna de destilação de alta pureza, do processo produtivo do 1,2 Dicloroetano (C2H4Cl2). A aquisição dos dados utilizados na construção dos sensores virtuais foi realizada através do modelo matemático do processo, simulado com dados reais de uma planta industrial. O estudo específico engloba a modelagem matemática/termodinâmica e avaliação do comportamento estacionário e dinâmico dessa torre, simulada aqui no software Aspen Plus e DynamicsTM. Desse modo, o modelo fornece os dados necessários para inferência das impurezas relacionadas, que são os teores dos compostos tetracloreto de carbono (CCl4) e clorofórmio (CHCl3), ambos devem ser mantidos, respectivamente, em valores ≤3000 e ≤400 ppm (partes por

milhão). A metodologia também aborda dois algoritmos de seleção de variáveis secundárias, que utilizam técnicas estatísticas multivariadas (algoritmo de todas as regressões possíveis-TRP e da análise de componentes principais-PCA). Verifica-se também nos dados gerados quanto a real ou não necessidade de remoção de erros grosseiros (outliers), por isso é também inserida na metodologia uma etapa de pré-processamento de dados. Foram selecionados os dez melhores modelos de inferência para cada uma das saídas. Diante dessa informação, os melhores modelos

produzidos não utilizavam as concentrações dos compostos das correntes de alimentação e sim medições de temperaturas ao longo da torre. Uma importante conclusão do ponto de vista de construção de sensores virtuais, porque na maioria dos trabalhos desenvolvidos essas variáveis são cruciais na produção de bons resultados. O treinamento dos sensores virtuais foi efetuado em um ambiente ruidoso, haja vista que foram simulados ruídos inerentes às medições (ruídos brancos Gaussianos). Na etapa final, os sensores virtuais são construídos utilizando uma técnica de modelagem empírica, redes neurais artificiais (RNA), onde foram utilizadas RNA do tipo Perceptron Multicamadas (MLP). Foram também avaliadas diversas variações quanto ao número

de neurônios e camadas ocultas das RNA, empregando como critério de parada a técnica de validação cruzada. Os sensores virtuais desenvolvidos apresentaram erros satisfatórios do ponto de vista de engenharia, uma boa análise de regressão e um bom erro médio quadrático. Logo, com essas estimativas espera-se a minimização e a previsão do comportamento transiente dos compostos no referido processo.This work proposes a methodology for software implementation to make soft sensors.

The goal is to estimate and predict the behavior of impurities in the bottom current of a highpurity distillation column, for 1,2-Dichloroethane or 1,2-DCE (C2H4Cl2) production. The data acquisition used in the construction of soft sensors was performed through a mathematical simulation of the process, with real industrial data taken from an industrial plant. A specific study involves the mathematical modeling, thermodynamics, evaluation of the steady state and the dynamic behavior of this process, simulated here in Aspen Plus and Aspen DynamicsTM software. Thus, the model provides the necessary data to infer the contents of the carbon impurities mentioned above, Tetrachloride (CCl4) and Chloroform (CHCl3), which are to be fixed approximately below 3000 and 400 ppm (parts per million) respectively. The methodology also covers selection algorithms of secondary variables, using multivariate statistical techniques: All Possible Regressions (TRP) and Principal Component Analysis (PCA). The data generated was checked in order to know whether to include or not a step for removal of outliers, so it was also included in the methodology one preprocessing data step. The ten best inference models

were selected for each output concentration. With this information, these models do not use concentrations measurements in the feed streams but measurements of the temperature along the column. This is an important conclusion from the point of view of virtual sensors building, because in most of the literature reported these variables are crucial in getting good results. The training of soft sensors was done in a noisy environment, considering that simulated noise was inherent to measurements (Gaussian noise). In the final step, the soft sensors devices are constructed using an empirical modeling technique of artificial neural networks (ANN), which were generated ANN type Multilayer Perceptron (MLP). Several variations were also evaluated on the number of neurons and hidden layers of networks, employing as a stopping criterion the

cross-validation technique. The developed soft sensors presented satisfactory errors from the engineering viewpoint, a good regression and a good mean square error. Finally, with these estimations it is expected to minimize and predict the transient behavior of the compounds in the referred process",industry,863,not included
,to_check,core,10.1115/IOWTC2018-1058,2018-11-04 00:00:00,core,'asme international',Operational Data to Maintenance Optimization: Closing the Loop in Offshore Wind O&M,"This is the author accepted manuscript. The final version is available from ASME via the DOI in this recordOffshore wind assets have reached multi-GW scale and additional
capacity is being installed and developed. To achieve demanding
cost of energy targets, awarded by competitive auctions,
the operation and maintenance (O&M) of these assets has to become
increasingly efficient, whilst ensuring compliance and effectiveness.
Existing offshore wind farm assets generate a significant
amount of inhomogeneous data related to O&M processes.
These data contain rich information about the condition of the
assets, which is rarely fully utilized by the operators and service
providers. Academic and industrial research and development
efforts have led to a suite of tools trying to apply sensor data
and build machine learning models to diagnose, trend and predict
component failures. This study presents a decision support
framework incorporating a range of different supervised and unsupervised
learning algorithms. The aim is to provide guidance
for asset owners on how to select the most relevant datasets, apply
and choose the different machine learning algorithms and
how to integrate the data stream with daily maintenance procedures.
The presented methodology is tested on a real case example
of an offshore wind turbine gearbox replacement at Teesside
offshore wind farm. The study uses kNN and SVM algorithms to
detect the fault using SCADA data and an autoregressive model
for the CMS data. The implementation of all the algorithms has
resulting in an accuracy higher than 94%. The results of this
paper will be of interest to offshore wind farm developers and operators to streamline and optimize their O&M planning activities
for their assets and reduce the associated costs.This research was funded by the Energy Technology Institute
and the RCUK Energy Programme (Grant number:
EP/J500847/1) and EDF Energ",industry,864,not included
,to_check,core,Springer,2016-11-24 00:00:00,core,fundamentals of predictive text mining,,"One consequence of the pervasive use of computers is that most documents originate in digital form. Widespread use of the Internet makes them readily available. Text mining – the process of analyzing unstructured natural-language text – is concerned with how to extract information from these documents.

Developed from the authors’ highly successful Springer reference on text mining, Fundamentals of Predictive Text Mining is an introductory textbook and guide to this rapidly evolving field. Integrating topics spanning the varied disciplines of data mining, machine learning, databases, and computational linguistics, this uniquely useful book also provides practical advice for text mining. In-depth discussions are presented on issues of document classification, information retrieval, clustering and organizing documents, information extraction, web-based data-sourcing, and prediction and evaluation. Background on data mining is beneficial, but not essential. Where advanced concepts are discussed that require mathematical maturity for a proper understanding, intuitive explanations are also provided for less advanced readers.

Topics and features: presents a comprehensive, practical and easy-to-read introduction to text mining; includes chapter summaries, useful historical and bibliographic remarks, and classroom-tested exercises for each chapter; explores the application and utility of each method, as well as the optimum techniques for specific scenarios; provides several descriptive case studies that take readers from problem description to systems deployment in the real world; includes access to industrial-strength text-mining software that runs on any computer; describes methods that rely on basic statistical techniques, thus allowing for relevance to all languages (not just English); contains links to free downloadable software and other supplementary instruction material.

Fundamentals of Predictive Text Mining is an essential resource for IT professionals and managers, as well as a key text for advanced undergraduate computer science students and beginning graduate students",industry,865,not included
,to_check,core,,2017-06-12 00:00:00,core,symbiolab/vitaprint_extruder: vitaprint extruder,,"Vitaprint extruder 1.0
Table of contents

Bill of materials
Housing
Movement
Thermoregulation


Manufacturing and Assembly
Overview
Part Manufacturing
Assembly Brief
Arduino Temperature Regulation


Extruder calibration
calibration set-up
calibration procedure


Thermistor calibration

Bill of materials <a id=""billmat""></a>
Housing: <a id=""house""></a>

Aluminium plates (300mm x 400mm x 5mm)
Aluminium blocks:
SYRINGE MOUNT: block 35mm x 45mm x 70mm (minimum)
FRONT CAP: block 40mm x 12mm x 60mm (minimum)
24BYJ48 NUT: block 15mm x 25mm x 30mm (minimum)


M3x12 DIN965 philips screw 10 pcs
M3x12 BN6404 torx sxrew 20pcs
2 x fi3 stainless steel rod (L = 110mm)
40mm x 8mm x 2mm rubber sheet

Movement <a id=""mov""></a>

Stepper motor Nema11 planetary (1:5 gear ratio)
28BYJ48 unipolar motor ([modified into a bipolar motor] http://www.jangeox.be/2013/10/change-unipolar-28byj-48-to-bipolar.html )
Linear rail with carrige (L = 100mm)
Sliding rods (stainles steel, fi 3mm)
Brass spindle (M5x0.5mm, L = 90mm)
NUT: 3D printed 100% infill ABS block (28x28x55mm)

Thermoregulation <a id=""thermo""></a>

Thermistor (read how to calibrate your own thermistor in the Calibration section)
40W ceramic heater for 3D printers
24V DC power supply
In our iteration we used PID thermoregulation algorithm (parameters need to be tuned by the user, depending on the control setup). Other temperature control systems can also be applied with this given hardware (relay switchin etc.).

Manufacturing and Assembly <a id=""manuass""></a>
1 Overview and Toolset <a id=""OVER""></a>
Here you can find manufacturing and assembly details for one Vitaprint unit. In this repository you can find STEP files for the Vitaprint unit in both orientations - Left and Right. The procedure is the same for both orientations (the final product is only mirrored) therefore you will find details for assembly for one unit in this repository.
1.1 General

Manufacturing time: approximately 8h
Assembly time: 30 min

NOTE: manufacturing and assembly time estimates assume well-skilled user and presence of all the tools listed below
1.2 Tool List and Skills Required

3D printer (ABS)
CNC mill
CNC router
band saw
lathe
HAND TOOLS:
torx srewdriver
phillips screwdriver
super glue
two-sided tape
scalpel



1.3 Necessary Steps

purchase of raw materials and components
manufacturing of parts
assembly
calibration

2 Part Manufacturing <a id=""MANUFACTURING""></a>
<img src=""https://cloud.githubusercontent.com/assets/14543226/24997531/3aaafcba-2037-11e7-8800-1aba4ec7eacb.png"" alt=""bubble"" width=""450"" height=""400"">
<img src=""https://cloud.githubusercontent.com/assets/14543226/24997449/fa3d8c7e-2036-11e7-8a56-a97ed070c891.png"" alt=""table"" width=""500"" height=""600"">
3 Assembly Details <a id=""ASSEMBLY""></a>

manufacture all parts
assemble each section separately, then join them together
| SECTION A | SECTION B | SECTION C |
|-----------|-----------|-----------|
|<img src=""https://cloud.githubusercontent.com/assets/14543226/24998545/c2ab57ce-203a-11e7-9a3c-8f3a5b85e27a.png"" alt=""sectionB"" width= ""150"" > | <img src=""https://cloud.githubusercontent.com/assets/14543226/24998406/4e023a1e-203a-11e7-8f65-00fdcdb5f56f.png"" alt=""sectionB"" width= ""150"" > |<img src=""https://cloud.githubusercontent.com/assets/14543226/24998669/3aab29d4-203b-11e7-93f6-95d102f12aa9.png"" alt=""sectionB"" width= ""150"">|



Use philips DIN965 screws wherever you see the hole chamfers, use torx BN6404 screws elsewhere
for the following steps gently apply hammer:
sliding the ball bearing into the rod holder
sliding the M5 threaded rod into the coupler
sliding the fi3 stainless steel rod into rod holder at the bottom and N11 motor plate at the top



4 Arduino: Temperature Regulation <a id=""CODE""></a>
To download Arduino firmware for temperature regulation please visit vitaprint_heat_regulator. This is an external temperature control system and can be replaced by any system you may have available.
Extruder calibration <a id=""extcal""></a>
To ensure the stepper movement translates into the piston movement accordingly to the g-code, it needs to be calibrated properly.
Calibration set-up <a id=""calset""></a>
In principle, the calibration set-up requires:

CNC control of the piston powering motor (we use Nema 11 with a planetary reductor)
the assembled extruder (at least the moving parts)
a precise displacement measurement device (calipers, etc., high precision indicators are preferrable)

The calibration protocol requires a set-up, where the piston translation, can be precisely measured in the axis of extrusion. For this purpose, the measurement device is fixed on the extruder in a way, that the extruder movement is translated directly into the measurement.
Calibration procedure <a id=""calpro""></a>

adjust the steps/unit value of the motor settings in the cnc control software (or use default values)
command piston movement for an exact distance
measure the real movement distance and compare
repeat steps 1-3, until the g-code value coincides with the measured value

For a Nema 11 motor (with a planetary reductor) + an M5 spindle (0.5mm pitch/rotation), the default STEPS/UNIT value is 37914.30.
Thermistor calibration <a id=""thermcal""></a>
Every thermistor analog output values have to be mapped to actual temperature values via an equation. Every thermistor needs to be calibrated as described in this section.
EQUIPMENT: thermo block/plate (heat source), alcochol thermometer, thermistor, setup for reading analog signal from the thermistor, oil, glass.

pour a small glass of oil (50mL or so)
place the glass on the heat source
fit a thermistor to the alcochol thermometer
sink the thermistor - thermometer setup into the glass with oil
prepare the system to read analog values 
turn on the heat source and record sensor analog output at every 1°C increment
plot data (x - axis: analog output, y-axis: real temperature)
fit the best trendline to the data obtained (MATLAB or EXCEL are good tools for completing this task but there are also others)
software will yield a fit line equation which then has to be transcribed into microcontroller firmware (we used Arduino Mega)

Now your thermistor will give out the right temperature values.
step_vitaprint.zi",industry,866,not included
,to_check,core,,2017-01-01 00:00:00,core,bayesian modeling for optimization and control in robotics,,"Robotics has the potential to be one of the most revolutionary technologies in human history. The impact of cheap and
potentially limitless manpower could have a profound influence on our everyday life and overall onto our society. As
envisioned by Iain M. Banks, Asimov and many other science fictions writers, the effects of robotics on our society might
lead to the disappearance of physical labor and a generalized increase of the quality of life. However, the large-scale
deployment of robots in our society is still far from reality, except perhaps in a few niche markets such as manufacturing.
One reason for this limited deployment of robots is that, despite the tremendous advances in the capabilities of the
robotic hardware, a similar advance on the control software is still lacking. The use of robots in our everyday life is still
hindered by the necessary complexity to manually design and tune the controllers used to execute tasks. As a result,
the deployment of robots often requires lengthy and extensive validations based on human expert knowledge, which
limit their adaptation capabilities and their widespread diffusion. In the future, in order to truly achieve an ubiquitous
robotization of our society, it is necessary to reduce the complexity of deploying new robots in new environments and
tasks.
The goal of this dissertation is to provide automatic tools based on Machine Learning techniques to simplify and
streamline the design of controllers for new tasks. In particular, we here argue that Bayesian modeling is an important tool
for automatically learning models from raw data and properly capture the uncertainty of the such models. Automatically
learning models however requires the definition of appropriate features used as input for the model. Hence, we present
an approach that extend traditional Gaussian process models by jointly learning an appropriate feature representation
and the subsequent model. By doing so, we can strongly guide the features representation to be useful for the subsequent
prediction task.
A first robotics application where the use of Bayesian modeling is beneficial is the accurate learning of complex dynamics models. For highly non-linear robotic systems, such as in presence of contacts, the use of analytical system
identification techniques can be challenging and time-consuming, or even intractable. We introduce a new approach for
learning inverse dynamics models exploiting artificial tactile sensors. This approach allows to recognize and compensate
for the presence of unknown contacts, without requiring a spatial calibration of the tactile sensors. We demonstrate
on the humanoid robot iCub that our approach outperforms state-of-the-art analytical models, and when employed in
control tasks significantly improves the tracking accuracy.
A second robotics application of Bayesian modeling is automatic black-box optimization of the parameters of a controller. When the dynamics of a system cannot be modeled (either out of complexity or due to the lack of a full state
representation), it is still possible to solve a task by adapting an existing controller. The approach used in this thesis is
Bayesian optimization, which allows to automatically optimize the parameters of the controller for a specific task. We
evaluate and compare the performance of Bayesian optimization on a gait optimization task on the dynamic bipedal
walker Fox. Our experiments highlight the benefit of this approach by reducing the parameters tuning time from weeks
to a single day.
In many robotic application, it is however not possible to always define a single straightforward desired objective.
More often, multiple conflicting objectives are desirable at the same time, and thus the designer needs to take a decision
about the desired trade-off between such objectives (e.g., velocity vs. energy consumption). One framework that is
useful to assist in this decision making is the multi-objective optimization framework, and in particular the definition of
Pareto optimality. We propose a novel framework that leverages the use of Bayesian modeling to improve the quality
of traditional multi-objective optimization approaches, even in low-data regimes. By removing the misleading effects
of stochastic noise, the designer is presented with an accurate and continuous Pareto front from which to choose the
desired trade-off. Additionally, our framework allows the seamless introduction of multiple robustness metrics which can
be considered during the design phase. These contributions allow an unprecedented support to the design process of
complex robotic systems in presence of multiple objective, and in particular with regards to robustness.
The overall work in this thesis successfully demonstrates on real robots that the complexity of deploying robots to solve
new tasks can be greatly reduced trough automatic learning techniques. We believe this is a first step towards a future
where robots can be used outside of closely supervised environments, and where a newly deployed robot could quickly
and automatically adapt to accomplish the desired tasks",industry,867,included
,to_check,core,Niterói,2017-07-14 00:00:00,core,aplicação de redes neurais artificiais na estimativa da permeabilidade usando perfis de poços do campo de namorado,,"A perfilagem de poços é uma ferramenta essencial para a indústria de petróleo. Em poucas palavras, pode ser resumida como a caracterização de propriedades petrofísicas e geológicas representadas graficamente pela associação com a profundidade do poço. É através da análise dos perfis que é possível identificar os tipos de rochas, a localização de hidrocarbonetos, pode-se estimar a viabilidade do poço e permite o reaproveitamento de poços já explorados.Esse trabalho aponta uma das medidas alternativas utilizadas hoje em dia para otimizar o processo de interpretação das respostas da perfilagem: o software Interactive Petrophysics que conta com a metodologia de redes neurais artificiais utilizando um algoritmo interpretativo usado para treinar a rede e simular uma saída esperada. As redes neurais artificiais possuem a capacidade de associar as informações de entrada e ponderá-las através de pesos no processo de aprendizagem. A metodologia do processo consistiu em utilizar informações petrofísicas reais provindas de perfis geofísicos e também dados de análise de testemunho fornecidos pela ANP (Agência Nacional de Petróleo, Gás Natural e Bicombustíveis) de três poços produtores pertencentes ao campo de Namorado (campo escola), os mais próximos possíveis, e assim tais informações foram usadas como os dados de entrada e para treinamento da rede para estimar a saída esperada, que, para esse caso, foi a permeabilidade dos poços. E, no final do trabalho, foi feita uma análise comparativa com os dados reais obtidos pela análise de testemunho com a saída do simulador.Well logging is an essential tool for the oil industry. In a few words, it can be described as the characterization of petrophysics and geological properties registered in association with the depth of the well. It is through the analysis of the logs that it is possible to determinate the type of rocks, the hydrocarbons localization, it can be estimated the viability of the well and the return of wells already explored. The well logging activity occurs during the exploration phase of the well. In this study, a brief presentation of the qualitative and quantitative properties of the logs will be carried out, as well as their importance for the characterization of the wells; on top of that the main logs used in the study of the case of the Namorado Field will be presented. This paper shows one of the alternative measures to accelerate the well logging process: The Interactive Petrophysics software presents an artificial neural network logic that uses an interpretative algorithm to train and simulate an expected output. The artificial neural networks can associate the input data and weight them through the learning process. This complex logic will also be presented in this paper so it can be better understood. The methodology of the process consisted in taking the input data, the logs and the core analysis, provided by ANP (Nacional Petroleum Agency) of three wells specifically selected, and then train this input data to provide the output permeability. At the end of this study it was made a comparative analysis with the real data and the one that came out of the process",industry,869,not included
,to_check,core,,2018-07-12 00:00:00,core,>doom jolt-ante,http://ualresearchonline.arts.ac.uk/15048/3/draft%232_DC_draftDOOMJOLT_ANTEV2.pdf,"Illustration as a mode of cultural production is an exponentially expanding phenomenon. Illustration students form a significant and substantial constituency within the Art and Design education sector. The future of Illustration disciplinarians is assured, or so it may seem. It is conceivable that the volume of Illustration graduates entering the ‘job (less) market’ exceeds real demand for conventional, transactional, freelance, commercial Illustration work. This constitutes an existential threat for the discipline and increased precarity for Illustration graduates. Add to this the imminence of wide scale Automation (Chui et al), the deployment of Artificial Intelligence in to the creative sector and the normalization of Self-Entrepreneurship (Deresiewicz) and we have a potent and destabilizing ‘mix’ that poses serious ethical and operational questions to educators, institutions and the industry at large",industry,870,not included
,to_check,core,,2016-03-28 00:00:00,core,'biblioteca central da unb',Intelligent agents for simultaneously control of width and height of weld beads of GMAW-S process,"Tese (mestrado)—Universidade de Brasília, Faculdade de Tecnologia, Departamento de Engenharia Mecânica, 2016.O processo de soldagem GMAW é um dos mais utilizado na produção industrial, devido, entre outras características, a seu alto grau de automação e a vantagem de se poder utilizar em diversas configurações com a maioria dos metais e ligas comerciais existentes. No caso da geometria dos cordões de solda no processo GMAW, diferentes pesquisas têm sido encaminhadas ao controle dos parâmetros operacionais que garantam as características geométricas requeridas, entre as mais importantes, a largura, o reforço e a penetração. Atualmente, diferentes modelos baseados em modelamento empírico ou em inteligência artificial são utilizados para controlar um parâmetro geométrico à vez. Este trabalho propõe uma estratégia que, independente de modelos predefinidos do processo, permite controlar simultaneamente a largura e o reforço dos cordões de solda no processo de soldagem GMAW no modo de transferência metálica por curto-circuito (GMAW-S). O controlador proposto é baseado em agentes inteligentes focados diretamente nas medições de largura e reforço dos cordões de solda. O monitoramento dos parâmetros geométricos é realizado em tempo real utilizando uma única câmera e diferentes metodologias de processamento digital de imagens. A avaliação da estabilidade do processo é realizada em tempo real e emprega-se para sair das regiões de instabilidade nas quais possa incorrer o processo durante a etapa de controle. A metodologia de monitoramento é avaliada como satisfatória utilizando o teste “t” para diferentes combinações de parâmetros de entrada. O tempo de processamento de cada imagem não supera os 3 ms, considerando-se adequado visando etapas de controle com uma taxa de amostragem de 100 Hz. Os resultados experimentais mostram que a implementação da estratégia de controle proposta é viável e consegue atingir simultaneamente diferentes valores de referência de largura e reforço dos cordões de solda.The GMAW process is widely used in industry due to, among others, its easer automation and high productivity. In the case of weld bead geometry in GMAW processes, different researches have been conducted to control operating parameters and to ensure required geometrical characteristics, among the most important: the width, the height and the penetration. Currently, different models, based on empirical modeling or artificial intelligence methodologies, are used to control individual geometric parameters. This work proposes a strategy that, regardless of predefined models, can simultaneously control the width and the height of the weld beads in GMAW-S process. The proposed control system is based on intelligent agents focus on measurements of the weld bead width and height. The geometric parameters monitoring is performed in real time using a single camera and different methods of digital image processing. The evaluation of process stability is performed in real time and employed to avoid the regions of instability in which may incur this process during the control stage. The monitoring methodology is assessed as satisfactory using the “t” test for different combinations of input parameters. The time of the image processing does not exceed 3 ms for each image and is considered appropriate to control steps, which use a 100 Hz sampling rate. The experimental results show that the implementation of the proposed control strategy is feasible in systems control without predefined model, achieving different width and height reference bead values",industry,871,not included
,to_check,core,,2016-01-01 00:00:00,core,ecms european council for modelling and simulation,A Software Framework For Intelligent Computer-Automated Product Design,"For many years, NTNU in Ålesund (formerly Aalesund University College) has maintained a close relationship with the maritime industrial cluster, centred in the surrounding geographical region, thus acting as a hub for both education, research, and innovation. Of many common relevant research topics, virtual prototyping is currently one of the most important. In this paper, we describe our first complete version of a generic and modular software framework for intelligent computer-automated product design. We present our framework in the context of design of offshore cranes, with easy extensions to other products, be it maritime or not. Funded by the Research Council of Norway and its Programme for Regional R&D and Innovation (VRI), the work we present has been part of two separate but related research projects (grant nos. 241238 and 249171) in close cooperation with two local maritime industrial partners. We have implemented several software modules that together constitute the framework, of which the most important are a server-side crane prototyping tool (CPT), a client-side web graphical user interface (GUI), and a client-side artificial intelligence for product optimisation (AIPO) module that uses a genetic algorithm (GA) library for optimising design parameters to achieve a crane design with desired performance. Communication between clients and server is achieved by means of the HTTP and WebSocket protocols and JSON as the data format. To demonstrate the feasibility of the fully functioning complete system, we present a case study where our computer-automated design was able to improve the existing design of a real and delivered 50-tonnes, 2.9 million EUR knuckleboom crane with respect to some chosen desired design criteria. Our framework being generic and modular, both clientside and server-side modules can easily be extended or replaced. We demonstrate the feasibility of this concept in an accompanying paper submitted concurrently, in which we create a simple product optimisation client in Matlab that uses readily available toolboxes to connect to the CPT and optimise various crane designs by means of a GA. In addition, our research team is currently developing a winch prototyping tool to which our existing AIPO module can connect and optimise winch designs with only small configuration changes. This work will be published in the near future",industry,872,not included
,to_check,core,Wiley-Blackwell,2011-01-01 00:00:00,core,optimizing the operating conditions in a high precision industrial process using soft computing techniques,,"This interdisciplinary research is based on the application of unsupervized connectionist architectures in conjunction with modelling systems and on the determining of the optimal operating conditions of a new high precision industrial process known as laser milling. Laser milling is a relatively new micro-manufacturing technique in the production of high-value industrial components. The industrial problem is defined by a data set relayed through standard sensors situated on a laser-milling centre, which is a machine tool for manufacturing high-value micro-moulds, micro-dies and micro-tools. The new three-phase industrial system presented in this study is capable of identifying a model for the laser-milling process based on low-order models. The first two steps are based on the use of unsupervized connectionist models. The first step involves the analysis of the data sets that define each case study to identify if they are informative enough or if the experiments have to be performed again. In the second step, a feature selection phase is performed to determine the main variables to be processed in the third step. In this last step, the results of the study provide a model for a laser-milling procedure based on low-order models, such as black-box, in order to approximate the optimal form of the laser-milling process. The three-step model has been tested with real data obtained for three different materials: aluminium, cooper and hardened steel. These three materials are used in the manufacture of micro-moulds, micro-coolers and micro-dies, high-value tools for the medical and automotive industries among others. As the model inputs are standard data provided by the laser-milling centre, the industrial implementation of the model is immediate. Thus, this study demonstrates how a high precision industrial process can be improved using a combination of artificial intelligence and identification techniques",industry,874,not included
,to_check,core,'Elsevier BV',2013-01-01 00:00:00,core,new trends on soft computing models in industrial and environmental applications,,"The twelve papers included in this special issue represent a selection of extended contributions presented at the Sixth International Conference on Soft Computing Models in Industrial and Environmental Applications, held in Salamanca, Spain, 6–8th April, 2011. Papers were selected on the basis of fundamental ideas and concepts rather than the direct usage of well-established techniques. This special issue is then aimed at practitioners, researchers and post-graduate students, who are engaged in developing and applying advanced Soft Computing Models to solving real-world problems in the Industrial and Environmental fields. The papers are organized as follows. In the first contribution, Graña and Gonzalez-Acuña develop a formulation of dendritic classifiers based on lattice kernels and train them using a direct Monte Carlo approach and a Sparse Bayesian Learning. The results of both kinds of training are compared with the Relevance Vector Machines on a collection of benchmark datasets. In the second contribution by Irigoyen and Miñano, the authors present the results of the identification of the relationship in time, between the required exercise (machine resistance) and the heart rate of the patient in medical effort tests, using a NARX neural network model. In the experimental stage, test data have been obtained by exercising with a cyclo-ergometer in two different tests: Power Step Response and Conconi. Carneiro et al. in the third contribution present a biologically inspired method to deal with the problem in which genetic algorithms are used to create possible solutions for a given dispute. The approach presented is able to generate a broad number of diverse solutions that cover virtually the whole search space for a given problem. The results of this work are being applied in a negotiation tool that is part of the UMCourt conflict resolution platform. In the fourth contribution by Donate et al., they propose a novel Evolutionary Artificial Neural Networks (EANN) approach, where a weighted n-fold validation fitness scheme is used to build an ensemble of neural networks, under four different combination methods: mean, median, softmax and rank-based combinations. Several experiments were held, using six real-world time series with different characteristics and from distinct domains. Overall, the proposed approach achieved competitive results when compared with non weighted n-fold EANN ensembles, the simpler 0-fold EANN and also the popular Holt–Winters statistical method. Dan Burdescu et al. in the fifth contribution, present a system used in the medical domain for three distinct tasks: image annotation, semantic based image retrieval and content based image retrieval. An original image segmentation algorithm based on a hexagonal structure was used to perform the segmentation of medical images. Image's regions are described using a vocabulary of blobs generated from image features using the K-means clustering algorithm. The annotation and semantic based retrieval task is evaluated for two annotation models: Cross Media Relevance Model and Continuous-space Relevance Model. Semantic based image retrieval is performed using the methods provided by the annotation models. The ontology used by the annotation process was created in an original manner starting from the information content provided by the Medical Subject Headings (MeSH). The experiments were made using a database containing colour images retrieved from medical domain using an endoscope and related to digestive diseases. In sixth paper by Pedraza et al., they develop a face recognition system based on soft computing techniques, which complies with privacy-by-design rules and defines a set of principles that context-aware applications (including biometric sensors) should contain to conform to European and US law. This research deals with the necessity to consider legal issues concerning privacy or human rights in the development of biometric identification in ambient intelligence systems. Clearly, context-based services and ambient intelligence (and the most promising research area in Europe, namely ambient assisted living, ALL) call for a major research effort on new identification procedures. The aim of the research by Redel-Macías et al. in paper seven is to develop a novel model which can be used in pass-by noise test in vehicles based on ensembles of hybrid Evolutionary Product Unit or Radial Basis function Neural Networks (EPUNN or ERBFNNs) at high frequencies. Statistical models and ensembles of hybrid EPUNN and ERBFNN approaches have been used to develop different noise identification models. The results obtained using different ensembles of hybrid EPUNNs and ERBFNNs show that the functional model and the hybrid algorithms proposed provide a very accurate identification compared to other statistical methodologies used to solve this regression problem. In the eighth paper, Wu et al. analyse the existence criterion of loop strategies, and then present some corollaries and theorems, by which the loop strategies and chain strategies can be found, also superfluous strategies and inconsistent strategies. It presents a ranking model that indicates the weak node in strategy set and it also introduces a probability-based model which is the basis of evaluation of strategy. Additionally, this research proposes a method to generate offensive strategy, and the statistic results of simulation game prove the validity of the method. Pop et al. in the ninth paper present an efficient hybrid heuristic algorithm obtained by combining a genetic algorithm (GA) with a local–global approach to the generalized vehicle routing problem (GVRP) and a powerful local search procedure. The computational experiments on several benchmarks instances show that the hybrid algorithm is competitive to all of the known heuristics published to date. In the tenth paper Kramer et al. illustrate how methods from neural computation can serve as forecasting, and monitoring techniques, contributing to a successful integration of wind into sustainable, and smart energy grids. The study is based on the application of kernel methods like support vector regression and kernel density estimation as prediction methods. Furthermore, dimension reduction techniques like self-organizing maps for monitoring of high-dimensional wind time series are applied. The methods are briefly introduced, related work is presented, and experimental case studies are exemplarily described. The experimental parts are based on real wind energy time series data from the NREL western wind resource dataset. Vera et al. in the eleventh contribution present a novel soft computing procedure based on the application of artificial neural networks, genetic algorithms and identification systems, which makes it possible to optimise the implementation conditions in the manufacturing process of high precision parts, including finishing precision, while saving both time and financial costs and/or energy. The novel proposed approach was tested under real dental milling processes using a high precision machining centre with five axes, requiring high finishing precision of measures in micrometres with a large number of process factors to analyse. The results of the experiment, which validate the performance of the proposed approach, are presented in this study. The final contribution, by Sakalauskas and Kriksciuniene, presents a research about financial market efficiency and to recognize major reversal points of long-term trend of stock market index, which could indicate forthcoming crisis or market raise periods. The study suggests a computational model of financial time series analysis, which combines several approaches of soft computing, including information efficiency evaluation methods (Shannon's entropy, Hurst exponent), neural networks and sensitivity analysis. The model aims to derive the aggregated measure for evaluating efficiency of the financial market and to find its interrelationships with the reversal of long-term trend. The radial basis function neural network was designed for forecasting moments of cardinal changes in stock market behaviour, expressed by its entropy values derived from the symbolized time series of stock market index. The performance of neural network model is explored by applying sensitivity analysis and resulted in selecting smoothing parameters of the input variables. The experimental research investigates behaviour of the long-term trend of the three emerging financial markets within NASDAQ OMX Baltic stock exchange. Introduction of information efficiency measures improve ability of the model to recognize the approaching reversal of long-term trend from temporary market “nervousness” and can be useful for calibrating stock trading strategy. First, we would like to thank all the authors for their valuable contributions, which made this special issue possible. We also like to thank our peer-reviewers for their timely diligent work and efficient efforts. We are also grateful to the Editor-in-Chief of Neurocomputing Journal, Prof. Tom Heskes, for his continued support for the SOCO series of conferences and for this Special Issue on this prestigious journal. Finally, we hope the reader will share our joy and find this special issue very useful",industry,875,not included
,to_check,core,,2009-01-28 00:00:00,core,real time 3-d graphics processing hardware design using field-programmable gate arrays.,,"Three dimensional graphics processing requires many complex algebraic and matrix based operations to be performed in real-time. In early stages of graphics processing, such tasks were delegated to a Central Processing Unit (CPU). Over time as more complex graphics rendering was demanded, CPU solutions became inadequate. To meet this demand, custom hardware solutions that take advantage of pipelining and massive parallelism become more preferable to CPU software based solutions. This fact has lead to the many custom hardware solutions that are available today. Since real time graphics processing requires extreme high performance, hardware solutions using Application Specific Integrated Circuits (ASICs) are the standard within the industry. While ASICs are a more than adequate solution for implementing high performance custom hardware, the design, implementation and testing of ASIC based designs are becoming cost prohibitive due to the massive up front verification effort needed as well as the cost of fixing design defects.Field Programmable Gate Arrays (FPGAs) provide an alternative to the ASIC design flow. More importantly, in recent years FPGA technology have begun to improve in performance to the point where ASIC and FPGA performance has become comparable. In addition, FPGAs address many of the issues of the ASIC design flow. The ability to reconfigure FPGAs reduces the upfront verification effort and allows design defects to be fixed easily. This thesis demonstrates that a 3-D graphics processor implementation on and FPGA is feasible by implementing both a two dimensional and three dimensional graphics processor prototype. By using a Xilinx Virtex 5 ML506 FPGA development kit a fully functional wireframe graphics rendering engine is implemented using VHDL and Xilinx's development tools. A VHDL testbench was designed to verify that the graphics engine works functionally. This is followed by synthesizing the design and real hardware and developing test applications to verify functionality and performance of the design. This thesis provides the ground work for push forward the use of FPGA technology in graphics processing applications",industry,876,not included
,to_check,core,,2007-01-01 00:00:00,core,1 unsupervised anomaly detection in large databases using bayesian networks,,"Today, there has been a massive proliferation of huge databases storing valuable information. The opportunities of an effective use of these new data sources are enormous, however, the huge size and dimensionality of current large databases call for new ideas to scale up current statistical and computational approaches. This paper presents an application of Artificial Intelligence technology to the problem of automatic detection of candidate anomalous records in a large database. We build our approach with three main goals in mind: 1)An effective detection of the records that are potentially anomalous, 2)A suitable selection of the subset of attributes that explains what makes a record anomalous, and 3)An efficient implementation that allows us to scale the approach to large databases. Our algorithm, called Bayesian Network Anomaly Detector (BNAD), uses 2 the joint probability density function (pdf) provided by a Bayesian Network (BN) to achieve these goals. By using appropriate data structures, advanced caching techniques, the flexibility of Gaussian Mixture models, and the efficiency of BNs to model joint pdfs, BNAD manages to efficiently learn a suitable BN from a large dataset. We test BNAD using synthetic and real databases, the latter from the fields of manufacturing and astronomy, obtaining encouraging results",industry,877,not included
,to_check,core,,2007-11-22 00:00:00,core,(al) and an application program (known as an,,"The development and deployment of multi-agent systems in real world settings raises a number of important research issues and problems which must be overcome if Distributed AI (DAI) is to become a widespread solution technology. Work undertaken in the context of the ARCHON project has provided a number of important insights into these issues. By providing an in depth analysis of ARCHON&apos;s electricity transportation management application, this paper draws together many of the experiences obtained when building one of the world&apos;s first operational DAI systems.  Introduction  In many industrial applications a substantial amount of time, effort and finance has been devoted to developing complex and sophisticated software systems. These systems are often viewed in a piecemeal manner as isolated islands of automation, when, in reality, they should be seen as components of a much larger business function (Jennings, 1994a). The main benefit of taking a holistic perspective is that the partial ..",industry,878,not included
,to_check,core,"Programa de Pós-Graduação em Engenharia de Materiais. Rede Temática em Engenharia de Materiais, Pró-Reitoria de Pesquisa e Pós-Graduação, Universidade Federal de Ouro Preto.",2008-01-01 00:00:00,core,modelo para predição da ocorrência de alarmes de colamento de aço no molde utilizando lógica fuzzy.,,"A crescente utilização de conceitos de programação avançada e inteligência artificial tem criado numerosas e novas aplicações desses algoritmos em processos industriais, resultando em expressivos ganhos de produtividade e economia de recursos. Algoritmos baseados em redes neurais tem demonstrado ser altamente acertivos, e quando empregados em conjunto a sistemas modelados em lógica fuzzy incorporam ao banco de dados subjetividade e experiências adquiridas ao longo do tempo, dotando o programa de capacidade de reagir de forma mais adequada frente a diferentes condições de processo e situações inesperadas. No processo de lingotamento contínuo é realizada a solidificação do aço na forma de placas, as quais serão enviadas para processamentos posteriores, exigindo que sua aparência superficial seja livre de irregularidades. Elevados requisitos de qualidade visam garantir maior produtividade, estabilidade operacional e acabamento superficial das bobinas laminadas. O primeiro estágio da solidificação do aço se inicia no molde onde o aço líquido, em contato com as faces do molde, forma uma casca externa solidificada. Um dos fenômenos mais prejudiciais à produção e manutenção da regularidade superficial da placa é o colamento do aço na face cobreada do molde, que leva a ruptura da pele de aço em formação. O “rompimento de pele” como é denominado, gera parada imediata e emergencial do lingotamento contínuo, bem como o sucatamento das placas em produção. De forma a se evitar a ocorrência de rompimentos de pele, os moldes de cobre possuem duas linhas de termopares instalados em suas faces para monitoramento em tempo real das condições de troca de calor durante o lingotamento. Um programa faz a comparação contínua da evolução do perfil de temperatura com os existentes em seu banco de dados, referentes a eventos de colamento de aço no molde. O programa Mold Sticker Detection (MSD) tem o objetivo de inibir a ocorrência de rompimentos de pele. Quando o MSD entende que em um determinado momento está ocorrendo o colamento de aço no molde, este gera um alarme reduzindo instantânemente a velocidade de lingotamento para valores de segurança. Essa redução abrupta tem por objetivo recompor a pele rompida, mas em contra partida causa marcas profundas na superfície da placa, além das geradas pelo próprio colamento. Este trabalho objetiva desenvolver um modelo baseado em lógica fuzzy a ser incorporado ao sistema atual de detecção de rompimento – MSD, procurando reduzir a geração de alarmes de colamento através da determinação da velocidade de lingotamento ideal condizentes com as condições atuais de extração de calor no molde. Para medição da eficácia do modelo proposto foram avaliadas as perdas de produtividade, bem como o número de ocorrência de alarmes de colamento e rompimentos de pele.A growing application of advanced programming concepts and artificial intelligence, has created a numerous and new usages for these algorithms in industrial processes resulting in expressive gains on productivity and resource savings. Algorithms based in neural networks has been proving its higher sharpness, and when applied together with fuzzy conceived systems may embody to the data basis subjectivity and experiences acquired through time, dowering the program of capacity to react more properly under different process conditions and uncommon situations. During the continuous casting process, the steel is solidified under a slab shape, which will be forward to post processes, demanding a smooth and roughness free surface appearance. Higher quality requirements aim to guarantee productivity increase, operational stability and greater superficial finishing of the laminated coils. The very first stage of steel solidification has its start on the mould where the liquid steel, facing the mould copper plates, forms an external solidified shell. One of the most harmful phenomena to the production regularity maintenance and superficial evenness of the slab is the steel sticking to the copper mould plates, which leads to the rupture of the solidifying shell. The shell break-out, as nominated, causes instant and emergency stoppage of the continuous casting machine and inproduction slabs scrapping. In order to avoid break-out occurrences, the mould has two thermocouple lines installed on the plates for real time monitoring of the heat transfer conditions during casting. A software compares the actual thermal profile with the existent data base concerning sticker events on the mould. The program Mold Sticker Detection (MSD) is used to inhibit break-outs. When the MSD understands that in a specific time a sticking is progress, it generates an alarm reducing instantaneously the casting speed to a safety value. This sudden reduction is provided to give enough time to have the steel shell rebuilt, but also marks deeply the slab surface besides those deriving from the sticking itself. This work has the objective to develop a fuzzy based model to be incorporated to the existing break-out detect system – MSD, aiming to reduce the occurrences of sticker alarms by establishing the ideal casting speed concerning the actual heat extraction conditions on the mould. To measure the effectiveness of the proposed model, will be evaluated the production losses, number of sticker alarms and break-outs as well",industry,879,not included
,to_check,core,,2008-04-03 00:00:00,core,a medical claim fraud/abuse detection system based on data mining: a case study in chile,,"Abstract — This paper describes an effective medical claim fraud/abuse detection system based on data mining used by a Chilean private health insurance company. Fraud and abuse in medical claims have become a major concern within health insurance companies in Chile the last years due to the increasing losses in revenues. Processing medical claims is an exhausting manual task carried out by a few medical experts who have the responsibility of approving, modifying or rejecting the subsidies requested within a limited period from their reception. The proposed detection system uses one committee of multilayer perceptron neural networks (MLP) for each one of the entities involved in the fraud/abuse problem: medical claims, affiliates, medical professionals and employers. Results of the fraud detection system show a detection rate of approximately 75 fraudulent and abusive cases per month, making the detection 6.6 months earlier than without the system. The application of data mining to a real industrial problem through the implementation of an automatic fraud detection system changed the original non-standard medical claims checking process to a standardized process helping to fight against new, unusual and known fraudulent/abusive behaviors. I",industry,880,not included
,to_check,core,,2009-10-28 00:00:00,core,the issue of quantization effect in direct implementation of adaptive lq controller with nn identification into plc,,"The direct implementation of the control algorithm from the simulation environment to the industrial process controller is the aim of many current papers. The sequence of direct implementation usually consists of three steps: simple simulation experiment, real time communication with the real process and monitoring of the control algorithm after implementation. The direct implementation enables us to describe the quantization effect. The issue of quantization effect disturbance given by A/D and D/A converters is often forgotten. This paper shows the second and the third implementation step where the first one was described in the 10th Zittau Fuzzy Colloquium. The advantage of identification based on neural networks in adaptive LQ controller that overcomes presented problem is introduced below",industry,881,not included
,to_check,core,,2009-01-01 00:00:00,core,establishing an ad hoc infrastructure for innovative technology deployment: the case of knowledge-based systems,,"In this paper, referring to the Model for  General Knowledge Management within the 
Enterprise (MGKME), we emphasize two of the operating elements of this model, which are 
essential to insure the organizational learning process that leads people to appropriate and use 
concepts, methods and tools of KM considered as an innovative technology: the “Ad hoc 
Infrastructures” element, and the “Organizational Learning Processes” element. The Nonaka’s SECI 
models, and the Japanese concept of Ba, underlie these two elements. The case of the “Semi-opened 
Infrastructure” model implemented to deploy Artificial Intelligence and Knowledge-based Systems 
within a large industrial company illustrates what could be the application of these concepts in the 
real field. Meanwhile, we partially validate MGKME. Furthermore, we consolidate the “Semi-
opened Infrastructure” model, which becomes a pattern of reference allowing implementing an “Ad 
hoc Infrastructure” for innovative technologies deployment.ou",industry,882,not included
,to_check,core,'EDUFU - Editora da Universidade Federal de Uberlandia',2011-09-05 00:00:00,core,integração de princípios de desenvolvimento ágil de software ao rup - um estudo empírico,https://core.ac.uk/download/195900126.pdf,"Software development processes are now essential for an organization to obtain acceptable
levels of productivity and quality. The integration of agile and traditional development
processes is an open and few explored research area, which has attracted the interest of
industrial and academic communities in order to take advantage of the strengths of both
approaches. However, little is known about the real benefits of existing proposals, as
studies are still preliminary and evidence is very sparse. This research aims to investigate
the best options for agile and traditional integration by defining a hybrid process that takes
advantage of both approaches. A proposal to integrate the practices of Scrum agile method
within a development process based on RUP   Rational Unified Process   was made based
on some indications and results in the literature. An empirical study aiming to evaluate the
productivity impact of that hybrid Scrum-RUP proposal was also carried out. Five groups
of similar projects from a CMMI-ML2 medium-sized company were compared with
respect to productivity, some of which were developed using the new Scrum-RUP process
and others were developed using the other RUP-based process the company was used to
employ. Also interviews were held with developers who participated in the projects to
identify the causes of productivity results. Quantitative results have shown that four out of
five project groups showed significant productivity increase in Scrum-RUP projects. The
results of the interviews have shown that the main causes of productivity increase were
related to process, of which the most frequent were communication, collaboration and
reduction of documentation. The study shows that it is possible to integrate Scrum
practices in the software development process without losing the rigor needed in the
desired subprocesses and still get real development productivity gain.Doutor em CiênciasProcessos de desenvolvimento de software são atualmente imprescindíveis para uma
organização obter níveis aceitáveis de produtividade e qualidade. A integração de
processos de desenvolvimento de software ágeis e tradicionais é uma área de pesquisa
aberta e pouco explorada que tem atraído o interesse das comunidades acadêmica e
industrial com o intuito de se aproveitar os pontos fortes das duas abordagens. Entretanto,
pouco ainda se sabe sobre os reais benefícios das propostas existentes, pois os estudos
ainda são preliminares e as evidências muito esparsas. Esta pesquisa tem o objetivo de
investigar as melhores opções de integração ágil e tradicional, definindo um processo
híbrido que aproveite os pontos fortes de ambas as abordagens. Foi elaborada uma
proposta de integração de práticas do método ágil Scrum dentro de um processo de
desenvolvimento baseado no processo RUP   Rational Unified Process com base em
algumas indicações e resultados encontrados na literatura. Também foi realizado um
estudo de caso comparativo multi-projeto com o intuito de avaliar o impacto em
produtividade com a adoção desta proposta híbrida Scrum-RUP. Foram comparadas as
produtividades de cinco grupos de projetos similares desenvolvidos em uma empresa
CMMI-ML2 de porte médio, dentre os quais alguns usaram o novo processo Scrum-RUP e
outros usaram um processo baseado em uma customização RUP que a empresa já utilizava.
Também foram realizadas entrevistas com desenvolvedores que participaram dos projetos
no intuito de investigar as possíveis causas dos resultados de produtividade. Os resultados
quantitativos mostraram que, dos cinco grupos comparados, quatro apresentaram aumento
significativo na produtividade dos projetos Scrum-RUP. Os resultados das entrevistas
mostraram que as principais causas de aumento de produtividade estavam relacionadas ao
processo Scrum-RUP, sendo comunicação, colaboração e diminuição da documentação as
mais frequentes. O estudo mostra que é possível inserir práticas Scrum no processo de
desenvolvimento de software sem eliminar o rigor nos subprocessos necessários e, mesmo
assim, obter ganhos reais de produtividade no desenvolvimento",industry,883,not included
,to_check,core,'Informa UK Limited',2011-01-01 00:00:00,core,implementation of a quantitative real-time pcr assay for the detection of mycobacterium immunogenum in metalworking fluids,,"The recently described human pathogen M. immunogenum (Wilson et al., 2001; Loots et al., 2005; Sampaio et al., 2006) has been implicated in the causation of hypersensitivity pneumonitis (HP) in automobile workers and other occupational workers exposed to metal working fluids (MWF; Moore et al., 2000; Falkinham, 2003; Beckett et al., 2005). The bacterium has as closest relatives M. chelonae, M. abscessus and collectively these bacteria form the M. chelonae complex. The early detection of M. immunogenum and other bacteria implicated in the causation of HP may be crucial to reducing its prevalence. As detection by conventional techniques such as cultured is severely limited, an M. immunogenum-specific quantitative real-time PCR system was developed (Rhodes et al., 2008).  This technique made use of a 5’ nuclease assay (Taqman) to detect fluorescence in real time upon specific amplification of target DNA. When initially validated laboratory strains and on industrial metal working fluids from the UK and USA from which no M. immunogenum CFU were recovered, the assay detected between 3.4 x101 and 1.9 x 104 cell equivalents (CE) per ml, and increased the detection rate over culture to 37.5% (12 of 32 samples; Rhodes et al., 2008). The initial description of the assay gave an indication as to its potential for routine assessment of coolant samples for contamination with this important organism. However, the true value of the technique will become apparent only after more in depth study and application to industrial systems.

The present report describes further testing of the assay and direct comparison against culture, with its implementation on a larger scale on a range of different contaminated coolant samples obtained from the USA and Europe over a 19 month period (from December 2006 to July 2008)",industry,884,not included
,to_check,core,Programa de Pós-Graduação em Engenharia Elétrica,2011-06-22 00:00:00,core,estimação da porcentagem de flúor em alumina fluoretada proveniente de uma planta de tratamento de gases por meio de um sensor virtual neural,,"The industries have been often seeking to reduce operating expenses, as to increase profits and competitiveness. To achieve this goal, it must take into account, among other factors, the design and implementation of new tools that accurately, efficiently and inexpensively allow access to information relevant to process. Soft sensors have been increasingly applied in
industry. Since it offers flexibility, it can be adapted to make estimations of any measurement, thus a reducing in operating costs without compromising the measurements, and in some cases even improve the quality of generated information. Since they are completely softwarebased, they are not subjected to physical damage as the real sensors, and are better adaptated to harsh environments with hard access. The success of this king of sensors is due to the use
of computational intelligence techniques, which have been widely used in the modeling of several nonlinear complex processes. This work aims to estimate the quality of alumina
fluoride from a Gas Treatment Center (GTC), which is the result of gaseous adsorption on
alumina virgin, using a soft sensor. The model that emulates the behavior of a alumina quality sensor the plant was created using an artificial intelligence technique known as Artificial Neural Network. The motivations of this work are: perform virtual simulations without compromising the GTC and make accurate decisions based not only on the operator's experience, to diagnose potential problems before they can interfere with the quality of alumina fluoride; maintain the aluminum reduction pot control variables within normal limits,
since the production from low quality alumina strongly affects the reaction of breaking the molecule that contains this metal. The benefits this project brings include: increasing the GTC efficiency, producing high quality fluoridated alumina and emitting fewer greenhouse gases into the atmosphere and increasing the pot lifespan.FAPESPA - Fundação Amazônia de Amparo a Estudos e PesquisasCVRD - Companhia Vale do Rio DoceAs indústrias têm buscado constantemente reduzir gastos operacionais, visando o aumento do lucro e da competitividade. Para alcançar essa meta, são necessários, dentre outros fatores, o projeto e a implantação de novas ferramentas que permitam o acesso às informações relevantes do processo de forma precisa, eficiente e barata. Os sensores virtuais têm sido aplicados cada vez mais nas indústrias. Por ser flexível, ele pode ser adaptado a qualquer tipo de medição, promovendo uma redução de custos operacionais sem comprometer, e em alguns
casos até melhorar, a qualidade da informação gerada. Como estão totalmente baseados em software, não estão sujeitos a danos físicos como os sensores reais, além de permitirem uma melhor adaptação a ambientes hostis e de difícil acesso. A razão do sucesso destes tipos de sensores é a utilização de técnicas de inteligência computacional, as quais têm sido usadas na modelagem de vários processos não lineares altamente complexos. Este trabalho tem como objetivo estimar a qualidade da alumina fluoretada proveniente de uma Planta de Tratamento de Gases (PTG), a qual é resultado da adsorção de gases poluentes em alumina virgem, via sensor virtual. O modelo que emula o comportamento de um sensor de qualidade de alumina foi criado através da técnica de inteligência computacional conhecida como Rede Neural Artificial. As motivações deste trabalho consistem em: realizar simulações virtuais, sem comprometer o funcionamento da PTG; tomar decisões mais precisas e não baseada somente
na experiência do operador; diagnosticar potenciais problemas, antes que esses interfiram na qualidade da alumina fluoretada; manter o funcionamento do forno de redução de alumínio dentro da normalidade, pois a produção de alumina de baixa qualidade afeta a reação de quebra da molécula que contém este metal. Os benefícios que este projeto trará consistem em: aumentar a eficiência da PTG, produzindo alumina fluoretada de alta qualidade e emitindo menos gases poluentes na atmosfera, além de aumentar o tempo de vida útil do forno de redução",industry,885,not included
,to_check,core,,2003-07-27 00:00:00,core,programa de pós-graduação em engenharia química,Inferência de variáveis do processo de produção de penicilina G acilase por Bacillus megaterium ATCC-14945.,"The enzyme penicillin G acylase (E.C.3.5.1.11) is used in the production of 6-aminopenicillanic acid (6-APA) and 7-aminocephalosporinic acid (7-ACA), which are key intermediates for the production of &#946;-lactam antibiotics. Its industrial importance was one of the motivations for this thesis. On-line measurement and monitoring of cultivations of Bacillus megaterium ATCC 14945 in a agitated and aerated bioreactor allowed the acquisition of variables such as mol fraction of CO2 and O2 in the exhaust
gases, pH, dissolved oxygen. Batch and fed-batch runs, using either enzymatically hydrolyzed casein or a pool of free amino acids provided information concerning the extend of the cultivation, preservation of the microorganism and addition/exclusion of nutrients. Usual carbon
sources as glucose, fructose and glycerol increased the cellular mass, but did not improve the productivity of PGA. The use of amino acids resulted in a 2.5-fold increase of productivity. Adding phenyl acetic acid at the beginning of the experiments did not inhibit cell growth.
Three non-structured models of microbial growth were put forth, assuming one, two and three limiting substrates. However, these models were too simple for our purposes. Another approach was then followed, using neural networks (NN) as
softsensors. Before implementing the inference algorithm, the blank noise of the instrumentation had to be reduced. A non-conventional filter was developed, combining
a recursive NN, a moving average and a second recursive NN. The smoothed variables were the input for a second NN, for pattern recognition, which classified the run in one of the main growth phases: lag, exponential and stationary. The main purpose of this NN was to identify the exponential phase, which would be the domain of the next NN, a multilayer perceptron (MLP) for inference of the cellular mass. Several NN, with
different topologies, were tested for this purpose. Finally, the product concentration (PGA activity in the medium) was estimated through a hybrid approach, using the growth rate inferred by the MLP NN, coupled to the cell-product yield, obtained from the fitting of the non-structured models. Another important information for this last algorithm was the knowledge that production of PGA was growth-associated, but with a 2hr-delay. The algorithm for inference was robust and accurate.Universidade Federal de Minas GeraisA grande importância da enzima penicilina G acilase (E.C.3.5.1.11), usada para a produção dos ácidos 6-aminopenicilânico (6-APA) e 7-aminocefalosporânico (7-ACA), compostos-chave na produção industrial de antibióticos &#946;-lactâmicos, foi a principal motivação deste trabalho. Através de realização de experimentos de produção
de PGA por Bacillus megaterium ATCC 14945 em reator convencional, utilizando sistema de aquisição de dados, foi possível monitorar em tempo real variáveis como, por exemplo, fração molar de oxigênio e dióxido de carbono nos gases de saída, pH, oxigênio dissolvido. Foram realizados cultivos em batelada e batelada alimentada tendo como
substratos limitantes tanto caseína hidrolisada enzimaticamente como aminoácidos livres. Obtiveram-se informações relacionadas ao tempo de cultivo, à estocagem do microrganismo e à adição e exclusão de nutrientes. Fontes usuais de carbono, como glicose, lactose e glicerol, quando utilizadas, promoviam o crescimento da massa celular sem, no entanto, aumentar a produção de PGA. O uso de aminoácidos como principal
substrato elevou em 2,5 vezes a produtividade. Observou-se, ainda, que a adição de ácido fenilacético (AFA) desde o início do cultivo não inibiu o crescimento do microrganismo.
As informações experimentais permitiram a proposição e validação de três modelos cinéticos não-estruturados deste processo, com um ou mais substrato(s) limitante(s). Como essas propostas se mostraram demasiadamente simplificadas, optouse
pelo uso de redes neurais como sensores baseados em  software , já que não se chegou a um modelo fenomenológico satisfatório. Para implementação do algoritmo de inferência baseado em rede neural,
mostrou-se necessário filtrar os ruídos das medidas provenientes do sistema de aquisição de forma a minimizar erros aleatórios da instrumentação. Propõe-se para isso filtro que utiliza redes recorrentes, combinadas a média móvel. As variáveis filtradas foram utilizadas numa segunda rede de identificação de padrões, que dividia o cultivo nas três principais fases do crescimento microbiano. O principal objetivo desta foi
identificar a fase de crescimento exponencial, que seria a enfocada pelo algoritmo de inferência. Para essa inferência, com base nas variáveis medidas em tempo real, treinaram-se redes com várias topologias e entradas, de modo a escolher as variáveis que possibilitassem o aprendizado dos aspectos essenciais da dinâmica do processo.
Por fim, a concentração de produto (atividade de PGA no meio de cultura) foi estimada através de enfoque híbrido, usando a velocidade de crescimento inferida por rede  feedforward , acoplada ao fator de rendimento célula-produto estimado no ajuste dos modelos não-estruturados. Outra informação importante utilizada neste último
algoritmo foi o fato da produção ser associada ao crescimento, mas com atraso de 2h. Novamente, os resultados quantitativos do algoritmo de inferência do produto foram muito satisfatórios",industry,886,not included
,to_check,core,,1994-01-01 00:00:00,core,neural networks: application to medical imaging,https://core.ac.uk/download/pdf/42781573.pdf,"The research mission is the development of computer assisted diagnostic (CAD) methods for improved diagnosis of medical images including digital x-ray sensors and tomographic imaging modalities. The CAD algorithms include advanced methods for adaptive nonlinear filters for image noise suppression, hybrid wavelet methods for feature segmentation and enhancement, and high convergence neural networks for feature detection and VLSI implementation of neural networks for real time analysis. Other missions include (1) implementation of CAD methods on hospital based picture archiving computer systems (PACS) and information networks for central and remote diagnosis and (2) collaboration with defense and medical industry, NASA, and federal laboratories in the area of dual use technology conversion from defense or aerospace to medicine",industry,888,not included
,to_check,core,,2017-01-01 00:00:00,core,'firenze university press',"Cultura tecnologica, ambiente, energia: prospettive della ricerca e della sperimentazione | Technological Culture, the Environment and Energy: the outlook for research and experimentation","Il ruolo della cultura tecnologica del progetto rispetto alle tre grandi sfide poste dalle questioni ambientali ed energetiche del nostro tempo: cambiamenti climatici, limitatezza delle risorse e eccessivo consumo di energia, è centrale e imprescindibile per affrontare consapevolmente la ricerca di una rinnovata dimensione delle condizioni dell'Abitare che nei differenti contesti europei e mondiali porti con sé i concetti - diversi terminologicamente ma affini nelle ampie accezioni che essi racchiudono e negli obiettivi che sottendono - di Sustainable Development, Nachhaltige Entwicklung, Dévéloppement Durable.

Nelle tre differenti aggettivazioni è racchiuso uno dei mandati della Tecnologia e della relativa Cultura tecnologica progettuale da sviluppare nel prossimo futuro: la richiesta di sostenibilità del fare umano per le generazioni future (Sustainable), di solidità e affidabilità dei comportamenti e delle prestazioni (Nachhaltige), di durabilità nel tempo dei prodotti delle trasformazioni (Durable).

L'altro grande mandato della Cultura tecnologica nella nostra epoca è quello di offrire non solo risposte 'dinamiche' nella dimensione temporale di medio-lungo termine alle crescenti esigenze di sostenibilità/affidabilità/durabilità, ma anche risposte 'dinamiche' nello spazio reale e nel tempo presente e di breve termine supportando l'Architettura nella sua altrettanto assoluta necessità di essere 'adattiva' e 'resiliente' ai cambiamenti già in atto sul piano climatico e ambientale.

Cultura tecnologica significa dunque profonda consapevolezza degli obiettivi da perseguire progettualmente, indissolubilmente legata ad un'attitudine alla visione sistemica dei problemi, ad un'impostazione metodologica delle strategie da tracciare e promuovere, ad un'intima sapienza degli aspetti di fattibilità e realizzabilità delle azioni da sperimentare, monitorare, consolidare nel tempo.

D'altra parte tutti i maggiori centri di ricerca e sperimentazione dell'area dell'Architettura nei Paesi avanzati pongono quale nodo focale dei processi di concezione, progettazione e realizzazione di qualsiasi tipo di intervento trasformativo delle nostre realtà quello caratterizzato dall'approccio tecnologico, in cui nei diversi contesti la Architectural Technology, la Baukonstruktion, la Technologie de l'Architecture, la Construcción en Arquitectura non rappresentano solo un ambito disciplinare (pealtro da sempre a vocazione fortemente interdisciplinare) ma, di più, la dimensione logica e culturale nella quale si coordinano e ruotano le complesse declinazioni e i differenti caratteri del progetto.

Nel rinvenire gli elementi di innovazione propri della Cultura tecnologica nell'approcciare e sviluppare ricerca e sperimentazione sui temi dell'ambiente e dell'energia vi è in primis la capacità di perseguire al contempo tre categorie di obiettivi ""alti"" prestazionali: efficienza delle azioni nel controllarne rendimenti e risultati; efficacia complessiva delle strategie nel verificarne il rapporto tra efficienza conseguita e quantità di risorse coinvolte e impiegate nei processi per raggiungere quei risultati; soddisfazione degli utenti nel vivere quelle condizioni di efficienza e nel percepire o addirittura esser coinvolti in quelle dimensioni di efficacia.

La concezione innovativa di scenario che è aperta dall'evoluzione del significato di efficacia pone in primo piano la questione della 'scarsità di risorse' e tiene in massima considerazione -fino al punto di farla sua nei recenti sviluppi a più ancora in quelli a venire nel prossimo futuro - l'accezione di 'Economia circolare' (asse portante della visione della Green Economy) che, secondo il recentissimo Piano di Azione Globale della Comunità Europea, attiene ad un sistema complesso in cui ""il valore dei prodotti, dei materiali, dell'energia e delle risorse è mantenuto quanto più a lungo possibile e la produzione di emissioni, inquinamento, scarti e rifiuti è ridotta al massimo"" (European Commission, 2017) e in cui, secondo il report della Agenzia Europea per l'Ambiente, centrale è ""contrastare il depauperamento delle risorse naturali, re-immettere nel mercato le risorse in dismissione ed agevolare il recupero delle risorse di valore"" (European Environment Agency, 2016).

In questo senso protagonista assoluto di tale innovazione è quell'approccio ecosistemico ai problemi, alle questioni in gioco e alle strategie ed azioni per risolverli, che fa fondamentalmente riferimento alla visione di tipo 'Life Cycle' strutturante il senso stesso di 'sostenibilità' di uno sviluppo capace di sostanziare la 'circolarità' del sistema economico, la 'mitigabilità' della crisi climatica, l''efficientabilità' della questione energetica, la 'capitalizzabilità' del patrimonio naturale, la 'inclusività' del benessere e la 'rigenerabilità' delle città; e di includere il concetto di 'Costo ambientale' in tutte le strategie e azioni da questi assi sottese.The role of technological culture in planning, seen in light of the three major challenges raised by the environmental and energy-related issues of our time, meaning climate change, limited resources and excessive energy consumption, is a key role, and one that we ignore at our peril when it comes to seeking out, in informed fashion, a renewal in the conditions of inhabiting the contexts of Europe and the rest of the world, a quest that necessarily brings into play concepts which differ with respect to their terminology, but yet prove quite similar when it comes to the board range of concepts covered and the wealth of their underlying objectives: Sustainable Development, Nachhaltige Entwicklung and Dévéloppement Durable.

Ensconced within each of these descriptors is one of the vital tasks that technology and the related cultural of technology must fulfil in the near future: responding to the request for sustainability in human  endeavours in future generations (Sustainable), as well as for integrity and reliability in terms of conduct and performance (Nachhaltige), together with the durability over time of the products of transformations (Durable).

The other major task of cultural technology in our time is to provide responses that prove 'dynamic' not only over the medium-long term, with respect to meeting the growing need for sustainability/reliability /durability, but that are also 'dynamic' within real space and at the present time, in the short term, supporting architecture’s equally absolute need to be 'adaptable' and 'resilient' in response to changes currently affecting the climate and the environment.

Technological culture, therefore, means possessing a thoroughgoing knowledge of the objectives to be pursued through planning, irrevocably linked to a leaning towards a systemic vision of issues, as well as a methodological approach to the strategies to be outline and promoted, plus an intimate familiarity with the considerations of feasibility and practicality tied to the initiatives to be tested, monitored and built up over time.

For that matter, all the major centres of research and experimentation in the architectural sectors of the developed countries posit the technological approach as a vital factor in the processes of conceptualisation, planning and implementation of any type of initiative meant to transform our existing realties, with the result that the various contexts of Architectural Technology, Baukonstruktion, Technologie de l'Architecture and Construcción en Arquitectura represent not only a disciplinary realm (albeit one with a marked interdisciplinary bent) but, even more to the point, the logical and cultural dimension within which the various manifestations and characteristics of planning are coordinated and brought into focus.

In identifying the innovative features displayed by technological culture as it addresses and develops research and experimentation on environmental and energy-related topics, the first observation to be made is its ability to simultaneously pursue three different categories of ""advanced"" objectives involving performance: the efficiency of initiatives in terms of controlling yields and results; the overall effectiveness of strategies in light of assessments of the ratio between the efficiency achieved and the quantities of resources involved and utilised in the processes enacted to reach those results; the satisfaction of the users of those conditions of efficiency as they perceive, or even become involved in, the new dimensions of effectiveness.

The innovative approach to conceptualisation of scenarios brought about by the evolution in the meaning of effectiveness highlights the issue of 'scarce resources' while placing the utmost importance – to the point of absorbing it in recent developments, a trend that shall become even more pronounced in the near future – on the framework of the 'Circular Economy' outlook (a cornerstone of the vision of a Green Economy), an outlook that, according to the recent Global Action Plan of the European  Community, points to a complex system in which, ""The value of products, materials, energy and resources is maintained for as long as possible, while the production of emissions, pollution, scrap and waste is reduced to the greatest possible extent"" (European Commission, 2017), with key importance placed on, ""Contrasting the impoverishment of natural resources, reintroducing discarded resources onto the market and facilitating the recovery of resources of value "" (European Environment Agency, 2016).

Seen in this light, the key ingredient to a similar innovation is the eco-systemic approach taken to the problems and issues addressed, as well as to the strategies and initiatives enacted to resolve them, an outlook that essentially consists of a 'Life Cycle' vision able to provide the structure for the fundamental 'sustainability' of a mode of development that succeeds in supporting the 'circularity' of the economic system, together with the potential for mitigating the climate crisis, for increasing energy efficiency, for capitalising on natural resources, for making wellbeing more inclusive while regenerating cities, with the concept of 'environmental cost' becoming an integral part of all the strategies and actions underlying such efforts",smart cities,889,not included
,to_check,core,,,core,行政院國家科學委員會,Hierarchical Cross-Layer Soft Computation Model for WiMAX Networks and Its Electronic Sytem Level Verification(II),"[[abstract]]本計畫原訂期程為三年，第一年已獲得執行，執行半年至今已經獲得豐碩之成果並發表於國際期刊獲得接受，本次申請依據評審意見修正(謹附於表C012-2 計畫內容中說明)，並將過去半年已完成成果以及已經為未來兩年所作之準備進行詳盡說明，以期研究成果能獲得延續執行。 IEEE 802.16 WiMax 無線網路的發展非常迅速，許多國家列為重要基礎建設之一(台灣也不例外)，就連標準的制定與更新也非常快，新的標準不斷出現，包含802.16e-2005 定義了Mobile WiMax，草擬中的802.16j (draft) 和802.16m (pre-draft)更嘗試在原來的高 throughput 下提昇relay 以及高mobility 的能力，在這些完稿或未確立的標準中，都是在媒體存取層(Media Access Control; MAC)以及實體層(Physical layer; PHY)中進行定義。其中並有對於服務品質(Quality of Service; QoS)的規範，足見QoS 不只是網路層以上的協定必需研究的問題，特別針對WiMax，在MAC 層實作QoS 的重要性更受到許多研究學者之重視。本計劃承接過計畫經驗，這些經驗包含使用跨協定控制方式完成之IEEE 802.11e EDCA QoS 之增進、IEEE 802.11e HCCA QoS 之增進、以及去年成功地完成IEEE 802.16e/j QoS 之增進，欲進行下列的研究，以將研究深入到IEEE 802.16e/j/m 以至於廣義Scalable OFDMA-based 的無線網路的QoS 控制： 1. 階層式跨協定模糊控制(Hierarchical Cross-Layer Control; HCLC) 使用於 802.16e/j/m Mobile 以及Relaying 之QoS 2. 定義於Lattice 上高階模糊宇集(高type 以及高Level 之fuzzy universe)之普適化模糊自動機(Generalized Fuzzy Automata; GFA) 來模仿(mathematically realize)WiMax 無線網路的行為，了解QoS 參數的特性。 3. 根據GFA 自動機理論設計HCLC 控制器，使用高階模糊宇集進行多參數以及多目標控制，以在相容於標準之條件下調整不同QoS 參數。 4. HCLC 之軟硬體共同設計與實作以及MAC 排程器矽智財之電子系統層級 (Electronic System Level; ESL)驗證，開發現今儀器無法量測以及測試的WiMax 驗證平台。 5. 通用於OFDM/Scalable OFDMA-based 之跨協定控制，以利未來延伸到其他之無線網路標準，例如IEEE 802.22 Cognitive Radio，IEEE 802.11n MIMO 之QoS 排程器等。本計畫從過去的研究成果來延伸，在跨協定QoS 控制中，在標準已經定義的Power Sleep Behaviors (Listen/Active)以及Ranging 以外，另加入路由控制、PDU 長度、排程器計算等造成能源消耗的考量，這是IEEE 802.16 標準中缺乏定義的部份。不同於使用最佳化理論之硬式計算(hard computing)演算法，我們說明使用軟式計算 (soft computing)的原因。除了IEEE Computational Intelligence Society (CIS)在CIS 雜誌中所揭出的Emergent Applications 包含了網路控制之外，我們定義了WiMax 問題，說明無線網路無法根據固定的機率模型假設以及固定的環境因素來進行推導，因此無法獲得以 constraints 與 controls 來表示 的objective function 的close-form。無線網路的與變數狀態非常多且動態，無法根據專家知識逐一定義模糊規則或進行最佳化之推導，本計畫使用簡單控制規則，在不同通訊協定層進行控制，每一協定層的模糊規則展開之後，規則數可以隨著所跨協定層數獲得exponential 成長，我們以Homomorphism 來說明並規範上下層之間狀態的關係。針對這樣的網路控制架構，我們稱之為階層式跨協定控制 (Hierarchical Cross-Layer Control; HCLC)。本計畫分析WiMax 在動態無線網路環境下的延遲與QoS 參數的關係，以廣義模糊自動機當作其行為模型，並整合能量消耗的模型。自動機之行為實現(output function)利用過去國科會計畫成果—單一維度HCLC控制架構來延伸。因為WiMax 之OFDMA 與802.16m 的 MIMO 中之特性，跨協定所必須完成的是多目標與多參數之控制，方法是延伸到高階宇集(universe)的HCLC。我們並發現這樣的一個架構是一個比傳統Hierarchical Tagaki-Sugeno Control 還要廣義的Paradigm，本計劃除針對這樣的架構進行穩定性以及效率的理論分析之外，並設計實驗證明這樣的架構即時有效地進行IEEE 802.16e/j 的QoS 控制，本計畫在第一年已經證明HCLC 應用於802.16e/j 的兩種重要的公平性：inter-class fairness 與intra-class fairness，這是大多文獻中未考慮到的部份。在以嵌入式系統以及矽智財設計、實作、驗證WiMax 跨協定排程器方面，我們考慮SoC 設計流程來說明，以往任何通訊網路之矽智財驗證平台無法驗證出在不明確網路下之可行性(特別是無線網路)，必須在花費大筆經費和時間整合到SoC 並且下線(tape out) 之後，再花許多時間進行embedded software 之porting，然後才發現所設計之SIP 是否有瑕疵(bug)，若發現瑕疵，現有之積體電路設計或模擬驗證軟體並無法針對通訊網路的部份進行除錯，即使是昂貴的SoC 驗證工具(例如具有AMBA SoC bus 輸出入介面之 ARM based 平台)也無法立即進行驗證晶片在通訊網路上之行為，僅能提供訊號波形、指令週期等層級之觀察與控制。而對於通訊網路，還必須花費成本複製多台才能夠架構出基本網路拓樸(topology)，也僅能進行基本實驗。即便最後成功，市場之優勢早已不再，對於許多design houses 來說，這是一個難關。本計畫所產出之Electronic System Level (ESL)驗證平台，可以大幅縮短Design House 在嵌入式系統設計驗證之時程，並大幅縮小開發成本。由於我們已經完成WLAN MANET 之ESL 架構，此架構基於HCLC 控制模型，成功驗證了FPGA 以及HCLC 跨協定演算法於MANET 下的行為，因此這樣的一個HCLC ESL Verification 平台， 最適用於Architecture/Algorithm 以及 Hardware/Software Co-design，對於無線網路矽智財以及嵌入式系統初期之開發最具幫助。綜合以上目標以及原因之說明，本計畫擬定接下來兩年之研究方法、步驟以及個別目標，本著研究應該持續且深入的觀念，沿著過去研究成果的脈絡，來規劃未來兩年的研究。第一年已經完成原訂第一年之目標： 高階宇集HCLC 控制模型以及 OFDMA-based 802.16e-2005 Mobile WiMax 以及802.16j (MMR)的跨協定排程器軟體元件；第二年將擴充到OFDMA-based 的802.16j/m，整合PHY 與MIMO 能量消耗模型(特別是mobile node 上行部份)，完成具Relaying 功能且支援MIMO 的IEEE 802.16 e/j/m Mobile WiMax 跨協定控制器；第三年依據WLAN 之ESL 經驗以及第一年與第二年之研究成果，實作OFDMA-based WiMax 排程器矽智財，並進行軟硬體系統整合，以及完成ESL 驗證平台。總結本計畫的創意以及特點即是：1. 所完成之系統以及QoS 矽智財能夠滿足QoS 之外，在透過跨協定的架構下還能同時inter-與intra-class 的公平性；2. 達到節能的目標；3. 完成支援Cross-Layer Design 的WiMAX 通訊網路晶片矽智財；4. 結合網路模擬器與Hierarchical Cross-Layer 控制模型完成低成本之ESL 平台。以未來之擴充性來說，ESL 驗證平台之應用，不僅用於矽智財之設計，也不僅用於MAC 層之協定驗證，對於跨協定設計中任何演算法之驗證、嵌入式系統之演算法驗證、…、等等，均能發揮其功能。而模糊自動機可以應用之範圍甚廣，例如機器學習，以智慧型的方式學習網路的狀態空間，朝實體層之跨協定調變控制以產生最佳頻寬利用等。對於本計畫利用人工智慧與軟式計算理論為工具，應用到通訊網路，並實現在SoC 領域，本著過去計劃之成功經驗，向系統整合方向邁進，成果將非常豐碩。[[abstract]]This project was planned in three-year execution. The first year was granted and till now very good results has been obtained and published in an international journal (SCIE indexed). In this proposal, we revise according to the reviewers’ comments (cf. Table C012-2). We also include the results descriptions of the first year execution and present how/what we have prepared for the following two years wishing that the research can be continued. Since IEEE 802.16 WiMax wireless networks are developed, it becomes the future infrastructure of many developed and developing countries. New related standards and their amendments are fervidly discussed. The IEEE 802.16e-2005 has defined Mobile WiMax and the latest drafting 802.16j and the “pre-drafting” 802.16m even try to respectively promote multi-hop relay and mobility beyond the original throughput. In these finalized, drafting, and pre-drafting standards, most are about Media Access Control (MAC) and PHYsical layers (PHY). What insights are with Quality of Service (QoS) and we see that QoS is a problem relating not only upper layers but also MAC and PHY. Especially for WiMax, QoS realization in MAC and PHY are emphasized by many researches. Inheriting the experiences of previous projects since IEEE 802.11e EDCA, HCCA, and currently the general TDMA-based QoS controls, we propose the following researches to strike into IEEE 802.16e/j Wave 2 and so as into general Scalable OFDMA (SOFDMA) based QoS control: 1. Define Hierarchical Cross-Layer Control (HCLC) for 802.16e/j/m mobile and multi-hop relay QoS 2. Study Generalized Fuzzy Automata (GFA) defined with high-type and high-level fuzzy universes over lattices to mathematically realize WiMax behaviors and characteristics 3. Design HCLC scheduler and allocator (together called controller) according to the GFA theory where high-type fuzzy universes are used for multi-parameter and multi-objective control. In this way, we can adjust QoS parameters while the wireless communications are still compatible with the standards.4. Perform HCLC software-hardware co-design, implement the MAC QoS controller Silicon Intellectual Property (SIP), and its Electronic System Level (ESL) verifications. Develop WiMax verification platform with cross-layer features that modern instruments cannot measure. 5. Research generic cross-layer control for the OFDM/Scalable OFDMA-based wireless such that we can extend the WiMax QoS control research into future standards such as IEEE 802.22 Cognitive Radio and IEEE 802.11n MIMO. This project extends the valuable results of previous executed/executing projects. Beside cross-layer, and beside defined power sleep behaviors (Listen/Active) and ranging, we also consider energy consumption about routing control, PDU length, and scheduling computation. These are what the IEEE 802.16 does not define. Unlike those hard computation algorithms using optimization theory, we explain the feasibility using soft computing. Beside the reason explained in the Computational Intelligence Magazine published by IEEE Computational Intelligence Society (CIS) that one of the emergent applications of soft computing is network control, we additionally and in detail defined problems of WiMax QoS control. In this proposal we explain why wireless networks cannot be modeled by fixed probability model and why their performances cannot be derived using fixed environmental factors. Consequently, optimization using hard computing algorithms can not express objective functions in terms of constraints and control parameters. However, using soft computing we still suffer difficulties. The variables and states of wireless networks forms a large and dynamic space such at that no expertise can define detail fuzzy rules. Therefore, in this project, we try to use simple fuzzy rules distributed in different protocol layers. Expanding the rules in the protocol layers, we found that the number of fuzzy rules exponentially grows along the number of layers. We adopt homomorphism concept to explain and constrain inter-layer states and variables. We called this network control architecture the Hierarchical Cross-Layer Control (HCLC). For WiMax, this project analyses the relationship between delay and QoS parameters when network is highly dynamic. Then, we use generalized fuzzy automata to realize the behavior of the WiMax network and integrate energy consumption model. The realization of automata (output function) inherits previous projects’ results – the one dimensional HCLC control to extend new SOFDMA-based control with multiple objectives and multiple parameters. The method used is adopting high-type and high-level fuzzy universes in the HCLC. We even find out that such HCLC architecture is a more generic paradigm than defined in so-called Hierarchical Tagaki-Sugeno fuzzy control. This project will analyze the stability and efficiency of this fuzzy control architecture. In the first half year, we design the experiment to prove that HCLC efficiently and effectively perform IEEE 802.16e/j QoS control while at the same time keep the fairness that few articles concern. Considering the embedded system design, silicon intellectual property implementation, and verification of the WiMax QoS controller, this project provides a low cost solution. According to known SoC design flow, no SIP verification platform can perform verification of SIP behavior under realistic wireless network environment before accomplishment of SoC integration, tapering, and embedded software porting. It’s a long journey to accomplish all these tasks. If a bug is found in the SIP, no EDA tool can remove it dedicatedly for network communication. Even using costly SoC verification tool such as ARM-based platform with AMBA SoC bus on the PCB, we still cannot observe the SIP behavior under wireless networks. Using costly SoC verification tool, what only we can do is signal wave measurement, observation and control at the instruction and cycle accuracy. For basic networking experiments, we still even have to replicate platforms to construct the basic network topology. After we success eventually, the market opportunity is gone. This is a tuff work for design houses. The verification platform of this project will be superior over traditional ones and obviously shorten the development of networking SIPs. Thus reduce the cost effectively. In previous project, we had accomplished WLAN MANET ESL verification where HCLC control model successfully cooperates with FPGA hardware. This is the ever first verification platform that verify behaviours of real hardware and cross-layer algorithm under MANET. The platform will be extended for WiMAX MMR since it optimally match hardware architecture and cross-layer algorithms and is the best tool for hardware/software co-design. According to the above descriptions, we project three-year research methods, steps, and objectives. Based on the concept that research should be always continuing and burrowing to the deep, we follow the direction of previous research projects. In the first half year, we already accomplished high-type HCLC control model and its cross-layer scheduler software component for OFDMA-based 802.16e/j Mobile Multi-hop Relay (MMR) WiMax. In the second year, we will extend the HCLC controller to 802.16j/m and integrate energy consumption model to accomplish embedded cross-layer scheduler software component for multi-hop relaying and mobile WiMax. In the third year, we will implement the OFDMA-based WiMax scheduler and its electronic system level (ESL) verification platform. Summarizing the originality and features, we have: first, the system and QoS SIP not only guarantee QoS but also intra- and inter-class fairness; sencond, the QoS and fairness guarantees are energy-aware; third, we accomplish HCLC SIP for general WiMax; fourth, we produce low cost ESL verification platform integrating the network simulator and hierarchical cross-layer control. For future extension, fuzzy automata theory applies in many areas such as machine learning, intelligent network state space learning, and cross-layer physical layer modulation for spectrum optimization. The resulting SIP verification platform is also useful in many cases in addition to SIP and MAC layer control. For example, it should be also useful in any cross-layer algorithms, embedded system algorithms, …, etc. This project covers several research fields including artificial intelligence, computing theory, communication networks, and VLSI. With the basis of previous successful projects, we progress toward system integration and the results will also be very plentiful and valuable.[[note]]NSC98-2221-E327-02",smart cities,890,not included
,to_check,core,Artificial Intelligence Applications Institute,2007-08-31 00:00:00,core,collaborative operations for personnel recovery final report on darpa/afrl,,"The University of Edinburgh and research sponsors are authorised to reproduce and distribute reprints and on-line copies for their purposes notwithstanding any copyright annotation hereon.  The views and conclusions contained herein are the author’s and shouldn’t be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of other parties.The Collaborative Operations for Personnel Recovery (Co-OPR) project sought to
provide collaborative task support for a Search and Rescue coordination center. The
project aimed to create a prototype “Personnel Recovery (Experimental) Pack” (PREP) and to demonstrate its use.

A number of requirements capture, knowledge gathering and transition workshops and
meetings were held. This included an initial requirements setting workshop early in 2005,
meetings at the USJFCOM Joint Personnel Recovery Agency’s (JPRA) Personnel
Recovery Education and Training Center (PRETC) in Fredericksburg, Virginia in June
2005, a review meeting in Edinburgh in August 2005, and attendance at a Command Post
Exercise (CPX) at the PRETC in November 2005. These initially established the potential
areas for use of Co-OPR and I-X tools in training exercises. In the second project year such tools were developed and tested using a training exercise as held at the PRETC, using observers from PRETC and USJFCOM and an evaluation expert from USJFCOM/J9.

The project was provided with a rich set of urban and rural scenarios by JPRA/PRETC
which together are unclassified versions of scenarios used within the PRETC training
courses and Command Post Exercises. At the time, these stretched the capabilities of the
current and envisaged technologies within Co-OPR/I-X. Refinement of the scenarios
alongside PRETC, and knowledge engineering to capture information on standard operating procedures and responses were a key part of making the work relevant to the potential real use of Co-OPR/I-X for Personnel Recovery.

The core I-X technology was packaged into a number of checkpoint releases to make
available the features required to meet the application needs. I-X version 4.3 released in November 2005 to checkpoint the results achieved on the first 12 months of work with
the PRETC. It formed a basis for the work on really using the technology at the PRETC.
New “white cell” aids for training were made available in an initial version. A new I-Sim
simulation capability, and advanced option exploration tools have all been improved
significantly to make them more usable, including the features of the I-Plan AI planner and its capability for plan repair after failures. The features of the Domain Editor (I-DE) and its ability to browse and update or augment standard operating procedure knowledge dynamically during missions were enhanced. The final release of I-X that includes all the new developments achieved in the Co-OPR project is version 4.5.

The results of the work were packaged, along with Personnel Recovery domain specific
models, as a web site and/or CD which could be considered as a prototype “Personnel
Recovery (Experimental) Pack” of tools to assist a Joint Personnel Recovery Center
(JPRC) and associated operational staff in performing their operations. The versions of
PREP produced were used in one workshop or Command Post Exercise at the PRETC
under guidance from Dr. Jeff Hansberger at training related workshops already organized
by USJFCOM/J9 Expt. and Fred Kleibacker, the (now former) Director of the PRETC in
Fredericksburg, Virginia. Co-OPR team members were engaged with these workshops to
to show the tools in realistic settings, to assist with training where possible, and to gather experimental feedback.

Realistic use of tools for Personnel Recovery requires that the systems can work with
emerging technology for geo-positioning, survival radios, evasion aids, robotic or semiautomated rescue aids or robots, and doctrine or tactics, techniques and procedures for Personnel Recovery. A number of short studies of these “complementary technologies” were made which explored deployment and inter-working aspects of these with the Co-OPR/I-X technology",smart cities,891,not included
,to_check,core,法政大学大学院デザイン工学研究科,2021-03-24 00:00:00,core,development of an object position and attitude sharing networked ar game system using omni directional laser scanner,https://core.ac.uk/download/426984051.pdf,"In recent years, the development environment for mobile applications using Augmented Reality (AR) technology, which superimposes virtual objects on real space, has improved dramatically, enabling the implementation of advanced applications by combining image processing using artificial intelligence (AI) and space sharing using network communication. In particular, the technology for simultaneous presentation of virtual objects to multiple users through space sharing is important for applications such as product demonstrations, design meetings, surgical support, or competitive games. However, such space-sharing technology alone can share virtual objects in the same space, but it cannot share remote objects as virtual objects with each other.In this research, as a proposal for the use of technology to mutually share remote objects as virtual objects, I will develop an AR game system that can communicate and compete with each other in the same way as real competitive games even in remote areas, by using a 360-degree omni-directional laser scanner to acquire the position and posture of a real radio-controlled car in each user\u27s space, and using network communication technology to mutually share this information and reproduce it as a virtual object in real time",multimedia,892,not included
,to_check,core,'Institute of Electrical and Electronics Engineers (IEEE)',2021-01-01 00:00:00,core,cat-like jumping and landing of legged robots in low gravity using deep reinforcement learning,10.1109/tro.2021.3084374,"In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.ISSN:1552-3098ISSN:1042-296XISSN:1941-046",multimedia,893,included
,to_check,core,Editoriale Scientifica,2021-08-01 00:00:00,core,confiance en l’intelligence artificielle et autorité des machines,,"International audienceThis contribution has two objectives. On the one hand, it considers the study of trust relationships towards the ""non-human agents"" that are the contemporary technological tools (algorithms and data that define artificial intelligence); on the other hand, it undertakes to conceptualize the authority that these tools are acquiring. The starting point is that the contemporary deployment of AI relies on forms of trust in machines shared between their designers and users, who appear to be the ""cement"" of the uses that guarantee the efficiency of the machines. However, this trust is less directed towards tools considered as neutral than towards personalized or even personified entities. Artificial intelligences, analyzed in the context of their uses, are already presented as something other than neutral instruments. Although still poorly qualified ontologically, they are already no longer simple tools, but agents. Through the services they render and the influence they already exert on behaviors, they acquire a real action that needs to be documented and conceptualized. This approach in political philosophy wants to examine the form of authority thus generated, notably by imagining experimental devices to test the hypotheses.Cette contribution se donne deux objectifs. D'une part, elle envisage l'étude des relations de confiance envers les « agents non-humains » que sont les outils technologiques contemporains (algorithmes et données qui définissent l'intelligence artificielle) ; de l'autre, elle entreprend de conceptualiser l'autorité que ces outils sont en train d'acquérir. Le constat de départ est que le déploiement contemporain de l'IA repose sur des formes de confiance envers les machines partagées entre leurs concepteurs et les usagers, qui apparaissent comme le « ciment » des usages garantissant l'efficacité des machines. Or cette confiance s'adresse moins à des outils considérés comme neutres qu'à des entités personnalisées voire personnifiées. Les intelligences artificielles, analysées dans le contexte de leurs usages, se présente déjà comme autre chose que des instruments neutres. Bien qu'encore mal qualifiés sur le plan ontologique, ce ne sont déjà plus de simples outils, mais des agents. Via les services qu'ils rendent et l'influence qu'ils exercent déjà sur les comportements, ils acquièrent une action réelle qu'il convient de documenter et de conceptualiser. Cette démarche en philosophie politique veut examiner la forme d'autorité ainsi engendrée, notamment en imaginant des dispositifs expérimentaux pour tester les hypothèses",multimedia,894,not included
,to_check,core,HAL CCSD,2021-06-08 00:00:00,core,eu h2020 neuronn: 2d oscillatory neural networks for energy efficient neuromorphic computing,,"National audienceIn this paper, we showcase a leading-edge implementation of oscillatory neural networks (ONNs) using beyond Complementary-Metal-Oxide-Semiconductor devices based on vanadium dioxide to mimick neurons, and 2D molybdenum disulfide memristors to emulate synapses. We explore the ONN technology through simulations from materials to devices up to circuits. We show that ONNs naturally behave like associative memories and can be used for pattern recognition, a task to be exploited in edge devices. Finally, we develop a reconfigurable digital ONN-on-FPGA to assess ONN functionality in real world applications",multimedia,895,not included
,to_check,core,,2021-01-22 00:00:00,core,learning audio and image representations with bio-inspired trainable feature extractors,,"Since when very young, we can quickly learn new concepts, and distinguish between different kinds of object or sound. If we see a single object or hear a particular sound, we are then able to recognize such sample or even different versions of it in other scenarios. As an example, if one sees a iron chair and associates the object to the general concept of ""chairs"", he will be able to detect and recognize also wooden or wicker chairs. Similarly, when we hear the sound of a particular event, such as a scream, we are then able to recognize other kinds of scream that occur in different environments. We continuously learn representations of the real world, which we then use in order to understand new and changing environments.In the field of pattern recognition, traditional methods typically require a careful design of data representations (i.e. features), which involves considerable domain knowledge and effort by experts. Recently, approaches for automated learning of representations from training data were introduced and based on popular deep learning techniques and convolutional neural networks (CNN). Representation learning aims at avoiding engineering of hand-crafted features and providing automatically learned features suitable for the recognition tasks. In this work, we proposed novel trainable filters for representation learning in audio and image processing. The structure of these filters is not fixed in the implementation but rather configured directly from single prototype patterns of interest [4].In the context of audio processing, we focused on the problem of audio event detection and classification in noisy environments, also in cases where the signal to noise ratio (SNR) is null or negative. We released two data sets, namely the MIVIA audio events and the MIVIA road events data sets, and obtained baseline results (recognition rate of about 85%) with a real-time method for event detection based on the bag of features classification scheme [3, 2].We designed novel trainable feature extractors, which we call COPE (Combination of Peaks of Energy), that are able to detect specific constellations of energy peak points in time-frequency representations of input audio signals [8]. The particular constellation of energy peaks to be detected by a COPE feature extractor is determined in an automatic configuration process performed on a given prototype sound. The design of COPE feature extractors was inspired by some functions of the cochlea membrane and the inner hair cells in the inner auditory system, which convert the sound pressure waves into neural stimuli on the auditory nerve.We proposed a method that uses COPE feature extractors together with a classification system to perform real-time audio event detection and classification, also in cases where sounds have null and negative SNR. The performance results (recognition rate over 90%) that we obtained on several benchmarking data sets for audio events detection in different contexts are higher than state-of-the-art approaches.In the second part of the work, we introduced B-COSFIRE filters for detection of elongated and curvilinear patterns in images and apply them to the delineation of blood vessels in retinal images [1, 6]. The B-COSFIRE filters are trainable, that is their structure is automatically configured from prototype elongated patterns. The design of the B-COSFIRE filters is inspired by the functions of some neurons, called simple cells, in area V1 of the visual system, which fire when presented with line or contour stimuli. A B-COSFIRE filter achieves orientation selectivity by computing the weighted geometric mean of the output of a pool of Difference-of-Gaussians (DoG) filters, whose supports are aligned in a collinear manner. Rotation invariance is efficiently obtained by appropriate shiftings of the DoG filter responses.After configuring a large bank of B-COSFIRE filters selective for vessels (i.e. lines) and vessel-endings (i.e. line-endings) of various thickness (i.e. scale), we employed different techniques based on information theory and machine learning to select an optimal subset of B-COSFIRE filters for the vessel delineation task [5, 7]. We considered the selected filters as feature extractors to construct a pixel-wise feature vector which we used in combination with a classifier to classify the pixels in the testing image as vessel and non-vessel pixels. We carried out experiments on public benchmarking data sets (DRIVE, STARE, CHASE DB1 and HRF data sets) and the results that we achieved are higher than many existing methods.We studied the computational requirements of the proposed algorithms in order to evaluate their applicabilityin real-world applications and the fulfillment of real-time constraints given by the considered problems. The MATLAB implementation of the proposed algorithms are publicly released for research purposes.This work contributes to the development of algorithms for representation learning in audio and imageprocessing and promotes their use in higher-level pattern recognition systems",multimedia,896,not included
,to_check,core,Bundle Adjustment on a Graph Processor,2020-03-30 00:00:00,core,http://arxiv.org/abs/2003.03134,,"Graph processors such as Graphcore's Intelligence Processing Unit (IPU) are
part of the major new wave of novel computer architecture for AI, and have a
general design with massively parallel computation, distributed on-chip memory
and very high inter-core communication bandwidth which allows breakthrough
performance for message passing algorithms on arbitrary graphs. We show for the
first time that the classical computer vision problem of bundle adjustment (BA)
can be solved extremely fast on a graph processor using Gaussian Belief
Propagation. Our simple but fully parallel implementation uses the 1216 cores
on a single IPU chip to, for instance, solve a real BA problem with 125
keyframes and 1919 points in under 40ms, compared to 1450ms for the Ceres CPU
library. Further code optimisation will surely increase this difference on
static problems, but we argue that the real promise of graph processing is for
flexible in-place optimisation of general, dynamically changing factor graphs
representing Spatial AI problems. We give indications of this with experiments
showing the ability of GBP to efficiently solve incremental SLAM problems, and
deal with robust cost functions and different types of factors.Comment: Published in Proceedings of the IEEE Conference on Computer Vision
  and Pattern Recognition (CVPR 2020). Video:
  https://www.youtube.com/watch?v=TqeN8aQNgd",multimedia,897,not included
,to_check,core,"How deep is your encoder: an analysis of features descriptors for an
  autoencoder-based audio-visual quality metric",2020-03-24 00:00:00,core,http://arxiv.org/abs/2003.11100,,"The development of audio-visual quality assessment models poses a number of
challenges in order to obtain accurate predictions. One of these challenges is
the modelling of the complex interaction that audio and visual stimuli have and
how this interaction is interpreted by human users. The No-Reference
Audio-Visual Quality Metric Based on a Deep Autoencoder (NAViDAd) deals with
this problem from a machine learning perspective. The metric receives two sets
of audio and video features descriptors and produces a low-dimensional set of
features used to predict the audio-visual quality. A basic implementation of
NAViDAd was able to produce accurate predictions tested with a range of
different audio-visual databases. The current work performs an ablation study
on the base architecture of the metric. Several modules are removed or
re-trained using different configurations to have a better understanding of the
metric functionality. The results presented in this study provided important
feedback that allows us to understand the real capacity of the metric's
architecture and eventually develop a much better audio-visual quality metric",multimedia,898,not included
,to_check,core,"Unsupervised Learning for Subterranean Junction Recognition Based on 2D
  Point Cloud",2020-06-07 00:00:00,core,http://arxiv.org/abs/2006.04225,,"This article proposes a novel unsupervised learning framework for detecting
the number of tunnel junctions in subterranean environments based on acquired
2D point clouds. The implementation of the framework provides valuable
information for high level mission planners to navigate an aerial platform in
unknown areas or robot homing missions. The framework utilizes spectral
clustering, which is capable of uncovering hidden structures from connected
data points lying on non-linear manifolds. The spectral clustering algorithm
computes a spectral embedding of the original 2D point cloud by utilizing the
eigen decomposition of a matrix that is derived from the pairwise similarities
of these points. We validate the developed framework using multiple data-sets,
collected from multiple realistic simulations, as well as from real flights in
underground environments, demonstrating the performance and merits of the
proposed methodology",multimedia,899,included
,to_check,core,"Accelerating Multi-attribute Unsupervised Seismic Facies Analysis With
  RAPIDS",2020-09-17 00:00:00,core,http://arxiv.org/abs/2007.15152,,"Classification of seismic facies is done by clustering seismic data samples
based on their attributes. Year after year, 3D datasets used by exploration
geophysics increase in size, complexity, and number of attributes, requiring a
continuous rise in the classification performance. In this work, we explore the
use of Graphics Processing Units (GPUs) to perform the classification of
seismic surveys using the well-established Machine Learning (ML) method
k-means. We show that the high-performance distributed implementation of the
k-means algorithm available at the RAPIDS library can be used to classify
facies in large seismic datasets much faster than a classical parallel CPU
implementation (up to 258-fold faster in NVIDIA V100 GPUs), especially for
large seismic blocks. We tested the algorithm with different real seismic
volumes, including Netherlands, Parihaka, and Kahu (from 12GB to 66GB)",multimedia,900,not included
,to_check,core,Application des algorithmes d’apprentissage automatique pour la détection de défauts de roulements sur les machines tournantes dans le cadre de l’Industrie 4.0,2020-01-01 00:00:00,core,https://core.ac.uk/download/335349820.pdf,,"L’internet des objets industriels (IIoT) fait partie d’un concept plus large connu sous le nom de L’Internet des Objets, ou IdO (en anglais Internet of Things, ou IoT). Les IIoT apportent de nouvelles opportunités aux sites de production telles que la diminution des coûts des opérations et l’augmentation de la productivité dans le but d’une exploitation optimale. La technologie IIoT révolutionnera les procédés de fabrication industrielle en permettant l’acquisition des quantités importantes de données, à des vitesses beaucoup plus élevées, et bien plus efficaces qu’auparavant. Un certain nombre d’entreprises innovantes ont commencé à mettre en oeuvre l’IIoT en exploitant des appareils connectés intelligents dans leurs usines (c’est ce qu’on appelle les usines intelligentes ou Industrie 4.0). Dans une perspective d’acquisition des données, l’internet des objets a favorisé l’inclusion des sous-systèmes ainsi que leurs analyses en temps réel. Pour ce faire, l’Industrie 4.0 introduit un concept de production numérisée en permettant une intégration souple et agile de nouveaux modèles commerciaux tout en maintenant les coûts de fabrications et l’efficacité à un niveau raisonnable. Dans ce projet de recherche, nous allons étudier la maintenance prédictive des installations industrielles. Cette tâche est essentielle au bon fonctionnement de l’usine et à la sécurité des personnes. Compte tenu des coûts, il est judicieux d’établir un bon équilibre entre entretien préventif systématique et entretien correctif. La surveillance des installations concourt à limiter le niveau d’entretien préventif. Dans ce contexte, l’analyse vibratoire constitue un outil de détection puis de diagnostic de défauts de fonctionnement des installations. Aussi, après avoir décrit les principales manifestations vibratoires des défauts de fonctionnement des machines, nous allons examiner les stratégies de détection et de surveillance dans le domaine temporel et fréquentiel et la démarche de diagnostic en nous appuyant sur l’intelligence artificielle et en analysant les signaux vibratoires permettant de déduire une politique de gestion de la maintenance. Notre objectif principal est de réaliser un système permettant d’assurer l’analyser les signaux vibratoires d’une machine tournante dans le domaine temps/fréquence. Il sera ensuite aisé de le comparer avec un système d’apprentissage automatique capable de détecter et classer les défauts grâce à des algorithmes d’intelligence artificielle. L’application vise à fournir un système de détection de défauts fiable afin de réduire les temps de dépannages et favoriser un diagnostic rapide des pannes des systèmes industriels. Par conséquent, le projet va de la conception jusqu’à la mise en oeuvre des algorithmes informatiques avec des exemples réels de signaux vibratoires. Un processus d’optimisation sera mis en oeuvre lors de la prise des décisions de l’équipe humaine afin d’augmenter l’efficacité des résultats et réduire les situations à risque qui mettront les machines hors d’usage. Ce projet de recherche permettra donc d’introduire un système intelligent dans un environnement de production de l’industrie 4.0. Durant cette étude, nous avons implémenté dans un premier temps des algorithmes qui nous ont permis d’extraire des caractéristiques des signaux d’une machine tournante. Ensuite, nous avons mis en place un système de surveillance de l’état de cette machine en fixant un seuil pour le bon fonctionnement et un autre pour déclencher une alarme quand ce dernier est atteint. Dans un second temps, nous avons utilisé des algorithmes d’apprentissage automatique (ou machine learning) pour classer les différents niveaux de défaillance. Après extraction des caractéristiques des signaux dans le domaine temporel et fréquentiel nous avons obtenu une fiabilité de 99.3% avec la méthode d’estimation dite « validation croisée » (en anglais cross-validation). Ce processus d’apprentissage optimise les paramètres du modèle afin que celui-ci corresponde aux données le mieux possible. Ensuite, nous avons évalué une autre technique de validation « testset validation » (en anglais holdout method). Cette technique est recommandée pour les grands ensembles de données. Après plusieurs tests, nous avons pu obtenir un taux de classification de 100% pour les différents niveaux de défauts considérés.



The internet of industrial objects (IIoT) is part of a larger concept known as the ""Internet of Things (IoT)"". IoT's bring new opportunities to production sites such as lower operating costs and increased productivity for optimal operation. The application of IoT to the manufacturing industry is called IIoT (or Industrial Internet or Industry 4.0). The IoT will revolutionize manufacturing by enabling the acquisition and accessibility of massive amounts of data at much higher speeds and far more efficiently than before. A few innovative companies have started to implement IoT by exploiting smart connected devices in their factories (so-called smart factories or Industry 4.0). From a data acquisition perspective, the Internet of Things has favoured the inclusion of subsystems and their analysis in real time. To achieve this, Industry 4.0 introduces a digitized production concept by allowing flexible and agile integration of new business models while keeping manufacturing costs and efficiency at a reasonable level. In this thesis, we will study the predictive maintenance for industry 4.0. This method of preventing asset failure by analyzing production data to identify patterns and predict issues before they happen. However, considering the costs, it is wise to strike a better balance between routine preventative maintenance and corrective maintenance. Facility monitoring helps to reduce the level of preventative maintenance. In this context, vibration analysis is a tool for detecting and then diagnosing malfunctions of installations. Also, after describing the main vibratory manifestations of failures of the machines, we will examine the detection and surveillance strategies at the time and frequency domain and the diagnosis process based on artificial intelligence by analyzing the vibratory signals. Our main objective is to design a system that will analyze the vibrating signals of a rotating machine in the time/frequency domain. A machine learning system will be used to detect and classify the defects based on artificial intelligence algorithms. The application aims to provide a reliable fault detection system to reduce repair times and promote rapid diagnosis of industrial operations. Therefore, the project goes from the design to the implementation of new algorithms using vibratory signals. An optimization process will be implemented when the decisions of the adequate staff are made to increase the efficiency of the results and reduce the risk situations that will put the machines out of use. Therefore, this research project will introduce an intelligent system into an industry 4.0 production environment. During this research project, firstly, we will implement algorithms that allowed us to extract characteristics of the vibration signals and set up a system to monitor the state of a rotating machine by setting a threshold of operation and trigger an alarm when this threshold is reached. In a second step, we will use signal processing and machine learning toolkits to classify the different levels of machine failure and use this method to detect the presence of error when the machine is running. After extracting the characteristics of our signals at the time and frequency domain, we will be obtained with the ""cross-validation"" a recognition rate of 99.3%. The final implementation with the ""holdout validation"" recommended for large datasets allowed us to have a classification rate of 100% of the different levels of defects considered",multimedia,901,not included
,to_check,core,Digital me ontology and ethics,2020-12-22 00:00:00,core,http://arxiv.org/abs/2012.14325,,"This paper addresses ontology and ethics of an AI agent called digital me. We
define digital me as autonomous, decision-making, and learning agent,
representing an individual and having practically immortal own life. It is
assumed that digital me is equipped with the big-five personality model,
ensuring that it provides a model of some aspects of a strong AI:
consciousness, free will, and intentionality. As computer-based personality
judgments are more accurate than those made by humans, digital me can judge the
personality of the individual represented by the digital me, other individuals'
personalities, and other digital me-s. We describe seven ontological qualities
of digital me: a) double-layer status of Digital Being versus digital me, b)
digital me versus real me, c) mind-digital me and body-digital me, d) digital
me versus doppelganger (shadow digital me), e) non-human time concept, f)
social quality, g) practical immortality. We argue that with the advancement of
AI's sciences and technologies, there exist two digital me thresholds. The
first threshold defines digital me having some (rudimentarily) form of
consciousness, free will, and intentionality. The second threshold assumes that
digital me is equipped with moral learning capabilities, implying that, in
principle, digital me could develop their own ethics which significantly
differs from human's understanding of ethics. Finally we discuss the
implications of digital me metaethics, normative and applied ethics, the
implementation of the Golden Rule in digital me-s, and we suggest two sets of
normative principles for digital me: consequentialist and duty based digital me
principles.Comment: 17 page",multimedia,902,not included
,to_check,core,Unsupervised Learning for Subterranean Junction Recognition Based on 2D Point Cloud,2020-09-01 00:00:00,core,https://core.ac.uk/download/345074841.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',"This article proposes a novel unsupervised learning framework for detecting the number of tunnel junctions in subterranean environments based on acquired 2D point clouds. The implementation of the framework provides valuable information for high level mission planners to navigate an aerial platform in unknown areas or robot homing missions. The framework utilizes spectral clustering, which is capable of uncovering hidden structures from connected data points lying on non-linear manifolds. The spectral clustering algorithm computes a spectral embedding of the original 2D point cloud by utilizing the eigen decomposition of a matrix that is derived from the pairwise similarities of these points. We validate the developed framework using multiple data-sets, collected from multiple realistic simulations, as well as from real flights in underground environments, demonstrating the performance and merits of the proposed methodology",multimedia,903,not included
,to_check,core,"CorGAN: Correlation-Capturing Convolutional Generative Adversarial
  Networks for Generating Synthetic Healthcare Records",2020-03-04 00:00:00,core,http://arxiv.org/abs/2001.09346,,"Deep learning models have demonstrated high-quality performance in areas such
as image classification and speech processing. However, creating a deep
learning model using electronic health record (EHR) data, requires addressing
particular privacy challenges that are unique to researchers in this domain.
This matter focuses attention on generating realistic synthetic data while
ensuring privacy. In this paper, we propose a novel framework called
correlation-capturing Generative Adversarial Network (CorGAN), to generate
synthetic healthcare records. In CorGAN we utilize Convolutional Neural
Networks to capture the correlations between adjacent medical features in the
data representation space by combining Convolutional Generative Adversarial
Networks and Convolutional Autoencoders. To demonstrate the model fidelity, we
show that CorGAN generates synthetic data with performance similar to that of
real data in various Machine Learning settings such as classification and
prediction. We also give a privacy assessment and report on statistical
analysis regarding realistic characteristics of the synthetic data. The
software of this work is open-source and is available at:
https://github.com/astorfi/cor-gan.Comment: Accepted to be published in the 33rd International FLAIRS Conference,
  AI in Healthcare Informatic",multimedia,905,not included
,to_check,core,Detection of traffic anomalies for a safety system of smart city,2020-01-01 00:00:00,core,https://core.ac.uk/download/346528589.pdf,,"For modern smart city with sustainable development we need to provide reasonable
level of safety and efficient management of the resources. Instant response to incidents and
abnormal situations will help to provide such high bars for city residents, which requires
deployment of application of intelligent information processing and data analytics into
infrastructure. Closed-circuit television (CCTV) is playing a key part in assurance of city
security - most of the modern large cities equip with powerful monitoring systems and
surveillance cameras. Video data covers most of the city and could be efficiently used to find
anomalies or trends. This hard task for non-stop video monitoring could be solved by modern
achievements in machine learning and computer vision techniques, which can automate the
process of video analysis and identify anomalies and incidents without human intervention. In
this paper, we used computer vision methods like object detection and tracking, as well as
neuron networks for classification and detection of anomalies on real time video. As a result of
this work we suggested the working approach for detection of vehicle/pedestrian violating
legal trajectory anomaly, which we tested on real-time video provided by surveillance cameras
of the city of Kazan",multimedia,906,not included
,to_check,core,"Unmasking Communication Partners: A Low-Cost AI Solution for Digitally
  Removing Head-Mounted Displays in VR-Based Telepresence",2020-11-06 00:00:00,core,http://arxiv.org/abs/2011.03630,,"Face-to-face conversation in Virtual Reality (VR) is a challenge when
participants wear head-mounted displays (HMD). A significant portion of a
participant's face is hidden and facial expressions are difficult to perceive.
Past research has shown that high-fidelity face reconstruction with personal
avatars in VR is possible under laboratory conditions with high-cost hardware.
In this paper, we propose one of the first low-cost systems for this task which
uses only open source, free software and affordable hardware. Our approach is
to track the user's face underneath the HMD utilizing a Convolutional Neural
Network (CNN) and generate corresponding expressions with Generative
Adversarial Networks (GAN) for producing RGBD images of the person's face. We
use commodity hardware with low-cost extensions such as 3D-printed mounts and
miniature cameras. Our approach learns end-to-end without manual intervention,
runs in real time, and can be trained and executed on an ordinary gaming
computer. We report evaluation results showing that our low-cost system does
not achieve the same fidelity of research prototypes using high-end hardware
and closed source software, but it is capable of creating individual facial
avatars with person-specific characteristics in movements and expressions.Comment: 9 pages, IEEE 3rd International Conference on Artificial Intelligence
  & Virtual Realit",multimedia,907,included
,to_check,core,"Practical sensorless aberration estimation for 3D microscopy with deep
  learning",2020-07-05 00:00:00,core,http://arxiv.org/abs/2006.01804,,"Estimation of optical aberrations from volumetric intensity images is a key
step in sensorless adaptive optics for 3D microscopy. Recent approaches based
on deep learning promise accurate results at fast processing speeds. However,
collecting ground truth microscopy data for training the network is typically
very difficult or even impossible thereby limiting this approach in practice.
Here, we demonstrate that neural networks trained only on simulated data yield
accurate predictions for real experimental images. We validate our approach on
simulated and experimental datasets acquired with two different microscopy
modalities, and also compare the results to non-learned methods. Additionally,
we study the predictability of individual aberrations with respect to their
data requirements and find that the symmetry of the wavefront plays a crucial
role. Finally, we make our implementation freely available as open source
software in Python",multimedia,908,not included
,to_check,core,All Too Well,2020-07-01 00:00:00,core,https://core.ac.uk/download/344752398.pdf,RIT Scholar Works,"All Too Well is a 2D animation graduate thesis film, with a length of 6 minutes and 49 seconds. The production phase went from September 2018 to January 2020.
A father who lost his son in a car accident recreates the family by bringing an artificial intelligence boy into the home to replace the lost son. Not only does their relationship fail to heal the father\u27s broken heart, but it also often brings back painful memories of his real boy. This only makes him realize that the robot son cannot replace his real son. So, he turns the power off of the robot son and deeply grieves. The mother witnesses her husband’s actions, approaches him, and then turns his power off. Themes of loss and healing in the age of artificial intelligence and the use of robots to fill our need for love and normal relationships are central to this abstracted narrative.
The major software used during the entire production process include: Adobe Photoshop, Adobe After Effects, Adobe Premiere, TVPaint and MOHO. The original music is a score by KaMan Chang. The final output format is 1080HD.
This paper will retrace the making of All Too Well and share the personal experience during its creation",multimedia,909,not included
,to_check,core,I-POST: Intelligent Point of Sale and Transaction System,2020-11-11 00:00:00,core,http://arxiv.org/abs/2011.06144,,"We propose a novel solution for the cashier problem. Current cashier
system/Point of Sale (POS) terminals can be inefficient, cumbersome and
time-consuming for the users. There is a need for a solution dependent on
modern technology and ubiquitous computing resources. We present I-POST
(Intelligent Point of Sale and Transaction) as a software system that uses
smart devices, mobile phone and state of the art machine learning algorithms to
process the user transactions in automated and real time manner. I-POST is an
automated checkout system that allows the user to walk in a store, collect his
items and exit the store. There is no need to stand and wait in a queue. The
system uses object detection and facial recognition algorithm to process the
authentication of the client and the state of the object. At point of exit, the
classifier sends the data to the backend server which execute the payments. The
system uses Convolution Neural Network (CNN) for the image recognition and
processing. CNN is a supervised learning model that has found major application
in pattern recognition problem. The current implementation uses two classifiers
that work intrinsically to authenticate the user and track the items. The model
accuracy for object recognition is 97%, the loss is 9.3%. We expect that such
systems can bring efficiency to the market and has the potential for broad and
diverse applications.Comment: 8 pages, 11 figure",multimedia,910,not included
,to_check,core,Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video,2020-02-06 00:00:00,core,http://arxiv.org/abs/1910.09430,,"Key challenges for the deployment of reinforcement learning (RL) agents in
the real world are the discovery, representation and reuse of skills in the
absence of a reward function. To this end, we propose a novel approach to learn
a task-agnostic skill embedding space from unlabeled multi-view videos. Our
method learns a general skill embedding independently from the task context by
using an adversarial loss. We combine a metric learning loss, which utilizes
temporal video coherence to learn a state representation, with an entropy
regularized adversarial skill-transfer loss. The metric learning loss learns a
disentangled representation by attracting simultaneous viewpoints of the same
observations and repelling visually similar frames from temporal neighbors. The
adversarial skill-transfer loss enhances re-usability of learned skill
embeddings over multiple task domains. We show that the learned embedding
enables training of continuous control policies to solve novel tasks that
require the interpolation of previously seen skills. Our extensive evaluation
with both simulation and real world data demonstrates the effectiveness of our
method in learning transferable skills from unlabeled interaction videos and
composing them for new tasks. Code, pretrained models and dataset are available
at http://robotskills.cs.uni-freiburg.deComment: Accepted at the 2020 IEEE International Conference on Robotics and
  Automation (ICRA). Video at https://www.youtube.com/watch?v=z8gG1k9kSqA
  Project page at http://robotskills.cs.uni-freiburg.d",multimedia,911,not included
,to_check,core,Augmented LiDAR Simulator for Autonomous Driving,2019-04-10 00:00:00,core,http://arxiv.org/abs/1811.07112,,"In Autonomous Driving (AD), detection and tracking of obstacles on the roads
is a critical task. Deep-learning based methods using annotated LiDAR data have
been the most widely adopted approach for this. Unfortunately, annotating 3D
point cloud is a very challenging, time- and money-consuming task. In this
paper, we propose a novel LiDAR simulator that augments real point cloud with
synthetic obstacles (e.g., cars, pedestrians, and other movable objects).
Unlike previous simulators that entirely rely on CG models and game engines,
our augmented simulator bypasses the requirement to create high-fidelity
background CAD models. Instead, we can simply deploy a vehicle with a LiDAR
scanner to sweep the street of interests to obtain the background point cloud,
based on which annotated point cloud can be automatically generated. This
unique ""scan-and-simulate"" capability makes our approach scalable and
practical, ready for large-scale industrial applications. In this paper, we
describe our simulator in detail, in particular the placement of obstacles that
is critical for performance enhancement. We show that detectors with our
simulated LiDAR point cloud alone can perform comparably (within two percentage
points) with these trained with real data. Mixing real and simulated data can
achieve over 95% accuracy.Comment: 10 page",multimedia,912,not included
,to_check,core,"Can a Robot Become a Movie Director? Learning Artistic Principles for
  Aerial Cinematography",2019-10-15 00:00:00,core,http://arxiv.org/abs/1904.02579,,"Aerial filming is constantly gaining importance due to the recent advances in
drone technology. It invites many intriguing, unsolved problems at the
intersection of aesthetical and scientific challenges. In this work, we propose
a deep reinforcement learning agent which supervises motion planning of a
filming drone by making desirable shot mode selections based on aesthetical
values of video shots. Unlike most of the current state-of-the-art approaches
that require explicit guidance by a human expert, our drone learns how to make
favorable viewpoint selections by experience. We propose a learning scheme that
exploits aesthetical features of retrospective shots in order to extract a
desirable policy for better prospective shots. We train our agent in realistic
AirSim simulations using both a hand-crafted reward function as well as reward
from direct human input. We then deploy the same agent on a real DJI M210 drone
in order to test the generalization capability of our approach to real world
conditions. To evaluate the success of our approach in the end, we conduct a
comprehensive user study in which participants rate the shot quality of our
methods. Videos of the system in action can be seen at
https://youtu.be/qmVw6mfyEmw",multimedia,913,included
,to_check,core,"Pose Estimation for Texture-less Shiny Objects in a Single RGB Image
  Using Synthetic Training Data",2019-09-23 00:00:00,core,http://arxiv.org/abs/1909.10270,,"In the industrial domain, the pose estimation of multiple texture-less shiny
parts is a valuable but challenging task. In this particular scenario, it is
impractical to utilize keypoints or other texture information because most of
them are not actual features of the target but the reflections of surroundings.
Moreover, the similarity of color also poses a challenge in segmentation. In
this article, we propose to divide the pose estimation process into three
stages: object detection, features detection and pose optimization. A
convolutional neural network was utilized to perform object detection.
Concerning the reliability of surface texture, we leveraged the contour
information for estimating pose. Since conventional contour-based methods are
inapplicable to clustered metal parts due to the difficulties in segmentation,
we use the dense discrete points along the metal part edges as semantic
keypoints for contour detection. Afterward, we exploit both keypoint
information and CAD model to calculate the 6D pose of each object in view. A
typical implementation of deep learning methods not only requires a large
amount of training data, but also relies on intensive human labor for labeling
the datasets. Therefore, we propose an approach to generate datasets and label
them automatically. Despite not using any real-world photos for training, a
series of experiments showed that the algorithm built on synthetic data perform
well in the real environment",multimedia,914,not included
,to_check,core,Internet of Things (IoT) for Live Monitoring and Analysis of 3D Printers,2019-04-23 00:00:00,core,https://core.ac.uk/download/249321724.pdf,Tennessee Tech University,"As the Internet of Things (IoT) continues to evolve, the need arises to keep a constant eye on the integrity of IoT implementations to ensure that everything is running as it should. This is especially true in industrial sector. Machine Health Monitoring (MHM) was created. The primary goal of MHM is to ensure that all devices within an Industrial IoT (IIoT) implementation are functioning as expected at all times. For our specific usage of MHM, we are monitoring various readings from a 3D printer. We have currently implemented an array of sensors, and a live camera feed, with an accelerometer and a power meter soon to come. We have also created a web server running off of a Raspberry Pi 3 to hold our database and provide accessibility to the data. Through this setup, a user could access our storage and view up to the last 60 readings from the sensors in real time. The user can also view the live camera feed. We plan to use that range to implement fault detection with an alert system. We also look to eventually add predictive AI in an attempt to prevent faults before they occur. We believe our implementation provides a more cost-effective and scalable architecture for MHM. It also provides large amounts of customizability and adaptability due to our usage of generalized sensor data inputs and storage. The system is also self-monitoring, providing proper detection of its own internal issues",multimedia,915,not included
,to_check,core,Perchage automatique de drones basé sur la vision artificielle,2019-11-01 00:00:00,core,https://core.ac.uk/download/269026357.pdf,,"RÉSUMÉ L'utilisation de l'intelligence artificielle et de la vision se développe considérablement dans l'industrie des drones, notamment pour la saisie ou le dépôt d'objets ou encore pour l'atterrissage. Le perchage de drone, étroitement lié à ces tâches, commence également à se développer. Cette faculté permettrait aux drones de réaliser de nouvelles tâches mais aussi de combler leurs inconvénients tels que leur faible durée de vol ou le fait qu'ils soient fragiles. Par exemple, cela assurerait à un drone de se poser en cas de fin de batterie ou de mauvaises conditions climatiques. Les techniques actuelles de perchage ou de saisie d'objet effectuées à partir de la vision artificielle et d'un système de préhension ajouté au drone, se basent seulement sur la détection d'objets. Les objets/supports, lors des tests, sont sélectionnés en avance par les chercheurs afin d'avoir une bonne concordance avec le préhenseur. Ainsi, dans le cas où le support possède une forme complexe ou encore des dimensions trop différentes par rapport à celles du préhenseur, le drone le détectera et essayera de s'y percher sans succès. L'objectif de cette maîtrise recherche est de développer un système de détection d'objets, par vision par ordinateur, qui selon les caractéristiques du préhenseur du drone, détecte les objets, leur attribue un score de concordance et renvoie le support idéal. Le score de concordance, que nous avons établi et nommé ""CSP"" (Concordance Support-Préhenseur) dans ce mémoire, se détermine à partir de la comparaison entre l'intervalle d'ouverture du préhenseur et les dimensions réelles des objets. La réalisation d'une comparaison ainsi que l'utilisation d'un score de concordance pour la détermination d'un support adéquat est une procédure que nous avons élaborée. Partant d'un algorithme de détection basé sur la classification et la segmentation des objets détectés, la solution proposée dans ce mémoire a été développée en 3 phases : 1. Entrainement supervisé du réseau de neurones de l'algorithme de détection pour de nouvelles classes d'objets adaptées au perchage de drone. 2. Conception d'un algorithme, permettant de réaliser une comparaison entre les dimensions des objets détectés et celles du système de préhension dans le but de déterminer le support idéal pour le perchage du drone. 3. Évaluation des performances du modèle de détection complet, regroupant les deux algorithmes, à partir de tests réalisés sur un ensemble d'objets avec des paramètres et des conditions d'environnement différents entre chaque test. Cette évaluation a démontré la précision et la fiabilité de notre système dans la détermination du support idéal à partir de la vision artificielle. Nous avons effectué un ensemble de 36 tests avec pour chaque test un paramétrage différent. De plus, chaque test a été effectué avec un nombre de 10 répétitions soit avec une configuration des paramètres identique dans le but d'avoir des résultats plus fiables. Ces tests ont été réalisés avec une caméra ayant une résolution 640 x 480 et une carte graphique GTX 1080Ti (GPU). La performance globale de notre système obtient un taux de succès de 84.17 % dans la détermination du support idéal. Ce taux de succès atteint même les 100 % dans le cas d'une différence de diamètre d'au moins 20 mm entre les objets détectés. La vitesse d'exécution du modèle, dans le cas d'un seul objet détecté par image, prend en moyenne 0.42 seconde pour l'analyse d'une image, ce qui correspond à 2.38 fps. Le fait d'obtenir un temps d'exécution efficace dans le traitement de chaque objet permet ainsi de garder une fluidité dans la détection lors du déplacement de la caméra et ainsi d'éviter les mauvaises détections et/ou mauvaises segmentations.----------ABSTRACT The use of artificial vision and artificial intelligence in drones applications is experiencing rapid growth, particularly in pick and drop applications but also in drone landing. Drone perching, being closely linked to applications stated above, has also begun to emerge. This research would allow the drones to realize new tasks but also to mitigate their disadvantages such as their short flight time or their fragility. Current perching and object grasping approaches are carried out by utilizing artificial vision and robotic grippers integrated with the drones. The target objects or their handles are chosen in a way to match the geometry of the gripper. Thus, in case where supports have complex form or dimensions too differents in comparison with gripper dimensions, perching won't be avaiblable. The purpose of this study is to develop a detection system that uses computer vision to return ideal support for drone perching. The ideal support corresponds to the object with the best matching score. The matching score, that we have developed and called ""CSP"" (Concordance Support-Préhenseur) in this study, is determined from the comparison between the gripper opening and the real object dimension. The implementation of this comparison as well as the utilization of matching score to the determination of the best support is a method that we created. The suggested solution in this study is based on a detection algorithm which uses the classification and segmentation of objects. The several stages of the uses method are enumerated as follows : 1. Supervised training of neural network detection algorithm for new objects classes adapted to drone perching. 2. Conception of an algorithm, allowing to realize a comparison between object dimensions and grasping system dimensions in order to determine the ideal support. 3. Evaluation of the global system performances thanks to tests performed on a set of objects. This evaluation has shown the precision and the reliability of our system in the ideal support determination based on artificial vision. We used a set of 36 tests with, for each test, different parameter values and different environmental conditions. Moreover, each test has been performed with a number of 10 repetitions for a given configuration in the purpose to obtain more reliable results. These tests have been made using a camera with a resolution of 640 x 480 and a graphic processor unit GTX 1080Ti (GPU). The global performance of our system obtains a success rate of 84.17 % in the ideal support determination. This success rate even reaches 100 % in the case where the difference between the diameter of each object is at least 20 mm. The execution speed of this model, with only one detected object by image, takes on average 0.42 second for the analysis of one image, which corresponds to 2.38 fps. To maintain fluidity and thus to avoid wrong detections and/or segmentations during camera motion, an efficient execution time is needed in the treatment of each object",multimedia,917,not included
,to_check,core,GAMESPECT: A Composition Framework and Meta-Level Domain Specific Aspect Language for Unreal Engine 4,2019-01-01 00:00:00,core,https://core.ac.uk/download/215374590.pdf,NSUWorks,"Game engine programming involves a great number of software components, many of which perform similar tasks; for example, memory allocation must take place in the renderer as well as in the creation routines while other tasks such as error logging must take place everywhere. One area of all games which is critical to the success of the game is that of game balance and tuning. These balancing initiatives cut across all areas of code from the player and AI to the mission manager. In computer science, we’ve come to call these types of concerns “cross cutting”. Aspect oriented programming was developed, in part, to solve the problems of cross cutting: employing “advice” which can be incorporated across different pieces of functionality.
Yet, despite the prevalence of a solution, very little work has been done to bring cross cutting to game engine programming. Additionally, the discipline involves a heavy amount of code rewriting and reuse while simultaneously relying on many common design patterns that are copied from one project to another. In the case of game balance, the code may be wildly different across two different games despite the fact that similar tasks are being done. These two problems are exacerbated by the fact that almost every game engine has its own custom DSL (domain specific language) unique to that situation. If a DSL could showcase the areas of cross cutting concerns while highlighting the ability to capture design patterns that can be used across games, significant productivity savings could be achieved while simultaneously creating a common thread for discussion of shared problems within the domain.
This dissertation sought to do exactly that- create a metalanguage called GAMESPECT which supports multiple styles of DSLs while bringing aspect-oriented programming into the DSL’s to make them DSAL (domain specific aspect languages). The example cross cutting concern was game balance and tuning since it’s so pervasive and important to gaming. We have created GAMESPECT as a language and a composition framework which can assist engine developers and game designers in balancing their games, forming one central place for game balancing concerns even while these concerns may cross different languages and locations inside the source code. Generality was measured by showcasing the composition specifications in multiple contexts and languages.
In addition to evaluating generality and performance metrics, effectiveness was be measured. Specifically, comparisons were made between a balancing initiative when performed with GAMESPECT vs a traditional methodology. In doing so, this work shows a clear advantage to using a Metalanguage such as GAMESPECT for this task. In general, a line of code reduction of 9-40% per task was achieved with negligible effects to performance. The use of a metalanguage in Unreal Engine 4 is a starting point to further discussions concerning other game engines. In addition, this work has implications beyond video game programming. The work described highlights benefits which might be achieved in other disciplines where design pattern implementations and cross-cutting concern usage is high; the real time simulation field and the field of Windows GUI programming are two examples of future domains",multimedia,918,not included
,to_check,core,'Wiley',2018-01-01 00:00:00,core,towards robust and domain invariant feature representations in deep learning,10.13016/M2MK65C49,"A fundamental problem in perception-based systems is to define

and learn representations of the scene that are more robust and adaptive to several nuisance

factors. Over the recent past, for a variety of tasks involving images, learned representations  have been empirically shown to outperform handcrafted ones.

However, their inability to generalize across varying data distributions poses the following

question: Do representations learned using deep networks just fit a given data distribution or do

they sufficiently model the underlying structure of the problem ? This question could be

understood using a simple example: If a learning algorithm is shown a number of images of a simple handwritten digit, then the representation learned should be generic enough to identify the same digit in

a different form. With regards to deep networks, although the learned representation has been shown

to be robust to various forms of synthetic distortions such as random noise, they fail in the presence of

more implicit forms of naturally occurring distortions. In this dissertation, we

propose approaches to mitigate the effect of such distortions and in the process, study some

vulnerabilities of deep networks to small imperceptible changes that occur in the given input.

The research problems that comprise this dissertation lie in the cross section of two open topics: (1)

Studying and developing methods that enable neural networks learn robust representations (2) Improving

generalization of neural nets across domains.  The first part of the dissertation  approaches the problem of robustness from two broad viewpoints:  Robustness to external nuisance factors that occur in the data and robustness

(or a lack thereof) to perturbations of the learned feature space.  In the second part, we focus on learning representations that are invariant to external covariate

shift, which is more commonly termed as domain shift. 

Towards learning representations robust

to external nuisance factors, we propose an approach that couples a deep convolutional neural

network with a low-dimensional discriminative embedding learned using triplet probability

constraints to solve the unconstrained face analysis problem. While previous approaches in this area have proposed scalable yet ad-hoc solutions to this problem, we propose a principled and parameter free formulation which is based on maximum likelihood estimation. In addition, we employ the principle of transfer learning to realize a deep network architecture that can train faster and on lesser data yet significantly outperforms existing approaches on the unconstrained face verification task. We demonstrate the robustness

of the approach to challenges including age, pose, blur and clutter by performing clustering

experiments on challenging benchmarks.

Recent seminal works have shown that deep neural networks are susceptible to visually imperceptible perturbations of the input. In this dissertation, we build on their ideas in two unique ways: (a) We show that neural networks that perform pixel-wise semantic segmentation tasks also suffer from this vulnerability, despite being trained with more extra information compares to simple classification tasks.  In addition, we present a novel self correcting mechanism in segmentation networks and provide an efficient way to generate such perturbations (b) We present a novel approach to regularize deep neural networks by perturbing intermediate layer activations in an efficient manner, thereby exploring the trade-off between conventional regularization and adversarial robustness within the context of very deep networks. Both of these works provide interesting directions towards understanding the secure nature of deep learning algorithms. 

While humans find it extremely simple to generalize their knowledge across domains, machine learning algorithms including deep neural networks suffer from the problem of domain shift across what are commonly termed as 'source' (S) 

and 'target' (T) distributions. Let the data that a learning algorithm

is trained on be sampled from  S. If the real data used to evaluate the model is

then sampled from T, then the learnt model under-performs on the target data. This inability to generalize is characterized as domain shift. 

Our attempt to address this problem involves learning a common

feature subspace, where distance between source and target distributions are minimized. Estimating the distance between different domains is highly non-trivial and is an open research

problem in itself. In our approach we parameterize the distance measure by using a Generative

Adversarial Network (GAN). A GAN involves a two player game between two mappings com-

monly termed as generator and discriminator. These mappings are learned simultaneously by

employing an adversarial game, i.e. by letting the generator fool the discriminator and enabling

the discriminator to outperform the generator. This adversarial game can be formulated as a

minimax problem. In our approach, we learn three mappings simultaneously: the generator,

discriminator and a feature mapping that contains information about both the content and the

domain of the input. We deploy a two-level minimax game, where the first level is a competition

between the generator and a discriminator similar to a GAN; the second level game is where the

feature mapping attempts to fool the discriminator thereby introducing domain invariance in

the learned feature representation. We have extensively evaluated this approach for different

tasks such as object classification and semantic segmentation, where we achieve state of the

art results across several real datasets. In addition to the conceptual novelty, our approach

presents a more efficient and scalable solution compared to other approaches that attempt to

solve the same problem.

In the final part of this dissertation, we describe some ongoing efforts and future directions of research. Inspired from the study of perturbations described above, we propose a novel metric on how to effectively choose pixels to label given an image, for a pixel-wise segmentation task. This has the potential to significantly reduce the labeling effort and our preliminary results for the task of semantic segmentation are encouraging.  While the domain adaptation approach proposed above considered static images, we propose an extension to video data aided by the use of recurrent neural networks.  Use of full temporal information, when available, provides the perceptual system additional context to disambiguate among smaller object classes  that commonly occur in real scenes",multimedia,919,not included
,to_check,core,'Robotics: Science and Systems Foundation',2018-04-09 00:00:00,core,safe visual navigation via deep learning and novelty detection,10.15607/RSS.2017.XIII.064,"Robots that use learned  perceptual  models in the real world must be able to safely handle cases where they are forced to make decisions in scenarios that are unlike any of their training  examples. However,  state-of-the-art  deep  learning methods are known to produce erratic or unsafe predictions when faced with novel inputs. Furthermore, recent ensemble, bootstrap and dropout methods for quantifying neural network uncertainty may not efficiently provide accurate uncertainty estimates when queried  with  inputs  that  are  very  different  from  their  training data. Rather than unconditionally trusting the predictions of a neural network for unpredictable real-world data, we use an autoencoder  to recognize when a query is novel, and revert to a safe prior behavior. With this capability,  we can deploy an autonomous deep learning system in arbitrary environments, without concern for whether it has received the appropriate training. We demonstrate our method with a vision-guided robot that can leverage its deep neural network to navigate 50% faster than  a  safe  baseline  policy in familiar types of  environments, while  reverting  to  the prior behavior in novel environments so that it can safely collect additional training data and continually improve. A video illustrating our approach is available at: http://groups.csail.mit.edu/rrg/videos/safe visual navigation",multimedia,921,not included
,to_check,core,[s.n.],2018-07-31 00:00:00,core,estimativa de exposições não continuas a ruido : desenvolvimento de um metodo e validação na construção civil,,"Orientador : Stelamaris Rolla BertoliTese (doutorado) - Universidade Estadual de Campinas, Faculdade de Engenharia CivilResumo: O objetivo geral deste trabalho foi desenvolver um método de estimativa de exposições não contínuas ao ruído. A aplicação desse método resulta em um índice representativo da real exposição ao ruído ocupacional de trabalhadores cujas atividades apresentam uma grande variação dos níveis médios de ruído quando computados diariamente. Esse índice, denominado nível normalizado de exposição não contínua, possibilitará a implantação de medidas mais efetivas na prevenção de danos auditivos associados ao ruído e um tratamento mais acertado que o atual em relação aos benefícios legais para certas categorias profissionais, como carpinteiros, armadores, pedreiros etc. O método proposto baseou-se no princípio de igual energia. Segundo esse princípio, a energia sonora é a única responsável pelo desenvolvimento das perdas auditivas induzidas pelo ruído (PAIRs) em uma população exposta ao ruído. Esse pressuposto, já adotado pela norma internacional ISO 1999 (1990), permitiu o estabelecimento de uma relação estatística entre a exposição contínua e as perdas auditivas induzidas pelo ruído. Neste trabalho, utilizando-se de outros pressupostos, estende-se o princípio de igual energia para exposições não contínuas, possibilitando as estimativas de perdas auditivas induzidas também em ambientes com grande variação dos níveis sonoros. Os resultados do método proposto estão relacionados pela efetividade de causar dano auditivo, isto é, as perdas induzidas pelo ruído oriundas de exposição não contínua são expressas, aqui, por um nível de exposição normalizado (NEN) de exposição contínua. Neste trabalho o método proposto é aplicado sobre dois grupos de profissionais da Construção Civil: ajudantes gerais e carpinteiros. Os resultados do método foram confrontados com as condições reais de trabalho nos canteiros de obras, visando estudar a coerência entre os resultados do método proposto e os resultados de campo. No final são apresentados várias medidas técnicas de controle de ruído para fontes específicas da Construção CivilAbstract: The general purpose of this work was developing an estimate method of noncontinuous exposures to noise. The application of this method results in a representative index of the actual exposure to the occupational noise of workers whose activities present a great variation of the average noise levels when computed on a daily basis. This index shall, named normalized level of noncontinuous exposure, enable the implementation of more effective actions in the prevention of auditory damages associated to noise and a more accurate treatment than the current one in relation to the legal benefits for certain professional categories, such as carpenters, undertaker, masons, etc. The proposed method is based on the principle of equivalent energy. According to this principle, the sound energy is the only responsible for the development of auditory damages induced by noise (PAIRs) in a population exposed to noise, so that the type of noise and its daily duration do not interfere in the auditory damage process. This assumption, already adopted by the international standard ISO 1999 (1990), allowed the establishment of a statistical relation between the continuous exposure and the auditory damages caused by noise. ln this work, using other assumptions, the principle of equal energy for noncontinuous exposures is included, allowing the estimates of auditory damages also caused in environments with great variation of sound levels. The results of the proposed method are related by the effectiveness of causing auditory damages, that is, the damages caused by the noise originated from noncontinuous exposure are expressed herein by a normalized exposure level (NEN) of continuous exposure. In this work the proposed method on two groups of professionals in the Civil Construction is also applied: general assistants and carpenters, for whom the exposure to noise was evaluated and the distribution of  PAIRs was determined. The method results were compared to the actual labor conditions in the work sites observed through examinations of the productive process and quantitative evaluations of the sound exposure of each group studied, with the purpose of studying the consistency between the results of the proposed method ai:1d the field results. Finally, we present several technical steps for noise control of specific sound sources of the Civil ConstructionDoutoradoSaneamentoDoutor em Engenharia Civi",multimedia,922,not included
,to_check,core,,2018-06-24 00:00:00,core,k-nearest neighbours based classifiers for moving object trajectories reconstruction,https://core.ac.uk/download/159622295.pdf,"This article presents an exemplary prototype implementation of an Application Programming Interface (API) for incremental reconstruction of the trajectories of moving objects captured by Closed-Circuit Television (CCTV) and High-Definition Television (HDTV) cameras based on KNearest Neighbor (KNN) classifiers. This paper proposes a model-driven approach for trajectory reconstruction based on machine learning algorithms which is more efficient than other approaches for dynamic tracking, such as RGB-D (Red, Green and Red Color model with Depth) images or scale or rotation approaches. The existing approaches typically need a low-level information from the input video stream but the environment factors (indoor light, outdoor light) affect the results. The use of a predefined model allows to avoid this since the data is naturally filtered. Experiments on different input video streams demonstrate that the proposed approach is efficient for solving the tracking of moving objects in input streams in real time because it needs less granular information from the input stream. The research reported here is part of a research program of the Cyber Security Research Centre of London Metropolitan University for real-time video analytics with applicability to surveillance in security, disaster recovery and safety management, and customer insight",multimedia,923,not included
,to_check,core,,2018-09-01 00:00:00,core,development of animal detection system for traffical collision avoidance,,"One of the main causes of road accidents in Malaysia is animal-vehicle collisions. This is mainly due to the rapid modernization of the country which affects animal habitat which increases the chance for animals to be caught in roads and highways across the country. Thus, this project is conducted to study the performance of an animal detection system to help solve and reduce AVCs. This project is driven by the fact that as of now, Malaysia does not have a specific system designed to avoid AVCs. Therefore, hopefully the implementation of an animal detection system can help reduce or probably solve the case of AVCs. In this project, a hardware system comprised of a Raspberry Pi 3, used as a microcontroller, and a webcam is used to implement a system to detect animals. The webcam is used to stream live video footages, where the live video feed will then be filtered through an image processing system to detect the presence of any animal. Any image detected in the camera will then be analyzed through a real time deep learning object detection system. This system will utilize a pre-trained Convolutional Neural Network classification system to detect our object of interest. The classification model is pretrained and stored in a dataset which will be used for the detection system. Any image which contains an animal throughout the video feed will be detected and shown in the camera and the image will then be captured and the images will then be saved. The result of the detection can be seen in the live video footage as it will display the type of animal which is detected in real time",multimedia,924,not included
,to_check,core,Shared Learning : Enhancing Reinforcement in $Q$-Ensembles,2017-09-14 00:00:00,core,http://arxiv.org/abs/1709.04909,,"Deep Reinforcement Learning has been able to achieve amazing successes in a
variety of domains from video games to continuous control by trying to maximize
the cumulative reward. However, most of these successes rely on algorithms that
require a large amount of data to train in order to obtain results on par with
human-level performance. This is not feasible if we are to deploy these systems
on real world tasks and hence there has been an increased thrust in exploring
data efficient algorithms. To this end, we propose the Shared Learning
framework aimed at making $Q$-ensemble algorithms data-efficient. For achieving
this, we look into some principles of transfer learning which aim to study the
benefits of information exchange across tasks in reinforcement learning and
adapt transfer to learning our value function estimates in a novel manner. In
this paper, we consider the special case of transfer between the value function
estimates in the $Q$-ensemble architecture of BootstrappedDQN. We further
empirically demonstrate how our proposed framework can help in speeding up the
learning process in $Q$-ensembles with minimum computational overhead on a
suite of Atari 2600 Games.Comment: Submitted to AAAI 201",multimedia,925,not included
,to_check,core,Learning audio and image representations with bio-inspired trainable feature extractors,2017-01-01 00:00:00,core,https://core.ac.uk/download/155003320.pdf,'Universitat Autonoma de Barcelona',"Since when very young, we can quickly learn new concepts, and distinguish between different kinds of object or sound. If we see a single object or hear a particular sound, we are then able to recognize such sample or even different versions of it in other scenarios. As an example, if one sees a iron chair and associates the object to the general concept of “chairs”, he will be able to detect and recognize also wooden or wicker chairs. Similarly, when we hear the sound of a particular event, such as a scream, we are then able to recognize other kinds of scream that occur in different environments. We continuously learn representations of the real world, which we then use in order to understand new and changing environments.In the field of pattern recognition, traditional methods typically require a careful design of data representations (i.e. features), which involves considerable domain knowledge and effort by experts. Recently, approaches for automated learning of representations from training data were introduced and based on popular deep learning techniques and convolutional neural networks (CNN). Representation learning aims at avoiding engineering of hand-crafted features and providing automatically learned features suitable for the recognition tasks. In this work, we proposed novel trainable filters for representation learning in audio and image processing. The structure of these filters is not fixed in the implementation but rather configured directly from single prototype patterns of interest [4].In the context of audio processing, we focused on the problem of audio event detection and classification in noisy environments, also in cases where the signal to noise ratio (SNR) is null or negative. We released two data sets, namely the MIVIA audio events and the MIVIA road events data sets, and obtained baseline results (recognition rate of about 85%) with a real-time method for event detection based on the bag of features classification scheme [3, 2].We designed novel trainable feature extractors, which we call COPE (Combination of Peaks of Energy), that are able to detect specific constellations of energy peak points in time-frequency representations of input audio signals [8]. The particular constellation of energy peaks to be detected by a COPE feature extractor is determined in an automatic configuration process performed on a given prototype sound. The design of COPE feature extractors was inspired by some functions of the cochlea membrane and the inner hair cells in the inner auditory system, which convert the sound pressure waves into neural stimuli on the auditory nerve.We proposed a method that uses COPE feature extractors together with a classification system to perform real-time audio event detection and classification, also in cases where sounds have null and negative SNR. The performance results (recognition rate over 90%) that we obtained on several benchmarking data sets for audio events detection in different contexts are higher than state-of-the-art approaches.In the second part of the work, we introduced B-COSFIRE filters for detection of elongated and curvilinear patterns in images and apply them to the delineation of blood vessels in retinal images [1, 6]. The B-COSFIRE filters are trainable, that is their structure is automatically configured from prototype elongated patterns. The design of the B-COSFIRE filters is inspired by the functions of some neurons, called simple cells, in area V1 of the visual system, which fire when presented with line or contour stimuli. A B-COSFIRE filter achieves orientation selectivity by computing the weighted geometric mean of the output of a pool of Difference-of-Gaussians (DoG) filters, whose supports are aligned in a collinear manner. Rotation invariance is efficiently obtained by appropriate shiftings of the DoG filter responses.After configuring a large bank of B-COSFIRE filters selective for vessels (i.e. lines) and vessel-endings (i.e. line-endings) of various thickness (i.e. scale), we employed different techniques based on information theory and machine learning to select an optimal subset of B-COSFIRE filters for the vessel delineation task [5, 7]. We considered the selected filters as feature extractors to construct a pixel-wise feature vector which we used in combination with a classifier to classify the pixels in the testing image as vessel and non-vessel pixels. We carried out experiments on public benchmarking data sets (DRIVE, STARE, CHASE DB1 and HRF data sets) and the results that we achieved are higher than many existing methods.We studied the computational requirements of the proposed algorithms in order to evaluate their applicabilityin real-world applications and the fulfillment of real-time constraints given by the considered problems. The MATLAB implementation of the proposed algorithms are publicly released for research purposes.This work contributes to the development of algorithms for representation learning in audio and imageprocessing and promotes their use in higher-level pattern recognition systems",multimedia,926,not included
,to_check,core,"Deep Learning-Based Multiple Object Visual Tracking on Embedded System
  for IoT and Mobile Edge Computing Applications",2018-07-31 00:00:00,core,http://arxiv.org/abs/1808.01356,,"Compute and memory demands of state-of-the-art deep learning methods are
still a shortcoming that must be addressed to make them useful at IoT
end-nodes. In particular, recent results depict a hopeful prospect for image
processing using Convolutional Neural Netwoks, CNNs, but the gap between
software and hardware implementations is already considerable for IoT and
mobile edge computing applications due to their high power consumption. This
proposal performs low-power and real time deep learning-based multiple object
visual tracking implemented on an NVIDIA Jetson TX2 development kit. It
includes a camera and wireless connection capability and it is battery powered
for mobile and outdoor applications. A collection of representative sequences
captured with the on-board camera, dETRUSC video dataset, is used to exemplify
the performance of the proposed algorithm and to facilitate benchmarking. The
results in terms of power consumption and frame rate demonstrate the
feasibility of deep learning algorithms on embedded platforms although more
effort to joint algorithm and hardware design of CNNs is needed.Comment: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessibl",multimedia,927,included
,to_check,core,"Improving Accuracy of Nonparametric Transfer Learning via Vector
  Segmentation",2017-10-24 00:00:00,core,http://arxiv.org/abs/1710.08637,,"Transfer learning using deep neural networks as feature extractors has become
increasingly popular over the past few years. It allows to obtain
state-of-the-art accuracy on datasets too small to train a deep neural network
on its own, and it provides cutting edge descriptors that, combined with
nonparametric learning methods, allow rapid and flexible deployment of
performing solutions in computationally restricted settings. In this paper, we
are interested in showing that the features extracted using deep neural
networks have specific properties which can be used to improve accuracy of
downstream nonparametric learning methods. Namely, we demonstrate that for some
distributions where information is embedded in a few coordinates, segmenting
feature vectors can lead to better accuracy. We show how this model can be
applied to real datasets by performing experiments using three mainstream deep
neural network feature extractors and four databases, in vision and audio",multimedia,928,not included
,to_check,core,Machine Learning Methods for Pipeline Surveillance Systems Based on Distributed Acoustic Sensing: A Review,2017-09-22 00:00:00,core,https://core.ac.uk/download/93126949.pdf,'MDPI AG',"There is an increasing interest in researchers and companies on the combination of Distributed Acoustic Sensing (DAS) and a Pattern Recognition System (PRS) to detect and classify potentially dangerous events that occur in areas above fiber optic cables deployed along active pipelines, aiming to construct pipeline surveillance systems. This paper presents a review of the literature in what respect to machine learning techniques applied to pipeline surveillance systems based on DAS+PRS (although its scope can also be extended to any other environment in which DAS+PRS strategies are to be used). To do so, we describe the fundamentals of the machine learning approaches when applied to DAS systems, and also do a detailed literature review of the main contributions on this topic. Additionally, this paper addresses the most common issues related to real field deployment and evaluation of DAS+PRS for pipeline threat monitoring, and intends to provide useful insights and recommendations in what respect to the design of such systems. The literature review concludes that a real field deployment of a PRS based on DAS technology is still a challenging area of research, far from being fully solved.Some authors were supported by funding from the European Research Council

through Starting Grant UFINE (grant number #307441), Water JPI, the WaterWorks2014 Cofunded Call, the

European Commission (Horizon 2020) through project H2020-MSCA-ITN-2016/722509-FINESSE, the Spanish

Ministry of Economy and Competitiveness, the Spanish “Plan Nacional de I+D+i” through projects

TEC2013-45265-R, TEC2015-71127-C2-2-R, TIN2013-47630-C2-1-R, and TIN2016-75982-C2-1-R, and the regional

program SINFOTONCM: S2013/MIT-2790 funded by the “Comunidad de Madrid”. H.F.M. acknowledges funding

through the FP7 ITN ICONE program, grant number #608099 funded by the European Commission. J.P.-G.

acknowledges funding from the Spanish Ministry of Economy and Competivity through an FPI contract. SML

acknowledges funding from the Spanish Ministry of Science and Innovation through a “Ramón y Cajal” contract.We acknowledge support by the CSIC Open Access Publication Initiative through its Unit of Information Resources for Research (URICI)",multimedia,931,not included
,to_check,core,Multivariate Regression with Incremental Learning of Gaussian Mixture Models,2017-01-01 00:00:00,core,https://core.ac.uk/download/132529727.pdf,'IOS Press',"La publicació definitiva d'aquest treball està disponible a IOS Press a través de http://dx.doi.org/10.3233/978-1-61499-806-8-196Within the machine learning framework, incremental learning of multivariate spaces is of special interest for on-line applications. In this work, the regression problem for multivariate systems is solved by implementing an efficient probabilistic incremental algorithm. It allows learning high-dimensional redundant non-linear maps by the cumulative acquisition of data from input-output systems. The proposed model is aimed at solving prediction and inference problems. The implementation introduced in this work allows learning from data batches without the need of keeping them in memory afterwards. The learning architecture is built using Incremental Gaussian Mixture Models. The Expectation-Maximization algorithm and general geometric properties of Gaussian distributions are used to train the models. Our current implementation can produce accurate results fitting models in real multivariate systems. Results are shown from testing the algorithm for both situations, one where the incremental learning is demonstrated and the second where the performance to solve the regression problem is evaluated on a toy example.Peer ReviewedPostprint (author's final draft",multimedia,932,not included
,to_check,core,ConfocalGN : a minimalistic confocal image simulator,2016-11-21 00:00:00,core,10.1101/088906,http://arxiv.org/abs/1610.10042,"SUMMARY : We developed a user-friendly software to generate synthetic
confocal microscopy images from a ground truth specified as a 3D bitmap with
pixels of arbitrary size. The software can analyze a real confocal stack to
derivate noise parameters and will use them directly to generate new images
with similar noise characteristics. Such synthetic images can then be used to
assert the quality and robustness of an image analysis pipeline, as well as be
used to train machine-learning image analysis procedures. We illustrate the
approach with closed curves corresponding to the microtubule ring present in
blood platelet. AVAILABILITY AND IMPLEMENTATION: ConfocalGN is written in
Matlab but does not require any toolbox. The source code is distributed under
the GPL 3.0 licence on https://github.com/SergeDmi/ConfocalGN",multimedia,933,not included
,to_check,core,,2015-12-01 00:00:00,core,identification automatique des problèmes fonctionnels dans les simulateurs de vol,https://core.ac.uk/download/213619981.pdf,"RÉSUMÉ Les simulateurs de vol sont des systèmes composés d’un système logiciel et d’actuateurs mécaniques qui recréés la dynamique d’un vrai vol avec un aéronef et qui sont notamment utilisés par les compagnies de transport aérien pour former leurs futurs pilotes. Ces simulateurs doivent recréer des scénarios de vol permettant d’exécuter des itinéraires réguliers de vol ou de confronter les pilotes à différentes situations d’urgence qui peuvent subvenir lors d’un vrai vol (p. ex., une tempête violente). Puisque les simulateurs de vol ont un impact direct sur la qualité de formation des pilotes, une batterie exhaustive de tests s’impose à chaque fois qu’une nouvelle version d’un simulateur doit être mise en opération. Une bonne partie de ces tests requièrent l’intervention d’un pilote qui stimule le système en réalisant une série d’opérations de contrôle provenant de scénarios de vol préparés par des experts en aéronautique, ce qui est gourmand en temps, en ressources humaines et en ressources financières. La raison de la nécessité de réaliser tous ces tests est que le logiciel en soit est constitué de plusieurs composants de type boîte noire dont le code source n’est pas disponible, ils sont fournis tels quels par les fournisseurs du composant réel d’origine devant être simulé (p. ex., un composant hydraulique d’un aéronef que nous voulons simuler). Puisque nous n’avons pas pas accès au code source, il n’est pas possible lors de la mise-à-jour de l’un de ces composants de savoir quel sera le changement dans le comportement du simulateur. Dans le meilleur des cas, un défaut dans un composant aura un impact local et, dans le pire des cas, il peut nuire à tous les autres composants du simulateur. Il est dans ce cas nécessaire d’utiliser une stratégie de test d’une large granularité couvrant à la fois le fonctionnement des composants eux-mêmes et l’ensemble des fonctionnalités de vol du simulateur afin d’en assurer sa qualité. Dans le cadre de ces tests, afin de réduire le temps requis pour vérifier le bon comportement d’une nouvelle version d’un simulateur, nous proposons dans ce mémoire d’automatiser l’analyse des résultats de test en modélisant le comportement normal du système logiciel en utilisant de l’apprentissage automatique. Un tel modèle tente de recréer le comportement stable du système en bâtissant des règles reliant ses entrées à chacune de ses sorties. Les entrées sont constituées de métriques provenant des contrôles qui stimulent le système et les sorties sont constituées de métriques que nous pouvons observer. Nous produisons donc un modèle initial d’une version fiable du simulateur et nous validons par la suite le bon comportement des versions subséquentes en comparant leur comportement avec celui du modèle initial. Nous cherchons à voir s’il y a un écart trop important avec le modèle initial, ce nous considérons comme étant une déviation. Le but final de notre recherche est de pouvoir appliquer notre méthodologie d’analyse des résultats de test dans l’industrie de la simulation. Afin d’avoir plus de flexibilité dans nos expérimentations, nous commençons d’abord par valider l’approche avec un simulateur plus simple et ouvert. Nous utilisons donc le simulateur de vol à source libre FlightGear comme cas d’étude pour évaluer notre approche de test. Ce simulateur utilise l’engin de vol très populaire JSBsim et la librairie SimGear pour modéliser les aéronefs et simuler leur comportement dynamique dans un environnement de vol. JSBsim est notamment utilisé par la Nasa à des fins de recherche. Nous réalisons d’abord un vol manuel en utilisant des périphériques de contrôle d’un ordinateur tout en enregistrant les données contenues dans le simulateur. Le vol doit être composé de toutes les opérations régulières de pilotage afin que le modèle que l’on en extrait représente fidèlement le comportement normal du simulateur. Nous réalisons par la suite plusieurs répétitions du même vol afin d’identifier des niveaux de seuils robustes pour déterminer au delà de quels écarts un métrique d’une nouvelle version doit être considéré comme déviant par rapport au modèle. Nous élaborons à cet effet cinq scénarios de mutation du comportement de la version initiale de notre simulateur afin de générer différentes versions ayant des métriques qui respectent le comportement normal du système ou présentant une déviation du comportement normal reliée à un problème fonctionnel. En sachant d’avance quelles mutations présentent des déviations et avec l’aide d’experts identifiant précisément les métriques qui dévient de leur comportement normal, nous pouvons construire un oracle avec lequel nous évaluons la précision et le rappel de notre approche de détection. En particulier, dans le cadre de notre étude empirique sur FlightGear, nous modifions une version initiale du simulateur en y injectant cinq différentes mutations qui n’ont pas d’impact grave ou qui engendrent des problème fonctionnels avec lesquels nous évaluons notre approche. Les résultats montrent qu’en choisissant des niveaux de seuil initiaux lors d’une première calibration, notre approche peut détecter les métriques ayant des déviations avec un rappel majoritairement de 100% (un seul cas de mutation à 50%) et une précision d’au moins 40%. En ce qui a trait aux mutations ne changeant pas le comportement normal du simulateur, notre approche a un taux de fausses alarmes de 60%. Nous montrons également qu’avec une base de données plus large, il est possible de calibrer notre approche afin d’améliorer sa performance. Avec des niveaux de seuil optimaux qui sont calibrés pour chacune des mutations, nous obtenons un taux de rappel de 100% pour tous les cas, une précision d’au moins 50% pour les versions ayant des problèmes fonctionnels et un taux de fausses alarmes de 0% pour les versions n’ayant pas de problèmes fonctionnels. Afin d’ouvrir la voie pour des améliorations futures, nous utilisons notre méthodologie pour faire une petite expérimentation connexe sur JSBsim afin de détecter les déviations dans les transitions entre les états stables de chacun des métriques. Nous utilisons la même modélisation du simulateur. Les résultats montrent que nous pouvons battre une classification aléatoire des déviations. Cette nouvelle approche n’en n’est qu’à ces débuts et nous sommes convaincu que nous pouvons obtenir des résultats plus significatifs avec de futurs travaux sur le sujet. Nous proposons également dans les améliorations futures de descendre le niveau de granularité de notre modélisation du simulateur au niveau de chacun de ses composants. Cela permettrait d’isoler exactement qu’elle composant présente une déviation, et ainsi trouver la source du problème.----------ABSTRACT Flight simulators are systems composed of software and mechanical actuators used to train crews for real flights. The software is emulating flight dynamics and control systems using components off-the-shelf developed by third parties. For each new version of these components, we do not know what parts of the system is impacted by the change and hence how the change would impact the behaviour of the entire simulator. Hence, given the safety critical nature of flight simulators, an exhaustive set of manual system tests must be performed by a professional pilot. Test experts then rely on the test pilot’s feedback and on the analysis of output metrics to assess the correctness of a simulator’s behaviour, which consumes time and money, even more these tests must be repeated each time one of the components is updated by its vendor. To automate the analysis of test results from new simulator versions, we propose a machine learning-based approach that first builds a model that mimics the normal behaviour of a simulator by creating rules between its input and output metrics, where input metrics are control data stimulating the system and output metrics are data used to assess the behaviour of the system. We then build a behavioural model of the last-known correctly behaving version of the simulator, to which we compare data from new versions to detect metric deviations. The goal of our research is to first build a model of the simulator, then detect deviations of new simulator versions using that model, and finally develop an automatic approach to flag deviations when there are functional problems. Our case study uses the open-source flight simulator Flight Gear, based on the well-known JSBsim flight dynamic engine currently used by the Nasa. Applying our approach, we first drive a manual basic flight using computer peripherals while recording metrics in the simulator. We then use the recorded metrics to build a model of this basic execution scenario. After repeating several times our flight we have enough data to identify robust metric deviation thresholds to determine when a metric in a new version is deviating too far from the model. We generate five different versions of FlightGear by injecting harmless and harmful modifications into the the simulator, then perform again our initial flight multiple times for each of those versions while recording input and output metrics. We rely on experts identifying which output metric should be deviating in each scenario to build an oracle. Using an initial threshold, our approach can detect most of the deviations in problematic versions with a near perfect recall (only one case at 50%) and a reasonable precision of at least 40%. For the versions without harmfull deviations we get a false alarm rate of 60%. By optimizing the threshold for each version, we get a perfect recall of 100%, a precision of at least 50%, and a false alarm rate of 0% for the good behaving versions. To open the path for future work, we perform a case study on JSBsim using a variant of our basic deviation detection approach to detect transient deviations using the same model. Our results show that we can beat a random classification for one kind of deviation, however for other deviations our models do not perform as well. This new approach is just a first step towards transient deviation detection, and we are convinced that we can get better results in future work. We also propose to use finer-grained models for each component to be able to isolate the exact component causing a functional problem",multimedia,934,not included
,to_check,core,,2016-04-01 00:00:00,core,"improving the extraction of crisis information in the context of flood, fire, and landslide rapid mapping using sar and optical remote sensing data",,"Optical and radar satellite remote sensing have proven to provide essential crisis information in case of natural disasters, humanitarian relief activities and civil security issues in a growing number of cases through mechanisms such as the Copernicus Emergency Management Service (EMS) of the European Commission or the International Charter ‘Space and Major Disasters’.

The aforementioned programs and initiatives make use of satellite-based rapid mapping services aimed at delivering reliable and accurate crisis information after natural hazards.

Although these services are increasingly operational, they need to be continuously updated and improved through research and development (R&D) activities. The principal objective of ASAPTERRA (Advancing SAR and Optical Methods for Rapid Mapping), the ESA-funded R&D project being described here, is to improve, automate and, hence, speed-up geo-information extraction procedures in the context of natural hazards response. This is performed through the development, implementation, testing and validation of novel image processing methods using optical and Synthetic Aperture Radar (SAR) data. The methods are mainly developed based on data of the German radar satellites TerraSAR-X and TanDEM-X, the French satellite missions Pléiades-1A/1B as well as the ESA missions Sentinel-1/2 with the aim to better characterize the potential and limitations of these sensors and their synergy. The resulting algorithms and techniques are evaluated in real case applications during rapid mapping activities. 

The project is focussed on three types of natural hazards: floods, landslides and fires. Within this presentation an overview of the main methodological developments in each topic is given and demonstrated in selected test areas. The following developments are presented in the context of flood mapping: a fully automated Sentinel-1 based processing chain for detecting open flood surfaces, a method for the improved detection of flooded vegetation in Sentinel-1data using Entropy/Alpha decomposition, unsupervised Wishart Classification, and object-based post-classification as well as semi-automatic approaches for extracting inundated areas and flood traces in rural and urban areas from VHR and HR optical imagery using machine learning techniques. Methodological developments related to fires are the implementation of fast and robust methods for mapping burnt scars using change detection procedures using SAR (Sentinel-1, TerraSAR-X) and HR optical (e.g. SPOT, Sentinel-2) data as well as the extraction of 3D surface and volume change information from Pléiades stereo-pairs. In the context of landslides, fast and transferable change detection procedures based on SAR (TerraSAR-X) and optical (SPOT) data as well methods for extracting the extent of landslides only based on polarimetric VHR SAR (TerraSAR-X) data are presented",multimedia,935,not included
,to_check,core,,2016-10-22 00:00:00,core,virtual mining model for classifying text using unsupervised learning,,"In real world data mining is emerging in various era, one of its most outstanding performance is held in various research such as Big data, multimedia mining, text mining etc. Each of the researcher proves their contribution with tremendous improvements in their proposal by means of mathematical representation. Empowering each problem with solutions are classified into mathematical and implementation models. The mathematical model relates to the straight forward rules and formulas that are related to the problem definition of particular field of domain. Whereas the implementation model derives some sort of knowledge from the real time decision making behaviour such as artificial intelligence and swarm intelligence and has a complex set of rules compared with the mathematical model. The implementation model mines and derives knowledge model from the collection of dataset and attributes. This knowledge is applied to the concerned problem definition. The objective of our work is to efficiently mine knowledge from the unstructured text documents. In order to mine textual documents, text mining is applied. The text mining is the sub-domain in data mining. In text mining, the proposed Virtual Mining Model (VMM) is defined for effective text clustering. This VMM involves the learning of conceptual terms; these terms are grouped in Significant Term List (STL). VMM model is appropriate combination of layer 1 arch with Analysis o",multimedia,936,not included
,to_check,core,,2016-12-01 00:00:00,core,qed-tutrix : système tutoriel intelligent pour l'accompagnement des élèves en situation de résolution de problèmes de démonstration en géométrie plane,https://core.ac.uk/download/213620715.pdf,"RÉSUMÉ
Au cours des dernières années, le système scolaire québécois impose une pression croissante sur les 
enseignants. En effet, ceux-ci doivent gérer des classes de plus en plus nombreuses tout en maintenant un soutien adéquat à l’apprentissage des élèves. Dans ce contexte, l’utilisation de systèmes tutoriels intelligents qui sont en mesure d’assister l’enseignant dans son travail 
pourrait permettre à ce dernier de consacrer plus de temps aux élèves qui en ont véritablement besoin. Malheureusement, dans le domaine de l’enseignement des preuves en géométrie, l’offre de systèmes tutoriels est limitée. De plus, ceux actuellement proposés forcent l’élève à travailler selon un ordre déterminé et ils ne fournissent pas de soutien dans le cadre d’une exploration libre du problème. Ils l’obligent aussi à rédiger des preuves formelles qui ne sont pas adaptées aux exigences des enseignants du secondaire.
Partant de ce constat, nous avons établi l’objectif principal de notre projet qui consiste à proposer un système tutoriel intelligent qui assiste l’élève dans une démarche d’exploration plutôt que de rédaction dans le cadre de l’élaboration d’une preuve en géométrie. Dans le but de l’atteindre, nous proposons le système QED-Tutrix qui a été conçu à la suite d’une analyse des 
interventions d’enseignants réels observés. Il permet à l’enseignant ou au didacticien de construire un ensemble de preuves acceptables pour un problème donné en fonction de l’objectif d’apprentissage visé. L’élève peut ensuite tenter de résoudre le problème choisi par l’enseignant 
en utilisant les différents outils offerts à l’interface de QED-Tutrix. Celui-ci a accès à une figure dynamique afin de découvrir des conjectures, à un répertoire d’énoncés pour composer sa preuve et à une démonstration qu’il doit compléter. Tous les énoncés proposés sont analysés par le 
système qui génère des rétroactions à l’intérieur d’une fenêtre de clavardage afin de guider l’élève lors de l’exploration et de la résolution du problème.
L’élaboration de QED-Tutrix a été réalisée par une équipe multidisciplinaire composée d’experts en didactique et en informatique. Le système a été construit itérativement par la mise en oeuvre du paradigme de la conception dans l’usage qui est constitué d’une succession de plusieurs cycles de recherche et de développement. Chaque cycle se clôture par une expé- rimentation qui vise à valider 
le travail accompli et à recueillir des informations qui sont réinvesties dans le cycle suivant. 
Une première version du système (GeoGebraTUTOR) a donc été créée afin d’étudier notamment les interventions d’enseignants réels qui ont inspiré l’élaboration de la seconde version (QED-Tutrix), qui est décrite et analysée dans cette thèse.
Nous ne prétendons pas que la version actuelle du système a un effet mesurable sur les résultats scolaires, car nous visons, pour l’instant, à permettre à un élève de travailler en conformité avec des théories didactiques reconnues. En effet, la conception de QED-Tutrix s’ancre 
principalement dans la théorie des situations didactiques qui permet de représenter une situation didactique par une relation élève-milieu. Nous utilisons cependant une version étendue de cette théorie dans laquelle un agent tutoriel, qui joue le rôle d’un enseignant virtuel, peut agir sur la 
relation élève-milieu. De plus, nous désirons offrir un système tutoriel qui est un véritable espace de travail géométrique, c’est-à-dire qu’il permet à l’élève de résoudre des problèmes en mettant en oeuvre les trois démarches définies dans cet espace.
Ces théories didactiques ainsi que les résultats de nos observations ont été implantés dans QED-Tutrix. Il en est résulté un système comportant quatre couches logicielles principales. La première permet de modéliser l’ensemble des démonstrations possibles pour résoudre un problème 
donné. Pour chaque problème, l’enseignant inscrit toutes les inférences, ou pas de démonstration, qui sont acceptables pour sa résolution selon l’objectif d’apprentissage visé. Chaque inférence contient une justification qui est utilisée pour produire un conséquent à partir de l’ensemble de ses antécédents. Il est possible de les combiner afin d’obtenir un graphe contenant toutes les solutions valides, car les conséquents peuvent être recyclés pour former les antécédents d’autres inférences. Le parcours du graphe contenu dans cette première couche permet donc d’énumérer les différentes solutions au problème représenté.
Afin de proposer une aide qui respecte l’état cognitif de l’élève lors de l’exploration d’un problème, il est essentiel de conserver la chronologie de ses actions. Nous l’avons donc modélisée à l’intérieur de la deuxième couche de notre système. Celle-ci contient des données dynamiques qui sont mises à jour au cours de la résolution d’un problème, à l’opposé du graphe qui est statique, et elle se superpose à ce dernier. En effet, nous indiquons, pour chaque noeud du graphe, le temps d’activation le plus récent qui correspond à l’écriture de l’énoncé qui lui est attaché. Cette approche se démarque de celle des autres systèmes, car ces derniers n’utilisent pas la chronologie 
des actions, étant donné qu’ils imposent une séquence de résolution.
Pour être en mesure de suggérer différentes pistes de solution à un élève bloqué dans son processus de résolution, nous avons choisi de traiter les inférences selon un ordre de priorité. Ce classement est réalisé par la troisième couche de QED-Tutrix, qui utilise les données des deux couches précédentes. Pour l’obtenir, nous recherchons d’abord la solution la plus avancée à l’aide 
d’une heuristique originale, que nous avons élaborée et qui permet d’éviter d’énumérer toutes les solutions. Nous affectons ensuite des priorités plus élevées aux inférences faisant partie de la solution déterminée et qui ont été travaillées récemment par l’élève, afin de respecter son état cognitif. La proposition d’autres pistes nous démarque des systèmes tutoriels traditionnels qui offrent de l’aide uniquement pour compléter une solution optimale.
La liste ordonnée d’inférences est utilisée par la dernière couche du système, soit celle qui produit les différentes rétroactions. Premièrement, QED-Tutrix offre des rétroactions instantanées, en réponse à l’écriture de chaque énoncé, sous forme d’émoticônes et de messages courts. Il permet aussi d’encoder des erreurs courantes afin de leur associer des messages précis. Il offre 
enfin une aide à la prochaine étape qui est inspirée des interventions des enseignants réels observés. Cette dernière forme d’aide a été modélisée par une machine à états finis qui traite séquentiellement les inférences ordonnées dans la liste et produit une série d’indices permettant de les compléter. Des messages doivent être composés pour cha- cune des inférences, mais des 
mécanismes ont été implantés afin de réduire leur nombre. Les rétroactions offertes sont comparables à celles d’autres systèmes tutoriels.
Environ 450 inférences ont été produites et près de 900 messages composés afin d’implanter les cinq problèmes actuellement offerts dans notre système. Son fonctionnement a d’abord été vérifié par un expert indépendant. Celui-ci a confirmé que les messages produits étaient conformes à la structure déterminée, mais que l’évaluation de la solution la plus avancée était parfois problématique. 
QED-Tutrix a ensuite été utilisé par des élèves de 4e secondaire. Ceux-ci ont généralement trouvé le système utile et ont apprécié l’expérience. L’analyse des enregistrements nous a permis de constater que la structure des messages générée permet d’ai- der certains élèves. De plus, nous avons observé la mise en oeuvre des différentes démarches, ce qui confirme le statut d’espace de travail géométrique de QED-Tutrix. L’efficacité de ce dernier est, par contre, limitée dans le cas d’élèves plus faibles, car la structure des messages est calibrée afin d’aider des élèves moyens. Le problème concernant l’évaluation de la solution la plus avancée a aussi provoqué la production 
de messages incohérents avec la stratégie de l’élève. Dans le but d’augmenter l’efficacité du système, nous envisageons, entre autres, de proposer des profils de tuteurs et d’élèves.
Malgré les lacunes qui ont été détectées, il n’en demeure pas moins que QED-Tutrix est un système tutoriel innovateur. En effet, dans le domaine des preuves en géométrie, il est le seul à utiliser des émoticônes et à proposer différentes pistes de solution. De plus, son élaboration itérative, par une équipe multidisciplinaire, permet d’obtenir un système respectueux du travail de l’élève, ce qui se démarque de l’approche traditionnelle qui consiste à tenter de reproduire le raisonnement 
d’un expert. Les étapes suivantes de conception visent à intégrer des rétroactions sous forme de problèmes connexes et à proposer une aide à la construction de la figure. Notre système pourrait facilement être adapté au traitement des démonstrations en logique de premier ordre. Une adaptation pour le traitement du raisonnement sous forme d’argumentation non formelle pourrait aussi être envisagée. Enfin, la suggestion de diverses pistes de solution pourrait être implantée dans d’autres systèmes tutoriels.----------ABSTRACT
In the past years, Quebec’s school system imposes a growing amount of pressure on its teaching staff. They must juggle classes with more and more students while assuring the quality of their teaching to each of them. In this context, the use of intelligent tutoring systems which can assist 
the teacher in his or her work could allow the teacher to dedicate more energy to each student when it’s needed. However, the offer for tutoring systems for the learning of proof is limited. 
Moreover, the available systems force the student to work according to a determined order and they don’t provide help in the context of a free exploration of the problem. They also force the student to write formal proofs when high school teachers rarely demand. With this assessment in mind, we established the principal objective for our project which aims at offering an intelligent tutoring system that assists the student in an exploratory approach when solving geometry proofs instead of a formal proof writing approach. The system we offer is QED-Tutrix which was designed taking into account actual teacher interventions observed in a classroom environment. QED-Tutrix allows the teacher or didactician to construct a number of admissible proofs for a given problem according to the learning goals. The student can then try to solve the problem chosen by the teacher by using the different tools QED-Tutrix puts at his disposal. The student has access to a dynamic geometric figure he can work with in order to make conjectures, as well as to a repertoire of statements to create their proof and an interactive 
written proof they can use to complete their proof. All the statements he provides are analyzed by the system which then generates feedback through a chat window in order to guide the student during the exploration and solving of the problem.
QED-Tutrix’s elaboration was carried out by a multidisciplinary team comprised of experts in didactics of mathematics and in computer science. The system is built in an iterative manner by adopting a design in use approach which consists of a series of many cycles of research and 
development. Each cycle is ended with an experimentation which aims at validating the work accomplished and at collecting information to be reinvested in the following cycle. A first version of the system (GeoGebraTUTOR) was created to study, among other things, real teacher interventions which inspired the implementation of the second version (QED-Tutrix). This last version is 
described and analyzed in the following thesis.
We do not claim that the present QED-Tutrix version has measurable effects on academic results, since our aim at the moment is to make sure it allows the student to work in a fashion put forward by known didactic principles. Indeed, QED-Tutrix’s conception is rooted mainly in the didactical situation’s theory which represents a didactical context by a student-milieu interaction. However, we use an extended version of this theory in which a tutorial system playing 
a virtual teaching role may influence this student-milieu interaction. Moreover, we aim at offering a tutoring system that is a true geometrical workspace, meaning that it allows the student to solve problems by engaging in three mathematical processes described in the geometrical workspace model.
The didactic theories and conclusions drawn from our observations were implemented in QED-Tutrix. This resulted in a system made of four main software layers. The first of these layers is for the organization of the different proof solutions for a given problem. For each solution, the teacher or didactician registers all the inferences or proof steps which are admissible according to a specific learning context. Each inference includes a justification that is used to produce a result stemming from a group of premises. It is possible to combine the different inferences in order to generate a graph of all the different admissible solutions, since the results of one inference can be recycled as a premise for another. The different pathways of this graph which is the output of 
the first software layer allow for the account of the different solutions to each implemented problem.
In order to offer help that takes into account the cognitive state of the student exploring the problem, it is essential to keep track of the chronology of his or her actions. This memory was implemented in the second layer of the system. It contains information that is dynamic and evolves as problem solving occurs. It also overlays the solution graph which is static. Therefore, we indicate for each of the graphs nodes, the most recent activation time associated with the writing of the statement attached to it. This approach stands out from the way other systems operate and in which chronology is usually not taken into account since a solving sequence is imposed.
In order to be able to suggest alternative paths to a student who is stuck in his or her solving process, we chose to treat inferences according to an order of priority. This ranking is carried out by QED-Tutrix’s third layer which uses the data from the first two layers. In order to achieve this ordering, we look for the most advanced solution with the help of an original heuristic, 
elaborated for this project, which spares the system from running through all the admissible solutions. We then assign the highest priorities to the inferences which are part of the identified solution and that has been worked on recently by the student keeping his or her cognitive state in mind. The ability to suggest other solution option distances us from traditional tutorial systems which offer help only to complete an optimal solution.
This list of ranked inferences is used by the fourth and last layer of the system, meaning the layer which generates various feedback. Firstly, QED-Tutrix replies to the writing of each statement with instant feedback in the shape of emojis or short messages. It also allows the 
programming of particular messages associated with known common mistakes. Lastly, this layer of the system offers help with the next step which is inspired by actual teacher interventions. This last form of help was modelled by a finite state machine that sequentially treats the ranked inferences from the list and produces a series of hints to help the student complete them. Messages must be 
created for each of the inferences, but mechanisms are implemented in order to reuse messages according to the inferences content, limiting the number of entries. The feedback offered by these messages is similar in form to the feedback offered by other tutorial systems.
Approximately 450 inferences were produced and close to 900 messages were created in order to implement the five problems currently available in QED-Tutrix. It’s operating has been verified by an independent expert, which confirmed that the output of messages is true to the identified 
structure, but the evaluation of the most advanced solution is sometimes problematic. QED-Tutrix was then put through a second trial in a class of 4th year of high school. The students generally found the system to be useful and appreciated the experience. The analysis of the session 
recordings revealed that the generated messages help some students. Also, we observed different mathematical processes which confirmed QED- Tutrix’s geometrical workspace status. However, the efficiency of QED-Tutrix is limited when helping students with less mathematical abilities since the message structure is built with the average student as a reference. Problems with the 
identification of the most advanced solution also lead to incoherence between messages and student strategies. In order to enhance the system’s efficiency in helping the student solve problems, we contemplate, among other things, to differentiate tutorial profiles according to students solving profiles.
In spite of witnessing shortcomings, QED-Tutrix is an innovative tutorial system. Indeed, in the field of geometry proofs, it is the only automated tutoring system to use emojis and to suggest alternative solution paths. Moreover, the iterative and multidisciplinary approach adopted for its design and development stands out from a traditional approach which aims at reproducing expert 
reasoning. The next design steps aims at including feedback in the form of related problems and to provide help with building the geometrical figure. Our system could easily be adapted to handle first order logical proofs. An adjustment to process non- formal argumentation could also be 
considered. Finally, the suggestion of alternative solution
paths could be implemented in other tutorial systems",multimedia,938,not included
,to_check,core,'Pisa University Press',2016-05-06 00:00:00,core,nuovi orizzonti del sistema dei controlli interni: continuous auditing/continuous monitoring,,"La domanda può, a prima vista, suonare un po’ accademica: si può misurare il valore di un sistema di controllo? La questione è, in realtà, molto concreta. In una fase, come quella che molte imprese stanno vivendo, di ripensamento dei modelli organizzativi e di business, rispondere è d’importanza cruciale per il management. Le imprese, in generale, hanno investito molto per rendere efficaci i controlli interni: più addetti, nuovi sistemi informativi, maggiore lavoro per tutta l’organizzazione. L’investimento è stato spesso dettato da recenti obblighi normativi e dalle sollecitazioni di regolatori e autorità di vigilanza. Altre volte si è trattato di una scelta determinata dall’operare in contesti di mercato impegnativi, per esempio quelli internazionali, che implicano l’adozione di standard adeguati. La voce controlli ha, in ogni caso, acquisito un peso crescente nel conto economico diventando, inevitabilmente, un possibile target quando, pressate dalla bassa congiuntura, molte imprese si sono trovate a dover stringere la cinghia. E allora anche per gli Internal Auditor è diventato importante dare una misura del proprio contributo al risultato.
È in quest’ottica che l’Internal Audit costituisce un supporto per la corporate governance, offrendo un prezioso contributo alla valutazione del sistema di governo strategico e operativo dell’impresa e assumendo un atteggiamento proattivo per il suo miglioramento continuo. Tutto ciò determina lo sviluppo di una cultura del controllo interno, inteso non come un mero proliferare di controllori rispetto agli esecutori, ma come un sistema integrato d’azienda ove le attività di controllo si coniugano con quelle di gestione del business.
La professione dell’Internal Auditor è stata caratterizzata, nel corso degli anni, da un’importante evoluzione storica, che ha determinato lo spostamento del suo raggio d’azione da verifiche limitate principalmente ad aspetti di conformità normativa e procedurale ad attività di maggiore ampiezza nell’ambito del controllo sistemico, della consulenza organizzativa e della governance aziendale. Questo processo evolutivo ha richiesto un incremento degli skill per lo svolgimento della professione e ha determinato una maggiore visibilità e credibilità della stessa Funzione aziendale.
A tal fine l’Associazione Italiana Internal Auditor ha fornito un notevole contributo poiché, oltre a promuovere lo sviluppo delle tematiche di controllo interno e di gestione dei rischi verso tutti gli stakeholder interessati alla corporate governance, garantisce la diffusione degli Standard Internazionali per la Pratica Professionale dell’internal auditing (IPPF), sostiene la formazione continua per la professione e il conseguimento di certificazioni specifiche riconosciute a livello internazionale e contribuisce alla realizzazione di programmi di quality assurance.
Le organizzazioni hanno la necessità di considerare nuovi approcci alternativi alle tradizionali attività di Internal Audit, spinti da business in continua trasformazione e dall’evoluzione del panorama normativo e regolatorio che esercitano pressioni sui costi, sulle persone e sui processi. Il Continuous Auditing e il Continuous Monitoring, che si configurano quali nuove opportunità da affiancare all’attività di Internal Audit, utilizzati in modo adeguato, possono aiutare le organizzazioni a gestire in modo efficace le esposizioni ai principali rischi, in quanto consentono di rilevare, monitorare e prevenire anomalie in modo più facile, completo e tempestivo rispetto agli approcci tradizionali.
Il processo di Continuous Auditing contribuisce a garantire la conformità alle politiche e procedure aziendali. In molti casi, questo sistema può operare come strumento di «allarme» per individuare preventivamente aspetti critici del Sistema di Controllo Interno.
Significa quindi, eseguire verifiche ad elevata frequenza (fino, addirittura, nel continuo) su base dati integrali (senza dover campionare) e in automatico (utilizzando i CAAT).
L’uso implicito di strumenti automatizzati permette anche l’applicazione di nuove tecniche di analisi euristica in grado di evidenziare l’esistenza di problemi nascosti o non anticipati. Il processo di Continuous Monitoring permette, invece, di avere una visibilità realtime del corretto funzionamento e dei problemi dei processi operativi o di business dal punto di vista del manager. Il collegamento e l’osmosi di approcci tra i due processi arriva in profondità, a tal punto che in determinati casi gli strumenti automatizzati possono essere i medesimi, impiegati con ottiche e scopi diversi, ma sempre con l’obiettivo di minimizzare i tempi di intervento e le conseguenze di errori, di non conformità e di potenziali frodi.
Appare evidente il vantaggio che le organizzazioni possono trarre dall’introduzione di approcci di questo tipo, in termini di risparmi diretti e di minori problemi dei sistemi operativi e di quelli di controllo. Non si tratta di abbandonare gli audit tradizionali, si tratta di introdurre nuove tecniche che consentano di aumentare la copertura della funzione e di focalizzare meglio la pianificazione degli audit tradizionali che per loro natura sono molto affidabili ma anche particolarmente dispendiosi in termini di tempo e costi.
In quest’ottica, l’ottimizzazione della qualità dei controlli si lega alla capacità di prevedere i comportamenti a maggior rischio nei vari cicli aziendali che possono impattare sul business e quindi sul conto economico. La capacità di identificare ex ante un insieme di comportamenti anomali (dall’inosservanza delle procedure interne fino alla frode), costituisce la nuova sfida cui sono chiamate a rispondere le funzioni di controllo, congiuntamente alle funzioni di business. L’introduzione di strumenti di Continuous Monitoring o Continuous Auditing nelle organizzazioni consente la realizzazione di questi obiettivi.
In una survey pubblicata cinque anni fa si prevedeva che “ nei prossimi cinque anni gli internal auditor si focalizzeranno sempre più sui temi del continuous auditing and assessment, nel tentativo di ottimizzare e migliorare i processi di audit. Gli audit diventeranno più dinamici e saranno effettuati secondo necessità dettate più dai cambiamenti dei profili di rischio di volta identificati che da calendari di audit prestabiliti secondo le logiche tradizionali. In questa ricerca di maggiore efficacia ed efficienza gli auditor faranno leva sulla tecnologia combinata con la loro innata capacità analitica di identificare key risk indicators che possono monitorare al meglio le condizioni di rischio”. Quella che cinque anni fa sembrava solo una possibile evoluzione, ottimisticamente azzardata, delle funzioni di controllo, in questi ultimi anni è diventata realtà: in tutto il mondo le organizzazioni più avanzate hanno compiuto questo fondamentale passo avanti nel percorso evolutivo della funzione di Internal Audit, e sono sempre più aziende che stanno seguendo l’esempio dei leader, affacciandosi con interesse a questa soluzione. La maturità raggiunta dalle funzioni di controllo e dai suoi protagonisti, unita a una tecnologia finalmente in grado di analizzare e rendere disponibili i risultati in real time, ci consente di vedere con chiarezza le future evoluzioni.
Ecco che le imprese sono sempre più alla ricerca di tecnologie specifiche che garantiscano gli imprescindibili requisiti di trasparenza, sicurezza e disponibilità dei dati, nonché l’integrità di ogni singola transazione.
Il software di data mining maggiormente adottato in relazione al rispettivo fabbisogno, è Audit Command Language (ACL), con la finalità di rielaborare le informazioni secondo alcuni algoritmi associati agli indicatori di rischio, i quali a loro volta hanno il compito di attivare dei segnali di “alert” in caso di superamento dei livelli di soglia predeterminati.
Specificamente concepite per le attività di controllo e gestione dei rischi, le soluzioni ACL di Audit Management si rivelano essere le migliori attualmente sul mercato e sono riconosciute come leader nel campo della Data Analysis. Per questo oltre 14.500 aziende in tutto il mondo, di cui più di 100 tra le maggiori imprese italiane di tutti i settori, hanno già adottato ACL",multimedia,939,not included
,to_check,core,'Pisa University Press',2016-06-01 00:00:00,core,monitoring and testing in lte networks: from experimental analysis to operational optimisation,https://core.ac.uk/download/79621522.pdf,"L'avvento di LTE e LTE-Adavanced, e la loro integrazione con le esistenti tecnologie cellulari, GSM e UMTS, ha costretto gli operatori di rete radiomobile ad eseguire una meticolosa campagna di test e a dotarsi del giusto know-how per rilevare potenziali problemi durante il dispiegamento di nuovi servizi.
In questo nuovo scenario di rete, la caratterizzazione e il monitoraggio del traffico nonchè la configurazione e l'affidibilità degli apparati di rete, sono di importanza fondamentale al fine di prevenire possibili insidie durante la distribuzione di nuovi servizi e garantire la migliore esperienza utente possibile.
Sulla base di queste osservazioni, questa tesi di dottorato offre un percorso completo di studio che va da un'analisi sperimentale ad un'ottimizzazione operativa.
Il punto di partenza del nostro lavoro è stato il monitoraggio del traffico di un eNodeB di campo con tre celle, operativo nella banda 1800 MHz. Tramite campagne di misura successive, è stato possibile seguire l'evoluzione della rete 4G dagli albori del suo dispiegamento nel 2012, fino alla sua completa maturazione nel 2015. I dati raccolti durante il primo anno, evidenziavano uno scarso utilizzo della rete LTE, dovuto essenzialmente alla limitata penetrazione dei nuovi smartphone 4G. Nel 2015, invece, abbiamo assistito ad un aumento netto e decisivo del numero di utenti che utilizzano la tecnolgia LTE, con statistiche aggregate (come gli indici di marketshare per i sistemi operativi degli smartphones, o la percentuale di traffico video) che rispecchiano i trend nazionali e internazionali. Questo importante risultato testimonia la maturità della tecnologia LTE, e ci permette di considerare il nostro eNodeB un punto di osservazione prezioso per l'analisi del traffico.
Di pari passo con l'evoluzione dell'infrastruttura, anche i telefoni cellulari hanno avuto una sorprendente evoluzione nel corso degli ultimi due decenni, a partire da dispositivi semplici con servizi di sola voce, fino agli smartphone di ultima generazione che offrono servizi innovativi, come Internet mobile, geolocalizzazione e mappe, servizi multimediali, e molti altri.
Monitorare il traffico reale ci ha quindi permesso di studiare il comportamento degli utenti e individuare i servizi maggiormente utilizzati. Per questo, sono state sviluppate diverse librerie software per l'analisi del traffico. In particolare, è stato sviluppato in C++14 un framework/tool per la classificazione del traffico. Il progetto, disponibile su github, si chiama MOSEC, un acronimo per MOdular SErvice Classifier. MOSEC consente di definire e utilizzare un numero arbitrario di plug-in, che processano il pacchetto secondo le loro logiche e possono o no ritornare un valore di classificazione. Una strategia di decisione finale consente di classificare i vari flussi, basandosi sulle classificazioni di ciascun plug-in. Abbiamo quindi validato la bontà del processi di classificazione di MOSEC utilizzando una traccia labellata come ground-truth di classificazione.
I risultati mostrano una eccellente capacità di classificazione di traffico TCP-HTTP/HTTPS, mediamente superiore a quella di altri tool di classificazione (nDPI, PACE, Layer-7), ed evidenzia alcune lacune per quanto riguarda la classificazione di traffico UDP.
Le carattistiche dei flussi di traffico utente (User Plane) hanno un impatto diretto sul consumo energetico dei terminali e indiretto sul traffico di controllo (Control Plane) che viene generato. Pertanto, la conoscenza delle proprietà statistiche dei vari flussi consente di affrontare un problema del cross-layer optimization, per ridurre il consumo energetico dei terminali variando dei parametri configurabili sugli eNodeB.
E' noto che la durata della batteria dei nuovi smartphone, rappresenta uno dei maggiori limiti nell'utilizzo degli stessi. In particolare, lo sviluppo di nuovi servizi e applicazioni capaci di lavorare in background, senza la diretta interazione dell’utente, ha introdotto nuovi problemi riguardanti la durata delle batterie degli smartphone e il traffico di segnalazione necessario ad acquisire/rilasciare le risorse radio.
In conformità a queste osservazioni, è stato condotto uno studio approfondito sul meccanismo DRX (Discontinuous Reception), usato in LTE per consentire all’utente di risparmiare energia quando nessun pacchetto è inviato o ricevuto. I parametri DRX e RRC Inactivity Timer influenzano notevolmente l’energia consumata dai vari device. A seconda che le risorse radio siano assegnate o meno, l’UE si trova rispettivamente negli stati di RRC Connected e RRC Idle.
Per valutare il consumo energetico degli smartphone, è stato sviluppato un algoritmo che associa un valore di potenza a ciascuno degli stati in cui l’UE può trovarsi. La transizione da uno stato all’altro è regolata da diversi timeout che sono resettati ogni volta che un pacchetto è inviato o ricevuto. Utilizzando le tracce di traffico reale, è stata associata una macchina a stati a ogni UE per valutare il consumo energetico sulla base dei pacchetti inviati e ricevuti.
Osservando le caratteristiche statistiche del traffico User Plane è stata ripetuta la simulazione utilizzando dei valori dell’Inactivity Timer diversi da quello impiegato negli eNodeB di rete reale, alla ricerca di un buon trade-off tra risparmio energetico e aumento del traffico di segnalazione. I risultati hanno permesso di determinare che l'Inactivity Timer, settato originariamente sull'eNodeB era troppo elevato e determinava un consumo energetico eccesivo sui terminali. Diminuendone il valore fino a 10 secondi, si può ottenere un risparmio energetico fino al 50\% (a secondo del traffico generato) senza aumentare considerevolemente il traffico di controllo.
I risultati dello studio di cui sopra, tuttavia, non tengono in considerazione lo stato di stress cui può essere sottoposto un eNodeB per effetto dell'aumento del traffico di segnalazione, nè, tantomeno, dell'aumento della contesa di accesso alla rete durante la procedura di RACH, necessaria per ristabilire il bearer radio (o connessione RRC) tra terminale ed eNodeB.
Valutare le performance di sistemi hardware e software per la rete mobile di quarta generazione, cosi come individuare qualsiasi possibile debolezza all’interno dell’architettura, è un lavoro complesso. Un possibile caso di studio, è proprio quello di valutare la robustezza delle Base Station quando riceve molte richieste di connessioni RRC, per effetto di una diminuzione dell'Inactivity Timer.
A tal proposito, all’interno del Testing LAB di Telecom Italia, abbiamo utilizzato IxLoad, un prodotto sviluppato da Ixia, come generatore di carico per testare la robustezza di un eNodeB. I test sono consistiti nel produrre un differente carico di richieste RRC sull'interfaccia radio, similmente a quelle che si avrebbero diminuendo l'Inactivity Timer. Le proprietà statistiche del traffico di controllo sono ricavate a partire dall'analisi dalle tracce di traffico reale. I risultati hanno dimostrato che, anche a fronte di un carico sostenuto di richieste RRC solo una minima parte (percentuale inferiore all'1\% nel caso più sfavorevole) di procedure fallisce. Abbassare l'inactivity timer anche a valori inferiori ai 10 secondi non è quindi un problema per la Base Station.
Rimane da valutare, infine, cosa succede a seguito dell'aumento delle richieste di accesso al canale RACH, dal punto di vista degli utenti. Quando due o più utenti tentano, simultaneamente, di accedere al canale RACH, utilizzando lo stesso preambolo, l’eNodeB potrebbe non essere in grado di decifrare il preambolo. Se i due segnali interferiscono costruttivamente, entrambi gli utenti riceveranno le stesse risorse per trasmettere il messaggio di RRC Request e, a questo punto, l’eNodeB può individuare la collisione e non trasmetterà nessun acknowledgement, forzando entrambi gli utenti a ricominciare la procedura dall’inizio. Abbiamo quindi proposto un modello analitico per calcolare la probabilità di collisione in funzione del numero di utenti e del carico di traffico offerto, quando i tempi d’interarrivo tra richieste successive é modellata con tempi iper-esponenziali. In più, abbiamo investigato le prestazioni di comunicazioni di tipo Machine-to-Machine (M2M) e Human-to-Human (H2H), valutando, al variare del numero di preamboli utilizzati, la probabilità di collisione su canale RACH, la probabilità di corretta trasmissione considerando sia il tempo di backoff che il numero massimo di ritrasmissioni consentite, e il tempo medio necessario per stabilire un canale radio con la rete di accesso. I risultati, valutati nel loro insieme, hanno consentito di esprimere delle linee guida per ripartire opportunamente il numero di preamboli tra comunicazioni M2M e H2H.
The advent of LTE and LTE-Advanced, and their integration with existing cellular technologies, GSM and UMTS, has forced the mobile radio network operators to perform meticulous tests and adopt the right know-how to detect potential new issues, before the activation of new services.
In this new network scenario, traffic characterisation and monitoring as well as configuration and on-air reliability of network equipment, is of paramount relevance in order to prevent possible pitfalls during the deployment of new services and ensure the best possible user experience.
Based on this observation, this research project offers a comprehensive study that goes from experimental analysis to operational optimization.
The starting point of our work has been monitoring the traffic of an already deployed eNodeB with three cells, operative in the 1800 MHz band. Through subsequent measurement campaigns, it was possible to follow the evolution of the 4G network by the beginning of its deployment in 2012, until its full maturity in 2015. The data collected during the first year, showed a poor use of the LTE network, mainly due to the limited penetration of new 4G smartphone. In 2015, however, we appreciate a clear and decisive increase in the number of terminals using LTE, with aggregate statistics (e.g. marketshare for smartphone operating systems, or the percentage of video traffic) that reflect the national trend. This important outcome testifies the maturity of LTE technology, and allows us to consider our monitored eNodeB as a valuable vantage point for traffic analysis.
Hand in hand with the evolution of the infrastructure, even mobile phones have had a surprising evolution over the past two decades, from simple devices with only voice services, towards smartphones offering novel services such as mobile Internet, geolocation and maps, multimedia services, and many more.
Monitoring the real traffic has allowed us to study the users behavior and identify the services most used. To this aim, various software libraries for traffic analysis have been developed. In particular, we developed a C/C++ library that analyses Control Plane and User Plane traffic, which provides corse and fined-grained statistics at flow-level.
Another framework/tool has been exclusively dedicated to the topic of traffic classification. Among the plethora of existing tool for traffic classification we provide our own solution, developed from scratch. The project, which is available on github, is named MOSEC, an acronym for Modular SErvice Classifier.
The modularity is given by the possibility to implement multiple plug-ins, each one will process the packet according to its logic, and may or may not return a packet/flow classification. A final decision strategy allows to classify the various streams, based on the classifications of each plug-in.
Despite previous approaches, the ability of keeping together multiple classifiers allows to mitigate the deficiency of each classifiers (e.g. DPI\nomenclature{DPI}{Deep Packet Inspection} does not work when packets are encrypted or DNS\nomenclature{DNS}{Domain Name Server} queries don't have to be sent if name resolution is cached in device memory) and exploit their full-capabilities when it is feasible.
We validated the goodness of MOSEC using a labelled trace synthetically created by colleagues from UPC BarcelonaTech.
The results show excellent TCP-HTTP/HTTPS traffic classification capabilities, higher, on average, than those of other classification tools (NDPI, PACE, Layer-7). On the other hand, there are some shortcomings with regard to the classification of UDP traffic.
The characteristics of User Plane traffic have a direct impact on the energy consumed by the handset devices, and an indirect impact on the Control Plane traffic that is generated. Therefore, the acquaintances of the statistical properties of the various flows, allows us to deal with the problem of cross-layer optimization, that is reducing the power consumption of the terminals by varying some control plane parameters configurable on the eNodeB.
It is well known that the battery life of the new smartphones is one of the major limitations in the use of the same. In particular, the birth of new services and applications capable of working in the background without direct user interaction, introduced new issues related to the battery lifetime and the signaling traffic necessary to acquire/release the radio resources.
Based on these observations, we conducted a thorough study on the DRX mechanism (Discontinuous Reception), exploited by LTE to save smartphones energy when no packet is sent or received. The DRX configuration set and the RRC Inactivity Timer greatly affect the energy consumed by the various devices. Depending on which radio resources are allocated or not, the user equipment is in the states of RRC Connected and Idle, respectively.
To evaluate the energy consumption of smartphones, an algorithm simulates the transition between all the possible states in which an UE can be and maps a power value to each of these states. The transition from one state to another is governed by different timeouts that are reset every time a packet is sent or received. Using the traces of real traffic, we associate a state machine to each for assessing the energy consumption on the basis of the sent and received packets.
We repeated these simulations using different values of the inactivity timer, that appear to be more suitable than the one currently configured on the monitored eNodeB, looking for a good trade-off between energy savings and increased signaling traffic. The results highlighted that the Inactivity Timer set originally sull'eNodeB was too high and determined an excessive energy consumption on the terminals. Reducing the value up to 10 seconds permits to achieve energy savings of up to 50\% (depending on the underling traffic profile) without up considerably the control traffic.
The results of the study mentioned above, however, do not consider neither the stress level which the eNodeB is subject to, given the raise of signaling traffic that could occur, nor the increase of collision probability during the RACH procedure, needed to re-establish the radio bearer (or RRC connection ) between the terminal and eNodeB .
Evaluate the performance of hardware and software systems for the fourth-generation mobile network, as well as identify any possible weakness in the architecture, it is a complex job. A possible case study, is precisely to assess the robustness of the base station when it receives many requests for RRC connections, as effect of a decrease of the inactivity timer.
In this regard, within the Testing LAB of Telecom Italia, we used IxLoad, a product developed by Ixia, as a load generator to test the robustness of one eNodeB. The tests consisted in producing a different load of RRC request on the radio interface, similar to those that would be produced by decreasing the inactivity timer to certain values. The statistical properties for the signalling traffic are derived from the analysis of real traffic traces. The main outcomes have shown that, even in the face of an high load of RRC requests only a small part (less than 1\% in the most unfavorable of the cases) of the procedure fails. Therefore, even lowering the inactivity timer at values lower than 10 seconds is not an issue for the Base Station.
Finally, remains to be evaluated how such surge of RRC request impacts on users performance. If one of the users under coverage in the RRC Idle is paged for an incoming packet or need to send an uplink packet a state transition from RRC Idle to RRC Connected is needed. At this point, the UE initiates the random access procedure by sending the random access channel preamble (RACH Preamble). When two or more users attempt, simultaneously, to access the RACH channel, using the same preamble, the eNodeB may not be able to decipher the preamble. If the two signals interfere constructively, both users receive the same resources for transmitting the RRC Request message and, at this point, the eNodeB can detect the collision and will not send any acknowledgment, forcing both users to restart the procedure from the beginning. We have proposed an analytical model to calculate the probability of a collision based on the number of users and the offered traffic load, when the interarrival time between requests is modeled with hyper-exponential times.
In addition, we investigated some performance for Machine-to-Machine (M2M) and Human-to-Human (H2H) type communications, including the probability of correct transmission considering either the backoff time either the maximum number of allowed retransmissions, and the average time required to established a radio bearer with the access network.
The results, considered as a whole, have made possible to express the guidelines to properly distribute the number of preambles in H2H and M2M communications",multimedia,940,not included
,to_check,core,Conception d'un outil de modélisation intégré pour l'indexation et l'analyse de trace,2015-12-01 00:00:00,core,https://core.ac.uk/download/213619955.pdf,,"RÉSUMÉ
Les simulateurs de vol sont des systèmes composés d'un système logiciel et d'actuateurs mécaniques qui recréés la dynamique d'un vrai vol avec un aéronef et qui sont notamment utilisés par les compagnies de transport aérien pour former leurs futurs pilotes. Ces simulateurs doivent recréer des scénarios de vol permettant d'exécuter des itinéraires réguliers de vol ou de confronter les pilotes à différentes situations d'urgence qui peuvent subvenir lors d'un vrai vol (p. ex., une tempête violente). Puisque les simulateurs de vol ont un impact direct sur la qualité de formation des pilotes, une batterie exhaustive de tests s'impose à chaque fois qu'une nouvelle version d'un simulateur doit être mise en opération. Une bonne partie de ces tests requièrent l'intervention d'un pilote qui stimule le système en réalisant une série d'opérations de contrôle provenant de scénarios de vol préparés par des experts en aéronautique, ce qui est gourmand en temps, en ressources humaines et en ressources financières. 
La raison de la nécessité de réaliser tous ces tests est que le logiciel en soit est constitué de plusieurs composants de type boîte noire dont le code source n'est pas disponible, ils sont fournis tels quels par les fournisseurs du composant réel d'origine devant être simulé (p. ex., un composant hydraulique d'un aéronef que nous voulons simuler). Puisque nous n'avons pas pas accès au code source, il n'est pas possible lors de la mise-à-jour de l'un de ces composants de savoir quel sera le changement dans le comportement du simulateur. Dans le meilleur des cas, un défaut dans un composant aura un impact local et, dans le pire des cas, il peut nuire à tous les autres composants du simulateur. Il est dans ce cas nécessaire d'utiliser une stratégie de test d'une large granularité couvrant à la fois le fonctionnement des composants eux-mêmes et  l'ensemble des fonctionnalités de vol du simulateur afin d'en assurer sa qualité. 
Dans le cadre de ces tests, afin de réduire le temps requis pour vérifier le bon comportement d'une nouvelle version d'un simulateur, nous proposons dans ce mémoire d'automatiser l'analyse des résultats de test en modélisant le comportement normal du système logiciel en utilisant de l'apprentissage automatique. Un tel modèle tente de recréer le comportement stable du système en bâtissant des règles reliant ses entrées à chacune de ses sorties. Les entrées sont constituées de métriques provenant des contrôles qui stimulent le système et les sorties sont constituées de métriques que nous pouvons observer. Nous produisons donc un modèle initial d'une version fiable du simulateur et nous validons par la suite le bon comportement des versions subséquentes en comparant leur comportement avec celui du modèle initial. Nous cherchons à voir s'il y a un écart trop important avec le modèle initial, ce nous considérons comme étant une déviation.
Le but final de notre recherche est de pouvoir appliquer notre méthodologie d'analyse des résultats de test dans l'industrie de la simulation. Afin d'avoir plus de flexibilité dans nos expérimentations, nous commençons d'abord par valider l'approche avec un simulateur plus simple et ouvert. Nous utilisons donc le simulateur de vol à source libre FlightGear comme cas d'étude pour évaluer notre approche de test. Ce simulateur utilise l'engin de vol très populaire JSBsim et la librairie SimGear pour modéliser les aéronefs et simuler leur comportement dynamique dans un environnement de vol. JSBsim est notamment utilisé par la Nasa à des fins de recherche. Nous réalisons d'abord un vol manuel en utilisant des périphériques de contrôle d'un ordinateur tout en enregistrant les données contenues dans le simulateur. Le vol doit être composé de toutes les opérations régulières de pilotage afin que le modèle que l'on en extrait représente fidèlement le comportement normal du simulateur. Nous réalisons par la suite plusieurs répétitions du même vol afin d'identifier des niveaux de seuils robustes pour déterminer au delà de quels écarts un métrique d'une nouvelle version doit être considéré comme déviant par rapport au modèle. Nous élaborons à cet effet cinq scénarios de mutation du comportement de la version initiale de notre simulateur afin de générer différentes versions ayant des métriques qui respectent le comportement normal du sytème ou présentant une déviation du comportement normal reliée à un problème fonctionnel. En sachant d'avance quelles mutations présentent des déviations et avec l'aide d'experts identifiant précisément les métriques qui dévient de leur comportement normal, nous pouvons construire un oracle avec lequel nous évaluons la précision et le rappel de notre approche de détection.
En particulier, dans le cadre de notre étude empirique sur FlightGear, nous modifions une version initiale du simulateur en y injectant cinq différentes mutations qui n'ont pas d'impact grave ou qui engendrent des problème fonctionnels avec lesquels nous évaluons notre approche. Les résultats montrent qu'en choisissant des niveaux de seuil initiaux lors d'une première calibration, notre approche peut détecter les métriques ayant des déviations avec un rappel majoritairement de 100% (un seul cas de mutation à 50%) et une précision d'au moins 40%. En ce qui a trait aux mutations ne changeant pas le comportement normal du simulateur, notre approche a un taux de fausses alarmes de 60%. Nous montrons également qu'avec une base de données plus large, il est possible de calibrer notre approche afin d'améliorer sa performance. Avec des niveaux de seuil optimaux qui sont calibrés pour chacune des mutations, nous obtenons un taux de rappel de 100% pour tous les cas, une précision d'au moins 50% pour les versions ayant des problèmes fonctionnels et un taux de fausses alarmes de 0% pour les versions n'ayant pas de problèmes fonctionnels.
Afin d'ouvrir la voie pour des améliorations futures, nous utilisons notre méthodologie pour faire une petite expérimentation connexe sur JSBsim afin de détecter les déviations dans les transitions entre les états stables de chacun des métriques. Nous utilisons la même modélisation du simulateur. Les résultats montrent que nous pouvons battre une classification aléatoire des déviations. Cette nouvelle approche n'en n'est qu'à ces débuts et nous sommes convaincu que nous pouvons obtenir des résultats plus significatifs avec de futurs travaux sur le sujet.
Nous proposons également dans les améliorations futures de descendre le niveau de granularité de notre modélisation du simulateur au niveau de chacun de ses composants. Cela permettrait d'isoler exactement qu'elle composant présente une déviation, et ainsi trouver la source du problème.----------ABSTRACT
Flight simulators are systems composed of software and mechanical actuators used to train crews for real flights. The software is emulating flight dynamics and control systems using components off-the-shelf developed by third parties. For each new version of these components, we do not know what parts of the system is impacted by the change and hence how the change would impact the behaviour of the entire simulator. Hence, given the safety critical nature of flight simulators, an exhaustive set of manual system tests must be performed by a professional pilot. Test experts then rely on the test pilot's feedback and on the analysis of output metrics to assess the correctness of a simulator's behaviour, which consumes time and money, even more these tests must be repeated each time one of the components is updated by its vendor. 
To automate the analysis of test results from new simulator versions, we propose a machine learning-based approach that first builds a model that mimics the normal behaviour of a simulator by creating rules between its input and output metrics, where input metrics are control data stimulating the system and output metrics are data used to assess the behaviour of the system. We then build a behavioural model of the last-known correctly behaving version of the simulator, to which we compare data from new versions to detect metric deviations. The goal of our research is to first build a model of the simulator, then detect deviations of new simulator versions using that model, and finally develop an automatic approach to flag deviations when there are functional problems
Our case study uses the open-source flight simulator Flight Gear, based on the well-known JSBsim flight dynamic engine currently used by the Nasa. Applying our approach, we first drive a manual basic flight using computer peripherals while recording metrics in the simulator. We then use the recorded metrics to build a model of this basic execution scenario. After repeating several times our flight we have enough data to identify robust metric deviation thresholds to determine when a metric in a new version is deviating too far from the model. 
We generate five different versions of FlightGear by injecting harmless and harmful modifications into the the simulator, then perform again our initial flight multiple times for each of those versions while recording input and output metrics. We rely on experts identifying which output metric should be deviating in each scenario to build an oracle. Using an initial threshold, our approach can detect most of the deviations in problematic versions with a near perfect recall (only one case at 50%) and a reasonable precision of at least 40%. For the versions without harmfull deviations we get a false alarm rate of 60%. By optimizing the threshold for each version, we get a perfect recall of 100%, a precision of at least 50%, and a false alarm rate of 0% for the good behaving versions.
To open the path for future work, we perform a case study on JSBsim using a variant of our basic deviation detection approach to detect transient deviations using the same model. Our results show that we can beat a random classification for one kind of deviation, however for other deviations our models do not perform as well. This new approach is just a first step towards transient deviation detection, and we are convinced that we can get better results in future work. We also propose to use finer-grained models for each component to be able to isolate the exact component causing a functional problem",multimedia,941,not included
,to_check,core,Prédiction de performance de matériel graphique dans un contexte avionique par apprentissage automatique,2015-08-01 00:00:00,core,https://core.ac.uk/download/213619757.pdf,,"RÉSUMÉ
Le matériel informatique graphique destiné aux ordinateurs de bureau ou aux systèmes embarqués traditionnels, ainsi que leur interface de programmation ne peuvent pas être utilisés dans les systèmes avioniques puisqu’ils ne se conforment pas aux règles de certifications DO-254 et DO-178B. Toutefois, on remarque le faible nombre d’outils de conceptions qui encadrent le développement d’applications graphiques avioniques, et ce malgré l’apparition de matériel graphique avionique de plus en plus performants. Suivant par exemple la méthode classique de conception en V, les ingénieurs doivent d’abord effectuer des choix de conception reliés à la sélection du matériel graphique avant de débuter une quelconque implémentation de code. Ainsi, il peut être difficile d’évaluer la pertinence de ces choix en évaluant les performances de traitement du matériel graphique puisque l’engin graphique n’aurait pas nécessairement été développé. Je propose donc un outil de conception permettant de prédire les performances de matériel graphique en termes d’images par secondes (FPS), basé sur OpenGL SC. L’outil crée des modèles non-paramétriques de performance du matériel en analysant, à l’aide d’algorithmes d’apprentissage, le temps de dessin de chaque image, lors du rendu d’une scène 3D synthétique. Cette scène est rendue à quelques reprises en faisant varié certaines de ses caractéristiques spatiales (nombre de sommets, taille de la scène, taille des textures, etc.) qui font parte intégrante du logiciel de vision synthétique habituellement développé dans ce domaine. Le nombre de combinaisons de ces caractéristiques utilisées durant l’entraînement supervisé des modèles de performance n’est qu’un très petit sous-ensemble de toutes les combinaisons, le but étant de prédire par extrapolation celles manquantes. Pour valider les modèles, une scène 3D fournie par un partenaire industriel est dessinée avec des caractéristiques non traitées durant la phase d’entraînement, puis le FPS de chaque image rendue est comparé au FPS prédit par le modèle. La tendance centrale de l’erreur de prédiction est ensuite démontrée comme étant moins de 4 FPS.----------ABSTRACT
Within the strongly regulated avionics engineering field, conventional graphical desktop hardware and software API cannot be used because they do not conform to the DO-254 and DO-178B certifications. We observe the need for better avionic graphical hardware, but system engineers lack system design tools related to graphical hardware. The endorsement of an optimal hardware architecture by estimating the performance of a graphical software, when a stable rendering engine does not yet exist, represents a major challenge.  There is also a high potential for development cost reduction, by enabling developers to have a first estimation of the performance of its graphical engine at a low cost. In this paper, we propose to replace expensive development platforms by a predictive software running on desktop. More precisely, we present a system design tool that helps predict the rendering performance of graphical hardware based on the OpenGL SC API. First, we create non-parametric models of the underlying hardware, with machine learning, by analyzing the instantaneous frames-per-second (FPS) of the rendering of a synthetic 3D scene and by drawing multiple times with various characteristics that are typically found in synthetic vision applications. The number of characteristic combinations used during this supervised training phase is a subset of all possible combinations, but performance predictions can be arbitrarily extrapolated. To validate our models, we render an industrial scene with characteristics combinations not used during the training phase and we compare the predictions to real values. We find a median prediction error of less than 4 FPS",multimedia,942,not included
,to_check,core,Simulation and HRI Recent Perspectives with the MORSE Simulator,2014-01-01 00:00:00,core,https://core.ac.uk/download/211824079.pdf,Springer International Publishing,"Lemaignan S, Hanheide M, Karg M, et al. Simulation and HRI Recent Perspectives with the MORSE Simulator. In: Brugali D, Broenink JF, Kroeger T, MacDonald B, eds. Simulation, Modeling, and Programming for Autonomous Robots. 4th International Conference, proceedings. Lecture Notes in Artificial Intelligence (LNAI). Vol 8810. Cham: Springer International Publishing;  2014.Simulation in robotics is often a love-hate relationship: while simulators do save us a lot of time and effort compared to regular deployment of complex software architectures on complex hardware, simulators are also known to evade many (if not most) of the real issues that robots need to manage when they enter the real world. Because humans are the paragon of dynamic, unpredictable, complex, real world entities, simulation of human-robot interactions may look condemn to fail, or, in the best case, to be mostly useless. This collective article reports on five independent applications of the MORSE simulator in the field of human-robot interaction: It appears that simulation is already useful, if not essential, to successfully carry out research in the field of HRI, and sometimes in scenarios we do not anticipate",multimedia,943,not included
,to_check,core,place:Riva del Garda,2013-01-01 00:00:00,core,plant disease models: from field observations to biological mechanisms,,"A plant disease model is a simplification of a real pathosystem (i.e., the relationships between a pathogen, a host plant, and the environment) that determine whether and how an epidemic develops over time and / or space. Different approaches have been used for the development of plant disease models, with relevant improvements in recent years. 
Empirical models have been elaborated using data collected under variable field conditions since the second half of the last century. The so called 3 10 rules for predicting first seasonal infection of grape downy mildew is a precursor of this empirical approach for understanding relationships between pathogens, plants and the environment. By using this approach, the model is developed by searching mathematical or statistical relationships between field collected data and these relationships do not necessarily have cause-effect meaning. Lack of knowledge, accuracy and, especially, robustness are the main weaknesses of these models, which impose accurate validation and, usually, proper calibration when these models are used in different environments or under changing climate. Recent methods of data analysis, like for instance neural networks, improve the capability of searching the mathematical structure of the model but they do not overcome the above mentioned weaknesses.
Mechanistic models are a new class of models based on knowledge of biological and epidemiological behaviour of the system under study. These models (also referred to as explanatory, theoretical, or fundamental) explain the pathosystem on the basis of what is known about how the system works in relation to the influencing variables. Mechanistic models are dynamic, because they analyse the changes over time of the components of an epidemic due to the external, influencing variables. Dynamic modelling is based on the assumption that the state of the pathosystem in every moment can be quantitatively characterised and that changes in the system can be described by mathematical equations. These models overcome the weakness of the empirical models. Compared to the 3 10 rule, a mechanistic model for grape downy mildew increased the overall accuracy of the predictions from ~60% to ~90%.
Complexity of mechanistic models has been regarded as a problem for the implementation of models in practical disease control, compared to the simplicity of the empirical models. This is a false problem, because confusing complexity of the mathematical framework of the model with complexity of model output is misleading. Indeed, it is possible to use complex models, able to depict the complexity of the biological systems, to produce simply, easy-to-use output for growers. Implementation of the above mentioned mechanistic model for grape downy mildew in a Decision Support System used by viticulturists clearly demonstrates the inconsistency of the \u201ccomplexity paradigm\u201d",multimedia,944,not included
,to_check,core,,2013-07-22 00:00:00,core,"jie ma,",,"In this paper, an online methodology for the detection of unsafe driving states while driving is presented. The detection is based on the multi-sensor approaches, including gyrometer, accelerometer, radar, video and so on. Various information comes from both the ego vehicle and its surroundings are fused to gain a comprehensive understanding of driving situations. Using subspace modeling techniques, we propose an unsupervised learning algorithm to perform the unsafe states detection. The feature space are decomposed into the normal and anomalous subspace, where the normal space are assumed as the major components of the driving patterns, and significant deviations from the modeled normal subspace are signaled as unsafe states. In addition, the algorithm works in a real-time way incorporating a implementation of sliding window, which enable the method to adapt over time to address changes in the new emerged driving situations. We have implemented our algorithm with a prototype system installed in a transit bus, validations are performed in real driving situations. Our experimental results demonstrate the effectiveness of the approach on forward risk predication. We gain a timely predication while with a low false positive when there occurs conflicts between the ego vehicle and front vehicles",multimedia,945,included
,to_check,core,,2013-01-01 00:00:00,core,absolute quantification of choline-related biomarkers in breast cancer biopsies by liquid chromatography electrospray ionization mass spectrometry,,"Abstract. It has been repeatedly demonstrated that choline metabolism is altered in a wide variety of cancers. In breast tumours, the choline metabolite profile is characterized by an elevation of phosphocholine and total choline-compounds. This pattern is increasingly being exploited as biomarker in cancer diagnosis. The majority of in vitro metabolomics studies, for biomarkers quantification in cell cultures or tissues, entail proton NMR spectroscopy. Although many &quot;targeted&quot; approaches have been proposed to quantify metabolites from standard one-dimensional (1D) NMR experiments, the task is often made difficult by the high degree of overlap characterizing 1 H NMR spectra of biological samples. Here we present an optimized protocol for tissue extraction and absolute quantification of choline, phosphocholine and glycerophosphocholine by means of liquid chromatography electrospray ionization mass spectrometry (LC-ESI-MS). The selected chromatographic separation system with a HILIC (hydrophilic interaction chromatography) amide column effectively separates free choline and its phopshorylated derivatives, contrary to failure observed using standard reversed-phase chromatography. The metabolite absolute quantification is based on external calibration with commercial standards, and is validated by a parallel 1D proton NMR analysis. The LC-MS/NMR analysis is applied to three breast carcinoma specimens obtained by surgical excision, each one accompanied by a control tissue sample taken outside the tumor margin. The metabolite concentrations measured are in good agreement with previous results on metabolic profile changes of breast cancer. Each of the three cancerous biopsies, when compared with the control tissue, exhibit a highly increased levels phosphocholine, total choline and phosphocholine/glycerophosphocholine ratio. demonstrated by numerous in vivo magnetic resonance spectroscopy (MRS) studies  In synthesis all of the altered metabolites participate in the biosynthetic and catabolic pathways of the major membrane phospholipid PtdCho, as shown in the schematic diagram  In vivo detection of choline metabolites by proton magnetic resonance spectroscopy ( 1 H MRS) has been proven to be useful in the diagnosis of cancer  The present work builds on the established correlation between choline metabolism modification and malignant tumor proliferation and focuses on three cancer indicators, namely tCho and PCho concentrations and PCho/GPCho ratio. A metabonomic study is conducted on three breast carcinoma specimens obtained by surgical excision, each one accompanied by a control tissue sample taken outside the tumor margin. The study entails tissue extraction of metabolites and a parallel analysis conducted by onedimensional (1D) 1 H NMR spectroscopy, taken as standard technique, and an optimized implementation of a LC/ESI-MS method. The key feature of the method, which was already reported in 2011  Scope of the present study is to prove that the assessment of breast cancer markers by LC/ESI-MS is feasible and diagnostically valuable. This technique shows a resolution advantage for assaying choline derivatives, and potentially many other target molecules, compared to NMR, and could complement the latter. After the necessary large scale standardization of protocols, mass spectrometry (MS) could reduce the traumatic impact of current bioptic procedures based on its sensitivity and, eventually, replace the current diagnostic protocols.  M.C. Mimmi et al. / Absolute quantification of choline-related biomarkers 73 Experimental section Reagents Standard compounds were obtained from SigmaAldrich. Choline, phosphocholine and glycerophosphocholine were respectively in the form of choline chloride, phosphorylcholine chloride calcium salt tetrahydrate and sn-glycero-3-phosphorylcholine 1:1 cadmium chloride adduct. All solvents used were of HPLC grade. Instrumentation The LC/MS system consisted of a QqTOF mass spectrometer (Q-STAR by Applied Biosystems-SCIEX) equipped by an electrospray ionization (ESI) source and an Agilent 1100 series micro LC pump. 1 H NMR experiments were acquired on a Bruker Avance spectrometer operating at 500 MHz. Tissue extraction Human breast-tissue specimens were obtained from three patients (referred to as patients A, B, C) by surgical excision  Choline compounds were extracted from tissue by a method derived from Bligh and Dyer  1) The frozen powder was suspended in 0.400 ml methanol/chloroform (2:1 v/v) and, after thorough vortexing, left overnight at -20 • C. 2) The sample was then centrifuged at 1500 g for 5 minutes and the supernatant extract was separated and stored at -20 • C. 3) The residue was submitted to re-extraction with 0.250 ml of methanol/chloroform/water (2:1:0.8 by volume) and left at -20 • C for at least four hours. 4) The supernatants from both extractions were combined and, after addition of first 0.100 ml of chloroform and then 0.100 ml of water, the resulting solution was submitted to 5 minutes centrifugation. The procedure separates two phases: an aqueous phase containing methanol at the top and an organic phase at the bottom; in some case denatured proteins was observed in the middle. 5) The aqueous phase (that contains the hydrosoluble choline compounds) was directly lyophylized and then redissolved in 0.020 ml water and 0.800 ml methanol (2.4% H 2 O by volume). The resulting minute amounts of precipitates (proteins) were removed by filtration with 0.2m filters. The filtrated solution was stored at -80 • C until it was used for either LC/ESI-MS or NMR analysis. LC/ESI-MS Analysis The LC-MS method previously reported  Positive ESI parameters for standard MS scan were tuned for best sensitivity under HPLC conditions. Nitrogen was used as nebulizer gas (GS1) and curtain gas (CUR): GS1 and CUR were set respectively at 20.00 and 15.00 psi. The DP (declustering potential), FP (focusing potential) and DP2 (secondary declustering potential) were set respectively at 40, 230 and 15 V. The ion spray voltage (IS) was 5 kV. The positively charged molecular ions of choline compounds were monitored by extracting the ion chromatogram of selected mass ranges including the target molecules mass. Data treatment The analytes peaks were integrated with the software Analyst QS 1.1 (Applied Biosystems-SCIEX) by evaluating the extracted ion content (XIC) counts. The correspondence between measured XIC counts and absolute concentrations was established by calibration curves with standard compounds. Details can be found in the Supplementary Material Section. NMR spectroscopy Analysis An aliquot of each aqueous extract (corresponding to 75 mg of frozen pulverized tissue) was dried under vacuum and redissolved in 0. Monodimensional 1 H spectra were collected using a sweep width of 8 kHz and a time domain of 8 K complex data points, with 64 acquisitions and 8 silent scans. Flip-angle pulses of 30 • were applied with a relaxation delay of 5 s. Presaturation was used for suppression of the HOD signal during the whole initial delay. The adopted pulse sequence includes the Electronic Reference Signal (ERETIC TM ) [2] synthesized by an electronic device, which was used for the determination of absolute concentrations. The reference signal provides a pseudo-FID that has all the characteristics of a real NMR signal, after proper calibration, and whose parameters (frequency, magnitude, phase, T2) are controlled from the spectrometer console. Spectra acquisition and processing were performed with the Bruker software TOPSPIN. Data treatment After FID apodization with a shifted sine square window function (90 • ) all spectra were phased, Fourier-transformed and chemical-shift referenced. The integral value of significant peaks were measured. In some cases Gaussian window multiplication of the FID was necessary to enhance resolution of signals: the deconvolution of the envelopes of interest was performed by fitting a mixed Lorentzian/Gaussian lineshape to the experimental pattern. The peak modeling,  M.C. Mimmi et al. / Absolute quantification of choline-related biomarkers 75 performed with TOPSPIN, was used only for the estimate of the integral fractions to be assigned to the single components of peak envelopes. The absolute quantification of metabolites of interest (namely Cho, PCho, GPCho) was based on N-trimethyl peaks  Results Extraction protocol The very first step of frozen tissue pulverization was modified with respect to our previously reported protocol  LC/ESI-MS The LC/ESI-MS analysis of the bioptic aqueous extracts focused on three significant metabolites for cancer diagnosis: Cho, PCho, and GPCho. The three molecules are hydrosoluble and positively charged, due to the choline moiety, hence all are detected by ESI-MS under positive mode. The protocol for chromatographic separation and mass spectrum acquisition had been already established  The Total Ion Current (TIC) Chromatogram of a standard mixture solution of Cho, PCho, GPCho is shown in  The concentration of free Cho and Cho derivatives obtained by LC/ESI-MS analysis are reported in  NMR Bioptic aqueous extracts, obtained as described in Experimental Section, were analyzed by 1 H 1D NMR without any buffer addition. Examples of full spectra relative to two representative biologic samples are shown in  The Cho compounds estimation resulting by 1D NMR analysis is reported in  Discussion Sample choice The first qualifying point of the present study is the availability of a double specimen for each of the three enrolled patients: the first derived from the proliferating carcinoma and the second from healthy tissue outside the tumor margin. Though in principle every metabonomic study should consider cancer biopsies and healthy tissue controls from the same individuals, i.e. internal controls, to account for the individual metabolome variability, this is not always possible and many theoretical conclusions on metabolic markers are based on the differential absolute concentrations in cancerous and normal tissues, without taking into account the tissue source. In this study, on the contrary, we could exclude that the concentration changes of choline compounds were related to the intrinsic variability among individuals. Methodology: NMR vs LC-MS NMR and Mass Spectrometry (MS) represent the principal approaches in metabolomics/metabonomics studies. In this type of application, NMR has many advantages such as high information content of the resulting spectra, ease of quantification and little need of optimization for the analysis conditions. MS techniques can cover a wide chemical diversity with high sensitivity and resolution, but require a preliminary optimization of the chromatographic separation. The present study highlights the complementarity of the two analytical approaches. We adopted 1D NMR as conventional reference technique to collect a full set of information on the aqueous extract of biopsies. The use of 1D NMR for the analysis of unfractionated tissue extract, with hundreds of overlapping resonances can not afford complete assignment and accurate peak integration. For the quantitative determination of our target biomarkers with well established assignment, however, we could adopt the approach of fitting overlapped 1D spectra with modeled peaks. The modeling Only the extracted ion chromatograms (XICs) are shown for Cho (blue traces), PCho (red traces), GPCho (green traces). The Cho and PCho components occurring at the retention time of GPCho (at 15-16 min) are due to in-source fragmentation of the latter metabolite. The splitting of the Cho peak into two peaks is likely to be ascribed to the different possible mechanisms of interactions of the molecule with the HILIC solid phase at the operating conditions. A remarkable difference in the relative intensity of Cho, PCho and GPCho signals is found and is consistent with cancer-related metabolism alteration. approach allowed the concentration estimate of Cho, PCho and GPCho in the whole set of sample, though for 2 out of 6 samples it was not possible to resolve PCho and GPCho, and only the total amount of the two compounds was reported. Differently, the combination of on-line HILIC separation and ESI-MS allowed to resolve the target peaks without the necessity of chemical derivatization. The LC-MS quantification of Cho derivatives in the test samples was obtained by interpolation of the LC-MS response, based on calibration curves obtained under the same experimental conditions. Although LC-MS quantitative assessment is safely carried out based on isotopically labeled internal standards, the response for Cho compounds is reasonably linear over the working range (correlation coefficients between 0.9686 and 0.9892 low R 2 values due to random errors). Some critical aspects of the LC-MS approach, that arise from reproducibility of the measurements, remain inherently unavoidable. Some significant metabolite level differences observed in  78 M.C. Mimmi et al. / Absolute quantification of choline-related biomarker",multimedia,946,not included
,to_check,core,All'Insegna del giglio,2013-01-01 00:00:00,core,"verso una metodologia condivisa per l'analisi del paesaggio antico: il progetto ""valle del tevere""",,"International audienceThe Tiber Valley Project aims to create a series of digital applications for 3D reconstructing, visualization and real time browsing of the ancient and current Tiber Valley landscape (particularly for the Villa dei Volusii and Lucus Feroniae areas), in four different historical phases. In this perspective, the first problem to face is the need for a valid methodology for ancient landscape ecosystem reconstruction, before dealing with monuments and building. On the basis of an intense multi-disciplinary discussion and the previous VH Lab experience in this field, in this article we are presenting a scheme for a standardized reconstruction procedure, where the landscape is built using all available sources and elevation data obtained by a photogrammetry process on historical pictures. Ecosystem areas are then calculated through GIS elaboration in GRASS-GIS environment, through a procedure which may be shared for any situation of historical landscape reconstruction, allowing the matching and the mathematical processing of geographical data aimed to the definition of different ecological areas (both in terms of natural vegetation and cultivated lands). Maps are then created to be imported in procedural landscape generation engines: the last part of the paper focuses on the lack of effective open source software in this field, and a possible proposal implementation in this sense1. Il progetto "" Valle del Tevere "" Il presente lavoro illustra la sperimentazione di una metodologia per la ricostruzione del paesaggio storico, con particolare riguardo agli aspetti di definizione delle diverse presenze vegetazionali e antropiche del territorio in determinate epoche del passato. Il caso di studio è rappresentato dal lavoro che l'Istituto per le Tecnologie Applicate ai Beni Culturali (ITABC) del CNR sta svolgendo nell'ambito del Programma "" Arcus "" , finalizzato alla creazione di un sistema integrato di conoscenza, valorizzazione e comunicazione del paesaggio culturale della Valle del Tevere (contesti archeologici, storico-arti-stici, naturalistici, antropici), in particolare dell'area compresa tra il Monte Soratte e Fiano Romano, in direzione N-S, e il tracciato della via Flaminia antica e Palombara Sabina, in direzione E-O. Il progetto prevede una variegata serie di prodotti finali, quali: – un'installazione di realtà virtuale ludico-educativa, caratterizzata da un sistema di natural interaction (interazione attraverso i movimenti del corpo), localizzata all'interno di un museo di Roma, quale porta privilegiata di accesso e promozione del territorio della Valle del Tevere; – una guida multimediale alla Villa dei Volusii e al sito di Lucus Feroniae, da fruire sia su mobile che presso il Museo di Lucus Feroniae; – una guida alla Riserva del Tevere-Farfa, per smartphone o tablet, da fruire durante la visita all'area naturalistica; – un'installazione multimediale-filmica dedicata alla Riserva naturale del Teve-re-Farfa e destinata alle scuole, da fruire nel Museo del Fiume di Nazzano; – un sito web sul paesaggio culturale, basato su un sistema informativo geo-grafico in 3D e dedicato al pubblico di turisti, studiosi, operatori, scuole. Il lavoro comporta una ricostruzione fotorealistica del paesaggio natu-rale e antropico rispetto alle seguenti fasi cronologiche: – fase preistorica (ricostruzione della storia geologica dell'area, formazione della Valle del Tevere e dell'alveo fluviale, tra 3 milioni di anni fa e 12.000 anni fa); – fase pre-romana (focalizzata sull'Età del Ferro e sul periodo Orientaliz-zante); – fase romana (focalizzata, in particolare, sul periodo augusteo); – fase medievale (focalizzata, in particolare, sul XII secolo); – fase contemporanea",multimedia,947,not included
,to_check,core,,2013-12-03 00:00:00,core,"evidence and recommendations to support the use of
a novel passive water sampler to quantify antibiotics in wastewaters",10.1021/es402662g.s001,"A novel passive water sampler (diffusive gradients in thin-films
for organics, o-DGT) was previously developed and successfully tested
in the laboratory, but has not yet been validated in the field. Here,
o-DGT samplers were deployed in the influent and effluent of a typical
UK wastewater treatment plant (WWTP); the influent was also sampled
with a conventional automatic sampler (Auto) and by grab (Grab) sampling.
All the samples were analyzed by LC-MS/MS for 40 target antibiotics
(including 16 sulfonamides (SAs), 12 fluoroquinolones, 6 macrolides,
2 ionophores, 2 diaminopyimidines, 1 aminocoumarin, and 1 lincosamide).
The diffusion coefficients (<i>D</i>) of these antibiotics
in o-DGT, measured in the laboratory, ranged from 0.58 × 10<sup>–06</sup> to 6.24 × 10<sup>–06</sup> cm<sup>2</sup> s<sup>–1</sup>. The derived surface area normalized sampling
rates (<i>R</i><sub>S/A</sub>, 0.54–5.74 mL d<sup>–1</sup> cm<sup>–2</sup>) were comparable with those
for another passive sampler called POCIS. Fourteen antibiotics were
detected in the actively sampled water samples, with 10 of the 14
detected in o-DGT devices deployed for more than 7 days. Most of the
antibiotics detected in o-DGT, except sulfapyridine, were continually
accumulated by o-DGT for ∼10 days. Deployment for 7 days is
recommended to integrate ambient concentrations over time, without
risks of reaching capacity and significant biofouling. Diffusive boundary
layer (DBL) thickness had less effect on the o-DGT measurement than
reported for other passive samplers. The comparison between o-DGT
and Auto and Grab samplings showed that o-DGT was more efficient in
terms of cost, time, and labor. This study demonstrates for the first
time in a real environment that o-DGT is an effective tool for the
routine monitoring of antibiotics in wastewaters and provides a powerful
approach to studying their occurrence, fate, and behavior in the environment",multimedia,948,not included
,to_check,core,Université de Strasbourg,2011-01-27 00:00:00,core,study and design of an analog neural processor of very low power consumption : appliced to the navigation of a new generation pacemaker,,"L’objectif de cette recherche est de développer un réseau de neurones impulsionnels analogiques afin d’améliorer la performance d’un pacemaker biventriculaire (aussi appelé le CRT-P) de nouvelle génération. L’implémentation sur silicium utilise l’approche réseau de neurones analogiques qui nécessite le développement d’une solution technique satisfaisant à une contrainte de très basse consommation énergétique. Nous proposons une approche de un réseau de neurones impulsionnels analogiques pour optimiser la prédiction des délais cardiaques avec l’algorithme d’apprentissage Hebb et l’algorithme d’apprentissage par renforcement dans des modes de fonctionnement différents. L’amélioration des prévisions permet au CRT-P de fournir des battements cardiaques optimaux en temps réel. Nous décrirons le comportement et les qualités de notre algorithme au travers de simulations mathématiques et comportementales. Des simulations complètes et cohérentes du système basées sur des modèles simples du coeur

(rythme cardiaque constant puis rythme cardiaque variable) avec des bruits uniformes aléatoires sont illustrées avec succès pour la validation de la faisabilité du système.

Nous proposons aussi une méthodologie renforcée de la conception analogique et mixte. Les simulations de tous niveaux (de hauts et bas niveaux)peuvent être faites rapidement afin de vérifier des performances du système dans chaque phase de conception et ainsi fournir une plage des spécifications acceptables facilitant la synthèse analogique et mixte suivante.The objective of this research is to develop an analog spiking neural network so as to improve the performance of a bi-ventricular pacemaker (also called the CRT-P) of new generation. The implementation on silicon using the analog neural network approach requires the development of a

satisfactory technical solution to meet the constraint of very low energy consumption. We propose an analog spiking neural network approach to optimize the cardiac delay prediction with the Hebbian learning algorithm and the reinforcement learning algorithm in different functional modes.

The delay improvement allows the CRT-P to provide optimal heartbeat in real time. We describe the behavior and the qualities of our algorithm through mathematical and behavioral simulations. The complete and coherent system simulations based on the simple heart models (constant heart

rate and variable heart rate) with random uniform noise are shown successfully to validate the system feasibility.

We also propose an enhanced methodology of the analog and mixed signal design. The simulations of all levels (high and low levels) can be carried out quickly in order to verify the system performance in each design phase and also carry out the acceptable specification space for facilitating the following analog and mixed signal synthesis",multimedia,949,not included
,to_check,core,Cambridge Scholar Press,2012-01-26 00:00:00,core,multi-label eurovoc classification for eastern and southern eu languages,,"Multi-label document classification is the task of automatically assigning multiple categories to the same document (e.g. a book is about cooking and about Austrian food). At least for Machine Learning approaches, this task is harder than standard (single label) classification because it is not clear for the learning software whether the presence of a feature (typically a word) is an indication of one class or another (e.g. whether the presence of the word ‘salt’ is an indication for the category cooking or for the category Austrian food). Multi-label classification is a real challenge if the number of classes is very high and if the number of training documents per category is unevenly distributed. We are presenting experiments with the JRC EuroVoc Indexer software JEX (Steinberger et al. 2012), which has been trained for all official EU languages on tens of thousands of documents per language to assign the thousands of class labels of the EuroVoc thesaurus . JEX is a multi-label classification system using a bag-of-words document representation. When applying such a tool that uses word forms as classifier features to languages as different as Germanic (e.g. English), Romance (e.g. French), Slavic (e.g. Czech or Polish) and Finno-Ugric languages (e.g. Estonian or Hungarian), the question arises how much the classifier performance differs. It can be expected that the significantly higher ratio of word forms to lemmas in Slavic and Finno-Ugric languages has a negative impact on the classifier performance, or that more training material would be needed for these more highly inflected languages to achieve the same performance. Similarly, one might wonder whether part-of-speech (POS) information is useful. JEX will soon be made available to parliamentary and other users. The experiments described in this chapter thus have a practical relevance as they can give an indication to the users and their technical partners as to whether they should invest in improving the software through linguistic pre-processing.JRC.G.2-Global security and crisis managemen",multimedia,950,not included
,to_check,core,place:ROMA,2011-01-01 00:00:00,core,la gestione del colore in real time rendering nella progettazione di spazi interni riconfigurabili,,"La personalizzazione dei prodotti rappresenta un argomento ampiamente trattato in letteratura, sia dal punto di vista dell\u2019evoluzione del mercato e dei clienti, sia dal punto di vista delle aziende, per le quali la personalizzazione \ue8 stata sovente descrit-ta come un\u2019opportunit\ue0, foriera di molte conseguenze nel modo in cui esse devono riorganizzare il loro processo produttivo per poterne sfruttare le potenzialit\ue0; a que-sto proposito si pensi, a titolo di esempio, alla sola necessit\ue0 di riorganizzare com-pletamente le logiche di gestione dei magazzini per i prodotti finiti che imponeva una forte rigidit\ue0 nelle possibili scelte da parte dei clienti finali.
All\u2019interno di questo vasto argomento, riteniamo che un elemento di grande interes-se possa essere costituito dalla possibilit\ue0, offerta dai nuovi strumenti digitali, di permettere ai progettisti che operano nel settore dell\u2019arredo e dell\u2019interior design di valutare in anticipo la personalizzazione sia delle \u201csuperfici\u201d dei prodotti, poich\ue9 questo costituisce uno dei pi\uf9 forti fattori di differenziazione e d\u2019identificazione, sia degli ambienti in cui quegli stessi prodotti devono essere inseriti per essere proposti all\u2019acquisto da parte del cliente.
Poich\ue9 un processo di personalizzazione soddisfacente nel settore dell\u2019arredo richie-de normalmente un ampio ventaglio di combinazioni di colori e di materiali, l\u2019uso di soluzioni di vendita tradizionali mal si adattano generalmente allo scopo. Questo fatto, vero in generale, \ue8 reso ancora pi\uf9 evidente nell\u2019ambito dell\u2019arredo a causa delle dimensioni dei prodotti e dell\u2019impossibilit\ue0 pratica di mostrare tutte le possibili combinazioni nei normali spazi vendita che, infatti, sono normalmente dotati di pic-coli campioni che i clienti faticano per\uf2 a mettere in rapporto con i prodotti e quindi ad immaginare correttamente nel risultato finale.
La progettazione virtuale costituisce un metodo e un processo del progetto relativa-mente nuovi, in grado di gestire meglio che in passato la necessit\ue0 di previsione e di valutazione delle possibili personalizzazioni.
Quella che \ue8 presentata di seguito \ue8 un\u2019applicazione innovativa, che combina modellazione virtuale e rappresentazione foto realistica dei modelli digitali, in cui i modelli digitali possono essere inseriti in un ambiente 3D navigabile, modificandone le caratteristiche dei colori e dei materiali in tempo reale senza una significativa per-dita di qualit\ue0 rispetto ai risultati tradizionalmente ottenibili con la fotografia.
Per indagare compiutamente il processo di progettazione e di personalizzazione nel settore arredo, abbiamo sviluppato il nostro sistema in collaborazione con B&B Ita-lia \u2013 un leader indiscusso del mobile contemporaneo \u2013 utilizzando come caso studio la collezione Maxalto progettata da Antonio Citterio.
Il sistema \ue8 stato realizzato avendo concordato con B&B Italia che il risultato potesse consentire: 

1.	Una visualizzazione fotorealistica in real time;
2.	Una semplificazione del processo di interior design;
3.	Una pi\uf9 semplice comprensione da parte del cliente dello spazio e delle at-mosfere proposte;
4.	Un controllo sul processo di progetto tale da poterne garantire una maggior qualit\ue0;
5.	Una riduzione dei costi di comunicazione;
6.	Un time-to-market pi\uf9 rapido. 

Per la visualizzazione dei modelli 3D in real time, abbiamo utilizzato un software commerciale, Deltagen della Real Time Technology, software che \ue8 considerato lo stato dell\u2019arte nel campo della visualizzazione real time per il product e per il car design. L\u2019azienda produttrice del software \ue8 stata coinvolta nel progetto, ed ha fornito un supporto sviluppando alcune funzionalit\ue0 specifiche, in quanto l\u2019efficacia del sistema proposto era direttamente correlato alla possibilit\ue0 di sperimentare ambiti con cui fosse possibile migliorare la resa grafica garantita dalla qualit\ue0 delle librerie grafiche OpenGL, uno standard normalmente utilizzato dalla quasi totalit\ue0 dei software di Real Time ..",multimedia,951,not included
,to_check,core,"Saint Louis University Libraries Special Collections, Archives & Manuscripts",2011-01-01 00:00:00,core,"folder 39: kusko, b.h. the development of particle induced x-ray emission for the study and analysis of museum objects, 1989",,"This folder contains two copies of a typewritten report: Kusko, Bruce H. ""The Development of Particle Induced X-Ray Emission for the Study and Analysis of Museum Objects."" Progress Report to J. Ligot, Director, LRMF. Only one copy has been digitized and is presented here.The items in this folder are part of the Thomas A. Cahill Papers--Crocker Historical and Archaeological Project, 1981-2009. They are from Series 1: Thomas A. Cahill Research Papers, 1981-1994. This series consists of various research papers and published articles based upon Dr. Cahill's research using Particle Induced X-ray Emission (PIXE) techniques in analyzing inks and papers.PROGRESS REPORT:
The Development of
Particle Induced X-ray Emission
for the
Study and Analysis of Museum Objects
Submitted by: Bruce H. Kusko
Fulbright Research Scholar
AGLAE
Laboratoire de Recherche
des Musees de France
to: M. Ligot
Directeur, LRMF
28 February, 1989
RESUME
Le Louvre est le premier musee au monde a posseder un
acceIerateur de particules pour l'etude et l'analyse des oeuvres
d'art et d'archeoLoqi.e. Le Laboratoire de Recherche des Musees de
France s'est donc equipe d'un outil t res puissant qui offrira aux
scientifiques de musee la palette des techniques d'analyse par
faisceaux d'ions acceleres qui comprend PIXE, PIGME, NRA, RBS et plus
tard datation C14.
Le rapport d'avancement consigne les travaux que j'ai accomplis
entre le 15 septembre et le 28 f'evr i.er 1989. Pendant cette peri ode
nous avons commence par monter un systeme PIXE d'analyse des oeuvres
d'art des collections de musee. PIXE est une technique multi-elementai
re, sensible, bon marche, rapide et ce qui est le plus
important dans le cas d'espece non destructive. Nous avons ainsi
obtenu des resultats preliminaires sur des echantillons de verre, des
etaIons qeoLoqi.ques , des pigments de peinture et des objets en or.
D'une certaine mani.ere, ces experiences ont ete plut6t un
apprentissage pour moi et l'equipe AGLAE. Nous nous sommes
familiarises avec l'accelerateur, la chambre d'analyse, le detecteur
de rayons X, l'eIectron.ique d'acquisition. Les premiers resul.t ats
sent; t res encourageants, il reste pourtant beaucoup de travail a
fournir pour que le systeme PIXE d'AGLAE soit operationnel en
routine.
Nous n'avons, de plus, qu'un temps limite de faisceau pour le
PIXE, puisque la priorite est donnee aux tests -verifier que
l'accelerateur est bien conforme aux specifications du constructeur.
L'accelerateur presente quelques problemes avec la source d'ions avec
comme consequence un courant de faisceau Leqeremerrt instable. De
plus, nous avons utilise une chambre provisoire. La nouvelle chambre
definitive a ete construite a Strasbourg et est en cours
d'installation. Il n'y a pas a l'heure actuelle un moyen sur pour
mesurer le courant de particules dans le cas d'echanti Ll.oris epai.s,
c'est pourquoi les resultats presentes ici ne s~nt que relatifs. Un
hacheur de faisceau (un composant pour mesurer le courant en
prelevant peri.odq.uiement; une partie du faisceau) est en cours de
construction a Jussieu-Universite Paris VII et sera monte au Louvre
en avril.
Nous avons porte nos efforts particulierement sur 'quatre
etalons de verre, qui represent erit les d.i f ferent s types de verre
etud.ies par les archeo Loques et les historiens d 'art. Ces etal.ons
sont bien connus et contiennent 27 elements chimiques entre le sodium
et le plomb dans des quarit i.tes variables comprises entre 100ppm et
35%. On a obtenus des resu l.tats avec une erreur relative comprise
entre 10 et 20% ,avec parfois des desaccords d'un facteur trois.
Les premiers echant i l.Lons que nous avons ree.l Lemerit analyses
sont des pigments de pe inture broyes provenant de la boutique ""A la
momie"", une maison de commerce en exercice pendant les 18 et 1gemes
siecles. Nous avons alors prouve que PIXE pouvait etre utilise pour
determiner les elements majeurs, mineurs et traces rent rant dans la
composition de ces pigments. De subtiles differences ont ete decelees
entre des pigments de meme denomination, et au contraire des pigments
avec des noms differents ont la meme composition chimique.
Le systeme de faisceau extrait a ete teste sur plusieurs
objets, y compris sur un tableau-faux primitif italien- , un cadre
dore du 15eme siecle et une petite statue en or antique. Pour
l'analyse de la plupart des objets de musee nous serons amene s a
utiliser ce systeme a l'air. 11 est donc crucial que ce systeme soit
parfaitement regle.
On a egalement commence a etudier l'atmosphere a l'interieur du
Louvre afin de determiner si des poussieres presentes dans les
galeries s~nt nocives pour les oeuvres d'art qui y sont conservees.
L'analyse des filtres aerosols pourra etre effectuee par AGLAE avec
seulement des modifications mineures sur le porte-cible.
Les resultats presentes ici ne s~nt pas entierement
satisfaisants. 11 est tout a fait rassurant que tous les equipements
et composants fonctionnent bien pour acquerir les spectres. Mais je
pense qu'il faut encore reduire les erreurs experimentales en
ameliorant le traitement des spectres(materiel et logiciel) pour que
le Louvre soit dote d'un systeme PIXE sur et precis.
EXECUTIVE SUMMARY
The Louvre Museum is the first museum in the world to have its own
particle accelerator for the study and analysis of works of art and
archaeology. The Laboratoire de Recherche des Musees de France has
thus taken a bold leap into the future with AGLAE, a powerful ""high-tech""
approach to the conservation of our cultural heritage.
This accelerator will enable the museum scientists to use a variety
of ion-beam techniques on works of art and archaeology, including
PIXE, PIGE, NRA, RBS and C-14 dating.
o
This progress report describes the work I accomplished between 15
September 1988 and 28 February 1989. During this time we have begun
to set up a PIXE system for the analysis of works of art in the
museum's collection. PIXE is multi-elemental, sensitive, inexpensive,
rapid, and most importantly, non-destructive. We have so far
acquired preliminary data on samples of glass, geological standards,
painting pigments, and gold artifacts. In a sense, these experiments
have been more of a learning experience for me and the AGLAE team. We
are becoming familiar with the accelerator, the target chamber, the
x-ray detector, the fast-pulse electronics, and the x-ray spectrum
reduction code. Although we have made a good start, much work still
needs to be done to make the AGLAE PIXE system ""state-of-the-art"".
We have had limited access to beam time for PIXE. Of course the
highest priority has been to ensure that the accelerator meets the
manufacturers specifications. The accelerator has had problems with
the ion source; as a result the beam current has not been very
stable. The target chamber we are using is a temporary one. A new one
has been built in Strasbourg and will be installed in March. There is
presently no reliable way of measuring the incident beam current for
thick samples, thus all the results presented here are relative. A
beam-chopper (a device for measuring the beam current) is being built
at Jussieu and will brought to the Louvre in April.
We have concentrated our efforts on four glass standards, which were
designed to duplicate the types of glass studied by archaeologists
and historians. These standards are well characterized and contain 27
elements between Na and Pb in quantities from 100 ppm to 35%. We have
been able to obtain results that are generally within 10 - 20% of the
given values, however, some occasionally there is a discrepancy of a
factor of three.
The firt ""real"" samples we have analyzed were pure pigments from the
boutique ""A la Momie"", a house of commerce in Paris during the 18th
and 19th centuries. We have seen how PIXE can be used to
quantitatively determine the major, minor, and trace elemental
composition of these paint pigments. Subtle differences were detected
between similar pigments, and pigments with different labels were
shown to have the same chemical composition.
The beam-in-air system has been tested on several samples, including
a false italian primitive, a gilded frame from the 15th century, and
a small gold statue from antiquity. For the analysis of most museum
objects we will be using this extracted beam system. It is thus vital
that we get it working well.
We are about start running air sampling equipment inside the Louvre
Museum in order to determine if any harmful pollutants are present in
the galleries that hold precious works of art. Analysis of the
aerosol filters can be done at AGLAE with only minor modifications of
the target holder.
I must say that I do not feel very confident about the numbers
presented here. I am glad we have all the equipment working to the
point that we can get results, but I have doubts that all the
hardware and software is working properly. We must now work to find
and reduce the experimental uncertainties, in order for the Louvre to
have a reliable and accurate PIXE system.
Progress Report, Fulbright Grant, 15 September 1988 - 28 February 1989
Bruce H. Kusko
AGLAE - LRMF
I. INTRODUCTION
During the last four years the Louvre Museum has been undergoing
a major renovation. Not only is the main entrance going to be a giant
glass pyramid, but three stories under the pyramid a team of
scientists will be bombarding precious works of art with a particle
accelerator. Analyse au Grand Louvre par Accelerateur Electrostatique
(AGLAE) is a part of the Laboratoire de Recherche des Musees de France
(LRMF), which has been given more space and equipment in a new
underground laboratory. It is only recently that methods of analysis
using high energy ion-beams (protons, alphas, 15N, etc) have been
applied to works of art and archaeology, and usually by physicists
working in nuclear laboratories in their spare time. The Louvre Museum
is thus the first museum to have an accelerator to be used exclusively
for the study and analysis of works of art and archaeology. The LRMF
has taken a bold leap into the future with this powerful ""high-tech""
approach to the conservation of our cultural heritage.
II. PIXE at AGLAE
My expertise is with particle induced x-ray emission (PIXE) and
I have been concerned with developing a PIXE system at AGLAE for the
study and analysis of works of art. We have had several opportunities
to do some PIXE analyses under vacuum, including pigment samples,
glass standards, geological standards, and gold samples. Although the
spectra were acquired under conditions that were not ideal, we were
able to perform tests of the target chamber, x-ray detector and
electronics, and the PIXE spectrum reduction code. All results
presented here are therefore considered preliminary and subject to
change.
a. Accelerator
The accelerator, a 2.0 Mev tandem pelletron, was purchased from
National Electrostatic Corporation in Middleton Wisconsin. It is
capable of accelerating protons from 0.3 to 4.0 Mev, alpha particles
from 0.3 to 6.0 Mev, and 15N ions to 8.0 Mev. In addition it can
accelerate deuterons and 3He ions. Guaranteed beam current for protons
is 5 pa through a 1 mm2 collimator. It was installed in the winter and
spring of 1988, and the first beam was realized in June 1988. Tests
and practice with the machine have been taking place ever since then,
and are expected to continue through the end of March.
B. Target area
The target chamber we are using is a temporary one. The target
ladder can hold up to six samples and is moved manually. The sample
surface is perpendicular to the axis of the incident beam. X-rays are
detected at a backward angle of 135 degrees. The detector is
collimated, and subtends a solid angle of 0.5 sr. A new target chamber
has been fabricated in Strasbourg and will be delivered to the Louvre
on 3 March. This new chamber is very versatile and will be used for
PIXE, PIGE, RBS, and NRA.
A beam-chopper for measuring the incident beam current has been
built at Jussieu. It is presently undergoing tests and will be brought
to the Louvre at the end of March. Presently the beam current is
determined by measuring the charge acquired by the target, target
holder, and target chamber, which are electrically isolated from the
rest of the accelerator. Since this method is not reliable, I have
designed and will be building a device to determine the beam current
by measuring the protons backscattered from a thin mylar foil placed
in front of the target. This thin foil monitoring technique has the
advantages of eliminating charge buildup on insulating samples
analyzed under vacuum, and it can be used to monitor the beam current
when analyzing samples in air with an extracted beam.
C. Electronics
The experimental setup for the x-ray electronics during PIXE
analyse is shown in figure 1. The detector is an EG&G Ortec 7900
2
Si(Li), with 30 mm2 area, 8.0 pm Be window, and a FWHM resolution of
147 ev at 5.895 kev. The high voltage bias is supplied by an Ortec 459
power supply. An Ortec 972 spectroscopy amplifier and an Ortec 444
biased amplifier are used together for pulse-processing and dead-time
corrections. A Seiko EGG 7800 multi-channel analyzer with a Seiko 1820
ADC interface is used to collect the spectra. An Enertec 7143 linear
ratemeter is used to monitor the x-ray count rate. The charge induced
by the beam current is measured by a Brookhaven Instruments
Corporation 1000a current integrator, and stored in an Ortec 996
counter and timer.
In order to obtain a resolution of 147 ev on a real sample it is
necessary to keep the amplifier close to the detector (less than 3 m)
and to acquire the data with the vacuum pumps off. Mechanical
vibration from the pumps adds almost 15 ev to the resolution. (We are
working to better isolate the pumps from the target chamber.)
D. PIXAN reduction code
Since my experience with computers has been with DEC and Apple
computers, I have had to become familiar with SUN (UNIX) and IBM (DOS)
computers. PIXE spectra are analyzed on a SUN MS 3/260 workstation.
The computer program PIXAN is used to reduce an x-ray spectrum
to its elemental composition, and it does this in two parts. The first
part of the program calculates the areas of the characteristic peaks.
The background is subtracted and then the peaks are fitted to a
modified gaussian form. The area of the characteristic peaks are
directly proportional to the number of x-rays coming from the
characteristic elements. The second part determines the theoretical x-ray
yield given the composition of the sample and the energy of the
proton beam. It is then necessary to combine the results of the two
parts to get the elemental composition in parts per million (ppm). The
program is designed to be used for both thin and thick targets.
I have had to spend most of my time geting PIXAN to run
correctly. It is an excellent program, but it evolved over a long
period of time at the Australian Atomic Energy Commission. Thus it is
3
suited to the types of samples they analyze in Australia and the
experimental conditions in their laboratory. The results are
critically dependent on the x-ray detector parameters (crystal
diameter, sensitive depth, silicon dead-layer, and the gold electrode
layer), any external detector filters, the x-ray electronics, and even
the target composition (which determines the self-absorption/self-enhancement
effects). The most important step, and perhaps the most
difficult one, is a proper subtraction of the background continuum.
With PIXAN the background can be modelled either by a polynomial (up
to 5th order), or by an iterative method that removes the
characteristic peaks and progressively reduces the spectrum to the
background continuum. We have found that small changes in the
parameters used to model the background leads to large changes in the
final results. We have found that the iterative method works better
than the polynomial method for our samples, and have achieved results
on glass standards that are generally within 20% of the given values
for the elements Na to Pb (see section 4).
The exact experimental uncertainties are hard to determine. The
precision when measuring an element well above the limit of detection
and isolated from any interfering peaks is better than 5%. Elements
found near the limit of detection have greater uncertainties. Elements
with low energy x-rays (less than 2 kev) and high energy x-rays
(greater than 30 kev) have larger uncertainties since the detector
efficiency is quite low and poorly known in these regions. The case of
interfering peaks must be considered individually. For example, the
uncertainty in the measurement of sulfur (Ka = 2.307 kev) depends on
the amount of lead (Ma = 2.346 kev) in the sample. The overall
accuracy of a PIXE measurement depends on the uncertainties in the
following factors: the number of x-rays in a characteristic peak; the
number of incident protons; the detector efficiency; and the
absorption/enhancement corrections. In general, uncertainties of
around 10% - 20% can be expected with PIXE.
Most of the results to date have been determined without seeing
graphically the background or least-squares fit to the spectrum, a
most unfortunate circumstance. Thomas Calligaro has recently written a
program that displays the x-ray spectrum, the background, and the fit
4
of the data, an indispensible step in proper spectrum analysis. Good
spectrum analysis, even with the best computer programs, still
requires experience. One must know what to look for in the background
subtraction, the gaussian fit, the calculation of the sum and escape
peaks, the relative peak intensities, the unfolding of overlapping
peaks, and so forth. One must be careful and not simply accept the
results given by the computer program.
The data files included with PIXAN lacked some vital information
to make it work correctly for the samples we have at the Louvre, and
needed to be supplemented. First of all they did not include any M-line
x-ray data, which are necessary for quantitative results on the
elements Na through Cl (1-3 kev). We have therefore added M-line data
for Pb, Hg, Pt, and Au into the PIXAN data files. (The elements whose
M-line data is important for us to know immediately.) Secondly, the
data file of relative peak heights is appropriate only for 2.5 Mev
protons. The relative peak heights are important to know since PIXAN
uses them to untangle the overlap of peaks. Although the Ka/KB ratio
is independent of energy, the LalLi ratio, (where i is anyone of the
many other L-line x-rays), and the Ma/Mi ratios vary unsystematically
as a function of incident proton energy. Therefore it will be
necessary for us to modify the data tables for energies lower and
higher than 2.5 Mev in order to obtain the most accurate results. (We
will use the tabulated theoretical x-ray cross sections of D.D. Cohen
and M. Harrington, Atomic Data and Nuclear Data Tables 33, 1985, 255-
343.) Thirdly, L- and M-shell sum (pile-up) peaks are not included in
the peak search and are therefore not removed from the x-ray spectra.
The justification for not including the L- and M-shell sum peaks was
to make it easier for PIXAN to determine accurate amounts of Ti
(Ka=4.508 kev) and Ba (La=4.467 kev) when both are present in the
sample. However, L- and M-line pileup peaks are a serious problem when
a sample contains major or minor amounts of platinum, gold, mercury,
lead, tin, or barium.
III. RESULTS
We have acquired 52 PIXE spectra to date: 12 on glass standards,
36 on paint pigments, 3 on ceramic standards, and 10 on gold samples.
5
A. Glass
We have concentrated our efforts on four colored glass samples.
These samples have been well characterized, and they contain 27
elements between Na and Pb in quantities of 100 ppm to 35%. The
elements between Na and K (X-rays between 1 and 3 Kev) are difficult
to determine yet important in archaeometric studies of glass,
pigments, ceramics, and geological samples. In addition these glass
samples are ideal PIXE targets, being completely homogeneous, flat,
and small enough to fit into a vacuum chamber. We felt it was
necessary to get good results on these standards before we started
analyzing unknown targets.
The glass samples were prepared in 1964 by R. H. Brill and A. A.
Erickson of the Corning Glass Works in Corning New York. The four
samples were designed to duplicate the types of glass studied by
archaeologists and art historians. Two samples (known as Brill A and
Brill B) are soda-lime-silica glasses, which are similar to ancient
Egyptian, Mesopotamian, Ionan, Byzantine, and Islamic glasses. One
sample (Brill C) is a glass with high-lead and high-barium levels,
which is similar to glasses from Eastern Asia. The fourth sample
(Brill D) is a potash-lime-silica glass which is similar to certain
medieval glasses and some glasses of the 17th to 19th centuries. Minor
and trace elements were also introduced at levels that are comparable
to those actually found in ancient glasses. The method of fabrication
of these samples is given in R. H. Brill, A Chemical-Analytical Round-robin
on Four Synthetic Glasses, Proceedings of the IX International
Congress on Glass, Versailles 27 September, 1971.
The glass samples are several millimeters long and are embedded
in amber. They were analyzed at 1.0 and 2.5 Mev, with the intention of
obtaining the light elements (Na to Cal at 1.0 Mev, when there is no
detector filter, and the heavy elements (K to Pb) at 2.5 Mev, while
using a detector filter which allows the use of higher beam currents
and better (lower) sensitivity. The spectrum for Brill C at 2.5 Mev is
shown in figure 2 and the best results to date for the glass samples
are given in table 1. The element K was chosen to be an ""internal
6
standard"", so all the results are relative and normalized to the K
concentration. A comparison between the experimental resu",multimedia,952,not included
,to_check,core,Implementation of Advanced Fuels and Combustion for Internal Combustion Engines,2009-04-01 00:00:00,core,https://core.ac.uk/download/62754107.pdf,,"Track II: Transportation and BiofuelsIncludes audio file (19 min.)Advanced engine designs for transportation has shown significant reduction in engine-out emissions while simultaneously achieving gains in fuel efficiency by using Low Temperature Combustion (LTC) modes and feedback control of the combustion processes. The work of this group has considered the difficulties encountered in using these combustion modes through implementation of advanced control methodologies, novel sensor techniques as well as expanding usage of fuels such as bio-fuels and hydrogen. The methods used to obtain the lower combustion temperatures include lean mixtures and high levels of exhaust gas recirculation. For example, LTC modes such as Homogeneous Charge Compression Ignition (HCCI) and Partially Premixed Compression Ignition (PCCI) engines show real gains in reduced engine out emissions with improved efficiency. However, implementation of these advanced combustion modes presents combustion timing and stability issues due to stronger dependence of these advanced combustion modes on the physical and chemical properties of the fuel, inlet temperature, and inlet composition than traditional diffusion burning (“diesel” type) modes. Progress in these advanced combustion modes requires a “smart” engine capable of sensing heat release patterns and adjusting combustion system parameters. Hence collaborative work between several researchers at Missouri S&T are considering the required combustion analysis, nonlinear control, sensor development and fuel property issues surrounding the implementation of several LTC modes. Analysis methods currently considered are based on surface accelerations for use on both conventional and premixed auto-ignited combustion types that can robustly indicate combustion characteristics. Surface mount accelerometers are being used to indicate combustion characteristics needed for closed loop engine control but which have minimal structural influence. Acceleration frequency bands are being identified where the structural characteristics has the most influence (i.e. structure resonant modes), thereby allowing indication of other surface acceleration frequency bands which are minimally affected by the structure and more indicative of the combustion behavior. Active control necessitates an advanced control strategy such as adaptive neural networks which we have shown can function satisfactorily even when the dynamics of the engine combustion process are unknown. A near optimal nonlinear adaptive controller using Approximate Dynamic Programming (ADP), based on a phenomenological LTC engine model is being developed. The conceived controller would reduce cyclic variability in start-of-combustion, limit pressure rise rates and control to maximize efficiency through control of heat release pattern phasing. With advanced control algorithms, low-cost sensor technologies need to be developed before robust control of auto-ignited combustion can be achieved on a production scale. Interferometer based sensors packaged in small fiber optics are being developed for the high temperature and pressure combustion chamber environment with response times on the order of microseconds. Finally, advancing the application of advanced LTC modes to enable the use of bio-fuels or hydrogen has become increasingly important for energy security. Consequently, the distinct characteristics of hydrogen combustion in engines are being investigated using advanced simulation techniques to examine more efficient and cleaner operating strategies (e.g., dual-fuel operation)",multimedia,953,not included
,to_check,core,A multi-FPGA distributed embedded system for the emulation of Multi-Layer CNNs in real time video applications,2010-03-29 00:00:00,core,https://core.ac.uk/download/60418869.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',"This paper describes the design and the implementation of an embedded system based on multiple FPGAs that can be used to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more CNN layers. The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates.This work has been partially supported by the Fundación Séneca de la Región de Murcia through the research projects 08801/PI/08 and 08788/PI/08, and by the Spanish Government through project TIN2008-06893-C03",multimedia,954,included
,to_check,core,,2003-08-22 00:00:00,core,"corvallis, or : oregon state university, dept. of computer science",Calibrating recurrent sliding window classifiers for sequential supervised learning,Sequential supervised learning problems involve assigning a class label to each item in a sequence. Examples include part of speech tagging and text to speech mapping. A very general-purpose strategy for solving such problems is to construct a recurrent sliding window (RSW) classifier which maps some window of the input sequence plus some number of previously predicted items into a prediction for the next item in the sequence. This paper describes a general-purpose implementation of RSW classifiers and discusses the highly practical issue of how to choose the size of the input window and the number of previous predictions to incorporate. Experiments on two real world domains show that the optimal choices vary from one learning algorithm to another. They also depend on the evaluation criterion number of correctly predicted items versus number of correctly predicted whole sequences.  We conclude that window sizes must be chosen by cross validation. The results have implications for the choice of window sizes for other models,multimedia,957,not included
,to_check,core,,2005-01-01 00:00:00,core,place:new york,A fuzzy expert approach for comparing alternative end uses for requalification of contaminated sites,"An important aspect in the issue of contaminated sites is their use after remediation. The choice of a certain use has got necessarily socio-economic implications that make it more or less suitable for that site. For this reason it is important to insert socio-economic analysis within the studies on contaminated site remediation and to implement approaches that calibrate extent and modality of the remediation on the basis of potential uses of the site after it. In particular it is important to provide decision-makers with tools that offer them the possibility to consider also this aspect, in order to support their decisions.The specific aim of the socio-economic module is providing the decision makers with a tool that makes possible to compare the different use destinations, outlining possible scenarios linked to alternative uses of the considered site, on the basis of socio-economic considerations (often founded on theories and methods of the spatial analysis) and of local characteristics. Comparing these scenarios it aims to give indications on which use is more suitable and why. The final objective is just to establish which is the \u201cbest\u201d use for that site. The term \u201cbest\u201d indicates that it is possible to rank the whole different use destinations. This is a typical multicriteria decision making problem and there is not a natural order in a multidimensional space, so it is necessary to find a device to do it. A method to rank them is to define a function from the attribute space in the real line and so obtain a total order induced by the real number one. The scientific literature is reach of several ways to approach this problem. Here we propose a method typical of Artificial Intelligence: a Fuzzy Expert Systems (FES). The final product is a software prototype for the remediation of contaminated sites: DESYRE (DEcision Support sYstem for the REqualification of contaminated sites, www.r3environmental.co.uk/dstdemo/), that represents a useful tool for decision-makers. Entering data, relative to the considered site, the software is able to elaborate them to give back one numerical index for each possible use destination (UD). These indexes represent synthetic and comparable expressions of the socio-economic implications deriving from each UD. In this way the decision-makers can compare the opportunities coming from the different uses and have a synthetic indicator, without losing the whole information. The software works in a very transparent way, so that it is possible to highlight which factors determine the high or low index value",multimedia,958,not included
,to_check,core,,,core,biologically-inspired machine intelligence technique for activity classification in smart home environments,https://core.ac.uk/download/200783174.pdf,"With the widespread adoption of Internet-connected devices and the prevalence of applications in the Internet of things (IoT), devices in smart homes can generate enormous amounts of data. There is a requirement for machine-learning techniques to learn from historical patterns and predict future activities. There is an increased interest in machine-learning techniques that can provide useful and interesting services in the smart home domain. The areas that machine-learning techniques can help advance are varied and ever-evolving. Predicting and classifying the Activities of Daily Living (ADLs) of inhabitants in a smart home environment are key modules to automate smart home devices. Some prominent examples include uses for entertainment, elderly care, healthcare, and security. The abilities of the machine-learning technique to find meaningful spatio-temporal relations of high-dimensional data and to learn from streaming datasets are important requirements.



Recently, the Hierarchical Temporal Memory (HTM) theory has been presented as a biologically-inspired machine-learning theory that attempts to mimic the neocortex, the front part of the human brain. The main features of the HTM theory include the ability to learn and predict temporal patterns. The HTM theory and its computational implementation, Cortical Learning Algorithms (CLA), present a potential alternative to traditional machine intelligence. This research aims to apply a new biologically inspired machine intelligence technique based on the HTM theory and its CLA implementation to classify ADLs of inhabitants by analysing data captured from different sensors in a smart home scenario.



This research started by reviewing existing research in classification and prediction of ADLs. A comprehensive evaluation of state-of-the-art machine learning techniques and their application in the context of smart homes has been carried out as a primary research. HTM theory and its implementation the CLA have been studied, and experiments were conducted to identify the weaknesses and limitations of applying the HTM theory and the CLA for activity classification in the smart home environment.



To test and evaluate the performance of a proposed machine intelligence technique, there is a need for a dataset that represents the ADLs in a smart home scenario. Due to the excessive cost of building real smart home datasets and the lack of real datasets from smart homes, to tackle this issue, as a secondary contribution to knowledge, this research used OpenSHS (Open Smart Home Simulator), an open-source, cross-platform 3D smart home simulator. In addition, forty-two ADL datasets, Simulated Activities of Daily Living Dataset (SIMADL) were extracted from the OpenSHS tool and made available publicly.



This research proposes multi-region CLA techniques to learn short- and long-term patterns. Two novel multi-region CLA techniques featuring a multiple spatial pooler and temporal memory regions that incorporate a hash encoder and a Multi-Layer Perceptron (MLP) classifier were proposed and applied. While, the hash encoder can deal with multi-dimensional datasets, because of the existing encoders of the standard NuPIC encoders are prepared to deal with a single column or a small number of columns and the MLP classifier was used rather than the classifiers used in the current implementation of CLA to produce meaningful predictions. Additionally, the two novel multi-region CLAs, Parallel Spatio-Temporal Memory Stream (CLA2) and Cascaded Temporal Memories Stream (CLA3) were developed to learn short- and long-term patterns from, streaming datasets. To remove the limitation of memory management of the original CLA that contains one spatial pooler and one temporal memory, the original CLA learns using one memory level, such model learns either short-term or long-term patterns, not both of them. A novel CLA2 was proposed and developed, to cope with learning both shortterm and long-term patterns. CLA2 can learn both short-term and long-term patterns in parallel. CLA2 includes two spatial poolers and two temporal memories to simulate short-term and long-term memories. The number of cells per column was decreased in the first region to learn short-term patterns. In the second region, the number of cells per column was increased to learn long-term patterns and the outputs of both regions were concatenated into one vector. The second proposed algorithm (CLA3) has three regions, one spatial pooler, and three cascaded temporal Memories (TM) regions, where the first region learns smaller features, and the second and third regions are more abstract in order to learn and recognise patterns and concatenates the outputs from all three regions into one vector.



An evaluation and comparison of the proposed algorithms against state-of-the-art supervised machine-learning techniques and the standard CLA for classification was conducted using both the simulated smart home SIMADL dataset, generated using the OpenSHS, and the ARAS dataset, that comprises data captured from real-world activities of residents residing in two houses, the obtained results for the real datasets offered less performance than the synthetic datasets. Because the inhabitants are asked to record their activities manually, it was prone to human errors. The real-world dataset ARAS House A is inconsistent because one of the inhabitants left the house for long period of time, which impacts the performance results. The results of the proposed algorithms for the classification of ADLs show that its performance are promising. For the CLA2, the average overall F-measure for all the synthetic datasets, SIMADL and the real-world datasets, ARAS is 84.88%, while the highest F-measure (86.87%) was achieved by the Convolutional Neural Network (CNN) model. The (CLA2) has achieved an F-measure of 92.63% for House B of the real-world ARAS dataset, which outperforms state-of-the-art classification models. For the CLA3, the average overall F-measure for all the synthetic and real datasets is 84.50% . The proposed algorithms improve the best performance of base-line (standard) CLAs by an average F-measure of 51.81% overall",multimedia,959,not included
,to_check,core,,2020-01-01 00:00:00,core,γεωπονικό πανεπιστήμιο αθηνών,Big data analytics in agricultural digital markets,"Nowadays, a lot of companies aim to tap into social media networking in order to maximize their profit by endorsing their products or services and for improving their brands’ names. The development of Web 2.0 has permitted Internet users to post, share and exchange their own self-generated opinions or thoughts on various topics on different websites. A large amount of data containing useful information concerning the consumers’ preferences is generated from a variety of sources such as reviews, posts, microblogs or online digital markets. In the case of Food and Beverage (F&B) sector, more and more review websites are established globally (Yelp, TripAdvisor, etc.) and most of them allow users to digitally make their own orders for delivery, or take away goods (just eat, etc.), as well as to digitally evaluate about the products or services that they have consumed. The produced information (evaluations) which is generated rapidly can be large and generally modifies consumers’ behavior. It is also notable that the e-evaluations could be characterized as mixed-documents, because users evaluate different companies’ functions at the same time. However, in most of the cases, the involved stakeholders (companies or customers) cannot mine and analyze in real time the useful information that is created due to the lack of resources and  humans’ physical or mental restrictions.In bibliography, there are various methodologies to face this problem, with the sentiment analysis being the preferred one. It is presented as the automatic detection of positive or negative expressions in user generated content. Also, in research there are two approaches: the machine learning approach and the semantic orientation approach and three levels of analysis: the document-level, the sentence-level and the aspect-level. A lot of frameworks have already proposed that use machine learning techniques, while the semantic orientation approach has not received the same interest yet. However, some previous researches concluded that the semantic orientation approach may be more efficient when implemented for classifying users’ opinions and for determining the polarity and the strength of opinions at the same time. Finally, while a lot has been written and researched about sentiment analysis in various domains and languages too, the F&B sector in the Greek language has drawn limited researchers’ attention. This study faces this problem by making a suggestion of a new hybrid framework of sentiment analysis using the semantic orientation approach. A thorough analysis of 91,504 customers’ reviews (9,150 reviews in the training set) collected from an e-ordering platform of some F&B companies distributed in almost all prefectures of Greece is occurred. After, the implementation of some well-known models and metrics in the training set (BOW, tf-idf and POS) resulted in the creation of an opinion dictionary concerning the F&B domain. The dictionary consists of three sub-dictionaries that correspond to three companies’ functions (quality of food, customer service and image of the company). Further, after the implementation of the PMI metric in the training set, the relationships between the parts of the speech and their positions in mixed customers’ e-valuations are identified and used for constructing the proposed patterns of tags. The patterns of tags are then used for detecting and quantifying the customers’ opinions per function in a review. The main innovation of this framework is based on the form of the analysis, first at sentence and after at aspect-level. Also, a new hybrid technique for constructing opinion dictionaries is proposed, using all the known techniques that are presented in bibliography (manually-technique, corpus-based technique and dictionary-based technique). Finally, since the extraction of useful information from the user generated content must usually be held in real-time in order to facilitate the stakeholders to make smart decisions on-time, a web-based system that able to mine and analyze customers’ evaluations in almost real-time, is introduced. For this, two modules will be designed and implemented. The first is a web scraping module for mining the reviews from all e-ordering platforms and the second is a module for implementing the proposed hybrid framework.Based on the confusion matrix of the designed sets, the well-known performance metrics of accuracy, precision, recall, and F-score were computed. The results showed a remarkable high performance on the sentiment classification and the aspect-level analysis respectively. Specifically, the proposed hybrid framework showed an average accuracy of 98.45% in the training set concerning the sentiment classification. Moreover, it showed an average accuracy of 93.61% in the annotated set concerning the aspect-analysis. Then, for the purposes of assessing the framework in a big set of customers’ reviews, it tested in the data set and it showed an extremely high average accuracy of 90.69%. However, in all designed sets were calculated lower values in negative predictions, mainly due to the identification difficulties of negation and sarcasm, in natural language texts. In more detail, the framework showed an average recall of 85.99% in negative predictions against of 99.43% in positive predictions in the training set, an average recall of 81.49% in negative predictions against of 96.43% in positive predictions in the annotated set and an average recall of 70.81% in negative predictions against of 95.79% in positive predictions in the data set. It is noteworthy that the performance of negative prediction remains at a high level, comparable to other similar researches. The average high values of the metrics lead us to conclude that the vast majority of aspects that described the examined functions in the F&B sector with the semantic orientation terms (adjectives) that con-currently occur, were detected. To conclude, a new technique for managing the instances that cannot be detected and evaluated by the system is proposed.Τη σημερινή εποχή, πολλές επιχειρήσεις χρησιμοποιούν τα μέσα κοινωνικής δικτύωσης με σκοπό να βελτιώσουν την επωνυμία τους και να μεγιστοποιήσουν τα κέρδη τους, μέσω της προβολής των αγαθών τους (προϊόντα ή υπηρεσίες). Η ανάπτυξη του Παγκόσμιου Ιστού 2.0 έχει επιτρέψει στους χρήστες να μεταφορτώνουν, να διαμοιράζονται και να ανταλλάζουν τις απόψεις ή τις ιδέες τους για διάφορα θέματα σε μια πληθώρα ιστοτόπων. Παράγεται, όπως είναι κατανοητό, ένας μεγάλος όγκος δεδομένων που περιέχει χρήσιμες πληροφορίες σχετικά με τις προτιμήσεις των καταναλωτών και από ποίκιλες πηγές, όπως μικρο-ιστολόγια, αξιολογήσεις σε ψηφιακές αγορές ή ιστοτόπους συζητήσεων κ.ά. Στην περίπτωση του αγροδιατροφικού κλάδου και κατ’ επέκταση της Εστίασης και Ποτών (Ε&Π), όλο και περισσότεροι ιστότοποι αξιολογήσεων λειτουργούν παγκοσμίως (Yelp, TripAdvisor κ.ά.) και πολλοί από αυτούς επιτρέπουν στους χρήστες να πραγματοποιούν ηλεκτρονικά τις παραγγελίες τους για τη διανομή στο σπίτι έτοιμου φαγητού (Volton, just-eat, e-food κ.ά.). Επίσης, οι χρήστες έχουν τη δυνατότητα να αξιολογούν δημόσια τα αγαθά που έχουν καταναλώσει. Όπως γίνεται αντιληπτό, παράγεται ταχύτατα ένας μεγάλος όγκος χρήσιμων πληροφορίων από τις ηλεκτρονικές αξιολογήσεις χρηστών στο Διαδίκτυο που μπορεί να επηρεάσει άλλους και να μεταβάλει την καταναλωτική τους συμπεριφορά. Αξίζει να σημειωθεί ότι οι ηλεκτρονικές αξιολογήσεις του κλάδου χαρακτηρίζονται ως μικτές, επειδή αξιολογούνται ταυτόχρονα διαφορετικές λειτουργίες ενός καταστήματος. Έτσι, τα ενδιαφερόμενα μέρη (επιχειρήσεις ή καταναλωτές) δεν είναι σε θέση να εξάγουν και να αναλύουν σε πραγματικό χρόνο τις χρήσιμες πληροφορίες που παράγονται, εξαιτίας της έλλειψης πόρων και των βιολογικών και διανοητικών περιορισμών του ανθρώπινου παράγοντα. Στη βιβλιογραφία παρουσιάζονται διάφορες μεθοδολογίες που αντιμετωπίζουν το συγκεκριμένο πρόβλημα με την ανάλυση συναισθήματος να είναι η προτιμότερη. Απαντάται ως η αυτόματη ανακάλυψη θετικών ή αρνητικών εκφράσεων σε περιεχόμενο που δημιουργείται από τους χρήστες των κοινωνικών δικτύων. Ερευνητικά παρουσιάζονται δύο προσεγγίσεις: η προσέγγιση της μηχανικής μάθησης και η προσέγγιση του σημασιολογικού προσανατολισμού. Επίσης παρουσιάζονται τρία επίπεδα ανάλυσης: το επίπεδο κειμένου, το επίπεδο πρότασης και το επίπεδο απόψεων. Αρκετά πλαίσια έχουν προταθεί που χρησιμοποιούν τεχνικές μηχανικής μάθησης, ενώ δεν παρουσιάζεται το ίδιο ενδιαφέρον για την προσέγγιση του σημασιολογικού προσανατολισμού. Ωστόσο, προηγούμενες έρευνες κατέληξαν στο συμπέρασμα ότι η προσέγγιση του σημασιολογικού προσανατολισμού θεωρείται αποδοτικότερη, όταν εφαρμόζεται για να ταξινομήσει τις απόψεις των χρηστών και ταυτόχρονα να υπολογίσει την ισχύ των απόψεων αυτών μέσω της ποσοτικοποίησης τους. Τέλος, ενώ αρκετές έρευνες έχουν διεξαχθεί σχετικά με την ανάλυση συναισθήματος σε διάφορες γλώσσες και κλάδους, η ελληνική και ειδικά ο κλάδος Ε&Π έχουν λάβει ελάχιστα την προσοχή του επιστημονικού κοινού. Αυτό οφείλεται κυρίως στο γεγονός ότι η ελληνική θεωρείται μια γλώσσα πολλών διακυμάνσεων με περίπλοκους γραμματικούς και συντακτικούς κανόνες. Η παρούσα διατριβή αντιμετωπίζει το πρόβλημα που περιγράφηκε, προτείνοντας ένα νέο πλαίσιο ανάλυσης συναισθήματος, εφαρμόζοντας την προσέγγιση του σημασιολογικού προσανατολισμού. Μέσω της εμπεριστατωμένης ανάλυσης 91.504 πραγματικών αξιολογήσεων πελατών (9.150 αξιολογήσεις στο σύνολο εκπαίδευσης) που εξορύχθηκαν από επιχειρήσεις που λειτουργούν σε όλη την Ελληνική επικράτεια, καθώς και με τη χρήση των κατάλληλων μέτρων και προτύπων (BOW, tf-idf, POS), κατασκευάζεται ένα λεξικό απόψεων που αναφέρεται στον κλάδο Ε&Π. Το προτεινόμενο λεξικό περιλαμβάνει τρία υπο-λεξικά που αντιστοιχούν στις λειτουργίες των καταστημάτων του κλάδου (ποιότητα φαγητού, εξυπηρέτηση & εικόνα καταστήματος). Στη συνέχεια, μέσω της εφαρμογής του προτύπου αμοιβαίας πληροφόρησης (PMI), εντοπίζονται οι σχέσεις των μερών του λόγου, καθώς και οι θέσεις τους στις μικτές αξιολογήσεις των πελατών και προτείνονται τα αντίστοιχα μοτίβα ετικετών, τα οποία χρησιμοποιούνται στη συνέχεια, με σκοπό να εντοπιστούν και να ποσοτικοποιηθούν οι απόψεις των χρηστών. Η καινοτομία του πλαισίου βασίζεται στη μορφή της ανάλυσης, πρώτα σε επίπεδο πρότασης και έπειτα σε επίπεδο απόψεων, με σκοπό να εξαχθούν και να ποσοτικοποιηθούν αυτόματα οι απόψεις για τις διαφορετικές λειτουργίες ενός καταστήματος. Επιπλέον, προτείνεται μια νέα υβριδική τεχνική κατασκευής λεξικών απόψεων, κάνοντας χρήση και των τριών τεχνικών που προτείνονται στη βιβλιογραφία (μηχανική τεχνική, τεχνική βασισμένη σε σώμα, τεχνική βασισμένη σε λεξικό). Τέλος, δεδομένου ότι η εξαγωγή των χρήσιμων πληροφοριών που παράγεται από το περιεχόμενο που δημιουργείται από τους χρήστες, πρέπει να λαμβάνει χώρα σε πραγματικό χρόνο, ώστε να διευκολύνει τα ενδιαφερόμενα μέρη να λάβουν ορθές αποφάσεις, κατασκευάστηκε ένα διαδικτυακό σύστημα εξόρυξης και ανάλυσης αξιολογήσεων πελατών σε σχεδόν πραγματικό χρόνο. Το σύστημα περιλαμβάνει δύο αρθρώματα: το άρθρωμα ιστο-συγκομιδής, που χρησιμοποιείται για να εξάγει τις ηλεκτρονικές αξιολογήσεις των πελατών από όλες τις σχετικές πλατφόρμες του κλάδου και το άρθρωμα εφαρμογής του προτεινόμενου υβριδικού πλαισίου ανάλυσης συναισθήματος. Βασιζόμενοι στις μήτρες σύγχυσης των συνόλων δεδομένων που κατασκευάστηκαν, υπολογίστηκαν τα γνωστά μέτρα αποδοτικότητας της ακριβείας, της ευστοχίας, της ανάκλησης και του F1-στόχου, τα οποία παρουσίασαν μια υψηλή αποδοτικότητα του προτεινόμενου υβριδικού πλαισίου, τόσο στην ταξινόμηση συναισθήματος, όσο και στην ανάλυση σε επίπεδο απόψεων. Συγκεκριμένα, το μέσο μέτρο ακριβείας στο σύνολο εκπαίδευσης σχετικά με την ταξινόμηση συναισθήματος υπολογίστηκε σε 98,45%. Επίσης, το αντίστοιχο μέτρο ακριβείας στο σχολιασμένο σύνολο σχετικά με την ταξινόμηση σε επίπεδο απόψεων υπολογίστηκε σε 93,61%. Στη συνέχεια, με σκοπό να αξιολογηθεί η αποδοτικότητα του πλαισίου σε ένα μεγάλο σύνολο αξιολογήσεων, δοκιμάστηκε στο σύνολο δεδομένων, οπότε και διαπιστώθηκε ένα εξαιρετικά υψηλό μέσο μέτρο ακριβείας ίσο με 90,69%. Ωστόσο, σε όλα τα προαναφερόμενα σύνολα παρουσιάστηκαν χαμηλότερα μέτρα αποδοτικότητας για τις αρνητικές προβλέψεις, κυρίως λόγω των δυσκολιών αναγνώρισης της άρνησης και του σαρκασμού σε κείμενα της φυσικής γλώσσας. Αναλυτικότερα, το μέσο μέτρο ανάκλησης για τις αρνητικές προβλέψεις στο σύνολο εκπαίδευσης υπολογίστηκε σε 85,99% έναντι του 99,43% στις θετικές προβλέψεις, στο σχολιασμένο σύνολο σε 81,49% έναντι του 96,43% και στο σύνολο δεδομένων σε 70,81% έναντι του 95,79%. Είναι άξιο προσοχής ότι η αποδοτικότητα της αρνητικής πρόβλεψης παραμένει σε υψηλά επίπεδα, συγκρίσιμη με άλλες παρεμφερείς έρευνες. Οι υψηλές μέσες τιμές των μέτρων μας οδήγησαν στο συμπέρασμα ότι η μεγαλύτερη πλειοψηφία των καταχωρήσεων απόψεων (ουσιαστικά), που περιγράφουν τις εξεταζόμενες λειτουργίες ενός καταστήματος στον κλάδο Ε&Π με τους αντίστοιχους όρους σημασιολογικού προσανατολισμού (επίθετα) που συνυπάρχουν, εντοπίστηκαν ορθά. Τέλος, προτείνεται μια νέα τεχνική διαχείρισης των κειμένων με ασάφειες, που συνήθως δεν αξιολογούνται από τα συστήματα ανάλυσης συναισθήματος, αφού δεν συμπεριλαμβάνονται στις μήτρες σύγχυσης, με αποτέλεσμα να μην επηρεάζουν τα αντίστοιχα μέτρα αποδοτικότητας. Η προτεινόμενη τεχνική επέφερε μέση μείωση 8,27% στους δείκτες αποδοτικότητας της αρνητικής πρόβλεψης και μέση μείωση 2,34% στους δείκτες αποδοτικότητας της θετικής πρόβλεψης",science,960,not included
,to_check,core,Assessing the relationship between macro-faunal burrowing activity and mudflat geomorphology from UAV-based Structure-from-Motion photogrammetry,2020-05-01 00:00:00,core,10.1016/j.rse.2020.111717,,"International audienceCharacterisation of the ecosystem functioning of mudflats requires insight on the morphology and facies of these coastal features, but also on biological processes that influence mudflat geomorphology, such as crab bioturbation and the formation of benthic biofilms, as well as their heterogeneity at cm or less scales. Insight into this fine scale of ecosystem functioning is also important as far as minimizing errors in upscaling are concerned. The realisation of high-resolution ground surveys of these mudflats without perturbing their surface is a real challenge. Here, we address this challenge using UAV-supported photogrammetry based on the Structure-from-Motion (SfM) workflow. We produced a Digital Surface Model (DSM) and an orthophotograph at 1 cm and 0.5 cm pixel resolutions, respectively, of a mudflat in French Guiana, and mapped and classed into different size ranges intricate morphological features, including crab burrow apertures, tidal drainage creeks and depressions. We also determined subtle facies and elevation changes and slopes, and the footprint of different degrees of benthic biofilm development. The results generated at this scale of photogrammetric analysis also enabled us to relate macrofaunal crab burrowing activity to various parameters, including mudflat elevation, spatial distribution and sizes of creeks and depressions, benthic biofilm distribution, and flooding duration. SfM photogrammetry offers interesting new perspectives in fine-scale characterisation of the geomorphology, benthic activity and degree of biofilm development of dynamic muddy intertidal environments that are generally difficult of access. The main shortcomings highlighted in this study are a drift of accuracy of the DSM outside areas of ground control points and the deployment of which perturb the mudflat morphology and biology, the waterlogged or very wet surfaces which generate reconstruction artefacts through the sun glint effect, and the timeconsuming task of manual interpretation of extraction of features such as crab burrow apertures. On-going developments in UAV positioning integrating RTK/PPK GPS solutions for image-georeferencing and precise orientation with high-quality inertial measurement units will limit the difficulties inherent to ground control points, while conduction of surveys during homogeneous cloudy conditions could reduce the sun-glint effect. Manual extraction of image features could be automated in the future through the use of deep-learning algorithms",science,962,not included
,to_check,core,How Deep is Your Encoder: An Analysis of Features Descriptors for an Autoencoder-Based Audio-Visual Quality Metric,2020-07-06 00:00:00,core,10.1109/qomex48832.2020.9123142,,"International Conference on Quality of Multimedia Experience (QoMEX), Athlone, Ireland (held online due to coronavirus outbreak), 26-28 May 2020The development of audio-visual quality assessment models poses a number of challenges in order to obtain accurate predictions. One of these challenges is the modelling of the complex interaction that audio and visual stimuli have and how this interaction is interpreted by human users. The No-Reference Audio-Visual Quality Metric Based on a Deep Autoencoder (NAViDAd) deals with this problem from a machine learning perspective. The metric receives two sets of audio and video features descriptors and produces a low-dimensional set of features used to predict the audio-visual quality. A basic implementation of NAViDAd was able to produce accurate predictions tested with a range of different audio-visual databases. The current work performs an ablation study on the base architecture of the metric. Several modules are removed or re-trained using different configurations to have a better understanding of the metric functionality. The results presented in this study provided important feedback that allows us to understand the real capacity of the metric's architecture and eventually develop a much better audio-visual quality metric.Science Foundation IrelandInsight Research CentreConselho Nacional de Desenvolvimento Cientfico eTecnol ogico (CNPq)Coordenacao de Aperfeicoamento de Pessoal de Nıvel Superior (CAPES)Fundacao de Apoio aPesquisa do Distrito Federal (FAPDF)University of Brasılia (UnB)2020-12-15 JG: external PDF cover page remove",science,963,not included
,to_check,core,"Straggler-aware Distributed Learning: Communication Computation Latency
  Trade-off",2020-04-10 00:00:00,core,10.3390/e22050544,http://arxiv.org/abs/2004.04948,"When gradient descent (GD) is scaled to many parallel workers for large scale
machine learning problems, its per-iteration computation time is limited by the
straggling workers. Straggling workers can be tolerated by assigning redundant
computations and coding across data and computations, but in most existing
schemes, each non-straggling worker transmits one message per iteration to the
parameter server (PS) after completing all its computations. Imposing such a
limitation results in two main drawbacks; over-computation due to inaccurate
prediction of the straggling behaviour, and under-utilization due to treating
workers as straggler/non-straggler and discarding partial computations carried
out by stragglers. In this paper, to overcome these drawbacks, we consider
multi-message communication (MMC) by allowing multiple computations to be
conveyed from each worker per iteration, and design straggler avoidance
techniques accordingly. Then, we analyze how the proposed designs can be
employed efficiently to seek a balance between the computation and
communication latency to minimize the overall latency. Furthermore, through
extensive simulations, both model-based and real implementation on Amazon EC2
servers, we identify the advantages and disadvantages of these designs in
different settings, and demonstrate that MMC can help improve upon existing
straggler avoidance schemes.Comment: This paper was presented in part at the 2019 IEEE International
  Symposium on Information Theory (ISIT) in Paris, France, and at the 2019 IEEE
  Data Science Workshop in Minneapolis, US",science,964,not included
,to_check,core,Iowa State University Digital Repository,2019-08-17 00:00:00,core,implementation of a blind quality control program in blood alcohol analysis,,"Declared proficiency tests are limited in their use for testing the performance of the entire system, because analysts are aware that they are being tested. A blind quality control (BQC) is intended to appear as a real case to the analyst to remove any intentional or subconscious bias. A BQC program allows a real-time assessment of the laboratory’s policies and procedures and monitors reliability of casework. In September 2015, the Houston Forensic Science Center (HFSC) began a BQC program in blood alcohol analysis. Between September 2015 and July 2018, HFSC submitted 317 blind cases: 89 negative samples and 228 positive samples at five target concentrations (0.08, 0.15, 0.16, 0.20 and 0.25 g/100 mL; theoretical targets). These blood samples were analyzed by a headspace gas chromatograph interfaced with dual-flame ionization detectors (HS-GC-FID). All negative samples produced `no ethanol detected’ results. The mean (range) of reported blood alcohol concentrations (BACs) for the aforementioned target concentrations was 0.075 (0.073–0.078), 0.144 (0.140–0.148), 0.157 (0.155–0.160), 0.195 (0.192–0.200) and 0.249 (0.242–0.258) g/100 mL, respectively. The average BAC percent differences from the target for the positive blind cases ranged from −0.4 to −6.3%, within our uncertainty of measurement (8.95–9.18%). The rate of alcohol evaporation/degradation was determined negligible. A multiple linear regression analysis was performed to compare the % difference in BAC among five target concentrations, eight analysts, three HS-GC-FID instruments and two pipettes. The variables other than target concentrations showed no significant difference (P \u3e 0.2). While the 0.08 g/100 mL target showed a significantly larger % difference than higher target concentrations (0.15–0.25 g/100 mL), the % differences among the higher targets were not concentration-dependent. Despite difficulties like gaining buy-in from stakeholders and mimicking evidence samples, the implementation of a BQC program has improved processes, shown methods are reliable and added confidence to staff’s testimony in court",science,966,not included
,to_check,core,,2019-01-01 00:00:00,core,the (social) role of the medical faculties in the dr. google era,,"The relationship between science and society seems to have acquired a renewed centrality in the current academic and scientific debate (Bucchi, Trench 2014; Tipaldo, Scamuzzi 2017). In the age of new media, increasingly characterized by the spread of fake news (AGCOM 2018), new communication “places”, such as institutional sites, blogs and social media, are becoming real reference points, not only in the collecting and diffusion of technical-scientific information for specialists (Martinelli et al. 2017), but also for those who previously had not come into direct contact with specific topics and are faced now with an excessive volume of data (Pellegrini, Rubin 2017). As a result, in the so-called ""Dr. Google Era"" the issue of public communication of science and health news becomes crucial, especially in a context characterized by a growing attention to the reliability of sources and the truthfulness of information. In this scenario, public authorities, in the broadest sense of the term, are called upon to face one of the most difficult challenges they have had to face so far:Public Engagement with Science and Technology (Greco, Pitrelli 2009; Scamuzzi, De Bortoli 2012). In the academic context, this approach recalls the two ""classic"" missions of the University as a social actor, namely education and research, and more immediately refers to the so-called ""Third Mission"" (ANVUR 2014). It is interesting to pay attention on two fundamental elements: (i) the role played by the medical faculties in these processes, as public bodies responsible for education and research in the field of biomedical studies par excellence; (ii) the implementation practices with which these educational institutions carry out the third mission and, therefore, can find legitimacy and be easily recognized by users. For these reasons, we will first analyse the Public Engagement activities carried out by the Italian medical faculties. Secondly, through a comparative analysis with the best practices, we will proceed to identify the set of specific measures in a perspective of future evaluation of quality and academic research, in order to build a possible model of Public Engagement for the medical faculties. AGCOM (2018), News vs. fake nel sistema dell’informazione, novembre, Roma. ANVUR (2014), Rapporto sullo Stato del Sistema Universitario e della Ricerca 2013, Roma. Bucchi M. e Trench B. (a cura di) (2014), Routledge Handbook of Public Communication of Science and Technology, London and New York, Routledge. Greco P. e Pitrelli N. (2009), Scienza e media ai tempi della globalizzazione, Codice Edizioni, Torino. Martinelli L. et al. (2017), ""Comunicazione digitale e riproduzione umana: opportunità e insidie fra scienza, diritti e bisogni sociali"", in Problemi dell'informazione, n. 3, pp. 529-544. Pellegrini G. e Rubin, A. (2017), ""Comunicare la ricerca. Uno studio sul ruolo dei ricercatori nello spazio pubblico della comunicazione"", in Problemi dell'informazione, n. 3, pp. 375-400. Scamuzzi S. e De Bortoli A. (a cura di) (2012), Come cambia la comunicazione della scienza. Nuovi media e terza missione dell’Università, il Mulino, Bologna. Tipaldo G. e Scamuzzi S. (2017), “Introduzione”, in Problemi dell’informazione, n. 3, pp. 367-37",science,967,not included
,to_check,core,LSU Digital Commons,2019-06-04 00:00:00,core,field drilling data cleaning and preparation for data analytics applications,,"Throughout the history of oil well drilling, service providers have been continuously striving to improve performance and reduce total drilling costs to operating companies. Despite constant improvement in tools, products, and processes, data science has not played a large part in oil well drilling. With the implementation of data science in the energy sector, companies have come to see significant value in efficiently processing the massive amounts of data produced by the multitude of internet of thing (IOT) sensors at the rig. The scope of this project is to combine academia and industry experience to analyze data from 13 different wells drilled in an area of 2 x 4 miles. The data was collected in the same rig and contains over 12 million electronic drilling recorder data points, driller’s activity logs and well profiles. The main focus is to propose a detailed workflow to clean and process real drilling data. Once cleaned, the data can be fed into data analytics platforms and machine learning models to efficiently analyze trends and plan future well more efficiently. This roadmap will serve as a basis for drilling optimization. The objective of this work is to detail the various steps needed to prepare field drilling data for business analysis, as well discuss about data analytics and machine learning application in drilling operations. The results to be presented are the detailed workflow and description of the data preparation steps, an example analysis of the drilling data and an example application of a machine learning model in drilling",science,969,not included
,to_check,core,place:Wilmington,2019-01-01 00:00:00,core,"the origins and ends of human rights education: enduring problematics, 1948-2018",,"Il capitolo fornisce una valutazione analitica della problematica dei diritti universali a partire dalla fondazione della Dichiarazione dei diritti umani da 1948 fino ai giorni nostri, nell'anno del suo
settantesimo anniversario e sostiene che tali problemi si rispecchiano nel
stesse origini moderne e nei fini dell'educazione ai diritti umani. Il contributo identifica tre
problematiche specifiche sia per i diritti umani sia per i diritti umani
educazione: proliferazione, priorit\ue0 e pluralismo. Questi tre ambiti hanno suggerito
problematiche molto reali non semplicemente per l'implementazione dell'educazione ai diritti umani ma anche per le implicazioni nel campo teorico pi\uf9 ampio in relazione a problematiche multidisciplinari rilevanti per la filosofia dei diritti umani, delle leggi, della politica sociale e della governance. 
.This chapter provides an analytical assessment of the enduring
problematics of universal rights from the United Nations\u2019 founding Universal
Declaration of Human Rights in 1948 to the present day, in the year of its
seventieth anniversary, and argues that such problematics are mirrored in the
same modern origins and ends of human rights education. It identifies three
such problematics specific to both human rights and human rights
education: proliferation, prioritisation and pluralism. These three suggested
problematics present very real issues not simply for the implementation of
rights through human rights education but raise too in the theoretical realm a
wider series of multi-disciplinary problematics relevant to the philosophy of
human rights, law and social policy, politics and governance. These three
problematics are, however, themselves framed by a larger, problematic grand
narrative. This is a coda of existential threat. It is this which draws human
rights and human rights education into the arena of security in relation to the
protection of rights from a diversity of threats. Indeed, the origins and ends of
human rights and human rights education are encircled both in historical
settings and contemporary contexts by a story of security against threat and a
narrative of protection",science,970,not included
,to_check,core,'IOP Publishing',2019-01-01 00:00:00,core,the asa (approccio sostenibile dell'abitare) app. a sustainable housing approach tool to raise tenants' awareness in the public housing sector,10.1088/1755-1315/296/1/011003,"Il progetto di ricerca presentato propone uno strumento innovativo che favorisce processi partecipativi nell'ambito della sostenibilità nel contesto edilizio. Il campo di intervento è quello dell'Edilizia Residenziale Pubblica, dove, nelle recenti costruzioni, in numerosi casi si riscontra, dal punto di vista tecnico, un elevato standard della qualità abitativa, dal punto di vista sociale, un'interessante varietà di tipologie di nuclei familiari (per consistenza numerica, modalità aggregativa, appartenenza culturale), entrambe caratteristiche che fanno dell'ERP un luogo stimolante per la ricerca.

Nel settore ERP convergono diversi attori. La ricerca si occupa del processo di conoscenza che coinvolge ognuno di essi (progettisti, tecnici di enti proprietari e di gestione, tecnici delle imprese di costruzione), con particolare attenzione alle persone che abitano negli alloggi. Di fatto, per ottenere risultati efficaci rispetto agli investimenti sostenuti, si presenta la necessità che gli inquilini assumano comportamenti consapevoli: per questo, la ricerca prospetta l'introduzione di nuovi attori, quali studenti (supportati dai loro insegnanti) che frequentano l'ultimo anno degli Istituti secondari superiori ad indirizzo tecnico, ai quali affidare il ruolo di formatori degli inquilini attraverso momenti di incontro da ripetere nel tempo.

La ricerca prevede l'ideazione dell'Applicazione ASA (Approccio Sostenibile all'Abitare), personalizzata per ogni alloggio ed edificio considerato attraverso il lavoro comune di studenti e inquilini; i dati sono quelli raccolti e trasferiti da un attore all'altro in tappe precedenti della ricerca.

Si descrivono le caratteristiche che la rendono utilizzabile come strumento di conoscenza e gestione per la propria abitazione, per ottenere risparmio nel rispetto delle condizioni di benessere; si illustrano aspetti che consentono di impiegarla per la gestione degli spazi comuni e del verde, per un miglioramento delle relazioni sociali. 

Pensata per essere messa a disposizione su iPad, accessibile a tutti gli inquilini in uno spazio comune dell'edificio dove si svolgono i momenti di formazione, ASA è scaricabile su dispositivi personali mobili. 

Tra i risultati della ricerca, si presenta una schematizzazione dei passaggi logici che costituiscono il processo dinamico dell'App. Si delineano i vantaggi economici e sociali per i diversi attori.

In un passaggio successivo, la ricerca passerà alla fase operativa, in cui gli inquilini potranno utilizzare l'App ASA anche per dialogare tra di loro e con i tecnici addetti alla gestione. Ulteriori sviluppi porteranno alla formazione della Community (ASA), che farà dialogare tra di loro tutti gli attori.This research project introduces an innovative tool promoting participatory processes for building sustainability. The action area is the Public Housing system where most recent buildings now show both a high technical standard and a captivating social variety of family households (including number of members, aggregations, cultural identity) – both characteristics making Public Housing an interesting research topic.

Different players are involved in a Public Housing system. This research deals with their specific knowledge process, with special attention paid to tenants. As a matter of fact, to get efficient outcomes against the investments made, tenants are needed to engage in responsible behaviors. This research envisages calling in new players, such as senior students from technical high schools who would be trained by their teachers to achieve a focused technical, psychological and pedagogical expertise. Students would then act as trainers themselves and coach the tenants through meetings scheduled over the time.

An Application called ASA as Sustainable Housing Approach will be designed to include the information provided by the tenants about their family households and housing units during the coaching sessions and collected by the student-coaches.  Data used will be that collected and transferred from a player to another in prior research phases.  The characteristics of the ASA as a knowledge and management tool intended to improve savings and wellbeing are described. The points whereby the ASA can be used for effectively managing common areas and green spaces to ease the social interactions are explained too.

Available on iPad and accessible to all tenants in the training environment, the ASA can be downloaded to mobile devices. 

The logical steps making up the ASA’s dynamic process are outlined and a real data simulation is proposed. The economic and social benefits for the different players are outlined as well.

Thanks to future developments of the ASA, tenants will be able to interact with each other and with the housing management institute’s experts. Further implementation of this App will lead to the creation of an ASA Community, allowing all players to interact",science,971,not included
,to_check,core,Iowa State University Digital Repository,2019-01-01 00:00:00,core,"deep learning for human engineered systems: weak supervision, interpretability and knowledge embedding",,"Pattern recognition has its origins in engineering while machine learning developed from computer science. Today, artificial intelligence (AI) is a booming field with many practical applications and active research topics that deals with both pattern recognition and machine learning. We now use software and applications to automate routine labor, understand speech (using Natural Language Processing) or images (extracting hierarchical features and patterns for object detection and pattern recognition), make diagnoses in medicine, even intricate surgical procedures and support basic scientific research.
This thesis deals with exploring the application of a specific branch of AI, or a specific tool, Deep Learning (DL) to real world engineering problems which otherwise had been difficult to solve using existing methods till date. Here we focus on different Deep Learning based methods to deal with several such problems. We also explore the inner workings of such models through an explanation stage for each of the applied DL based strategies that gives us a sense of how such typical black box models work, or as we call it, an explanation stage for the DL model.
This explanation framework is an important step as previously, Deep Learning based models were thought to be frameworks which produce good results (classification, object detection, object recognition to name a few), but with no explanations or immediately visible causes as to why it achieves the results it does. This made Deep Learning based models hard to trust amongst the scientific community. In this thesis, we aim to achieve just that by deploying such explanation frameworks, which will be discussed later in the subsequent chapters. We use one such explanation framework to develop a surrogate model to predict properties of microstructures as well.
Furthermore, we dig deep into the realm of semi-supervised or weakly supervised learning which utilizes minimal data for its training phase and develop a novel framework capable of accurately predicting yields of crops like sorghum. We utilize this framework to learn from available data in an online fashion, annotate new data, significantly reduce manual annotation time and at the same time predict crop yield, a first of its kind within the target application domain.
We also propose a new generative modeling approach, Surrogate Invariance Network (S-InvNet), that can efficiently model data spaces with known invariances. We devise an adversarial training algorithm to encode such invariances into the data distribution. We validate our framework by reconstructing two-phase microstructures with desired physical properties. The physical properties are governed by a data-driven surrogate model which acts as the invariance for our case. In essence, we fuse a generative and a classification model with the classification model acting as an invariance checker that enforces the property of the target microstructure.
How a Deep Learning model can iteratively solve a non-linear Partial Differential Equation (PDE) based on a given set of initial conditions and using a Physics-governed loss function (instead of traditional loss functions used to optimize Deep Learning models) is presently an area of great research interest. We explore this as well. Previous literature deal with utilizing a single loss function to solve such examples of PDE-governed systems. Moreover, the most recent work proposes to solve only a simple spatially varying PDE (Darcy Flow). We extend the framework to deal with both spatial and time-varying PDEs (Burgers\u27 Equation). Furthermore, we also propose an alternating minimization process for optimizing neural networks as PDE-solvers. This alternatively minimizes two separate loss functions. We note that this process works just as well as the single loss minimization and in certain cases, performs even better. From a neural network standpoint, we use a Convolutional Encoder-Decoder framework as our PDE-surrogate",science,972,not included
,to_check,core,,2019-09-01 00:00:00,core,jerico-next. first valorisation results for each region,,"WP4 of JERICO-NEXT aims to synthesize the project’s activities in the other WPs and gather the contributions around applied Joint Research Activity Projects (JRAPs) selected to benefit of and highlight JERICO-NEXT activities. In order to fulfil this objective, methodologies developed or improved in WP3 were applied in the JRAPs;the provision of data assembled and distributed was undertaken according to the WP5 recommendations; dedicated topical approaches of the scientific strategy matured jointly with WP1&4 (Deliverable D4.1) were applied within the JRAPs, providing then in return essential input to the future road map of the research infrastructures. Indeed, six JRAPs were implemented to address different key environmental issues and/or policy needs such as those considered by the MSFD, and according to the 6 JERICO-NEXT scientific areas:
1- JRAP#1 on pelagic biodiversity
2- JRAP#2 on benthic biodiversity
3- JRAP#3 on chemical contaminant occurrence and related biological responses
4- JRAP#4 on hydrography and transport
5- JRAP#5 on carbon fluxes and carbonate system
6- JRAP#6 on operational oceanography and forecasting.
These JRAPs were not intending to implement similar actions at each JERICO-RI site but only to a selection of sites/regions according to the consortium interests and requirements from local to regional scales. Consequently, it is paramount to regionally synthesize the preliminary results after deployments in JRAPs, which is the purpose of this document.
The main document is organised according to the following regions and sites:
	- Bay of Biscay (South East Bay of Biscay, Portuguese Margin and Nazaré Canyon,  Girond Mud patch and Bay of Brest)
	- Channel and North Sea
	- Kattegat and Skagerrak Sea
	- Baltic Sea
	- Norwegian Sea
	- Med. Sea: From Liguria to the Ibiza Channel
	- Med. Sea: Northern Adriatic Sea
	- Med. Sea: Cretan Sea.
Results for those regions are presented in chapter 3 after consideration of the regional or site specificities related to the most relevant scientific issues of the area and the most relevant societal and policy needs, respectively. As the project and the JRAPs were not a priori organised to fit with regional to local needs, the reader may identify a weak point in the way JERICO-NEXT is addressing scientific syntheses per region. Nevertheless, it is a preliminary work towards the regional structuration of the RI, as expected, and a significant effort was put forth to identify discrepancies in the level of regional integration, and the recommended way to progress on this issue is presented in Chapter 4.
 
Synthesis of main achievements per region of JERICO-RI
Bay of Biscay: SE BoB
	- Involved JRAPs: JRAPs #1 3 4 6
	- Progress in the study of coastal small scale and mesoscale features from the combined use of multiplatform in-situ and satellite data.
	- Progress in the application of innovative techniques (developed in WP3) in this area for data-gap filling, data-blending, advanced Lagrangian diagnostics and performing observing system experiments (OSEs) and observing system simulation experiments (OSSEs).
	- Success in the gathering of new high-resolution datasets from different surveys and actions in the area (ETOILE with MASTODON-2D moorings deployment and TNA BB-TRANS).
 
Bay of Biscay: Gironde and Bay of Brest
	- Involved JRAPs: JRAP #2
	- Success in gathering biological and biogeochemical observations on a major marine mudpatch located in a high energetic environment. New evidence on the major role of hydrodynamics in controlling benthic diversity and associated biogeochemical processes in this mudpatch.
	- Success in deriving high resolution spatial maps of clam dredging pressure in the Bay of Brest and in using these maps to show the deleterious effect of this activity on benthic diversity hosted by maerl beds.
	- Success in the field testing of several techniques and tools (e.g., Image acquisition via mobile platforms and sediment profiling, image processing via dedicated software, and O2 sediment microprofiling) developed within both JERICO-FP7 and JERICO-NEXT.
 
Bay of Biscay: Nazare Canyon
	- Involved JRAPs: JRAPs #4 & 6
	- Success in the implementation of a high-resolution model with data assimilation able to describe the energetic dynamics and coastal ocean impacts of this long submarine canyon.
	- Progress in understanding the crucial importance of real-time monitoring infrastructures (fixed platforms, HF radars) to the characterization and operational forecasting of coastal ocean areas marked by the presence of submarine canyons, view the energetic and short spatial scale processes that are associated with these topographic features and the impacts canyons promote on larger domains of the coastal ocean. 
	- Progress in understanding the dominant processes of subinertial dynamics in the Nazare Canyon area of influence, contributing to define the main components of a real-time monitoring system for this area.
 
Channel and North Sea
	- Involved JRAPs: JRAPs #1, 4, 5
	- Successful implementation of innovative (semi-)automated techniques for the monitoring of phytoplankton dynamics and C cycle in the English Channel and the North Sea  - an extended shelf system influenced by multiple sources of human pressure and contrasting hydrodynamical conditions
	- Some of these methods were compared with traditional laboratory analysis which helped to better address the added value of innovative techniques in terms of improving the spatial and temporal resolution (both in surface and in the water column), making it possible to consider functional and, sometimes, even taxonomical characterization of phytoplankton communities composition as well as photo-physiology, at high  resolution, in almost real-time.
	- New insights into the seasonality of the spatial distribution of delta partial pressure of carbon dioxide (pCO2) measured continuously on ships of opportunity at a regional scale in the North Sea, and the relation between marine sinks of CO2 with high total chlorophyll a fluorescence.
	- Automated techniques made it possible to characterize the size and functional composition of phytoplankton communities (from pico- to microphytoplankton) through the main bloom episodes including outburst of potential HABs as Pseudo-nitszchia spp. and Phaeocystis globosa (characterized as high or low red fluorescence nano-eukaryotes: Nano high and Low FLR) from the Eastern English Channel (EEC) towards the southern North Sea, in international cross-border (UK, FR, BE, NL) common research cruises, following the spatial and temporal succession of spring blooms.
	- There is a need to increase the combined implementation of innovative and reference techniques both on current monitoring of discrete stations as well as in continuous automated measurements performed on cruises and ships of opportunity (as FerryBoxes), in order to increase the spatial and temporal resolution of the surveys of the different eco-hydrodynamic regions of the area.
 
Kattegat and Skagerrak
	- Involved JRAPs: JRAPs #1, 3, 5, 6
	-  Harmful algae, phytoplankton diversity and abundance were observed in near real time at an aquaculture site on the Swedish west coast. In situ imaging flow cytometry combined with machine learning and wireless communications provided data every 20 minutes.
	- Data from HF radar, FerryBox, research vessels and oceanographic buoys were used together with results from the 3D-NEMO Nordic ocean circulation model and remote sensing to describe the Kattegat-Skagerrak system.
	- Data from a FerryBox system revealed, as expected, the occurrence of strong concentration gradients reflecting progressive dilution along the South-North transect and highlighted harbours areas (Oslo and Kiel) as hotspots for some chemical compounds.
	- Microbial molecular markers representing bacterial species and genes were used to identify hydrocarbon pollution or high nutrient loads.
	- Barcoding of phytoplankton and bacteria revealed previously unknown diversity in the pelagic communities (see deliverable D3.8).
	- Carbon fluxes and carbonate system variability in the Skagerrak/Kattegat region is primarily driven by changes in salinity resulting from the balance of freshwater inputs from riverine and Baltic sources and saline waters from the Atlantic Ocean.
 
Baltic
	- Involved JRAPs: JRAPs #1, 5, 6
	- Different technologies for phytoplankton research have been successfully evaluated in the Baltic Sea. Operational monitoring of phycoerythrin fluorescence started after in-depth study during JRAP1, which identified different origins of this signal. To study filamentous cyanobacteria blooms, various sensors were tested and they were largely complementary. New absorption method seems to provide reliable estimates for Chlorophyll-a concentration, but still lacks automated maintenance procedures. Better understanding was obtained on the range of conversion factor between electron transport rate (measured with fluorescence induction) and carbon fixation rate, as well as of the reasons behind this variability. 
	- Carbonate system components of the Baltic Sea showed large seasonal variability indicating high impact of biological activity for pH and pCO2. Alkalinity of the Baltic Sea is difficult to model from other carbonate system components and online sensors are required to understand its variability.
	- The joint studies between different (multinational) research groups using different technologies provided good know-how exchange and should be encouraged. As well, multidisciplinary research efforts, including physics, chemistry, biology and modelling, should be encouraged, to gain knowledge on the environmental challenges more in detail. 
 
Norwegian Sea
	- Involved JRAPs: JRAPs #1, 3, 6
	- The Norwegian Sea plays a major role as an area where potentially highly polluted waters from the North Sea mix with water transported from the North Atlantic Ocean. This water is then transported into the Arctic region. Analysis of the transport pathway of waterborne contaminants along the Norwegian coast was instrumental for assessing the spatial range of contaminants with different properties and address questions regarding exposure of the Arctic.
	- 42 currently used pesticides in Europe, 5 artificial sweeteners and 11 pharmaceuticals and personal care products were targeted during the study. Several compounds were detected in the Norwegian Sea, including current use pesticides, artificial food additives and some pharmaceuticals. Their presence in this coastal area, and also in high-latitude more open waters, highlight the potential for these contaminants to undergo long range transport with marine currents.
	- Seasonal variability in temperature in the coastal area (up to a 15 ˚C differential between summer and winter depending on latitude) is a large driving force on carbonate system variability, including a decrease in surface water fCO2 due to lower wintertime temperatures as well as the uptake of atmospheric CO2 as water cools during its northward journey from the North Atlantic to the Arctic Ocean.
	- In addition, a focal point was the improvement of systems that provides knowledge for the transport of parasites and harmful algae in the Norwegian Sea coastal area. Here the observations by FerryBoxes, fixed stations as well as repeated transects were used to validate and improve numerical model simulations.
 
Mediterranean Sea: Ligurian to Ibiza channel
	- Involved JRAPs: JRAPs #1, 4, 6
	- Major investigation effort was led in the Liguro-Provencal area and the Catalan Margin to study the variability of the Northern current through the combination of independent and complementary observational platforms. The dynamics of the boundary currents were studied to identify the interplay between various forcings (remote, thermohaline and wind), the generation of mesoscale and submesoscale instabilities, and data blending and assimilation techniques were investigated.
	- Major results have shown a clear correlation between hydrographic changes led by climatic interannual variability and the community composition of phytoplankton and zooplankton. The role of (sub)mesoscale processes have shown to modulate biochemical processes and to locally enhanced marine biomass productions/accumulation.
	- The multiplatform observing system in the NW Mediterranean Sea, combined with growing centralized frameworks of data management and distribution, i.e. Copernicus Marine Environment Monitoring Service and SeaDataNet/SeaDataCloud, will provide the basis for an extended European coastal infrastructure.
 
Mediterranean Sea: Adriatic Sea
	- Involved JRAPs: JRAPs #5 & 6
	- During the project, important steps forward have been made on the development of capabilities to integrate new kinds of experimental data and oceanographic models to support ecosystem management. The implementation of an Adriatic oceanographic model assimilating surface current from coastal radar and temperature profiles from fishing vessels (FOOS fleet) has been developed and tested, providing encouraging results. Surface currents from coastal radars were integrated with results from drifter deployments to investigate zones of recruitment for small pelagic fishes, highlighting the role of remote areas in supporting the ecological role of these environments.
	- One year of high frequency data of sea surface pCO2 was successfully gathered at a fixed station in the northernmost Adriatic. Results highlighted how the biological CO2 uptake during phytoplankton blooms was able to keep the central basin a strong CO2 sink not only in winter, when low temperatures favor CO2 dissolution, but through most of the year, even when temperatures raised above 25°C.
	- The work carried on so far highlighted the potentiality of the area, where historical data and many observational systems are available to support both ecosystem management and advanced marine researches. On the other hand, they pointed out the need for integration of the existing facilities and observational systems also at a trans-border level to address the climate and ecological challenges facing this basin.
 
Mediterranean Sea: Cretan Sea
	- Involved JRAPs: JRAPs #2 & 6
	- An interesting case of how the additional assimilation of glider profiles and FerryBox observations used in the OSE experiment is beneficial for the Aegean Sea forecasting system in terms of reducing the system biases. This improved the sea surface salinity model bias over the south Aegean (north of Crete and south of 37°N).
	- In the Cretan Sea, the vertical migration of mesopelagic organisms (macroplanktonic and micronektonic) was observed by acoustical means for almost 2.5 years in the epipelagic and mesopelagic layers. The observed organisms were categorized into four groups according to their migration patterns which appeared to occur at diel and seasonal scale. The variability of the migration patterns was inspected in relation to the physical and biological environmental conditions of the study area. Stratification of the water column does not act as a barrier for the vertical motion of the strongest migrants that move up to 400 m every day. Instead, changes in light intensity (lunar cycle, daylight duration, cloudiness) and the presence of prey and predators seem to explain the observed daily, monthly and seasonal.
 
This report clearly demonstrates how the consortium of the project is willing to progress on monitoring strategies in several JERICO studies (besides the simulation experiments for transport studies, the analysis of search radius from contaminant sources and the use of covariance in highly relevant low-concentration persistent contaminants, use of multi-functional sensors, etc.); despite the difficulty and time-consuming activity of operating both fixed and mobile platforms working in the highly dynamic complex and densely utilised coastal areas. By progressing on the integration of scientific fields, it also shows the benefit of operating several platforms types (fixed & mobile, at sea, remote, & numerical). For instance, we can emphasize what deployments in JRAPs have proved:
	- JRAP #1: interest in deploying complementary observing systems for algal blooms to get information interoperable at EU level.
	- JRAP #2: success in monitoring highly dynamic benthic ecosystems.
	- JRAP #3: possibility to successfully perform monitoring of contaminants in an interoperable manner.
	- JRAP #4: the highly resolved low-cost sensor and mooring deployments for specific transport or contaminant studies, which shows the intent to be cost-efficient. The complementarity of remote + at sea and numerical systems.
	- JRAP#5: coastal carbon fluxes and biogeochemical cycling: The relatively large variability of conditions keeps being a challenge for sensor developers, with necessary periodic calibration needs that are possible to tackle as shown.
	- JRAP#6: makes a strong case of the need for in-situ data vs models, in particular for coastal processes.
 
A vision: a possible geographical structure of JERICO-RI per region and site
As a consequence, JERICO-RI already proved his capability to gather information and tools to qualify and quantify processes, their scales, related challenges and the possible solution to progress on. As a next step, in agreement with regional stakeholder, JERICO-RI should develop regional forum/center to share information (data and products), expertises, practices, solution and training in line with regional purposes to support scientists and regional stakeholders. This would support application of policies and regulations, based on applied collaborations between scientists and other stakeholders to tackle common societo, environmental and scientific questions from local to regional scales.
According to the monitoring purposes in regions and sites, the need of integration in scientific fields is diverse and JERICO-NEXT presented only a first steps. In the future, JRAPs, TNA & regions should engage with outermost regions where regional projects take place and could be liaised with JERICO RI to better connect these regions in the coastal observing RI landscape. Because of these considerations, the consortium progressed towards regional integrated coastal observatories and preliminary elements are presented in chapter 4.
A main lesson learned is that societal challenges and priorities at the regional level are important elements for the structuring of a coastal observing system. Therefore, a key challenge for the future is to improve regionalisation of the observatories for a better understanding of region-specific processes and an improved fit-for-purpose of the JERICO-RI. Furthermore, the observatories need to be consolidated in terms of performance, reliability and variables to optimally address and answer to key regional and pan-European environmental challenges. The two above-mentioned aspects are the integration challenge that the consortium wishes to tackle by implementing a regional structure of JERICO-RI.
The structuring process will be challenging because coastal observatories are not operated by the same organisations and therefore may have differing objectives and means of operation (financial, logistical, etc.). Based on these differences, we have proposed that the coastal observing systems in JERICO-RI can be structured hierarchically in which all sizes and types of coastal observatories can function in JERICO-RI in an integrated and mutually beneficial way. This will be reported in deliverable D1.4. This way towards a regional structuring of JERICO-RI is included in the proposal for the 3rd project of the JERICO series of projects, JERICO-S3, selected for funding during 4 years and to start in early 2020. With regards to the WP4 of the present project, the final deliverable: D4.5 is in progress to report results of each of the six JRAP activities and will be available by Sept. 2019 on the JERICO-RI website",science,973,not included
,to_check,core,TRACE: Tennessee Research and Creative Exchange,2019-12-15 00:00:00,core,improving manufacturing data quality with data fusion and advanced algorithms for improved total data quality management,,"Data mining and predictive analytics in the sustainable-biomaterials industries is currently not feasible given the lack of organization and management of the database structures. The advent of artificial intelligence, data mining, robotics, etc., has become a standard for successful business endeavors and is known as the ‘Fourth Industrial Revolution’ or ‘Industry 4.0’ in Europe. Data quality improvement through real-time multi-layer data fusion across interconnected networks and statistical quality assessment may improve the usefulness of databases maintained by these industries. Relational databases with a high degree of quality may be the gateway for predictive modeling and enhanced business analytics.	Data quality is a key issue in the sustainable bio-materials industry. Untreated data from multiple databases (e.g., sensor data and destructive test data) are generally not in the right structure to perform advanced analytics. Some inherent problems of data from sensors that are stored in data warehouses at millisecond intervals include missing values, duplicate records, sensor failure data (data out of feasible range), outliers, etc. These inherent problems of the untreated data represent information loss and mute predictive analytics. The goal of this data science focused research was to create a continuous real-time software algorithm for data cleaning that automatically aligns, fuses, and assesses data quality for missing fields and potential outliers. The program automatically reduces the variable size, imputes missing values, and predicts the destructive test data for every record in a database. Improved data quality was assessed using 10-fold cross-validation and the normalized root mean square error of prediction (NRMSEP) statistic. 	The impact of outliers and missing data were tested on a simulated dataset with 201 variations of outlier percentages ranging from 0-90% and missing data percentages ranging from 0-90%.  The software program was also validated on a real dataset from the wood composites industry. One result of the research was that the number of sensors needed for accurate predictions are highly dependent on the correlation between independent variables and dependent variables. Overall, the data cleaning software program significantly decreased the NRMSEP ranging from 64% to 12% of quality control variables for key destructive test values (e.g., internal bond, water absorption and modulus of rupture)",science,974,not included
,to_check,core,'MDPI AG',2019-01-01 00:00:00,core,exploiting scalable machine-learning distributed frameworks to forecast power consumption of buildings,10.3390/en12152933,"The pervasive and increasing deployment of smart meters allows collecting a huge amount of fine-grained energy data in different urban scenarios. 

The analysis of such data is challenging and opening up a variety of interesting and new research issues across energy and computer science research areas. 

The key role of computer scientists is providing energy researchers and practitioners with cutting-edge and scalable analytics engines to effectively support their daily research activities, hence fostering and leveraging data-driven approaches.

This paper presents SPEC, a scalable and distributed engine to predict building-specific power consumption.

SPEC addresses the full analytic stack and exploits a data stream approach over sliding time windows to train a prediction model tailored to each building. The model allows us to predict the upcoming power consumption at a time instant in the near future. 

SPEC integrates different machine learning approaches, specifically 

ridge regression, artificial neural networks, and random forest regression, to predict fine-grained values of power consumption, and  

a classification model, the random forest classifier, to forecast a coarse consumption level. 

SPEC exploits state-of-the-art distributed computing frameworks to address the big data challenges in harvesting energy data: the current implementation runs on Apache Spark, the most widespread high-performance data-processing platform, and can natively scale to huge datasets.

As a case study, SPEC has been tested on real data {of an heating distribution network and power consumption data} collected in a major Italian city.

Experimental results demonstrate the effectiveness of SPEC to forecast both fine-grained values and coarse levels of power consumption of~buildings",science,976,not included
,to_check,core,Análise da implantação do Parque Aquícola da UHE do Lajeado através de métodos multicritérios de decisão – Uma abordagem socioambiental e dos fatores de risco,2018-01-01 00:00:00,core,https://core.ac.uk/download/270093220.pdf,,"This study is derived from the policy of the Brazilian government to increase fish production through the promotion of fish farming in network ponds in aquaculture parks. This project aims to explore the implementation and operation of the Sucupira Aquaculture Park (PAS) of the Lajeado Hydroelectric Power Plant (Palmas / TO) through the use of multicriteria decision methods, aiming at a socioenvironmental approach of the risk factors of the implementation of this project through the proposition of sustainability indicators. Scientometry has demonstrated that the production of knowledge related to fish culture in net tanks and artisanal fishing in reservoirs is indexed and has a multidisciplinary character. The articles are technicist and productivist, with little regard for environmental and socioeconomic factors, nor the public policies for the development and promotion of fishery and fishery activities, which in a way explains the dissatisfaction with the activity by the fish farmers of the Bom Peixe Association , as well as the inefficiency of the public policy of transforming the artisanal fisherman into an entrepreneur. The Social Network Analysis (ARS) makes clear that there is interconnection among all actors, but the protagonism of this network is dominated by public institutions and the process of communication between the various institutional actors is two-way, while between fish farmers and fishermen is of a unidirectional character, making them only subjects of actions. The Trophic State Index (EIT) was used as a monitoring tool for aquaculture activities in PAS. Limnological variables, with the exception of total phosphorus, despite small variations of oxygen and total solids, are in accordance with the recommendations of CONAMA Resolution 357/05. The data allow us to conclude that the aquaculture area presented a predominance of supereutrophic trophic state and that the trophic state of the reservoir in the vicinity of the tanks is strongly influenced by anthropic activities, mainly in the rainy season. In the dry season, there is an increase in the phosphorus concentration in the water, leading to a hypereutrophic state. It is not possible to say that aquaculture is an impactful activity and its effects are difficult to predict in the long term, as well as the behavior of the reservoir through increased production in the parks, however, if there is no monitoring and efficient control over water quality , fish farming may promote degradation of the aquatic ecosystem. The analysis of the technical, economic and financial feasibility of investment in aquaculture projects of tambaqui production in network tanks used the economic indicators Effective Operational Cost (COE), Total Operating Cost (COT), Total Gross Income (RBT), Gross Margin (MB), Net Margin (ML), Profit (L) and Return Rate of Capital (TRC). The analysis of TRC presented negative results and the system of production in excavated nurseries proved to be more efficient. The system of production in network tanks is an alternative of lower cost and time of implantation, but with uncertain return. The socioeconomic profile of the producers is considered to be incompatible with the business and technological activity of aquaculture in network tanks. In the producers' view, the activity is still a future promise and the public policy of turning fishermen into fish farmers proved ineffective. According to TRC, the activity is unsustainable and has been showing negative results, which indicates its low competitiveness. Sustainability and governance indicators were developed for the development of fish farming in the PAS, which, through social, economic and environmental subindices, generated a Sustainability Index (ISUS) of the enterprise. The data allowed to conclude that the ISUS was classified as not very sustainable (0.37), mainly in the economic and social dimensions. The analysis of governance was identified in interviews with the main agents of the productive chain, making it clear that the lack of governance structures was noticeable in all segments analyzed, which portrays the real stage of disorganization and inefficiency of the productive chain of fish farming in PAS.Este estudo deriva da política do governo brasileiro para incremento da produção de pescado por meio do fomento à piscicultura em tanques rede nos parques aquícolas. Este projeto visa explorar a implantação e operação do Parque Aquícola Sucupira (PAS) da UHE Lajeado (Palmas/TO) através do uso de métodos multicritérios de decisão, com objetivo de realizar uma abordagem socioambiental dos fatores de risco da implantação deste empreendimento através da proposição de indicadores de sustentabilidade. A cienciometria, demonstrou que a produção de conhecimento relacionado ao cultivo de peixes em tanques rede e a pesca artesanal em reservatórios está indexada e é de caráter multidisciplinar. Os artigos possuem caráter tecnicista e produtivista, pouco considerando os fatores ambientais e sócio econômicos, nem tampouco as políticas públicas de desenvolvimento e fomento às atividades piscícolas e ou pesqueiras, o que de certa forma explica a insatisfação com a atividade pelos piscicultores da Associação Bom Peixe, bem como a ineficiência da política pública de transformar o pescador artesanal em empreendedor. A Análise de Redes Sociais (ARS) deixa claro que há interligação entre todos os atores, porém o protagonismo dessa rede está dominado por instituições públicas e o processo de comunicação entre os diversos atores institucionais é de caráter bidirecional, enquanto que entre os piscicultores e pescadores é de caráter unidirecional, tornando-os apenas sujeitos das ações. O Índice de Estado Trófico (IET) foi utilizado como ferramenta de monitoramento de atividades aquícolas no PAS. As variáveis limnológicas, com exceção do fósforo total, apesar das pequenas variações de oxigênio e sólidos totais, estão de acordo com as recomendações da Resolução CONAMA 357/05. Os dados permitem concluir que a área aquícola apresentou predominância de estado trófico supereutrófico e que o estado trófico do reservatório nas proximidades dos tanques rede sofre forte influência externa de atividades antrópicas, principalmente no período chuvoso. No período da seca, ocorre aumento na concentração de fósforo na água, levando a um estado hipereutrófico. Não é possível afirmar que a aquicultura seja uma atividade impactante e seus efeitos são difíceis de serem previstos em longo prazo, bem como o comportamento do reservatório mediante aumento de produção nos parques, porém, caso não haja monitoramento e controle eficiente sobre a qualidade da água, a piscicultura pode vir a promover degradação do ecossistema aquático. A análise da viabilidade técnica, econômica e financeira de investimento em projetos aquícolas de produção de tambaqui em tanques rede utilizou os indicadores econômicos Custo Operacional Efetivo (COE), Custo Operacional Total (COT), Renda Bruta Total (RBT), Margem Bruta (MB), Margem Líquida (ML), Lucro (L) e Taxa de Retorno do Capital (TRC). A análise da TRC apresentou resultados negativos e o sistema de produção em viveiros escavados se mostrou mais eficiente. O sistema de produção em tanques rede é uma alternativa de menor custo e tempo de implantação, porém com retorno incerto. O perfil socioeconômico dos produtores é considerado incompatível com a atividade empresarial e tecnificada de aquicultura em tanques rede. Na visão dos produtores a atividade ainda é uma promessa futura e a política pública de transformar pescadores em piscicultores se mostrou ineficaz. De acordo com a TRC, a atividade é insustentável e vem apresentando resultados negativos, o que denota sua baixa competitividade. Foram elaborados indicadores de sustentabilidade e governança para o desenvolvimento da piscicultura no PAS, que através de subíndices sociais, econômicos e ambientais geraram um Índice de Sustentabilidade (ISUS) do empreendimento. Os dados permitiram concluir que o ISUS foi classificado como pouco sustentável (0,37), principalmente nas dimensões econômicas e sociais. A análise de governança foi identificada em entrevistas aos principais agentes da cadeia produtiva, ficando claro que a falta de estruturas de governança foi perceptível em todos os segmentos
analisados, o que retrata o real estagio de desorganização e ineficiência da cadeia produtiva da piscicultura no PAS",science,977,not included
,to_check,core,Towards Democratizing Data Science with Natural Language Interfaces,2018-01-01 00:00:00,core,https://core.ac.uk/download/224407156.pdf,,"Data science has the potential to reshape many sectors of the modern society. This potential can be realized to its maximum only when data science becomes democratized, instead of centralized in a small group of expert data scientists. However, with data becoming more massive and heterogeneous, standing in stark contrast to the spreading demand of data science is the growing gap between human users and data: Every type of data requires extensive specialized training, either to learn a specific query language or a data analytics software. Towards the democratization of data science, in this dissertation we systematically investigate a promising research direction, natural language interface, to bridge the gap between users and data, and make it easier for users who are less technically proficient to access the data analytics power needed for on-demand problem solving and decision making.One of the largest obstacles for general users to access data is the proficiency requirement on formal languages (e.g., SQL) that machines use. Automatically parsing natural language commands from users into formal languages, natural language interfaces can thus play a critical role in democratizing data science. However, a pressing question that is largely left unanswered so far is, how to bootstrap a natural language interface for a new domain? The high cost of data collection and the data-hungry nature of the mainstream neural network models are significantly limiting the wide application of natural language interfaces. The main technical contribution of this dissertation is a systematic framework for bootstrapping natural language interfaces for new domains. Specifically, the proposed framework consists of three complimentary methods: (1) Collecting data at a low cost via crowdsourcing, (2) leveraging existing NLI data from other domains via transfer learning, and (3) letting a bootstrapped model to interact with real users so that it can refine itself over time. Combining the three methods forms a closed data loop for bootstrapping and refining natural language interfaces for any domain.The developed methodologies and frameworks in this dissertation hence pave the path for building data science platforms that everyone can use to process, query, and analyze their data without extensive specialized training. With such AI-powered platforms, users can stay focused on high-level thinking and decision making, instead of overwhelmed by low-level implementation and programming details --- ``\emph{Let machines understand human thinking. Don't let humans think like machines}.'",science,978,not included
,to_check,core,"Can We Assess Mental Health through Social Media and Smart Devices?
  Addressing Bias in Methodology and Evaluation",2018-07-19 00:00:00,core,http://arxiv.org/abs/1807.07351,,"Predicting mental health from smartphone and social media data on a
longitudinal basis has recently attracted great interest, with very promising
results being reported across many studies. Such approaches have the potential
to revolutionise mental health assessment, if their development and evaluation
follows a real world deployment setting. In this work we take a closer look at
state-of-the-art approaches, using different mental health datasets and
indicators, different feature sources and multiple simulations, in order to
assess their ability to generalise. We demonstrate that under a pragmatic
evaluation framework, none of the approaches deliver or even approach the
reported performances. In fact, we show that current state-of-the-art
approaches can barely outperform the most na\""ive baselines in the real-world
setting, posing serious questions not only about their deployment ability, but
also about the contribution of the derived features for the mental health
assessment task and how to make better use of such data in the future.Comment: Preprint accepted for publication in the European Conference on
  Machine Learning and Principles and Practice of Knowledge Discovery in
  Databases (ECML-PKDD 2018 Applied Data Science Track",science,979,not included
,to_check,core,'Kyiv Politechnic Institute',2018-01-01 00:00:00,core,підвищення цифрової грамотності майбутніх фахівців із видавничої справи та редагування: досвід створення та впровадження масового відкритого онлайн-курсу,10.20535/2522-1078.2018.1.132896,"У статті розглянуто досвід розробки, створення та впровадження у
змішаному форматі масового відкритого онлайн-курсу «Цифрові комунікації
в глобальному просторі» з метою підвищення цифрової грамотності майбутніх фахівців з видавничої справи та редагування. Такий курс дав змогу
навчити студентів користуватись цифровими ресурсами та технологіями,
які потрібні для організації редакційно-видавничого процесу сучасних медіа.
Під час навчання розглянуто створення та наповнення веб-ресурсів, підготовка текстів для інтернет-видань та соціальних мереж, графічний дизайн,
хмарні технології, основи роботи пошукових систем, штучного інтелекту,
головні засади інформаційної безпеки. Впровадження масового відкритого
онлайн-курсу в змішаному форматі дозволило доповнити дисципліну «Інтенет-журналістика» та створити проблемно-орієнтоване середовище для
студентів. Розгляд реального досвіду фахівців із цифрових комунікацій підвищив загальний рівень цифрової грамотності студентів та створив підґрунтя
для майбутніх наукових та професійних пошуків.The article is dedicated to the experience of the development, creation, and
implementation in blended for the massive open online course ""Digital communication
in the global society"" to improve digital awareness of future specialists
in publishing and editing. This course has enabled students to use the digital resources
and technologies needed to organize the editorial and publishing process
in modern media. During the study examined the creation and content development
for web resources, drafting texts for online publications and social media, graphic
design, cloud technologies, fundamentals of search engines, artificial intelligence,
the information security’s main principles. The introduction of the massive open
online course in blended made it possible to give a new way of teaching for the
course „Web journalism” and have created a problem-oriented environment for
students. The real experience case studies of digital communication professionals
have raised the overall level of digital literacy among students and created the basis
for future academic and professional research",science,980,not included
,to_check,core,HAL CCSD,2018-07-10 00:00:00,core,hatp: hierarchical agent-based task planner: demonstration,,"International audienceHierarchical Task Network (HTN) planning is a proven approach to solving complex, real world planning problems more efficiently than planning from first principles when “standard operating procedures” (or “recipes”) can be supplied by the user. By planning for tasks in the same order that they are later executed, total-order HTN planners always know the complete state of the world at each planning step. This enables writing more expressive planning domains than what is possible in partial-order HTN planning, such as preconditions with calls to external procedures.Such features have facilitated the use of total-order HTN planners in agent systems and seen them excel in AI games.This paper describes the Hierarchical Agent-based Task Planner (HATP), a total-order HTN planner. Since its first implementation, HATP has had various extensions and integrations over the years, such as support for splitting a solution into multiple streams and assigning them to the agents in the domain; modelling their beliefs as distinct world states; allowing “social rules” to be included by the user to define what kind of agent behaviour is appropriate; allowing tasks to be planned by taking the human’s safety and comfort into account; and to interleave HTN and geometric planning.Since many of these implementations have remained prototypes, we have significantly enhanced them as well as HATP itself, and integrated them into a stand-alone HATP distribution, which is now available as open source software (under a BSD 2-Clause License).This paper presents some of our recent improvements to HATP, and gives an overview of its user-friendly language, which treats agents as distinct entities; its mechanisms for effective control over decomposition; and its integration into our robotics framewor",science,981,not included
,to_check,core,Palmas,2018-09-29 00:00:00,core,análise da implantação do parque aquícola da uhe do lajeado através de métodos multicritérios de decisão – uma abordagem socioambiental e dos fatores de risco,,"This study is derived from the policy of the Brazilian government to increase fish production through the promotion of fish farming in network ponds in aquaculture parks. This project aims to explore the implementation and operation of the Sucupira Aquaculture Park (PAS) of the Lajeado Hydroelectric Power Plant (Palmas / TO) through the use of multicriteria decision methods, aiming at a socioenvironmental approach of the risk factors of the implementation of this project through the proposition of sustainability indicators. Scientometry has demonstrated that the production of knowledge related to fish culture in net tanks and artisanal fishing in reservoirs is indexed and has a multidisciplinary character. The articles are technicist and productivist, with little regard for environmental and socioeconomic factors, nor the public policies for the development and promotion of fishery and fishery activities, which in a way explains the dissatisfaction with the activity by the fish farmers of the Bom Peixe Association , as well as the inefficiency of the public policy of transforming the artisanal fisherman into an entrepreneur. The Social Network Analysis (ARS) makes clear that there is interconnection among all actors, but the protagonism of this network is dominated by public institutions and the process of communication between the various institutional actors is two-way, while between fish farmers and fishermen is of a unidirectional character, making them only subjects of actions. The Trophic State Index (EIT) was used as a monitoring tool for aquaculture activities in PAS. Limnological variables, with the exception of total phosphorus, despite small variations of oxygen and total solids, are in accordance with the recommendations of CONAMA Resolution 357/05. The data allow us to conclude that the aquaculture area presented a predominance of supereutrophic trophic state and that the trophic state of the reservoir in the vicinity of the tanks is strongly influenced by anthropic activities, mainly in the rainy season. In the dry season, there is an increase in the phosphorus concentration in the water, leading to a hypereutrophic state. It is not possible to say that aquaculture is an impactful activity and its effects are difficult to predict in the long term, as well as the behavior of the reservoir through increased production in the parks, however, if there is no monitoring and efficient control over water quality , fish farming may promote degradation of the aquatic ecosystem. The analysis of the technical, economic and financial feasibility of investment in aquaculture projects of tambaqui production in network tanks used the economic indicators Effective Operational Cost (COE), Total Operating Cost (COT), Total Gross Income (RBT), Gross Margin (MB), Net Margin (ML), Profit (L) and Return Rate of Capital (TRC). The analysis of TRC presented negative results and the system of production in excavated nurseries proved to be more efficient. The system of production in network tanks is an alternative of lower cost and time of implantation, but with uncertain return. The socioeconomic profile of the producers is considered to be incompatible with the business and technological activity of aquaculture in network tanks. In the producers' view, the activity is still a future promise and the public policy of turning fishermen into fish farmers proved ineffective. According to TRC, the activity is unsustainable and has been showing negative results, which indicates its low competitiveness. Sustainability and governance indicators were developed for the development of fish farming in the PAS, which, through social, economic and environmental subindices, generated a Sustainability Index (ISUS) of the enterprise. The data allowed to conclude that the ISUS was classified as not very sustainable (0.37), mainly in the economic and social dimensions. The analysis of governance was identified in interviews with the main agents of the productive chain, making it clear that the lack of governance structures was noticeable in all segments analyzed, which portrays the real stage of disorganization and inefficiency of the productive chain of fish farming in PAS.Este estudo deriva da política do governo brasileiro para incremento da produção de pescado por meio do fomento à piscicultura em tanques rede nos parques aquícolas. Este projeto visa explorar a implantação e operação do Parque Aquícola Sucupira (PAS) da UHE Lajeado (Palmas/TO) através do uso de métodos multicritérios de decisão, com objetivo de realizar uma abordagem socioambiental dos fatores de risco da implantação deste empreendimento através da proposição de indicadores de sustentabilidade. A cienciometria, demonstrou que a produção de conhecimento relacionado ao cultivo de peixes em tanques rede e a pesca artesanal em reservatórios está indexada e é de caráter multidisciplinar. Os artigos possuem caráter tecnicista e produtivista, pouco considerando os fatores ambientais e sócio econômicos, nem tampouco as políticas públicas de desenvolvimento e fomento às atividades piscícolas e ou pesqueiras, o que de certa forma explica a insatisfação com a atividade pelos piscicultores da Associação Bom Peixe, bem como a ineficiência da política pública de transformar o pescador artesanal em empreendedor. A Análise de Redes Sociais (ARS) deixa claro que há interligação entre todos os atores, porém o protagonismo dessa rede está dominado por instituições públicas e o processo de comunicação entre os diversos atores institucionais é de caráter bidirecional, enquanto que entre os piscicultores e pescadores é de caráter unidirecional, tornando-os apenas sujeitos das ações. O Índice de Estado Trófico (IET) foi utilizado como ferramenta de monitoramento de atividades aquícolas no PAS. As variáveis limnológicas, com exceção do fósforo total, apesar das pequenas variações de oxigênio e sólidos totais, estão de acordo com as recomendações da Resolução CONAMA 357/05. Os dados permitem concluir que a área aquícola apresentou predominância de estado trófico supereutrófico e que o estado trófico do reservatório nas proximidades dos tanques rede sofre forte influência externa de atividades antrópicas, principalmente no período chuvoso. No período da seca, ocorre aumento na concentração de fósforo na água, levando a um estado hipereutrófico. Não é possível afirmar que a aquicultura seja uma atividade impactante e seus efeitos são difíceis de serem previstos em longo prazo, bem como o comportamento do reservatório mediante aumento de produção nos parques, porém, caso não haja monitoramento e controle eficiente sobre a qualidade da água, a piscicultura pode vir a promover degradação do ecossistema aquático. A análise da viabilidade técnica, econômica e financeira de investimento em projetos aquícolas de produção de tambaqui em tanques rede utilizou os indicadores econômicos Custo Operacional Efetivo (COE), Custo Operacional Total (COT), Renda Bruta Total (RBT), Margem Bruta (MB), Margem Líquida (ML), Lucro (L) e Taxa de Retorno do Capital (TRC). A análise da TRC apresentou resultados negativos e o sistema de produção em viveiros escavados se mostrou mais eficiente. O sistema de produção em tanques rede é uma alternativa de menor custo e tempo de implantação, porém com retorno incerto. O perfil socioeconômico dos produtores é considerado incompatível com a atividade empresarial e tecnificada de aquicultura em tanques rede. Na visão dos produtores a atividade ainda é uma promessa futura e a política pública de transformar pescadores em piscicultores se mostrou ineficaz. De acordo com a TRC, a atividade é insustentável e vem apresentando resultados negativos, o que denota sua baixa competitividade. Foram elaborados indicadores de sustentabilidade e governança para o desenvolvimento da piscicultura no PAS, que através de subíndices sociais, econômicos e ambientais geraram um Índice de Sustentabilidade (ISUS) do empreendimento. Os dados permitiram concluir que o ISUS foi classificado como pouco sustentável (0,37), principalmente nas dimensões econômicas e sociais. A análise de governança foi identificada em entrevistas aos principais agentes da cadeia produtiva, ficando claro que a falta de estruturas de governança foi perceptível em todos os segmentos analisados, o que retrata o real estagio de desorganização e ineficiência da cadeia produtiva da piscicultura no PAS",science,982,not included
,to_check,core,Modern innovations in corporate reporting,2017-01-01 00:00:00,core,10.21272/mmi.2017.1-10.,,"В умовах перегляду ролі обліку, звітності та її аудиту у посткризовий період особливого значення
набувають технологічні інновації, пов’язані зі зростанням масиву облікових даних, швидкості їх
поширення, необхідності своєчасного та поглибленого аналізу та аудиту з метою прийняття рішень
стейкхолдерами на фінансових ринках. Проведений структурний аналіз сучасних інноваційних підходів у
складанні та поданні корпоративної звітності на основі теорії дифузії інновацій дозволяє окреслити
перспективи у їх застосуванні з метою забезпечення транспарентності корпоративної звітності.В условиях пересмотра роли учета, отчетности и ее аудита в посткризисный период особое значение
приобретают технологические инновации в этих сферах, связанные с ростом массива учетных данных, скорости их распространения, необходимости своевременного и углубленного анализа и аудита в целях принятия решений стейкхолдерами на финансовых рынках. Проведенный структурный анализ современных инновационных подходов в составлении и представлении корпоративной отчетности на основе теории диффузии инноваций позволяет очертить перспективы в их применении с целью обеспечения транспарентности корпоративной отчетности.Fragmentation of reformatting trends in corporate reporting, differentiation national approaches to the regulation of accounting systems and the lack of academic studies of accounting innovations cosed an urgent need to identify key innovations in the development of corporate reporting, analyze the feasibility of their use.
The aim of the article. The aim of the article is structural analysis of modern innovations in preparing and presenting corporate reporting and researching the prospects of their application based on the theory of diffusion of innovations The results of the analysis. Innovation diffusion theory as in instrument of interdisciplinary analysis allows to describe key parameters in spreading of actual accounting, audit and corporate reporting innovations, for example: relative advantage, compatibility, complexity of innovation, trialability and observability. Located in the order of accounting cycle (accounting of
different types of operations, reporting after the end of the period, and rendering an auditor’s opinion), discussed technological innovations give an idea about the prospects of modern system of corporate reporting. Big data, accounting for Environmental, Social and corporate Governance – criteria, preparation and presentation of integrated reporting, real-time reporting, conducting continuous audit with computer assisted audit technologies (CAAT), Global General Accepted Accounting Principles (GGAAP) and development of XBRL are the typical technological innovations that accompany the transformation corporate reporting process based on a new paradigm. Conclusions and directions of further researches. Big data is not only the product of accounting and reporting in real time, but also the basis for the continuous audit of the real-time reporting with advanced analytical technologies – CAAT. Taking into
account ESG-criteria in the course of business, the implementation of responsible investment requires the development of appropriate indicators and its accounting measurement for the demonstration of progress in achieving sustainability and corporate social responsibility. This ESG-criteria and indicator (non-financial information), spreading real-time reporting lead to the emergence of integrated reporting. It causes changes in approaches to public audit and confirmation of this type of reporting with the CAAT, artificial intelligence systems. GGAAP development, improvement of methodology of integrated reporting standards and International Standards on Quality, Control, Auditing, Review Other Assurance and Related Services acts as a response to the need to restoring confidence in the reporting on globalized financial marketsand independent verification of its quality. XBRL as a technology innovation serving as connecting-link the one that helps transmit understandable reporting to all stakeholders, minimizing the time and costs for its processing and analysis through the taxonomy. The combination of considered innovation are grounded necessitates of construction of convergent model of accounting, reporting and auditing, which fully meets the information needs of stakeholders on globalized financial markets and with the spreading of sustainability concept",science,984,not included
,to_check,core,Predicting large scale fine grain energy consumption,2017-01-01 00:00:00,core,10.1016/j.egypro.2017.03.271,https://core.ac.uk/download/pdf/234915328.pdf,"Today a large volume of energy-related data have been continuously collected. Extracting actionable knowledge from such data is a multi-step process that opens up a variety of interesting and novel research issues across two domains: energy and computer science. The computer science aim is to provide energy scientists with cutting-edge and scalable engines to effectively support them in their daily research activities. This paper presents SPEC, a scalable and distributed predictor of fine grain energy consumption in buildings. SPEC exploits a data stream methodology analysis over a sliding time window to train a prediction model tailored to each building. The building model is then exploited to predict the upcoming energy consumption at a time instant in the near future. SPEC currently integrates the artificial neural networks technique and the random forest regression algorithm. The SPEC methodology exploits the computational advantages of distributed computing frameworks as the current implementation runs on Spark. As a case study, real data of thermal energy consumption collected in a major city have been exploited to preliminarily assess the SPEC accuracy. The initial results are promising and represent a first step towards predicting fine grain energy consumption over a sliding time window",science,985,included
,to_check,core,A new method for unveiling open clusters in Gaia New nearby open clusters confirmed by DR2,2018-10-16 00:00:00,core,10.1051/0004-6361/201833390,,"International audienceContext. The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in astronomy. It includes precise astrometric data (positions, proper motions, and parallaxes) for more than 1.3 billion sources, mostly stars. To analyse such a vast amount of new data, the use of data-mining techniques and machine-learning algorithms is mandatory.Aims. A great example of the application of such techniques and algorithms is the search for open clusters (OCs), groups of stars that were born and move together, located in the disc. Our aim is to develop a method to automatically explore the data space, requiring minimal manual intervention.Methods. We explore the performance of a density-based clustering algorithm, DBSCAN, to find clusters in the data together with a supervised learning method such as an artificial neural network (ANN) to automatically distinguish between real OCs and statistical clusters.Results. The development and implementation of this method in a five-dimensional space (l, b, ϖ, μα*, μδ) with the Tycho-Gaia Astrometric Solution (TGAS) data, and a posterior validation using Gaia DR2 data, lead to the proposal of a set of new nearby OCs. Conclusions. We have developed a method to find OCs in astrometric data, designed to be applied to the full Gaia DR2 archive",science,987,not included
,to_check,core,The overlooked potential of Generalized Linear Models in astronomy-II: Gamma regression and photometric redshifts,2015-01-01 00:00:00,core,https://core.ac.uk/download/333612214.pdf,'Elsevier BV',"Machine learning techniques offer a precious tool box for use within astronomy to solve problems involving so-called big data. They provide a means to make accurate predictions about a particular system without prior knowledge of the underlying physical processes of the data. In this article, and the companion papers of this series, we present the set of Generalized Linear Models (GLMs) as a fast alternative method for tackling general astronomical problems, including the ones related to the machine learning paradigm. To demonstrate the applicability of GLMs to inherently positive and continuous physical observables, we explore their use in estimating the photometric redshifts of galaxies from their multi-wavelength photometry. Using the gamma family with a log link function we predict redshifts from the PHoto-z Accuracy Testing simulated catalogue and a subset of the Sloan Digital Sky Survey from Data Release 10. We obtain fits that result in catastrophic outlier rates as low as ~1% for simulated and ~2% for real data. Moreover, we can easily obtain such levels of precision within a matter of seconds on a normal desktop computer and with training sets that contain merely tho nds of galaxies. Our software is made publicly available as a user-friendly package developed in Python, R and via an interactive web application. This software allows users to apply a set of GLMs to their own photometric catalogues and generates publication quality plots with minimum effort. By facilitating their ease of use to the astronomical community, this paper series aims to make GLMs widely known and to encourage their implementation in future large-scale projects, such as the Large Synoptic Survey Telescope",science,988,not included
,to_check,core,La Cartella Clinica Informatizzata nel percorso diagnostico-assistenziale del Malato Raro: sviluppo e implementazione di un sistema di raccolta e analisi dell'informazione clinica fenotipica e genotipica. Computerized Medical Record in the diagnostic-care of the Rare Diseases: development and implementation of a system for the collection and analysis of clinical phenotypic and genotypic data,2016-01-29 00:00:00,core,https://core.ac.uk/download/78395279.pdf,,"Internationally there is no single definition of a rare disease: in European countries and beyond is determined on the basis of different thresholds of prevalence, in accordance with regulatory requirements: in the European Union (EU), a disease is rare if it has a population prevalence of less than or equal to 1/2000 inhabitants (0.05 %). Rare diseases, from a clinical and epidemiological point of view-, are a very heterogeneous group, with about 5000-8000 entities, mostly with genetic origin, often severe, chronically debilitating, progressive and potentially fatal, representing a major public health problem; nevertheless, in the past, have received little scientific interest, are hardly included in international classifications and barely visible at the level of health information, for many of them also epidemiologic data are not yet available. The main reason seems to be the absence of a system of correct coding and traceability in health information systems: the existing classification systems for collecting phenotypic and genotypic data, are widely heterogeneous, lacking a standard terminology, slightly inter-operable, solely dedicated to the classification of the phenotype or supplied with database features of variants and related phenotypes profiles. Some of these databases are already operating, other developing, along with the implementation of NGS technologies, which guaranteed the medical scientific community, compared to traditional techniques, the study of a wide range of genetic diseases, with the collapse of cost and time of data analysis. Worldwide were established centers and research projects in highly specialized techniques of genome sequencing and development of databases of genomic variants, that have made available a large variety and quantity of new information, often difficult to manage and interpret. The commitment of the scientific community is therefore the systematic organization of that huge amount of data, potentially useful to the correct interpretation of the pathogenic significance of genomic variants in relation to the clinical phenotype. Based on emerging data from the literature, the records of patients and pathologies constitute interesting tools for the development of knowledge useful for the clinical and epidemiological aspect and in the field of rare diseases. Many existing registers must address issues of quality, accessibility and standardization of data, optimization of resources, and interoperability, related to fragmentation of coding systems of diseases. The situation on European and international scene seems to favor the establishment of global registers or common platforms for coordination of all existing and future registers, following a uniform approach, coordinated and of the highest quality. The implementation of this model is currently a strategic objective of the European community within the project EPIRARE and of the US community in the project on Global Rare Diseases Registry (GRDR). In the evolutionary path that supports the implementation of the public health service to improve the exercise of the right to healthcare of citizens, the current development of innovative information technologies, which make available comprehensive information, shared and able to guarantee citizens the best continuity of care, is critical. In Italy, the Electronic Health Record (FSE) is the computerized collection of data of the clinical history of an individual, designed as part of the National Health Service (SSN) according to the National Guidelines issued by the Ministry of Health in November 2010. The process is slow, but with the introduction of the SFE is to be hoped that in the near future, we witness a revolutionary change in the relationship between health infrastructure and citizen, so far characterized by communication barriers, rigid and cumbersome bureaucratic procedures and paperwork with improvement of the flow of data and the diagnostic process and healthcare. Also in the international context is documented the usefulness of the so-called ""Electronic Health Record"" (EHR) or ""Personal Health Record"" (PHR) to assist this process in healthcare, as well as for measuring and monitoring the quality of care and for translational research, but the construction and development of the FSE are still at a preliminary stage and the challenges of making the application infrastructure are numerous, especially for developing countries.

Another major challenge on many fronts to health systems in the field of rare diseases is that represented by the heterogeneous group of patients without diagnosis, individuals with known diseases incorrectly recognized, experiencing delays and diagnostic errors, or individuals with unknown diseases; these patients over-use inadequate diagnostic pathways and their care needs are not satisfied, with high human and social costs.

To date the problem of patients without diagnosis was universally recognized worldwide as central, to the planning of health care networks for patients with a rare disease, but its still unresolved: the attempts made in the USA with Undiagnosed Diseases Program (UDP) of National Institute of Health (NIH) clinical Center have shown promising results, but appeared poorly sustainable in clinical practice and still have limited epidemiological value.

Nationally some Accredited Centres for excellence in the diagnosis and treatment of specific rare diseases or groups of related rare diseases, have established projects targeted on finding solutions to methodological collection of clinical phenotype-genotype correspondence, supporting the diagnostic process of patients with rare diseases, with special attention to patients without a diagnosis or with a generic presumptive diagnosis: Region-University research Project ""Next Generation Sequencing and Gene Therapy to diagnose and Cure rare Diseases in Emilia Romagna (rarer)"" and Ministerial Project ""A multicenter collaborative research network for the identification and study of rare undiagnosed patients: the impact on the rare disease National Health Service network (UnRareNet)"".

In the broader context of these research projects present PhD project designed to create a unique collection of analytical data, describing phenotype and genotype features of patients with known or suspected diagnosis, between groups of selected rare diseases, based on a computerized platform, with implementation of the model of the Regional Registry for Rare Diseases, already used in the Veneto Region.

A further objective was the creation of a specific product useful for clinical management in the field of rare diseases, especially with respect to efficiency, speed and accuracy of diagnostic assessment, through the transfer of the instrument for collection of clinical phenotype-genotype data in patients with diseases or groups of selected rare diseases in the form of generalized Computerized Medical Record (CCI).

The form CCI should have features which cut across rare diseases registry and the hospital medical records, dedicated to a single episode of care, and should structur on the emerging pattern of computerized personal health record, such as in Italy, the ""Fascicolo Sanitario Elettronico"".

The added value is the integration of the relational database research potential with the clinical applications of CCI in the practical management and care of patients with rare diseases. It was also planned the networking of CCI as part of a shared use of clinical, genetic and healthcare data in real contexts of clinical excellence, such as those offered by the RARER and UnRareNet projects.

The project is structured in three different phases:

1. identification and sharing, within the common RARER protocol, of a suitable methodology for the construction of 8 different disease registries, with implementation of the clinical database and structuring a form of collection of genotypic data; 

2. realization and network sharing of module CCI such as a sustainable product in a clinical setting, containing clinical information in a relational database; 

3. implementation of the acquisition data system and module CCI, with the expansion of the patient population in the context of the project UnRareNet, data analysis, identifying the correlation between genotype and phenotype pictures, and design of an expert system able to manage complex procedures for data processing. 

The research project proposed the implementation of a relational database with the collection through CCI of clinical data in patients with rare diseases with known or suspected diagnosis, in the context of some rare diseases of specific interest.

The clinical entities and major and minor entities, describing phenotype and genotype features are classified in the database according to a classificatory nodular and multihierarchical like a tree method, which integrates uni and multidimensional internationally recognized classification systems and, at the same timem respects the anatomical structure and function of the human organism.

The database includes 72,360 records, distributed between the main and secondary entities (signs, symptoms, comorbidity, impairments, diagnostic, attributes) and relationships between entities, 117 different clinical entities, 63 different genes and 1,186 total patients. The results obtained during the development and implementation of the system of collection of clinical information, with the implementation of the module CCI in the context of the project RARER, have proved the usability of the product in a preliminary context; the expansion of the clinical application in the context of the project UnRareNet, has produced promising results and the system showed a good performance in terms of similar and dissimilar phenotypes, managing fully clinical information regarding total number of different rare diseases and proving thus be generalized flexible and interoperable. The data are still limited at the time, but having analytical characteristics pre-coded, are immediately usable for future complex processing. The collection of genetic information and integration with clinical information appeared feasible with promising results, although the project UnRareNet this is still absolutely at a preliminary stage; patients without diagnosis is about 10% of the total, but more than half of them do not yet have genetic investigation, which proved to be useful to confirm the diagnosis in 53% of cases with known disease. On the total sample of patients, the use of NGS technologies seems to be limited at this stage, but the results highlight the potential of the modern techniques of genome sequencing in the implementation of the diagnostic process; expanding the population of patients enrolled and the targeted application of these new technologies could allow wider knowledge on the phenotypic and genotypic profile of the considered diseases. Having a broader patient population, the creation of the expert system, of which the foundations were laid in the context of this project, will be possible through the collection of theoretical frequencies of clinical and diagnostic findings of some groups of rare diseases related (metabolic, mitochondrial), arising from the international literature and to be included in the system for processing of clinical scenarios. The expert system will allow the validation of diagnoses simulated by collecting new cases with known diagnosis, and the formulation of new diagnoses of known diseases or identification of new clinical entities, even with use of neural networks or Kohonen networks and Fuzzy methods. The methodology for collecting phenotypic and genotypic information in patients with rare diseases, proposed in this PhD project, proved sufficient to satisfy the requirements of applicability and suggested a number of future applications, which aim the expansion of knowledge on genomic variability of rare diseases and to reduce the number of patients without a diagnosis",science,990,not included
,to_check,core,,2014-01-15 00:00:00,core,the deep sound of a global tweet: sonic window #1 (a real time sonification),,"Abstract. People listen music, than they share emotions writing thinks about music on Twitter, a software analyzes the tweet with music as argument, and report some informations about these spoken emotions. I wrote a patch in Max/MSP that sonify in real time the global emotion lived by the twitter user music writers, it produce new music and this new music produce emotions also, and if you want you can write about that in Twitter, in this way the social network produce new emotions from its previous emotions, an AI generated emotion",science,991,not included
,to_check,core,,2015-11-26 00:00:00,core,grasp strategy for the scheduling of oil well development activities [estratégia grasp para escalonar atividades de desenvolvimento de poços de petróleo],,"Before promising oil basins are effectively developed into productive oil wells, it is necessary to conduct several drilling, completion and interconnection activities at such sites. The scheduling of these activities must satisfy several conflicting restrictions and also try to maximize the oil production within a given time frame. The present study describes a Greedy Randomized Adaptive Search Procedure (GRASP) to schedule the development of offshore oil wells. The results are compared to a Constraint Program (CP) tool developed by Petrobras, used and well accepted. Comparative results conducted in real instances indicate that the implementation of GRASP exceeds the RP tool, and produces significant production increase solutions.21217239Aiex, R.M., Binato, S., Resende, M.G.C., Parallel GRASP with pathrelinking for job shop scheduling (2003) Parallel Computing, 29 (4), pp. 393-430. , AprAiex, R.M., Resende, M.G.C., Pardalos, P.M., Toraldo, G., Grasp with path relinking for the three-index assignment problem (2001) AT&T Labs Research Technical Report, p. 43. , Florham Park, NJBard, J.F., Feo, T.A., Operations sequencing in discrete parts manufacturing (1989) Management Science Hannover, 35 (2), pp. 249-255. , FebBinato, S., Hery, W.J., Loewenstern, D., Resende, E.M.G.C., A greedy randomized adaptive search procedure for job shop scheduling (2002) Essays and Surveys on Metaheuristics, pp. 59-79. , Dordrecht: Kluwer AcademicBresina, J.L., Heuristic-biased stochastic sampling (1996) National Conference on Artificial Intelligence, (13). , Portland. Proceedings of the-K [S.l.: s.n.], 1996. p. 271-278Canuto, S.A., Resende, M.G.C., Ribeiro, E.C.C., Local search with perturbations for the prize-collecting steiner tree problem in graphs (2001) Networks [S.L.], 38, pp. 50-58Colorni, A., Dorigo, M., Maniezzo, V., Ant system for job-shop scheduling (1994) Belgian Journal of Operations Research, Statistics, and Computer Science, [S.L.], 1 (34), pp. 39-53Dorigo, M., Di Caro, G., Gambardella, L.M., Ant algorithms for discrete otimization (1998) Bruxelles: Universite Libre de Bruxelles, p. 36. , Technical Report IRIDIA/98-10). To appear in Artificial Life MIT Press 1999Feo, T.A., Bard, J.F., Holland, E.S., Facility-wide planning and scheduling of printed wiring board assembly (1995) Operations Research New York, 43, pp. 219-230Fleurent, C., Glover, F., Improved constructive multistart strategies for the quadratic assignment problem using adaptive memory (1999) Informs Journal on Computing, Cincinnati, 11 (2), pp. 198-204Glover, F., Tabu search and adaptive memory programming: Advances, applications and challenges (1996) Interfaces in Computer Science and Operations Research, p. 75. , BARR, R.S.HELGASON, R.V.KENNINGTON, J.L. (Ed.). [Dordrecht]: Kluwer AcademicGlover, F., Multi-start and strategic oscillation methods: Principles to exploit adaptive memory (2000) Interfaces in Computer Science and Operations Researchs, p. 24. , [Dordrecht]: BARR, R.S.HELGASON, R.V.KENNINGTON, J.L. (Ed.). Kluwer AcademicGlover, F., Laguna, M., (1997) Tabu Search, p. 382. , Boston: Kluwer AcademicGlover, F., Laguna, M., Marti, E.R., Fundamentals of scatter search and path relinking (2000) Control and Cybernetics Warszawa, 39 (3), pp. 653-684(1999) ILOG Solver 4.4 Reference Manual. [S.l.]: ILOG, , ILOGNascimento, J.M., (2002) Hybrid Computational Tools for the Optimization of the Production of Petroleum in Deep Waters, , Dissertacao (Mestrado) - Universidade de CampinasMarriott, K., Stuckey, P.J., (1998) Programming with constraints: an introduction, p. 467. , Cambridge: Massachusetts Institute of Technology PressResende, M.G.C., Festa, P., An updated bibliography of GRASP (2003) AT&T Labs Research Technical Report TD-5SB7BK, Florham Park, NJ, p. 10. , October 14Resende, M.G.C., Ribeiro, C.C., A GRASP with path-relinking for private virtual circuit routing (2001) AT&T Labs Research Technical Report, Florham Park, NJ, p. 19. , June 15Resende, M.G.C., Ribeiro, C.C., Greedy randomized adaptive search procedure (2001) AT&T Labs Research Technical Report, p. 29. , Florham Park, NJ Sept. Revision 2, Aug. 29, 2002. To appear in -State of the Art Handbook in Metaheuristics-, F. Glover and G. Kochenberger, eds. Kluwer 2002Rodrigo, M., Maniezzo, V., Colorni, A., The Ant System: Optimization by a colony of cooperating agents (1996) IEE Transactions on Systems Man and Cybernetics, 26 (1), pp. 1-13Roy, B., Sussmann, B., Les problemes d-Ordonnancement avec contraintes disjonctive (1964) Note Ds No. 9 Bis, , Paris: SEMAYannakakis, M., Computational complexity (1997) Local search in combinatorial optimization, pp. 19-55. , AARTS, E.LENSTRA, J.K. (Ed.). Chichester: J. Wile",science,992,not included
,to_check,core,,2012-11-21 00:00:00,core,'mit press - journals',Installation of a very broad band borehole seismic station in Ferrara (Emilia),"The Istituto Nazionale di Geofisica e Vulcanologia (INGV) is the Italian agency devoted to monitor in real time the seismicity on the Italian territory.  The seismicity in Italy is of course variable in time and space, being also very much dependant on local noise conditions. Specifically, monitoring seismicity in an alluvial basin like the Po one is a challenge, due to consistent site effects induced by soft alluvial deposits and bad coupling with the deep bedrock (Steidl et al., 1996). This problem was tackled by INGV first with the Cavola experiment (Bordoni et al., 2007), where a landslide was seismically characterized using a seismic array and also down-hole logging of P- and S-wave travel times at a borehole drilled within the array; later, with an ad hoc project in 2000-2001, with the first installation of a broad band seismic station nearby Ferrara in a borehole of 135 meters depth. Comparison of recordings with a surface seismic station indicated a noise reduction of 2 decades in power spectral density at frequencies larger than 1.0 Hz (Cocco et al., 2001). The instrumentation in Ferrara has been working for several months but after that the seismic station was discontinued due to lack of maintenance manpower.
The Centro di Ricerche Sismologiche (CRS, Seismological Research Center) of the Istituto Nazionale di Oceanografia e di Geofisica Sperimentale (OGS, Italian National Institute for Oceanography and Experimental Geophysics) in Udine (Italy) after the strong earthquake of magnitude M=6.4 occurred in 1976 in the Italian Friuli-Venezia Giulia region, started to operate the Northeastern Italy (NI) Seismic Network: it currently consists of 15 very sensitive broad band and 21 simpler short period seismic stations, all telemetered to and acquired in real time at the OGS-CRS data center in Udine (Fig. 1).
Real time data exchange agreements in place with other Italian, Slovenian, Austrian and Swiss seismological institutes lead to a total number of about 100 seismic stations acquired in real time, which makes the OGS the reference institute for seismic monitoring of Northeastern Italy. Since 2002 OGS-CRS is using the Antelope software suite on several workstations plus a SUN cluster as the main tool for collecting, analyzing, archiving and exchanging seismic data, initially in the framework of the EU Interreg IIIA project “Trans-national seismological networks in the South-Eastern Alps”. SeisComP is also used as a real time data exchange server tool (Bragato et al., 2011).
Among the various Italian institution with which OGS is cooperating for real time monitoring of local seismicity there is the Regione Veneto (Barnaba et al., 2012). The Southern part of the Veneto Region stands on the Po alluvial basin: earthquake localization and characterization is here again affected in this area by the presence of soft alluvial deposits. OGS ha already experience in running a local seismic network in difficult noise conditions making use of borehole installations (Priolo et al., 2012) in the case of the monitoring of a local storage site for the Italian national electricity company ENEL. Following the ML=5.9 earthquake that struck the Emilia region around Ferrara in Northern Italy on May 20, 2012 at 02:03:53 UTC, a cooperation of INGV, OGS, the Comune di Ferrara and the University of Ferrara lead to the reinstallation of the very broad band borehole seismic station in Ferrara. The aim of the OGS intervention was on one hand to extend its real time seismic monitoring capabilities toward South-East (Fig. 1), including Ferrara and its surroundings, and on the other hand to evaluate the seismic  response at the site.
As concerns the superficial geology of the area where the borehole seismic station  has been installed, the outcropping materials are represented by alluvial deposits of different environments, like channel and proximal levee, inter-fluvial, meander and swamps deposits. As  a consequence, the outcropping deposits are everywhere Holocene in age substantially loose or poorly compacted in the first meters-decameters and granulometrically could vary from clay to coarse sand.
Two preliminary reports prepared by the Italian Department of Civil Defense (Dipartimento Nazionale di Protezione Civile) in collaboration with other institutions describe the data  recorded by the national accelerometric network and complemented by additional data recorded by a number of temporary stations (Dolce et al., 2012a; Dolce et al., 2012b). These reports bear witness of strong ground motion values with an acceleration peak of about 0.9 g in the vertical component recorded during the ML=5.8 earthquake of May 29, 2012 by the Mirandola station, located at about 2 km from the epicentre. The analysis of the seismic noise recorded at some stations shows a quite pronounced peak of the horizontal-to-vertical spectral ratio (H/V) in the frequency range of 0.6 – 0.9 Hz common to all stations. Finally, strong evidence of liquefaction phenomena are reported at several sites (e.g.: S. Carlo, S. Agostino and Mirabello), most of which have been attributed to the occurrence of saturated sandy layer(s) at shallow depth deposited along an abandoned reach of the Reno River (Papathanassiou et al., 2012).
Details of the station configuration and installation will be outlined, with first results.PublishedPotenza (Italy)1.1. TTC - Monitoraggio sismico del territorio nazionaleope",science,993,not included
,to_check,core,,2014-01-01 00:00:00,core,country:prt,Analysis of Robust Implementation of an EMG Pattern Recognition Based Control,"Control of active hand prostheses is an open challenge. In fact, the advances in mechatronics made available prosthetic hands with multiple active degrees of freedom; however the predominant control strategies are still not natural for the user, enabling only few gestures, thus not exploiting the prosthesis potential. Pattern recognition and machine learning techniques can be of great help when applied to surface electromyography signals to offer a natural control based on the contraction of muscles corresponding to the real movements. The implementation of such approach for an active prosthetic system offers many challenges related to the reliability of data collected to train the classification algorithm. This paper focuses on these problems and propose an implementation suitable for an embedded system. Copyright \ua9 2014 SCITEPRESS - Science and Technology Publications. All rights reserved",science,994,not included
,to_check,core,OGS,2012-11-21 00:00:00,core,installation of a very broad band borehole seismic station in ferrara (emilia),https://core.ac.uk/download/pdf/41152694.pdf,"The Istituto Nazionale di Geofisica e Vulcanologia (INGV) is the Italian agency devoted to monitor in real time the seismicity on the Italian territory.  The seismicity in Italy is of course variable in time and space, being also very much dependant on local noise conditions. Specifically, monitoring seismicity in an alluvial basin like the Po one is a challenge, due to consistent site effects induced by soft alluvial deposits and bad coupling with the deep bedrock (Steidl et al., 1996). This problem was tackled by INGV first with the Cavola experiment (Bordoni et al., 2007), where a landslide was seismically characterized using a seismic array and also down-hole logging of P- and S-wave travel times at a borehole drilled within the array; later, with an ad hoc project in 2000-2001, with the first installation of a broad band seismic station nearby Ferrara in a borehole of 135 meters depth. Comparison of recordings with a surface seismic station indicated a noise reduction of 2 decades in power spectral density at frequencies larger than 1.0 Hz (Cocco et al., 2001). The instrumentation in Ferrara has been working for several months but after that the seismic station was discontinued due to lack of maintenance manpower.
The Centro di Ricerche Sismologiche (CRS, Seismological Research Center) of the Istituto Nazionale di Oceanografia e di Geofisica Sperimentale (OGS, Italian National Institute for Oceanography and Experimental Geophysics) in Udine (Italy) after the strong earthquake of magnitude M=6.4 occurred in 1976 in the Italian Friuli-Venezia Giulia region, started to operate the Northeastern Italy (NI) Seismic Network: it currently consists of 15 very sensitive broad band and 21 simpler short period seismic stations, all telemetered to and acquired in real time at the OGS-CRS data center in Udine (Fig. 1).
Real time data exchange agreements in place with other Italian, Slovenian, Austrian and Swiss seismological institutes lead to a total number of about 100 seismic stations acquired in real time, which makes the OGS the reference institute for seismic monitoring of Northeastern Italy. Since 2002 OGS-CRS is using the Antelope software suite on several workstations plus a SUN cluster as the main tool for collecting, analyzing, archiving and exchanging seismic data, initially in the framework of the EU Interreg IIIA project “Trans-national seismological networks in the South-Eastern Alps”. SeisComP is also used as a real time data exchange server tool (Bragato et al., 2011).
Among the various Italian institution with which OGS is cooperating for real time monitoring of local seismicity there is the Regione Veneto (Barnaba et al., 2012). The Southern part of the Veneto Region stands on the Po alluvial basin: earthquake localization and characterization is here again affected in this area by the presence of soft alluvial deposits. OGS ha already experience in running a local seismic network in difficult noise conditions making use of borehole installations (Priolo et al., 2012) in the case of the monitoring of a local storage site for the Italian national electricity company ENEL. Following the ML=5.9 earthquake that struck the Emilia region around Ferrara in Northern Italy on May 20, 2012 at 02:03:53 UTC, a cooperation of INGV, OGS, the Comune di Ferrara and the University of Ferrara lead to the reinstallation of the very broad band borehole seismic station in Ferrara. The aim of the OGS intervention was on one hand to extend its real time seismic monitoring capabilities toward South-East (Fig. 1), including Ferrara and its surroundings, and on the other hand to evaluate the seismic  response at the site.
As concerns the superficial geology of the area where the borehole seismic station  has been installed, the outcropping materials are represented by alluvial deposits of different environments, like channel and proximal levee, inter-fluvial, meander and swamps deposits. As  a consequence, the outcropping deposits are everywhere Holocene in age substantially loose or poorly compacted in the first meters-decameters and granulometrically could vary from clay to coarse sand.
Two preliminary reports prepared by the Italian Department of Civil Defense (Dipartimento Nazionale di Protezione Civile) in collaboration with other institutions describe the data  recorded by the national accelerometric network and complemented by additional data recorded by a number of temporary stations (Dolce et al., 2012a; Dolce et al., 2012b). These reports bear witness of strong ground motion values with an acceleration peak of about 0.9 g in the vertical component recorded during the ML=5.8 earthquake of May 29, 2012 by the Mirandola station, located at about 2 km from the epicentre. The analysis of the seismic noise recorded at some stations shows a quite pronounced peak of the horizontal-to-vertical spectral ratio (H/V) in the frequency range of 0.6 – 0.9 Hz common to all stations. Finally, strong evidence of liquefaction phenomena are reported at several sites (e.g.: S. Carlo, S. Agostino and Mirabello), most of which have been attributed to the occurrence of saturated sandy layer(s) at shallow depth deposited along an abandoned reach of the Reno River (Papathanassiou et al., 2012).
Details of the station configuration and installation will be outlined, with first results",science,995,not included
,to_check,core,,2012-09-01 00:00:00,core,theory and practice of coordination algorithms exploiting the generalised distributive law,,"A key challenge for modern computer science is the development of technologies that allow interacting computer systems, typically referred as agents, to coordinate their decisions whilst operating in an environment with minimal human intervention. By so doing, the decision making capabilities of each of these agents should be improved by making decisions that take into account what the remaining agents intend to do. Against this background, the focus of this thesis is to study and design new coordination algorithms capable of achieving this improved performance.In this line of work, there are two key research challenges that need to be addressed. First, the current state-of-the-art coordination algorithms have only been tested in simulation. This means that their practical performance still needs to be demonstrated in the real world. Second, none of the existing algorithms are capable of solving problems where the agents need to coordinate over complex decisions which typically require to trade off several parameters such as multiple objectives, the parameters of a sufficient statistic and the sample value and the bounds of an estimator. However, such parameters typically characterise the agents’ interactions within many real world domains. For this reason, deriving algorithms capable of addressing such complex interactions is a key challenge to bring research in coordination algorithms one step closer to successful deployment.The aim of this thesis is to address these two challenges. To achieve this, we make two types of contribution. First, we develop a set practical contributions to address the challenge of testing the performance of state-of-the-art coordination algorithms in the real world. More specifically, we perform a case study on the deployment of the max-sum algorithm, a well known coordination algorithm, on a system that is couched in terms of allowing the first responders at the scene of a disaster to request imagery collection tasks of some of the most relevant areas to a team of unmanned aerial vehicles (UAVs). These agents then coordinate to complete the largest number of tasks. In more detail, max-sum is based on the generalised distributive law (GDL), a well known algebraic framework that has been used in disciplines such as artificial intelligence, machine learning and statistical physics, to derive effective algorithms to solve optimisation problems. Our iv contribution is the deployment of max-sum on real hardware and the evaluation of its performance in a real world setting. More specifically, we deploy max-sum on two UAVs (hexacopters) and test it a number of different settings. These tests show that max-sum does indeed perform well when confronted with the complexity and the unpredictability of the real world.The second category of contributions are theoretical in nature. More specifically, we propose a new framework and a set of solution techniques to address the complex interactions requirement. To achieve this, we move back to theory and tackle a new class of problem involving agents engaged in complex interactions defined by multiple parameters. We name this class partially ordered distributed constraint optimisation problems (PO-DCOPs). Essentially, this generalises the well known distributed constraint optimisation problem (DCOP) framework to settings in which agents make decisions over multiple parameters such as multiple objectives, the parameters of a sufficient statistic and the sample value and the bounds of an estimator. To measure the quality of these decisions, it becomes necessary to strike a balance between these parameters and to achieve this, the outcome of these decisions is represented using partially ordered constraint functions.Given this framework, we present three sub-classes of PO-DCOPs, each focusing on a different type of complex interaction. More specifically, we study (i) multi-objective DCOPs (MO-DCOPs) in which the agents’ decisions are defined over multiple objectives, (ii) risk-aware DCOPs (RA-DCOPs) in which the outcome of the agents’ decisions is not known with certainty and thus, where the agents need to carefully weigh the risk of making decisions that might lead to poor and unexpected outcomes and, (iii) multiarm bandit DCOPs (MAB-DCOPs) where the agents need to learn the outcome of their decisions online. To solve these problems, we again exploit the GDL framework. In particular, we employ the flexibility of the GDL to obtain either optimal or bounded approximate algorithms to solve PO-DCOPs. The key insight is to use the algebraic properties of the GDL to instantiate well known DCOP algorithms such as DPOP, Action GDL or bounded max-sum to solve PO-DCOPs. Given the properties of these algorithms, we derive a new set of solution techniques. To demonstrate their effectiveness, we study the properties of these algorithms empirically on various instances of MO-DCOPs, RA-DCOPs and MAB-DCOPs. Our experiments emphasize two key traits of the algorithms. First, bounded approximate algorithms perform well in terms of our requirements. Second, optimal algorithms incur an increase in both the computation and communication load necessary to solve PO-DCOPs because they are trying to optimally solve a problem which is potentially more complex than canonical DCOPs",science,996,not included
,to_check,core,,2011-03-05 00:00:00,core,neural networks non-linear scaling,,"Project (M.S., Computer Science)-- California State University, Sacramento, 2011.Training a neural network with backpropagation algorithm is a systematic process to model a set of given data. This training process involves, among other things, scaling the input and output datasets provided to the neural network. The reason that the scaling process is required, is that real world problem datasets might not be in the range [0, 1], whereas the neural networks work with data only in the range [0, 1], i.e. the neurons fire or they do not. To achieve this, linear scaling is typically used, which for certain datasets can make it difficult for the neural network to properly differentiate between values that are close together. \ud
	This project will show how the implementation of non-linear median scaling can be applied to the training datasets and compares its performance against the linear scaling methodology for a variety of training datasets both for speed of learning and subsequent ability to generalize.  This project demonstrates that after introduction of non-linear scaling into the backpropagation and applying it to the various datasets, there is an improvement in performance to some extent.Computer Scienc",science,997,not included
,to_check,core,,2011-05-27 00:00:00,core,simulating a quasi-simulation: a framework for using multi agent simulation techniques,,"MMORPGs The use of computer simulation techniques for the study of social phenomena, or Social Simulation, is a relatively new field (Gibert &amp; Troitzch, 2005). By using Multi Agent Simulation (MAS) techniques, among others, social scientists are able to explore “what if ” scenarios of emergent behaviors in complex social systems. However, the Social Simulation method faces many challenges: a) human subjectivity; there is no computer, mathematical model nor software powerful and exhaustive enough to replicate subjective aspects like love, free will, etc; b) pervasive contingency; even if we can simulate the interaction of a great number of variables and environmental factors, a computational simulation will never attain the level of complexity that actual human social phenomena has and c) validation; is it not always easy to extract from the real world the kind of research results needed to validate social simulation models. In the other hand, Massive Multiplayer Online Role Playing Games (MMORPGs) share many characteristics with MAS models. MMORPGs resemble in many ways the social complexities of the real world, they are also coded through a programming language and they are also based on a hardware/software platform … but they feature one thing that MAS models don’t: real human beings participate on them, instead of mere AI based agents. Therefore MMORPGs are quasi-simulations that offer unprecedented opportunities for studying complex social phenomena. Since it is humans and not only AI NPCs who play them, the “human subjectivity ” problem can be bypassed. Their “sandbox ” nature minimizes the “pervasive contingency ” problem. Moreover, the wide arrange of data gathering possibilities they offer (see for example Ducheneaut et al, 2004, Williams et al 2008a) empower researchers to obtain appropriate results for computer model validation purposes. In this sense, Gee (2004) and Burke (2005) call for the need of bridging complex systems simulation techniques with MMORPGs studies, and in this paper I intend to further the discussion of the kind framework that is needed for such enterprise. The importance of this theme for the Game Studies discipline is put into perspective by a relatively recent bu",science,998,not included
,to_check,core,'Springer Fachmedien Wiesbaden GmbH',2012-01-01 00:00:00,core,how to make a robot smile? perception of emotional expressions from digitally-extracted facial landmark configurations,,"Abstract. To design robots or embodied conversational agents that can
accurately display facial expressions indicating an emotional state, we need
technology to produce those facial expressions, and research that investigates
the relationship between those technologies and human social perception of
those artificial faces. Our starting point is assessing human perception of core
facial information: Moving dots representing the facial landmarks, i.e., the
locations and movements of the crucial parts of a face. Earlier research
suggested that participants can relatively accurately identity facial expressions
when all they can see of a real human full face are moving white painted dots
representing the facial landmarks (although less accurate than recognizing full
faces). In the current study we investigated the accuracy of recognition of
emotions expressed by comparable facial landmarks (compared to accuracy of
recognition of emotions expressed by full faces), but now used face-tracking
software to produce the facial landmarks. In line with earlier findings, results
suggested that participants could accurately identify emotions expressed by the
facial landmarks (though less accurately than those expressed by full faces).
Thereby, these results provide a starting point for further research on the
fundamental characteristics of technology (AI methods) producing facial
emotional expressions and their evaluation by human users",science,999,not included
,to_check,core,,2012-01-01 00:00:00,core,a comparative study of absent features and unobserved values in software effort data,,"Software effort data contains a large amount of missing values of project attributes. The problem of absent features, which occurred recently in machine learning, is often neglected by researchers of software engineering when handling the missingness in software effort data. In essence, absent features (structural missingness) and unobserved values (unstructured missingness) are different cases of missingness although their appearance in the data set are the same. This paper attempts to clarify the root cause of missingness of software effort data. When regarding missingness as absent features, we develop Max-margin regression to predict real effort of software projects. When regarding missingness as unobserved values, we use existing imputation techniques to impute missing values. Then, &Epsilon; -SVR is used to predict real effort of software projects with the input data sets. Experiments on ISBSG (International Software Benchmarking Standard Group) and CSBSG (Chinese Software Benchmarking Standard Group) data sets demonstrate that, with the tasks of effort prediction, the treatment regarding missingness in software effort data set as unobserved values can produce more desirable performance than that of regarding missingness as absent features. This paper is the first to introduce the concept of absent features to deal with missingness of software effort data. &copy; 2012 World Scientific Publishing Company.National Natural Science Foundation of China 60903050, 71101138; Beijing Natural Science Fund 4122087; Scientific Research Foundation for the Returned Overseas Chinese Scholars, State Education MinistrySoftware effort data contains a large amount of missing values of project attributes. The problem of absent features, which occurred recently in machine learning, is often neglected by researchers of software engineering when handling the missingness in software effort data. In essence, absent features (structural missingness) and unobserved values (unstructured missingness) are different cases of missingness although their appearance in the data set are the same. This paper attempts to clarify the root cause of missingness of software effort data. When regarding missingness as absent features, we develop Max-margin regression to predict real effort of software projects. When regarding missingness as unobserved values, we use existing imputation techniques to impute missing values. Then, &Epsilon; -SVR is used to predict real effort of software projects with the input data sets. Experiments on ISBSG (International Software Benchmarking Standard Group) and CSBSG (Chinese Software Benchmarking Standard Group) data sets demonstrate that, with the tasks of effort prediction, the treatment regarding missingness in software effort data set as unobserved values can produce more desirable performance than that of regarding missingness as absent features. This paper is the first to introduce the concept of absent features to deal with missingness of software effort data. &copy; 2012 World Scientific Publishing Company",science,1000,not included
,to_check,core,,2012-01-01 00:00:00,core,comment: evaluating model performance in fictitious prediction problems,,"I congratulate the author on an impressive and important contribution. Multinomial Inverse Regression (MIR) has wide applicability to a diverse array of problems. Further, the model is technologically impressive and the software is easy to apply to real problems. I hope that MIR will gain widespread use in the machine learning literature and across the social sciences. This comment will focus on a basic question that arises when applying MIR: how do we know that it is performing well? The daunting challenge is that when using MIR to discover characteristics of words, the usual evaluation methods are inappropriate. The standard eval-uation for methods like MIR is to assess its performance in out of sample prediction tasks. When this is the goal—as in supervised learning tasks—then this is the ideal evaluation. And as the author shows, MIR excels at this task, with superior classification accuracy for the set of restaurant reviews. Text classification is certainly important, but perhaps the most promising application of MIR is to measuring how words convey sentiment. For example, the author uses MIR to identify differences in language between Democrats and Republicans (and differences in lan-guage based on constituency characteristics). Social scientists use other methods to mak",science,1001,not included
,to_check,core,,2021-01-21 00:00:00,core,trifinger: an open-source robot for learning dexterity,http://arxiv.org/abs/2008.03596,"Dexterous object manipulation remains an open problem in robotics, despite
the rapid progress in machine learning during the past decade. We argue that a
hindrance is the high cost of experimentation on real systems, in terms of both
time and money. We address this problem by proposing an open-source robotic
platform which can safely operate without human supervision. The hardware is
inexpensive (about \SI{5000}[\$]{}) yet highly dynamic, robust, and capable of
complex interaction with external objects. The software operates at 1-kilohertz
and performs safety checks to prevent the hardware from breaking. The
easy-to-use front-end (in C++ and Python) is suitable for real-time control as
well as deep reinforcement learning. In addition, the software framework is
largely robot-agnostic and can hence be used independently of the hardware
proposed herein. Finally, we illustrate the potential of the proposed platform
through a number of experiments, including real-time optimal control, deep
reinforcement learning from scratch, throwing, and writing",robotics,1002,not included
,to_check,core,,2020-01-01 00:00:00,core,δημοκρίτειο πανεπιστήμιο θράκης (δπθ),Μελέτη και υλοποίηση πλήρως αυτόνομου ιπτάμενου ρομποτικού συστήματος για την παροχή υπηρεσιών εκτάκτου ανάγκης,"The major issue during crisis events occurrence is the immediate reaction and the provision of emergency services to humans in imminent danger. As human survival chances are negatively correlated with detection time, the response time is a vital factor in Search and Rescue (SAR) operations, thus potential delays may have dramatic and even lethal results. Moreover, the safety of rescue workers is another major issue and must be ensured in any circumstance. Unmanned Aerial Vehicles (UAVs) technology can provide a critical and multifaceted support in SAR missions. Given the fact that the faster these operations are planned and executed, the greater the chances of success they get, these aerial robots are ideal candidates for such applications owed to their ability of being nimble, quick-moving and easily programmed to exhibit autonomous behaviors. Hence, with the aim to increase the prospects of distressed people to remain alive, robotic rescue systems—either remote controlled or autonomous ones—must be instantaneously available, to provide a robust helping hand to rescuers, thus boosting their performance and ensuring their own safety. The doctoral dissertation in hand deals with the study and implementation of a novel fully autonomous UAV-based rescue platform for life-saving services, by combining Global Navigation Satellite System (GNSS) methods and state-of-the-art deep learning techniques. Specifically, the proposed Autonomous Aerial Rescue System (AARS), namely the ROLFER (RObotic Lifeguard For Emergency Rescue), is capable of precisely locating and providing instantaneous emergency services to humans in peril during critical events. The main system’s components are: a completely autonomous rescue UAV and its bearing equipment, the Ground Control Station (GCS) including all the apparent software and hardware for the system’s proper functionality, as well as the wearable equipment carried by humans in peril for the distress signal transmission. The rescue system’s Key Performance Indicators (KPI) are to: (i) be fully autonomous, i.e. guided solely by data provided by the distressed human’s wearable equipment without requiring to include human-in-the-loop, (ii) provide fast reaction as possible, i.e. to minimize the intervention time a UAV needs to provide invaluable assistance to a human in the context of an emergency and (iii) precisely detect the human in peril as well to accurately release the rescue equipment. The importance of not wasting the critical response time, as well as taking into serious consideration any other vital parameter during a SAR operation is obvious. The main objectives of this research are summarized as follows: The study, design and implementation of a fully autonomous aerial robotic system for the provision of life-saving services to humans in peril, The deployment of two human rescue scenarios in both maritime and terrestrial environments, The detection of human in peril using several accurate advanced Global Positioning System (GPS)/Global Navigation Satellite System (GNSS)-based localization methods, The detection, tracking and localization of human in peril using advanced image processing techniques as well as the precise rescue apparatus release, The hazard identification and risk analysis of the proposed AARS for the elimination of potential threats to both human in peril and rescue workers, by applying System Theoretic Process Analysis (STPA). Thus, the structure of the dissertation in hand is mainly based on the aforementioned objectives. In particular, Chapter 1 introduces the utilization of robotic systems in SAR operations including some crucial information about UAVs, a review of the related literature, as well the contribution of this dissertation within the state of art. Chapter 2 presents the proposed aerial rescue support system’s architecture and functionality, concerning details about both software and hardware configuration, as well as the system’s performance evaluation in terms of both accuracy and intervention time. Moreover, Chapter 3 addresses the issue of precise localization of the distressed human by implementing advanced geolocalization techniques (e.g. Real Time Kinematics (RTK)-GPS). Thus, an enhanced positioning system which grounds on a specially customized wearable device is proposed, allowing the precise positioning of human in peril. Both hardware and software configuration of the designed novel prototype are described. Experimental results from real flight tests of the proposed AARS and the effectiveness of the implemented moving-base RTK-GPS-based wearable apparatus are also discussed in detail. Chapter 4 presents the vision-based punctual and precise detection of humans in peril from the proposed AARS. In particular, human detection is implemented based on two different approaches:a)by utilizing deep learning techniques on-board the autonomous system’s rescue UAV for the human in peril in maritime environmentb)by combining RTK-GPS techniques and advanced image processing algorithms on the proposed rescue system’s GCS for the human in peril in terrestrial environment. Two suitable datasets, including human in peril in both maritime and terrestrial environments have been created for the computer vision algorithms training. Hardware and software configuration, as well experimental results for the accurate human detection and rescue apparatus fall down are presented in detail.  In Chapter 5, a hazard identification and risk analysis of the proposed AARS, for the elimination of potential threats to both human in peril and rescue workers from the proposed system functionality is presented. The objectives of this analysis are a) to introduce the idea of analyzing the system by dividing it in separate operational modes and b) to feature the advantages of applying a systemic STPA-based safety approach on a SAR mission recruiting the use of UAVs. The dissertation is concluded in Chapter 6 where our findings are summarized and future work is mentioned.The doctoral dissertation in hand has been conducted in the Laboratory of Robotics and Automation of the Department of Production and Management of DUTh, which expertizes in such systems implementations.Το μεγαλύτερο πρόβλημα κατά την διάρκεια καταστάσεων εκτάκτου ανάγκης είναι η άμεση αντίδραση και η παροχή βοηθειών πρώτης ανάγκης στα υπό κίνδυνο άτομα. Λαμβάνοντας υπ’ όψιν ότι οι πιθανότητες επιβίωσης του υπό κίνδυνο ατόμου μειώνονται δραματικά με την πάροδο του χρόνου ανίχνευσης του, ο χρόνος αντίδρασης αποτελεί καίριας σημασίας παράγοντα κατά τη διάρκεια των επιχειρήσεων έρευνας και διάσωσης καθώς ενδεχόμενες καθυστερήσεις μπορεί να οδηγήσουν σε δραματικά αποτελέσματα. Επιπλέον, η ασφάλεια των μελών των ομάδων διάσωσης αποτελεί ένα άλλο μείζoν θέμα και θα πρέπει να διασφαλιστεί σε κάθε περίπτωση. Τα μη-επανδρωμένα εναέρια οχήματα (UAVs) μπορούν να παρέχουν αποφασιστικής σημασίας και πολυπρόσωπη στήριξη στις επιχειρήσεις έρευνας και διάσωσης αφού πρόκειται για γρήγορα και ευέλικτα εναέρια ρομποτικά συστήματα τα οποία μπορούν να προγραμματιστούν εύκολα προκειμένου να λειτουργούν πλήρως αυτόνομα. Συνεπώς, έχοντας ως στόχο να αυξήσουν τις πιθανότητες διάσωσης των υπό κίνδυνο ατόμων, τα ρομποτικά εναέρια οχήματα-είτε τηλεκατευθυνόμενα είτε πλήρως αυτόνομα- οφείλουν να είναι πάντα διαθέσιμα να προσφέρουν ένα χρήσιμο χέρι βοηθείας στις διασωστικές ομάδες ενισχύοντας παράλληλα την απόδοση τους και εξασφαλίζοντας την σωματική ακεραιότητα των εργαζόμενων σε αυτές. Η παρούσα διδακτορική διατριβή πραγματεύεται την μελέτη και την υλοποίηση ενός καινοτόμου πλήρως αυτόνομου ρομποτικού συστήματος διάσωσης βασισμένο στη τεχνολογία των UAVs. Η υλοποίηση και η λειτουργικότητα του προτεινόμενου συστήματος βασίζεται στην συνδυαστική χρήση προηγμένων μεθόδων γεωεντοπισμού και αλγορίθμων βαθιάς μάθησης. Συγκεκριμένα, το προτεινόμενο σύστημα με το ακρωνύμιο ROLFER (Robotic Lifeguard For Emergency Rescue), καθιστά εφικτό τον ακριβή εντοπισμό και την παροχή του κατάλληλου εξοπλισμού βοήθειας στα υπό κίνδυνο άτομα κατά τη διάρκεια καταστάσεων εκτάκτου ανάγκης. Τα κύρια μέρη του προτεινόμενου συστήματος είναι: το πλήρως αυτόνομο διασωστικό UAV με τον απαραίτητο φερόμενο εξοπλισμό βοήθειας, ο επίγειος σταθμός βάσης με το απαραίτητο υλικό (hardware) και λογισμικό (software) για την εύρυθμη λειτουργία του συστήματος, καθώς επίσης και ο κατάλληλος φορητός εξοπλισμός του χρήστη (άτομο σε κίνδυνο) για την εκπομπή του σήματος κινδύνου. Επιπλέον, η μελέτη και υλοποίηση ενός σεναρίου διάσωσης σε υδάτινο και χερσαίο περιβάλλον εξετάζεται διεξοδικά. Οι κύριοι στόχοι της παρούσας διδακτορικής διατριβής συνοψίζονται ως εξής: Η μελέτη, ο σχεδιασμός  και η υλοποίηση ενός πλήρως αυτόνομου ρομποτικού συστήματος εναέριας παροχής βοήθειας για την παροχή απαραίτητου εξοπλισμού σε άτομα που βρίσκονται σε κίνδυνο, Η ανάπτυξη σεναρίου διάσωσης σε θαλάσσιο και χερσαίο περιβάλλον, Ο εντοπισμός του ατόμου σε κίνδυνο με τη χρήση διαφόρων προηγμένων μεθόδων που βασίζονται στη χρήση γεωεντοπισμού (παγκόσμιο σύστημα δορυφορικής πλοήγησης), Ο εντοπισμός, παρακολούθηση και χωροθέτηση του ατόμου σε κίνδυνο με τη χρήση προηγμένων τεχνικών επεξεργασίας εικόνας για την απελευθέρωση του κατάλληλου εξοπλισμού με υψηλή ακρίβεια, Η ανάλυση επικινδυνότητας του προτεινόμενου εναέριου ρομποτικού συστήματος με εφαρμογή της εξειδικευμένης μεθόδου STPA, για την εύρεση των προδιαγραφών ασφαλείας του καθώς και για την ελαχιστοποίηση των πιθανών απειλών τόσο για το άτομο που κινδυνεύει όσο και για την ομάδα διάσωσης. Συνεπώς, η δομή της παρούσας διδακτορικής διατριβής βασίζεται στους παραπάνω σκοπούς. Συγκεκριμένα, στο Κεφάλαιο 1 γίνεται εισαγωγή της χρήσης των ρομποτικών συστημάτων στις επιχειρήσεις έρευνας και διάσωσης, συμπεριλαμβάνοντας χρήσιμες πληροφορίες σχετικά με τα μη-επανδρωμένα εναέρια οχήματα (UAVs), εκτενή ανασκόπηση της σχετικής βιβλιογραφίας, καθώς επίσης  και την συνεισφορά της παρούσας διατριβής στην επιστήμη. Στο Κεφάλαιο 2 παρουσιάζεται η δομή και η αρχιτεκτονική του προτεινόμενου διασωστικού συστήματος συμπεριλαμβανομένων και των λεπτομερειών τόσο του υλικού (hardware) όσο και του λογισμικού (software), καθώς επίσης και την αξιολόγηση του σε θέματα σχετικά με την ακρίβεια και του χρόνου παρέμβασης του. Στο Κεφάλαιο 3 εξετάζεται ο ακριβής εντοπισμός της θέσης του υπό κίνδυνο ατόμου με χρήση προηγμένων τεχνικών γεωεντοπισμού (RTK-GPS). Συγκεκριμένα παρουσιάζεται ένα βελτιωμένο σύστημα εντοπισμού το οποίο βασίζεται σε μια προσαρμοσμένη φορητή συσκευή που επιτρέπει τον εντοπισμό της θέσης του ατόμου σε κίνδυνο με ακρίβεια μερικών εκατοστών. Αποτελέσματα από την αξιολόγηση της χρήσης της συγκεκριμένης πρωτότυπης συσκευής περιγράφονται λεπτομερώς. Η υλοποίηση και εφαρμογή προηγμένων αλγορίθμων επεξεργασίας εικόνας για την βέλτιστη δυνατή ακρίβεια εντοπισμού της θέσης του ατόμου που κινδυνεύει παρουσιάζεται στο Κεφάλαιο 4. Συγκεκριμένα, η υλοποίηση αυτή πραγματοποιείται με δύο διαφορετικές προσεγγίσεις: α) με την χρήση αλγορίθμων βαθιάς μάθησης ενσωματωμένων στο αυτόνομο μη-επανδρωμένο εναέριο όχημα, για τον εντοπισμό του υπό-κίνδυνο ατόμου σε υδάτινο περιβάλλον, β) με συνδυαστική χρήση προηγμένων μεθόδων γεωεντοπισμού (RTK-GPS) και προηγμένων αλγορίθμων υπολογιστικής όρασης στον επίγειο σταθμό βάσης του συστήματος, για τον εντοπισμό του ατόμου σε χερσαίο περιβάλλον. Για το σκοπό αυτό, έχουν δημιουργηθεί δύο διαφορετικά σετ δεδομένων (dataset) που εμπεριέχουν εικόνες ατόμων σε κίνδυνο τόσο σε χερσαίο όσο και σε υδάτινο περιβάλλον. Με βάση αυτά τα σετ δεδομένων πραγματοποιήθηκε η εκπαίδευση των αλγορίθμων υπολογιστικής όρασης. Επιπλέον, παρουσιάζονται αναλυτικά πειραματικά αποτελέσματα από δοκιμές του συστήματος σε πραγματικές συνθήκες σχετικά με την ακρίβεια του εντοπισμού του ανθρώπου όσο και της απελευθέρωσης του διασωστικού εξοπλισμού. Τέλος, το Κεφάλαιο 5 παρουσιάζει την ανάλυση επικινδυνότητας του προτεινόμενου συστήματος για την απαλοιφή πιθανών απειλών και κινδύνων που θα μπορούσε να ελλοχεύει η χρήση του τόσο για το υπό κίνδυνο άτομο όσο και για τις διασωστικές ομάδες. Σκοπός της ανάλυσης αυτής είναι α) να εισάγει την ιδέα της ανάλυσης του συστήματος διαιρώντας το σε επιμέρους φάσεις λειτουργίας και β) να παρουσιάσει τα πλεονεκτήματα της εφαρμογής αναλύσεων επικινδυνότητας που βασίζονται στη συστημική θεωρία και συγκεκριμένα της STPA, στον τομέα των αποστολών έρευνας και διάσωσης με χρήση UAVs. Η παρούσα διατριβή ολοκληρώνεται στο Κεφάλαιο 6 όπου παρουσιάζονται τα συμπεράσματα και τα ευρήματα της υλοποίησης του συστήματος καθώς και η πιθανή μελλοντική δουλειά . Η παρούσα διδακτορική διατριβή εκπονήθηκε στο Εργαστήριο Ρομποτικής και Αυτοματισμού του Τμήματος Μηχανικών Παραγωγής και Διοίκησης του Δ.Π.Θ. που ειδικεύεται σε τέτοιου είδους υλοποιήσεις συστημάτων",robotics,1004,not included
,to_check,core,,2020-05-07 00:00:00,core,"using taint analysis and reinforcement learning (tarl) to repair
  autonomous robot software",http://arxiv.org/abs/2005.03813,"It is important to be able to establish formal performance bounds for
autonomous systems. However, formal verification techniques require a model of
the environment in which the system operates; a challenge for autonomous
systems, especially those expected to operate over longer timescales. This
paper describes work in progress to automate the monitor and repair of
ROS-based autonomous robot software written for an a-priori partially known and
possibly incorrect environment model. A taint analysis method is used to
automatically extract the data-flow sequence from input topic to publish topic,
and instrument that code. A unique reinforcement learning approximation of MDP
utility is calculated, an empirical and non-invasive characterization of the
inherent objectives of the software designers. By comparing off-line (a-priori)
utility with on-line (deployed system) utility, we show, using a small but real
ROS example, that it's possible to monitor a performance criterion and relate
violations of the criterion to parts of the software. The software is then
patched using automated software repair techniques and evaluated against the
original off-line utility.Comment: IEEE Workshop on Assured IEEE Workshop on Assured Autonomous Systems,
  May, 202",robotics,1006,not included
,to_check,core,,2020-03-27 00:00:00,core,μάθηση μέσω παρατήρησης για την επίτευξη ρομποτικών δράσεων χειρισμού,,"Η παρούσα διδακτορική διατριβή αφορά τη μελέτη, ανάπτυξη και εφαρμογή, μεθόδων
Μηχανικής Μάθησης μέσω Παρατήρησης (Learning from Demonstration) με στόχο την
ρομποτική αναπαραγωγή δράσεων χειρισμού. Η μεθοδολογία αυτή στηρίζεται στην
δημιουργία μιας αντιστοίχισης (mapping) μεταξύ της κινηματικής του ανθρώπινου χεριού και
ενός ρομποτικού βραχίονα, ή πιο συγκεκριμένα μεταξύ του πολυδιάστατου χώρου των
κινήσεων του ανθρώπου (human actor) με τον επίσης πολυδιάστατο χώρο δράσης του
ρομπότ. Η συσχέτιση των ανθρώπινων ενεργειών με αντίστοιχες ρομποτικές, επιτυγχάνεται
μέσω μιας άδηλης αναπαράστασης, που ονομάζεται λανθάνουσα απεικόνιση χώρου (latent
space). Πιο συγκεκριμένα, μελετάμε την αμοιβαία αλληλεπίδραση της αντίληψης και της 
δράσης, προκειμένου να διδάξουμε τα ρομπότ μια ποικιλία από νέες κινήσεις χειρός. Ως εκ
τούτου, υλοποιήθηκε ένα μεθοδολογικό πλαίσιο μάθησης μέσω παρατήρησης, το οποίο
ονομάζεται IMFO (Imitation Framework by Observation), που διευκολύνει την αναπαραγωγή
μαθημένων και νέων κινήσεων χειρισμού από ένα ρομπότ (manipulation tasks) και,
παράλληλα, έχει ευρεία εφαρμογή σε σενάρια αλληλεπίδρασης ανθρώπου-ρομπότ (HRI) σε
καθημερινά περιβάλλοντα.
Επιπλέον, σε αυτή τη διατριβή, εξετάζουμε το ρόλο της χρονικής διάρκειας εκτέλεσης μιας
κίνησης μέσα από τη διαδικασία μάθησης από παρατήρηση, ενισχύοντας το διαμορφωμένο
πλαίσιο IMFO με την δυνατότητα αναπαράστασης και αναπαραγωγής τόσο των χωρικών όσο
και των χρονικών χαρακτηριστικών των ανθρώπινων κινήσεων. Σε αντίθεση με άλλες
μεθόδους μάθησης μέσω παρατήρησης (LfD) που περιγράφουν την εκτελούμενη δράση μόνο
με βάση τα χωρικά χαρακτηριστικά της, η προτεινόμενη μεθοδολογία ενισχύει την
αναπαραγωγή των χωροχρονικών πτυχών μιας κίνησης επιτρέποντας την αποτελεσματική
εφαρμογή της σε πιο σύνθετα σενάρια HRI, όπου η χρονική αλληλουχία των δράσεων είναι
σημαντική. Επιπρόσθετα, εισάγεται ένα σύνολο καλά καθορισμένων μετρικών αξιολόγησης
(evaluation metrics) για να αποτιμηθεί η εγκυρότητα της προτεινόμενης προσέγγισης
λαμβάνοντας υπόψη τη χρονική και χωρική συνέπεια των αναπαραγόμενων συμπεριφορών.
Μια αξιοσημείωτη επέκταση του προαναφερθέντος πλαισίου αναφέρεται στην εκμάθηση
της δύναμης που επιβάλλεται από τον χρήστη για την επιτυχημένη εκτέλεση λεπτών
χειρισμών. Αυτή η διαδικασία παρουσιάζεται επίσης στην παρούσα διατριβή μέσω ενός
νέου πλαισίου εποπτευόμενης μάθησης, το οποίο ονομάζεται SLF (Supervised Learning
scheme for Force-based manipulation). Το SLF διατυπώνεται ως μία διαδικασία τριών
σταδίων: (α) επιβλεπόμενη διαδικασία εκτέλεσης κινήσεων χειρισμού σε προσομοίωση για
την απόκτηση επαρκών δεδομένων, (β) διαδικασία εκπαίδευσης (training) για τη
διευκόλυνση της μάθησης κινήσεων χειρισμού με την κατάλληλη προσαρμογή του καρπού
και της δύναμη πιασίματος και μεταφοράς και (γ) εκτέλεση της κίνησης από ρομποτικό
βραχίονα σε προσομοίωση. Στη συνέχεια, με τη χρήση της μεθόδου sim-to-real transfer,
επιτυγχάνεται αναπαραγωγή των μαθημένων δράσεων σε πραγματικά περιβάλλοντα
γενικεύοντας την εφαρμογή του πλαισίου μάθησης σε επιπλέον συνθήκες χειρισμού
εύθραυστων αντικειμένων. Τα αποτελέσματα με τη χρήση του ρομποτικού βραχίονα YuMi,
σε πειράματα με διαφορετικά αντικείμενα με παρόμοιους συντελεστές τριβής, και
εναλλακτικές πόζες πιασίματος, αποδεικνύουν ότι το ρομπότ είναι σε θέση να αναπαράγει
αποτελεσματικά απαιτητικές κινήσεις μεταφοράς και χειρισμού μετά την ολοκλήρωση της
διαδικασίας μάθησης.
Συνοπτικά, η παρούσα διατριβή μελετά την διαδικασία μάθησης μέσω παρατήρησης
συνεισφέροντας με μια νέα προσέγγιση που εισάγει την μελέτη δράσεων χειρισμού
αντικειμένων μέσα από έναν χώρο μειωμένων διαστάσεων, για την εύκολη και συμπαγή
κωδικοποίηση των επιμέρους χαρακτηριστικών των δράσεων. Ταυτόχρονα μελετώνται τα
χρονικά χαρακτηριστικά των κινήσεων ώστε να ενισχυθεί η εφαρμογή της μεθόδου σε
σύνθετες, πραγματικές συνθήκες που απαιτούν χρονική ακρίβεια αναπαραγωγής. Τέλος, η
διαμόρφωση μιας γενικευμένης διαδικασίας εποπτευόμενης μάθησης για τον χειρισμό
εύθραυστων αντικείμενων αναβαθμίζει περαιτέρω το αρχικό πλαίσιο μάθησης.The current PhD thesis addresses the formulation and implementation of a methodological
framework for robot Learning from Demonstration (LfD). The latter refers to methodologies
that develop behavioral policies from example state-to-action mappings. To this
end, we study the reciprocal interaction of perception and action, in order to teach robots
a repertoire of novel action behaviors. Based on that, we design, develop and implement
a robust imitation framework, termed IMFO (IMitation Framework by Observation), that
facilitates imitation learning and relevant applications in human-robot interaction (HRI)
tasks. IMFO can cope with the reproduction of learned (i.e. previously observed) actions,
aswell as novel ones. Mapping of human actions to the respective robotic ones is achieved
via an indeterminate depiction, termed latent space representation. The latter accomplishes
a compact, yet precise abstraction of action trajectories, effectively representing
high dimensional raw actions in a low dimensional space.
Moreover, throughout this thesis, we examine the role of time in LfD by enhancing
the aforementioned framework with the notion of learning both the spatial and temporal
characteristics of human motions. Accordingly, learned actions can be subsequently reproduced
in the context of more complex time-informed HRI scenarios. Unlike previous
LfD methods that cope only with the spatial traits of an action, the formulated scheme
effectively encompasses spatial and temporal aspects. Extensive experimentation with a
variety of real robotic platforms demonstrates the robustness and applicability of the introduced
integrated LfD scheme.
Learned actions are reproduced under the high level control of a time-informed task
planner. During the implementation of the studied scenarios, temporal and physical constraints
may impose speed adaptations in the performed actions. The employed latent
space representation readily supports such variations, giving rise to novel actions in the
temporal domain. Experimental results demonstrate the effectiveness of the proposed
enhanced imitation scheme in the implementation of HRI scenarios. Additionally, a set
of well defined evaluation metrics are introduced to assess the validity of the proposed
approach considering the temporal and spatial consistency of the reproduced behaviors.
A noteworthy extension of the above regards force-based object grasping for executing
sensitive manipulation tasks. This is also treated in the current thesis via a novel supervised
learning scheme, termed SLF (Supervised Learning for Force-based manipulation).
SLF is formulated as a three-stage process: (a) supervised trial-execution in simulation
to acquire sufficient training data; (b) training to facilitate grasp learning with suitable
robot-arm pose and lifting force; (c) grasp execution in simulation. Subsequently, following
sim-to-real transfer, operation in real environments is achieved in addition to simulated
ones, generalizing also for objects not included in the trial sessions. The proposed
learning scheme is demonstrated in object lifting tasks where the applied force varies for
different objects with similar contact friction coefficients, and likewise the grasping pose.
Experimental results on the manipulator YuMi show that the robot is able to effectively
reproduce demanding lifting and manipulation tasks after learning is accomplished.
In summary, our thesis has studied LfD and has contributed with a novel approach that
introduced latent space representations to encode the action characteristics. A framework
implementation (IMFO) of our approach allowed extensive experimentation and also conduction
of HRI scenarios. The inclusion of temporal aspects in our approach enhanced it
to cope with complex, real-life interactions. Finally, the extension of IMFO with forcebased
grasping facilitated manipulation tasks with sensitive objects",robotics,1008,not included
,to_check,core,,2016-07-01 00:00:00,core,exploiting heterogeneity in networks of aerial and ground robotic agents,,"By taking advantage of complementary communication technologies, distinct sensing functionalities and varied motion dynamics present in a heterogeneous multi-robotic network, it is possible to accomplish a main mission objective by assigning specialized sub-tasks to specific members of a robotic team. An adequate selection of the team members and an effective coordination are some of the challenges to fully exploit the unique capabilities that these types of systems can offer. Motivated by real world applications, we focus on a multi-robotic network consisting off aerial and ground agents which has the potential to provide critical support to humans in complex settings. For instance, aerial robotic relays are capable of transporting small ground mobile sensors to expand the communication range and the situational awareness of first responders in hazardous environments.

In the first part of this dissertation, we extend work on manipulation of cable-suspended loads using aerial robots by solving the problem of lifting the cable-suspended load from the ground before proceeding to transport it. Since the suspended load-quadrotor system experiences switching conditions during this critical maneuver, we define a hybrid system and show that it is differentially-flat. This property facilitates the design of a nonlinear controller which tracks a waypoint-based trajectory associated with the discrete states of the hybrid system. In addition, we address the case of unknown payload mass by combining a least-squares estimation method with the designed controller.

Second, we focus on the coordination of a heterogeneous team formed by a group of ground mobile sensors and a flying communication router which is deployed to sense areas of interest in a cluttered environment. Using potential field methods, we propose a controller for the coordinated mobility of the team to guarantee inter-robot and obstacle collision avoidance as well as connectivity maintenance among the ground agents while the main goal of sensing is carried out. For the case of the aerial communications relays, we combine antenna diversity with reinforcement learning to dynamically re-locate these relays so that the received signal strength is maintained above a desired threshold.

Motivated by the recent interest of combining radio frequency and optical wireless communications, we envision the implementation of an optical link between micro-scale aerial and ground robots. This type of link requires maintaining a sufficient relative transmitter-receiver position for reliable communications. In the third part of this thesis, we tackle this problem. Based on the link model, we define a connectivity cone where a minimum transmission rate is guaranteed. For example, the aerial robot has to track the ground vehicle to stay inside this cone. The control must be robust to noisy measurements. Thus, we use particle filters to obtain a better estimation of the receiver position and we design a control algorithm for the flying robot to enhance the transmission rate. Also, we consider the problem of pairing a ground sensor with an aerial vehicle, both equipped with a hybrid radio-frequency/optical wireless communication system. A challenge is positioning the flying robot within optical range when the sensor location is unknown. Thus, we take advantage of the hybrid communication scheme by developing a control strategy that uses the radio signal to guide the aerial platform to the ground sensor. Once the optical-based signal strength has achieved a certain threshold, the robot hovers within optical range.

Finally, we investigate the problem of building an alliance of agents with different skills in order to satisfy the requirements imposed by a given task. We find this alliance, known also as a coalition, by using a bipartite graph in which edges represent the relation between agent capabilities and required resources for task execution. Using this graph, we build a coalition whose total capability resources can satisfy the task resource requirements. Also, we study the heterogeneity of the formed coalition to analyze how it is affected for instance by the amount of capability resources present in the agents.Electrical EngineeringDoctoralUniversity of New Mexico.  Dept. of Electrical and Computer EngineeringFierro, RafaelOishi, MeekoTapia, LydiaSadler, Bria",robotics,1010,not included
,to_check,core,,2016-09-20 00:00:00,core,lessons learned from the trading agent competition,,"Major advancements in artificial intelligence have beenstimulated by well-designed competitions that tackleintriguing and often very complex problems. Chess, poker, stock trading, real-time strategy games, robot soccer, robot rescue or planning, and autonomous vehicles are among the most well known. Adaptability, proactiveness, and interop-erability are essential characteristics of these games. Many of these competitions focus on development of rational behavior that would allow dominance over competitors. Apart from game play, though, one of the main reasons for these competi-tions is to bridge the gap between AI and real-life domains. In many real-life domains, such as trading environments, self-interested entities need to operate subject to limited time and information. Additionally, the web has mediated an ever broad-er range of transactions, urging participants to concurrently trade across multiple markets. All these have generated the need for technologies that empower prompt investigation of large vol-umes of data and rapid evaluation of numerous alternative strategies in the face of constantly changing market conditions (Bichler, Gupta, and Ketter 2010). AI and machine-learning tech-niques, including neural networks and genetic algorithms, are continuously gaining ground in the support of such trading sce-narios. User modeling, price forecasting, market equilibrium pre-diction, and strategy optimization are typical cases where AI typ-ically provides reliable solutions. Yet, the adoption and deployment of AI practices in real trading environments remains limited, since the proprietary nature of markets precludes open benchmarking, which is critical for further scientific progress. T",robotics,1011,not included
,to_check,core,,2014-11-19 00:00:00,core,lego© mindstorms nxt and q-learning: a teaching approach for robotics in engineering,,"Robotics has become a common subject in many engineering degrees and postgraduate programs. Although at undergraduate levels the students are introduced to basic theoretical concepts and tools, at postgraduate courses more complex topics have to be covered. One of those advanced subjects is Cognitive Robotics, which covers aspects like automatic symbolic reasoning, decision-making, task planning or machine learning. In particular, Reinforcement Learning (RL) is a machine learning and
decision-making methodology that does not require a model of the environment where the robot operates, overcoming this limitation by making observations. In order to get the greatest educational benefit, RL theory should be complemented with some hands-on RL task that uses a real robot, so students get a complete vision of the learning problem, as well as of the issues that arise when dealing with a physical robotic platform. There are several RL techniques that can be studied in such a subject; we have chosen Q-learning, since is a simple, effective and well-known RL algorithm.

In this paper we present a minimalist implementation of the Q-learning method for a Lego Mindstorms NXT mobile robot, focused on simplicity and applicability, and flexible enough to be adapted to several tasks. Starting from a simple wandering problem, we first design an off-line model of the learning process in which the Q-learning parameters are studied. After that, we implement the algorithm on the robot, gradually enlarging the number of states-actions of the problem. The final result of this work is a teaching framework for developing practical activities regarding Q-learning in our Robotics subjects, which will improve our teaching.Universidad de Málaga. Campus de Excelencia Internacional Andalucía Tech",robotics,1012,not included
,to_check,core,,2014-12-21 00:00:00,core,virtual experiments environment for mobile robots design and testing,,"Abstract:- This paper refers to a virtual environment which represents the main support for experiments with mobile robots in the design and testing stage. This software environment is very useful because, compairing to the experiments with real robots, it allow the testing and evaluation of different types of interfaces and different working environments with diverse configurations. A very important facility of this interactive software environment is the fact that the designers of the robots sensors and interfaces are able to work in parallel to design test, optimize and realize different control devices for the robot. Key-Words:- artificial intelligence, mobile robots, virtual experiments, virtual environment, simulator, synchronization ",robotics,1013,not included
,to_check,core,,2015-08-23 00:00:00,core,nubot team description paper 2008,,"Abstract. The paper mainly presents the developments of our middle-size league robot team “NuBot ” for RoboCup 2008 Suzhou. The improvements lie in robot hardware like new panoramic mirror and kicking device, and in robot software such as algorithms for panoramic image processing and robot’s self-localization, multi-robot cooperation, path planning and motion control. Our current research focuses on robust robot vision, multi-robot cooperation, new learning controller for DC motors, and reinforcement learning for real robots. ",robotics,1014,not included
,to_check,core,,2014-12-29 00:00:00,core,and,,Computers are often used to simulate real world activities and events. The usefulness of a software simulation is determined by the validity of the underlying model of the system. Software simulation often treats the real world in a less random and more predictable fashion than is the case in nature. The use of an actual physical controlled object creates a more realistic environment to carry out real-time or AI experiments. These physical objects exhibit a more chaotic behavior due to the mechanical issues independent of the controlling algorithm. The construction and use of robots to accomplish this behavior has been successfully done in the past. The research discussed in this paper established a semi-adversarial environment. In this type of an environment one of the parties neither hinders nor assists in the process undertaken. To demonstrate this behavior the most appropriate group of animals is a herd of sheep. To create this environment the basic behaviors and characteristics of sheep had to be understood and encoded. This paper describes those necessary attributes and the resulting sheep algorithms. Future use of these algorithms in semi-adversarial environments will also be discussed,robotics,1015,not included
,to_check,core,,2008-01-01 00:00:00,core,towards autonomous design of experiments for robots,,"For understanding a real-world environment on a conceptual level, any agent requires the capability for autonomous, open-ended learning. One of the main challenges in Artificial Intelligence is to bias the learning phase sufficiently in order to obviate complexity issues, while at the same time not restricting the agent to a certain environment or to a particular task. In this paper we describe a framework for autonomous design of experiments for a robotic agent, which enables the robot to improve and increase its conceptual knowledge about the environment through open-ended learning by experimentation. We specify our implementation of this framework and describe how its modules can recognize situations in which learning is useful or necessary, gather target-oriented data and provide it to machine learning algorithms, thus reducing the search space
for the learning target significantly. We describe the integration of these modules and the real world scenarios in which we tested them",robotics,1017,not included
,to_check,core,,2009-01-01 00:00:00,core,autonomous design of experiments for learning by experimentation,,"In Artificial Intelligence, numerous learning paradigms have been developed over the past decades. In most cases of embodied and situated agents, the learning goal for the artificial agent is to „map“ or classify the environment and the objects therein [1, 2], in order to improve navigation or the execution of some other domain-specific task. Dynamic environments and changing tasks still pose a major challenge for robotic learning in real-world domains. In order to intelligently adapt its task strategies, the agent needs cognitive abilities to more deeply understand its environment and the effects of its actions. In order to approach this challenge within an open-ended learning loop, the XPERO project (http://www.xpero.org) explores the paradigm of Learning by Experimentation to increase the robot's conceptual world knowledge autonomously. In this setting, tasks which are selected by an actionselection mechanism are interrupted by a learning loop in those cases where the robot identifies learning as necessary for solving a task or for explaining observations. It is important to note that our approach targets unsupervised learning, since there is no oracle available to the agent, nor does it have access to a reward function providing direct feedback on the quality of its learned model, as e.g. in reinforcement learning approaches. In the following sections we present our framework for integrating autonomous robotic experimentation into such a learning loop. In section 1 we explain the different modules for stimulation and design of experiments and their interaction. In section 2 we describe our implementation of these modules and how we applied them to a real world scenario to gather target-oriented data for learning conceptual knowledge. There we also indicate how the goaloriented data generation enables machine learning algorithms to revise the failed prediction model",robotics,1018,not included
,to_check,core,,2009-01-01 00:00:00,core,research article bootstrap learning and visual processing management on mobile robots,,"which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. A central goal of robotics and AI is to enable a team of robots to operate autonomously in the real world and collaborate with humans over an extended period of time. Though developments in sensor technology have resulted in the deployment of robots in specific applications the ability to accurately sense and interact with the environment is still missing. Key challenges to the widespread deployment of robots include the ability to learn models of environmental features based on sensory inputs, bootstrap off of the learned models to detect and adapt to environmental changes, and autonomously tailor the sensory processing to the task at hand. This paper summarizes a comprehensive effort towards such bootstrap learning, adaptation, and processing management using visual input. We describe probabilistic algorithms that enable a mobile robot to autonomously plan its actions to learn models of color distributions and illuminations. The learned models are used to detect and adapt to illumination changes. Furthermore, we describe a probabilistic sequential decision-making approach that autonomously tailors the visual processing to the task at hand. All algorithms are fully implemented and tested on robot platforms in dynamic environments. 1",robotics,1019,included
,to_check,core,,2010-01-01 00:00:00,core,d.: multilevel darwinist brain (mdb): artificial evolution in a cognitive architecture for real robots,,"Abstract—The multilevel Darwinist brain (MDB) is a cognitive architecture that follows an evolutionary approach to provide au-tonomous robots with lifelong adaptation. It has been tested in real robot on-line learning scenarios obtaining successful results that reinforce the evolutionary principles that constitute the main orig-inal contribution of the MDB. This preliminary work has lead to a series of improvements in the computational implementation of the architecture so as to achieve realistic operation in real time, which was the biggest problem of the approach due to the high compu-tational cost induced by the evolutionary algorithms that make up the MDB core. The current implementation of the architecture is able to provide an autonomous robot with real time learning ca-pabilities and the capability for continuously adapting to changing circumstances in its world, both internal and external, with min-imal intervention of the designer. This paper aims at providing an overview or the architecture and its operation and defining what is required in the path towards a real cognitive robot following a developmental strategy. The design, implementation and basic op-eration of the MDB cognitive architecture are presented through some successful real robot learning examples to illustrate the va-lidity of this evolutionary approach. Index Terms—Adaptive systems, artificial neural networks, au-tonomous robotics, cognitive architecture, developmental robotics, evolutionary computation. I",robotics,1020,included
,to_check,core,,2011-03-01 00:00:00,core,ambientes inteligentes para racionalização de energia utilizando localização rfid,,"Monografia (graduação)—Universidade de Brasília, Faculdade de Tecnologia, 2011.Há tempos o Laboratório de Automação e Robótica da Universidade de Brasília vem desenvolvendo projetos na área de ambientes inteligentes visando a racionalização de energia e o conforto térmico por meio automação predial. Inserido nesse projeto está o presente trabalho de graduação, seu objetivo é acionar aparelhos de ar condicionado por meio de um sistema supervisório, decisão essa de ligar ou desligar os aparelhos tomada de acordo com a localização de usuários em um ambiente fechado fazendo-se uso da tecnologia Radio Frequency Identi cator (RFID) e de redes neurais implementadas e disponibilizadas pelo laboratório. O foco deste trabalho é a comunicação entre o sistema supervisório e o sistema de localização, a comunicação entre o sistema supervisório e os aparelhos de ar condicionado e por  m uma tentativa de melhoramento da localização de usuários quando estes estiverem em movimento, dado que o sistema previamente implementado funcionava apenas para os casos em que o usuário estivesse parado. Vários softwares e equipamentos foram utilizados, RFID ativos em três ambientes distintos do laboratório, o software supervisório ActionView, o módulo de comunicação ModBus ERS 1050, scripts em MatLab para aquisição de dados das leitoras RFID e para treinamento e uso das redes neurais arti ciais Perceptron Multicamadas. O Acionamento foi realizado com êxito, quando o supervisório envia o sinal de liga, ou de desliga, os comandos são executados imediatamente e logo são mostrados na tela do software, a comunicação com o sistema de localização está funcionando sem erros através do protocolo OLE for Process Control (OPC), e as redes neurais estão realizando localizações com uma precisão aceitável para ambientes fechados. Contudo faz-se necessário um desenvolvimento melhor na parte de localização para obter-se uma robustez maior para o projeto.For a long time the Automation and Robotics Laboratory of the University of Brasilia is developing works in Ambient Intelligence, studding ways to save energy while ensureing thermal comfort to the users, through building automation. This graduation work is inserted in this context, its goal is to actuate air conditioning equipments through a supervisory system, this decision of turning on or turning o  the equipments is taken according to the localization of the users in teh environment, using the Radio Frequency Identi cator technology, and neural networks implemented and available at the Laboratory. This work focuses on communication between the supervisory and the localization system, the communication between the supervisory and the air conditioning equipments and a try of improvement of the localization of users when these are moving around, because the localization system that exists works only if the user is static. Many software and equipments were used in this work: active RFID in three rooms of the laboratory, the supervisory software ActionView, the ModBus communication module ERS 1050 [SPIN Engenharia de automação], Matlab scripts for data acquisition of the RFID readers and for the Multilayer Perceptron neural network train and use. The actuation of the air conditioning equipments is considered successfully when the supervisory send the command to turn on and to turn o , the command is executed in real time and immediately after it has been sent, it appears on the software screen. The communication between the locallization system and the supervisory is working without errors using the OLE for Process Control Protocol (OPC), and the neural networks are locating the user with a acceptable precision for indoor localization applications, nervertheless it is still necessary to improve the localization to get into a system that is reliable",robotics,1021,not included
,to_check,core,,2010-02-17 00:00:00,core,using rfid and a low cost robot to evolve foraging behavior,,"The process of developing genetic algorithms, genetic programs or training neural networks is a time consuming task. When the target device is an autonomous mobile robot, this development is often performed using software simulation. Software simulations are a cost effective tool and provide researchers with the ability to test out multiple algorithms quickly and efficiently. However, the end result is that the optimized algorithm(s) must be implemented and tested on an actual robot to evaluate performance in the real world. Significant cost can be associated with this final step. In this paper we propose to leverage Radio Frequency Identification (RFID) and a low-cost RFID capable mobile robot with the intent of creating basic foraging behavior. Additionally, we will present experimental results that demonstrate the effectiveness of using Genetic Programming (GP) and a low-cost RFID capable robot t",robotics,1022,included
,to_check,core,,2007-01-01 00:00:00,core,project group: 1030 groupmembers:,,"This thesis concerns the development of an autonomous mobile robot intended for use in a human environment. From a primary focus on HRI, a novel robotic behaviour al-gorithm has been developed and validated through simulation as well as real world ex-periments. Furthermore, the behaviour of the robot is continuously adjusted to that of encountered people, by incorporating CBR based artificial intelligence, implemented by use of aMySQL database. Besides interacting with people, the robot is capable of navigating and localizing itself in a given environment while avoiding un-known obstacles. All robot software, including the implemen-tation of the above algorithm, has been de-veloped for use with the Player robot soft-ware framework, and implemented on a FESTO Robotino®. Consequently, due to the Player framework offering a wide range of robot features, the developed system can form the basis of future robotic research. Experiments performed on the robot show promising results, in relation to both the behavioural capabilities and the developed system in general",robotics,1023,not included
,to_check,core,,2008-03-04 00:00:00,core,artificial emotions,,"Abstract. The question of implementing emotions in robots is twofold: on the on hand it should be verified whether such an effort is valuable, and on the other it should be determined whether the implementation is feasible. The answer to the first question seems easy: besides and beyond the reasons of pure intellectual curiosity and scientific research, emotions should be studied and implemented if the overall behavior of such robots is better than their unemotional counterparts with respect to behaving efficiently in a real world environment. Two diverse opinions have emerged in the previous discussion. One, due to McCarthy, asserts that emotions will introduce obstacles in the communication among robots and human beings [6]. On the other hand, Minsky sustains the opinion that it is impossible to implement intelligence without emotions [7]. In this paper we analyze these perspectives, discuss a possible way to approach the topic, and provide an architecture to implement emotions, which has shown some very interesting characteristics. We sustain that the research on emotions — from the Artificial Intelligence point of view — is valuable and worth pursuing. 1 The need for emotions You [humans] are, after all, essentially irrational",robotics,1024,not included
,to_check,core,,2008-02-04 00:00:00,core,speaking autonomous intelligent devices,,"Abstract. The development of speech tools suitable for use in real world environments requires collaboration between computational linguistics and new implementation fields e.g. robotics, and the incorporation of new AI techniques to improve overall system performance. In this paper we present the core development concepts of SAID (Speaking Autonomous Intelligent Devices). The work presented centres around four key strands of research, namely the recasting of the Time Map model as a Multi-Agent System (MAS), the development of a MAS based audio feature extraction system, the deployment of a BDI based dialog agent and the design of a MAS based social robot architecture",robotics,1025,not included
,to_check,core,,2005-01-01 00:00:00,core,"an architecture for behaviorbased reinforcement learning,” adaptive behavior",,"This paper introduces an integration of reinforcement learning and behavior-based control designed to produce real-time learning in situated agents. The model layers a distributed and asynchronous reinforcement learning algorithm over a learned topological map and standard behavioral substrate to create a reinforcement learning complex. The topological map creates a small and task-relevant state space that aims to make learning feasible, while the distributed and asynchronous nature of the model make it compatible with behavior-based design principles. We present the design, implementation and results of an experiment that requires a mobile robot to perform puck foraging in three artificial arenas using the new model, a random decision making model, and a layered standard reinforcement learning model. The results show that our model is able to learn rapidly on a real robot in a real environment, learning and adapting to change more quickly than both alternative models. We show that the robot is able to make the best choices it can given its drives and experiences using only local decisions and therefore displays planning behavior without the use of classical planning techniques. ",robotics,1026,included
,to_check,core,,2008-06-02 00:00:00,core,francisco cervantes,,"As autonomous robots become more complex in their behavior, more sophisticated software architectures are required to support the ever more sophisticated robotics software. These software architectures must support complex behaviors involving adaptation and learning, implemented, in particular, by neural networks. We present in this paper a neural based schema [2] software architecture for the development and execution of autonomous robots in both simulated and real worlds. This architecture has been developed in the context of adaptive robotic agents, ecological robots [6], cooperating and competing with each other in adapting to their environment. The architecture is the result of integrating a number of development an",robotics,1027,included
,to_check,core,,2008-03-12 00:00:00,core,neural network on fpga for command surface approximation,,"Abstract: The hardware implementation of neural networks is a new step in the evolution and use of neural networks in practical applications. The CMAC cerebellar model articulation controller is intended especially for hardware implementation, and this type of network is used successfully in the areas of robotics and control, where the real time capabilities of the network are of particular importance. The implementation of neural networks on FPGA’s has several benefits, with emphasis on parallelism and the real time capabilities.This paper discusses the hardware implementation of the CMAC type neural network, the architecture and parameters and the functional modules of the hardware implemented neuro-processor",robotics,1028,not included
,to_check,core,,2000-01-01 00:00:00,core,design and implementation of the agent-based evms system,,"This report describes the design and implementation of the agent-based EVMs system. A prototype is available at www.sims.berkeley.edu/research/metadata/demo.html. 1.0 Intelligent Agents Since 1990, intelligent agents have been broadly used in complex, dynamic, and open applications such as production planning, robotics, and in searching the Internet. Though theoretical research as well as real implementation of agent technology are common, there is no commonly agreed-upon definition of intelligent agents. From an Artificial Intelligence point of view, an intelligent agent is a hardware or (more usually) softwarebased computer system that has the following properties: (Wooldridge &amp; Jennings 1995) • Autonomy: agents work on their own without direct interventions of humans or others. • Social ability: agents interact with each other using an agent communication language (ACL, e.g., Telescript, Safe-Tcl, KQML, etc.). • Reactivity: agents perceive their environment and respond in a timely manner. • Pro-activeness: agents are goal-oriented",robotics,1029,not included
,to_check,core,,1998-01-01 00:00:00,core,golex --- bridging the gap between logic (golog) and a real robot,,"The control of mobile robots acting autonomously in the real world is one of the long-term goals of the field of artificial intelligence. So far the field lacks methods bridging the gap between the sophisticated symbolic techniques to represent and reason about action and more and more reliable low-level robot control and navigation systems. In this paper we present GOLEX, an execution and monitoring system for the logic-based action language GOLOG and the complex and distributed RHINO control software which operates on RWI B21 and B14 mobile robots. GOLEX provides the following features: it maps abstract primitive actions into low-level commands of the robot control system, thus allowing the user to concentrate on the application rather than the inner workings of the robot; it monitors the execution of the primitive GOLOG actions, making it possible to detect simple execution failures and timeouts; and it includes means to deal with sensing and user input and to continue the operati..",robotics,1030,not included
,to_check,core,,2000-01-01 00:00:00,core,natural landmark recognition using neural networks for autonomous vacuuming robots,,"Two types of neural networks were trained and tested on a real robot for a natural landmark recognition task. The neural networks investigated were the multilayer perceptron (MLP) and learning vector quantisation (LVQ). The intended application is for autonomous vacuuming robots in completely unknown indoor environments, using a novel topological world model and region filling algorithm. A topological world model based on natural landmarks is built incrementally while the robot systematically cleans the environment. The implementation of this world model depends on robust and accurate recognition of natural landmarks. Both types of neural network were found to be able to successfully recognise the natural landmarks selected",robotics,1031,included
,to_check,core,,1998-01-01 00:00:00,core,a neural schema architecture for autonomous robots,,"As autonomous robots become more complex in their behavior, more sophisticated software architectures are required to support the ever more sophisticated robotics software. These software architectures must support complex behaviors involving adaptation and learning, implemented, in particular, by neural networks. We present in this paper a neural based schema [2] software architecture for the development and execution of autonomous robots in both simulated and real worlds. This architecture has been developed in the context of adaptive robotic agents, ecological robots [6], cooperating and competing with each other in adapting to their environment. The architecture is the result of integrating a number of development and execution systems: NSL, a neural simulation language; ASL, an abstract schema language; and MissionLab, a schema-based mission-oriented simulation and robot system. This work contributes to modeling in Brain Theory (BT) and Cognitive Psychology, with applications in D..",robotics,1032,not included
,to_check,core,Sociedade Brasileira de Automática,2004-01-09 00:00:00,core,desempenho de algoritmos de aprendizagem por reforço sob condições de ambiguidade sensorial em robótica móvel,,"Analisamos a variação de desempenho de algoritmos de aprendizagem por reforço em situações de ambigüidade de estados comumente produzidas pela baixa capacidade sensorial de robôs móveis. Esta variação é produzida pela violação da condição de Markov, importante para garantir a convergência destes algoritmos. As conseqüências práticas desta violação em sistemas reais não estão avaliadas de maneira definitiva na literatura. São estudados neste artigo os algoritmos Q-learning, Sarsa e Q(lambda), em experimentos realizados em um robô móvel Magellan Pro™. De modo a definir um verificador de desempenho para os algoritmos testados, foi implementado um método para criar mapas cognitivos de resolução variável. Os resultados mostram um desempenho satisfatório dos algoritmos, com uma degradação suave em função da ambigüidade sensorial. O algoritmo Q-learning teve o melhor desempenho, seguido do algoritmo Sarsa. O algoritmo Q(lambda) teve seu desempenho limitado pelos parâmetros experimentais. O método de criação de mapas se mostrou bastante eficiente, permitindo uma análise adequada dos algoritmos.We analyzed the performance variation of reinforcement learning algorithms in ambiguous state situations commonly caused by the low sensing capability of mobile robots. This variation is caused by violation of the Markov condition, which is important to guarantee convergence of these algorithms. Practical consequences of this violation in real systems are not firmly established in the literature. The algorithms assessed in this study were Q-learning, Sarsa and Q(lambda), and the experiments were performed on a Magellan Pro™robot. A method to build variable resolution cognitive maps of the environment was implemented in order to provide realistic data for the experiments. The implemented learning algorithms presented satisfactory performance on real systems, with a graceful degradation of efficiency due to state ambiguity. The Q-learning algorithm accomplished the best performance, followed by the Sarsa algorithm. The Q(lambda) algorithm had its performance restrained by experimental parameters. The cognitive map learning method revealed to be quite efficient, allowing adequate algorithms assessment",robotics,1033,not included
,to_check,core,,2003-01-01 00:00:00,core,high-level robot programming in dynamic and incompletely known environments,,"This thesis advocates the usefulness and practicality of a logic-based approach to AI and in particular to high-level control of mobile robots. The contribution of the research work reported here is twofold: 1) the development of theoretical frameworks that account for uncertainty and unmodeled dynamics in an environment where an acting agent has to achieve certain goals and 2) the implementation of the developed ideas on a mobile robot. We have elaborated the approach to designing efficient and reliable controllers in Golog following two different perspectives on the environment where the control program is supposed to operate. According to one perspective, investigated in Chapter 4, the agent has a logical model of the world, but there is no probabilistic information about the environment where the agent is planning to act, and the agent is not capable or has no time for acquiring probabilities of different effects of its actions. In this case, the uncertainty and dynamics of the environment can be accounted only by observing the real outcomes of actions executed by the agent, by determining possible discrepancies between the observed outcomes and the effects expected according to the logical model of the world and then by recovering, if necessary, from th",robotics,1034,not included
,to_check,core,,1992-01-01 00:00:00,core,artificial intelligence for monitoring and diagnosis of robotic spacecraft,,"In this thesis the application of artificial intelligence to monitoring and diagnosis of robotic spacecraft is discussed. Several software prototype systems were developed to serve as testbeds for the research and to evaluate the effectiveness of the approach against real problems and current techniques used in NASA\u27s planetary exploration program.  Software prototypes were used to investigate the verification of robot plan execution. New artificial intelligence algorithms for monitoring and diagnosis of robot systems were designed, programmed, and tested. These included plan analysis for monitoring, sensor planning, generation of expected sensor values, and diagnosis of execution failures caused by hardware, environmental or plan anomalies. Testing was performed on a laboratory telerobotic hardware testbed for satellite servicing and on a mobile planetary rover robot operating in natural terrain.  Artificial intelligence algorithms, software prototypes, and more advanced, operationally capable systems for monitoring ground support systems and actual spacecraft in flight were designed, programmed, and tested. A ground support system that served as one test domain was the mirror cooling circuit of the 25-foot Space Simulator at the Jet Propulsion Laboratory (JPL) in Pasadena, California. A prototype monitoring system for this device based on a theory of ""predictive monitoring"" was developed and tested. Mission operations for the Voyager II spacecraft served as another test domain for an intelligent spacecraft health-monitoring and diagnosis system. This system was successfully tested in support of telecommunications operations during Voyager II\u27s encounter with the planet Neptune in 1989. This was the one of the first artificial intelligence systems to be used in planetary spacecraft operations at NASA/JPL. Subsequently, this system was adapted and tested in support of operations of the Magellan spacecraft telecommunications subsystem and the Galileo spacecraft power and pyro subsystem.  Some of the specific artificial intelligence algorithms that were developed for monitoring and diagnosis included the use of heuristic and causal model-based reasoning techniques for predictive generation of sensor values, sensor selection planning, dynamic alarm limit checking, hierarchical procedure specialists for fault diagnosis, and integration of Al with conventional systems in full-scale monitoring and diagnosis applications.  In support of this overall program of research, novel software engineering tools for artificial intelligence research and application development were also developed and will be discussed in the thesis.  The application of artificial intelligence techniques to the monitoring and diagnosis of robotic space systems was shown to be very effective with specific benefits in the areas of systems autonomy, spacecraft safety, ground operations productivity and automation. As a result of this work in part, artificial intelligence is now considered by senior mission designers to be an enabling technology for on-board automation of planetary rovers and for automation in mission operations at the Jet Propulsion Laboratory",robotics,1035,included
,to_check,core,,1971-01-01 00:00:00,core,research and applications:  artificial intelligence,https://core.ac.uk/download/pdf/80640932.pdf,"Research in the field of artificial intelligence is discussed. The focus of recent work has been the design, implementation, and integration of a completely new system for the control of a robot that plans, learns, and carries out tasks autonomously in a real laboratory environment. The computer implementation of low-level and intermediate-level actions; routines for automated vision; and the planning, generalization, and execution mechanisms are reported. A scenario that demonstrates the approximate capabilities of the current version of the entire robot system is presented",robotics,1036,not included
,to_check,core,,1971-01-01 00:00:00,core,approved by:,,"This is the final report for the most recent year of a program research in the field of artificial intelligence. The focus of recent work has been the design, implementation, and integration of a completely new system for the control of a robot that plans, learns, and carries out tasks autonomously in a real laboratory environment. The report includes sections that describe the computer implementation of low-level and intermediate-level actions; routines for automated vision; and the planning, generalization, and execution mechanisms. Section II of the report contains a scenario that demonstrates the approximate capabi Ii ties of the current version of the entire robot system. iii CONTENT",robotics,1037,not included
10.1109/isaect50560.2020.9523700,to_check,2020 International Symposium on Advanced Electrical and Communication Technologies (ISAECT),IEEE,2020-11-27 00:00:00,ieeexplore,edge-cloud architectures using uavs dedicated to industrial iot monitoring and control applications,https://ieeexplore.ieee.org/document/9523700/,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud.",autonomous vehicle,1038,included
334c96fd3595cf14477eaa52455f14b17d2b8ffc,to_check,semantic_scholar,J. Intell. Robotic Syst.,2020-01-01 00:00:00,semantic_scholar,pie: a tool for data-driven autonomous uav flight testing,https://www.semanticscholar.org/paper/334c96fd3595cf14477eaa52455f14b17d2b8ffc,"In this paper, a novel technique is presented to test the flight of an unmanned aerial vehicle autonomously in a real-world scenario using a data-driven technique without intervening with its onboard software. With the growing applications of such vehicles, testing of autonomous flight is a very important task for rapid deployment. There are different tools for modeling and simulating unmanned vehicles in virtual worlds such as Gazebo, MATLAB, Simulink, and Webots to name a few. None of these simulation tools are able to model all possible physical parameters of a real-world environment. Hence, the flight controller or mission planning software has to be tested in the physical world in the presence of an expert before deployment for a specific task. A Perception Inference Engine evaluation tool is presented that can infer internal states of the autonomous system from external observations only. The Gazebo simulation platform is used to collect data to develop the perception model. For real-time data collection, a VICON motion capture system is used to observe the autonomous flight of a small unmanned aerial vehicle. A state-of-the-art decision tree algorithm is used to implement the data-driven approach. The technique was tested using simulation data and verified with real-time data from Intel Aero Ready to Fly and Parrot AR. 2.0 drones. Moreover, we analyzed the robustness of the proposed system by introducing noise in sensor measurement and ambiguity in the testing scenario. We compared the performance of the decision tree classifier with Naïve bayes and support vector machine classifiers. It is shown that the developed system can be used for the performance evaluation of a UAV operating in the physical world by significantly reducing uncertainty in mission failure due to environmental parameters.",autonomous vehicle,1039,not included
48c6c90f708013906e4fb9e82635a94f1db386db,to_check,semantic_scholar,2021 IEEE Intelligent Vehicles Symposium (IV),2021-01-01 00:00:00,semantic_scholar,digimobot: digital twin for human-robot collaboration in indoor environments,https://www.semanticscholar.org/paper/48c6c90f708013906e4fb9e82635a94f1db386db,"Human-robot collaboration and cooperation are critical for Autonomous Mobile Robots (AMRs) in order to use them in indoor environments, such as offices, hospitals, libraries, schools, factories, and warehouses. Since a long transition period might be required to fully automate such facilities, we have to deploy AMRs while improving safety in the mixed environments of human and mobile robots. In addition, human behaviors in such environments might be difficult to predict. In this paper, we present a Digital Twin for Autonomous Mobile Robots system named DigiMobot to support, manage, monitor, and validate AMRs in indoor environments. First, DigiMobot can simulate human behaviors and robot movements to verify and validate AMRs to improve safety in a virtual world. Secondly, DigiMobot can monitor and manage AMRs in the physical world by collecting sensor data from each robot in real-time. Since DigiMobot enables us to test the robot systems in the virtual world, we can deploy and implement AMRs in each facility without any modifications. To show the feasibility of DigiMobot, we develop a software framework and two different types of autonomous mobile robots. Finally, we conduct real-world experiments in a warehouse located in Saitama, Japan, in which more than 400, 000 items are stored for commercial purposes.",autonomous vehicle,1040,included
fab176450e9142a31f47c0dbe1990f1e647cb66d,to_check,semantic_scholar,International Journal of Parallel Programming,2019-01-01 00:00:00,semantic_scholar,guest editorial: special issue on emerging technology for software define network enabled internet of things,https://www.semanticscholar.org/paper/fab176450e9142a31f47c0dbe1990f1e647cb66d,"The Internet of Things (IoT) has been considered as a technology, which takes the first step towards a smarter world bridging the physical world with the cyber world. Even though IoT notion has gained much attention during the last few decades, real-world implementation of large-scale IoT network is still evolving in its infancy. Since deployment lags far behind the theoretic notion, comprehensive research efforts on innovative applications, architecture, and a network of IoT are required to promote the implementation of large-scale, high-quality, efficient, and secure IoT scenarios. Software Defined Networks (SDN) facilitates a variety of opportunities for network evolution. The key feature of SDN is to decouple data and control planes, which removes control plane from network hardware. As a result, it offers a remarkable resilience in programming, while providing a broad range of opportunities to optimize the utilization of network resources. Owing to the characteristics of SDN, experts in both industry and academia claims SDN as one of the ideal technologies to bridge the gaps and to overcome drawbacks of IoT deployment. Exploiting the benefits of scalable and adaptable network devices, SDN is considered as highly promising in empowering smart, powerful, and open IoT services and communication functionalities. In fact, SDN is capable of addressing numerous challenges in IoT varying from innumerable service requests and responses, the enormous data flow of IoT sensors, devices, and appli-",autonomous vehicle,1041,not included
10.1109/sensysml50931.2020.00007,to_check,2020 IEEE Second Workshop on Machine Learning on Edge in Sensor Systems (SenSys-ML),IEEE,2020-04-21 00:00:00,ieeexplore,welcome message from sensys-ml'20 chairs,https://ieeexplore.ieee.org/document/9111711/,"We are very excited to welcome you to the second ACM/IEEE International Workshop on Machine Learning on Edge in Sensor Systems (SenSys-ML 2020). SenSys-ML’20 will be held in conjunction with the CPS-IoT Week 2020 and focuses on work that combines sensor signals from the physical world with machine learning, particularly in ways that are distributed to the device or use edge and fog computing. The development and deployment of ML at the very edge remains a technological challenge constrained by computing, memory, energy, network bandwidth, and data privacy and security limitations. This is especially true for battery-operated devices and always-on use cases and applications. In recent years this has gained attention from both academia and industry and many TinyML initiatives have been started focusing both in hardware and software advancements. This workshop will provide a forum for sensing, networking and machine learning researchers to present and share their latest research on building machine learning-enabled sensor systems. Sensys-ML focuses on providing extensive feedback on Work-In-Progress papers involving machine learning (TinyML/ UltraML) on sensor systems. Many papers were submitted from multiple countries, and four papers were selected for publication. This year our articles had themes including techniques for collecting low-resolution images from thermal cameras for human activity recognition, VAE-based approach for privacy preservation of sensor data, deep model compression techniques, and novel GRU based shallow Neural Networks. We see advancement and contributions made by research work will have a significant impact on real-world applications and future research directions.",industry,1042,not included
