id,status,doi,publisher,database,query_name,query_value,url,publication_date,title,abstract,semantic_score
1,included,0ef4f7572bb6a7321de2841245946f6151efb839,,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/0ef4f7572bb6a7321de2841245946f6151efb839,,managing ethical risks of artiﬁcial intelligence in business applications,"The introduction of artiﬁcial intelligence (AI) capabilities in business applications provides signiﬁcant beneﬁts but requires organizations to manage critical risks of AI ethical consequences. We survey a range of large organizations on their use of enterprise risk management (ERM) processes and toolsets to predict and control the ethical risks of AI. Four serious gaps in current ERM systems are identiﬁed from analyses of the survey results: (1) AI ethical principles do not translate eﬀectively to ethical practices; (2) Real-time monitoring of AI ethical risks is needed; (3) ERM systems emphasize economic not ethical risks; and (4) When ethical risks are identiﬁed, no solutions are readily at hand. To address these gaps, we propose a proactive approach to manage ethical risks by extending current ERM frameworks. An enhanced ERM (e-ERM) framework is designed and evaluated by subject matter expert focus groups. We conclude with observations and future research directions on the need for more aggressive pro-ethical management oversight as organizations move to ubiquitous use of AI-driven business applications.",0.8062968552112579
2,excluded,883e2aea0b82b8c16e0bafcc7160f4eedb276849,"2022 IEEE 28th International Conference on Engineering, Technology and Innovation (ICE/ITMC) & 31st International Association For Management of Technology (IAMOT) Joint Conference",semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/883e2aea0b82b8c16e0bafcc7160f4eedb276849,2022.0,ai governance and ethics in public procurement: bridging the gap between theory and practice,"As Artificial Intelligence (AI) systems have become increasingly widespread, research in AI ethics has sparked. When developing the systems, many tools and methods are available for implementing AI ethics in practice. In addition, the research in AI governance is starting to activate, and some models for AI governance have already been introduced. Simultaneously, the role of the information systems (IS) procurement function has developed from its traditional operative role to a more strategic position, as the investments in IT have been on a constant rise. Success in procurement is found to be critical regarding the success of the development and implementation of information systems. But how are the existing tools and methods in AI ethics related to procurement practices? And how is procurement positioned in the proposed AI governance frameworks? This study answers these questions by setting up a research framework based on AI governance models and analyzing existing tools and methods in AI ethics.",0.8154128462076187
3,excluded,5d64050bd6ad24b2853079af619c434898ce7740,arXiv.org,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/5d64050bd6ad24b2853079af619c434898ce7740,2024.0,ai governance and accountability: an analysis of anthropic's claude,"As AI systems become increasingly prevalent and impactful, the need for effective AI governance and accountability measures is paramount. This paper examines the AI governance landscape, focusing on Anthropic's Claude, a foundational AI model. We analyze Claude through the lens of the NIST AI Risk Management Framework and the EU AI Act, identifying potential threats and proposing mitigation strategies. The paper highlights the importance of transparency, rigorous benchmarking, and comprehensive data handling processes in ensuring the responsible development and deployment of AI systems. We conclude by discussing the social impact of AI governance and the ethical considerations surrounding AI accountability.",0.8206197261810303
4,excluded,ace2ad5f557fc104927ddbdaa80d240604435bc9,Policy & Society,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/ace2ad5f557fc104927ddbdaa80d240604435bc9,2025.0,responsible governance of generative ai: conceptualizing genai as complex adaptive systems,"
 Organizations increasingly use Generative Artificial Intelligence (AI) to create strategic documents, legislation, and recommendations to support decision-making. Many current AI initiatives are technology-deterministic, whereas technology co-evolves with the social environment, resulting in new applications and situations. This paper presents a novel view of AI governance by organizations from the perspective of complex adaptive systems (CASs). AI is conceptualized as a socio-technological and adaptive system in which people, policies, systems, data, AI, processes, and other elements co-evolve. The CAS lens draws attention to focusing AI governance on the entire organization, taking an outward perspective and considering public values and societal concerns. Although there is no shortage of AI governance instruments, they differ in their effectiveness, and combinations of appropriate mechanisms should be selected to deal with AI’s evolving nature and complexity. A major challenge is that no responsibility, and therefore accountability, is taken due to the lack of understanding of the full socio-technological CAS. As such, joint accountability is needed in which involved parties work together.",0.8411970287561417
5,excluded,49a50fb2078540ca059b3d6d282acdf4a7abd0b9,Strategic Analysis,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/49a50fb2078540ca059b3d6d282acdf4a7abd0b9,2023.0,trustworthy artificial intelligence: design of ai governance framework,"Abstract This article presents the various challenges in the current system of AI governance and the correlation between data, algorithm, technology, governance, and geopolitics surrounding its successful implementation. The focal point of the article is the Adaptive-Hybrid AI Governance framework based on technical, ethical, and societal regulatory mechanisms that models trustworthy AI and the risks associated with it. The article highlights the need for trustworthy AI and how major countries are shaping their AI regulatory mechanisms. It presents a case study on AI governance in defence that elucidates ethical AI governance through various use cases.",0.8252739846706391
6,excluded,31c6e226d88ea6611dd6e7aa03febfcf8c1beb36,IEEE technology & society magazine,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/31c6e226d88ea6611dd6e7aa03febfcf8c1beb36,2021.0,ai4eq: for a true global village not for global pillage,"<bold>The last few</bold> years have seen a large number of initiatives on artificial intelligence (AI) ethics: intergovernmental-institution initiatives such as “Ethics Guidelines for Trustworthy AI” from the high-level expert group on AI of the European Commission <xref ref-type=""bibr"" rid=""ref1"">[1]</xref> or the Organisation for Economic Cooperation and Development (OECD) Council Recommendation on Artificial Intelligence <xref ref-type=""bibr"" rid=""ref2"">[2]</xref>, government initiatives such as that of the U.K. Parliament Select Committee on Artificial Intelligence <xref ref-type=""bibr"" rid=""ref3"">[3]</xref>, industry initiatives on AI ethical codes such as those of Google, IBM, Microsoft, and Intel, academic initiatives such as the Montreal declaration for the responsible development of AI <xref ref-type=""bibr"" rid=""ref4"">[4]</xref>, the Stanford University 100 Year Study on AI <xref ref-type=""bibr"" rid=""ref5"">[5]</xref> or the Alan Turing Institute’s “Understanding Artificial Intelligence Ethics and Safety” <xref ref-type=""bibr"" rid=""ref6"">[6]</xref>, and finally professional body initiatives such as the IEEE Global Initiative on Ethics of Autonomous/Intelligent Systems (A/IS) <xref ref-type=""bibr"" rid=""ref7"">[7]</xref>. These initiatives, while acknowledging the potential of A/IS technologies to contribute to global socioeconomic solutions, highlight the increasing challenges posed by these technologies in the ethical, moral, legal, humanitarian, and sociopolitical domains.",0.838510024547577
7,excluded,7c8a533175c24ceff2cdb82dfd340065cc93a1ce,Applied Sciences,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/7c8a533175c24ceff2cdb82dfd340065cc93a1ce,2023.0,"re-thinking data strategy and integration for artificial intelligence: concepts, opportunities, and challenges","The use of artificial intelligence (AI) is becoming more prevalent across industries such as healthcare, finance, and transportation. Artificial intelligence is based on the analysis of large datasets and requires a continuous supply of high-quality data. However, using data for AI is not without challenges. This paper comprehensively reviews and critically examines the challenges of using data for AI, including data quality, data volume, privacy and security, bias and fairness, interpretability and explainability, ethical concerns, and technical expertise and skills. This paper examines these challenges in detail and offers recommendations on how companies and organizations can address them. By understanding and addressing these challenges, organizations can harness the power of AI to make smarter decisions and gain competitive advantage in the digital age. It is expected, since this review article provides and discusses various strategies for data challenges for AI over the last decade, that it will be very helpful to the scientific research community to create new and novel ideas to rethink our approaches to data strategies for AI.",0.8206197261810303
8,excluded,31564919d4d826e28388d6a63a6af5f6ca5aafb9,Internet Research,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/31564919d4d826e28388d6a63a6af5f6ca5aafb9,2022.0,how to explain ai systems to end users: a systematic literature review and research agenda,"PurposeInscrutable machine learning (ML) models are part of increasingly many information systems. Understanding how these models behave, and what their output is based on, is a challenge for developers let alone non-technical end users.Design/methodology/approachThe authors investigate how AI systems and their decisions ought to be explained for end users through a systematic literature review.FindingsThe authors’ synthesis of the literature suggests that AI system communication for end users has five high-level goals: (1) understandability, (2) trustworthiness, (3) transparency, (4) controllability and (5) fairness. The authors identified several design recommendations, such as offering personalized and on-demand explanations and focusing on the explainability of key functionalities instead of aiming to explain the whole system. There exists multiple trade-offs in AI system explanations, and there is no single best solution that fits all cases.Research limitations/implicationsBased on the synthesis, the authors provide a design framework for explaining AI systems to end users. The study contributes to the work on AI governance by suggesting guidelines on how to make AI systems more understandable, fair, trustworthy, controllable and transparent.Originality/valueThis literature review brings together the literature on AI system communication and explainable AI (XAI) for end users. Building on previous academic literature on the topic, it provides synthesized insights, design recommendations and future research agenda.",0.8145611852407455
9,excluded,70043a0b612b6253b37df7d363b3bf2ec3d581c7,ACM Computing Surveys,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/70043a0b612b6253b37df7d363b3bf2ec3d581c7,2021.0,trustworthy ai: from principles to practices,"The rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people’s trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, and accountability. To unify currently available but fragmented approaches toward trustworthy AI, we organize them in a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to system development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items for practitioners and societal stakeholders (e.g., researchers, engineers, and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges for the future development of trustworthy AI systems, where we identify the need for a paradigm shift toward comprehensively trustworthy AI systems.",0.8023776173591614
10,excluded,fe0dd6c6f09d9efe49148205237d66602fe9f06d,arXiv.org,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/fe0dd6c6f09d9efe49148205237d66602fe9f06d,2022.0,explainability for identification of vulnerable groups in machine learning models,"If a prediction model identifies vulnerable individuals or groups, the use of that model may become an ethical issue. But can we know that this is what a model does? Machine learning fairness as a field is focused on the just treatment of individuals and groups under information processing with machine learning methods. While considerable attention has been given to mitigating discrimination of protected groups, vulnerable groups have not received the same attention. Unlike protected groups, which can be regarded as always vulnerable, a vulnerable group may be vulnerable in one context but not in another. This raises new challenges on how and when to protect vulnerable individuals and groups under machine learning. Methods from explainable artificial intelligence (XAI), in contrast, do consider more contextual issues and are concerned with answering the question""why was this decision made?"". Neither existing fairness nor existing explainability methods allow us to ascertain if a prediction model identifies vulnerability. We discuss this problem and propose approaches for analysing prediction models in this respect.",0.8554961800575256
11,excluded,0065be6d1ad6a14c3bf69c7f1443cf1be6c65705,"Conference on Fairness, Accountability and Transparency",semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/0065be6d1ad6a14c3bf69c7f1443cf1be6c65705,2021.0,an action-oriented ai policy toolkit for technology audits by community advocates and activists,"Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.",0.8351606994867324
12,excluded,65f0e1debbc22274681b3169822f6cc08e28a47e,XVII Brazilian Symposium on Information Systems,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/65f0e1debbc22274681b3169822f6cc08e28a47e,2021.0,ethical guidelines and principles in the context of artificial intelligence,"The interest in Artificial Intelligence (AI) based systems has been gaining momentum at a fast pace, both for software development teams and for society as a whole. This work aims to identify the guidelines and ethical principles for systems based on Artificial Intelligence. Design Science Research methodology was adopted in order to understand the various guidelines and principles existing in the literature. From the current landscape, a body of knowledge in the field of AI ethics is presented, with the purpose of supporting developers and Product Owners in identifying the guidelines and ethical principles in the literature so that they can be used during the software development process. Thus, this work will contribute to the various stakeholders in the development of ethical systems in the context of AI, such as: policy makers, ethicists, users, organizations, data scientists, development teams, among others.",0.8062968552112579
13,excluded,367b89b469cf901005f6eb35c67d9ff6b24291fc,Journal of health care for the poor and underserved,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/367b89b469cf901005f6eb35c67d9ff6b24291fc,2021.0,a proposed framework on integrating health equity and racial justice into the artificial intelligence development lifecycle,"Abstract:The COVID-19 pandemic has created multiple opportunities to deploy artificial intelligence (AI)-driven tools and applied interventions to understand, mitigate, and manage the pandemic and its consequences. The disproportionate impact of COVID-19 on racial/ethnic minority and socially disadvantaged populations underscores the need to anticipate and address social inequalities and health disparities in AI development and application. Before the pandemic, there was growing optimism about AI's role in addressing inequities and enhancing personalized care. Unfortunately, ethical and social issues that are encountered in scaling, developing, and applying advanced technologies in health care settings have intensified during the rapidly evolving public health crisis. Critical voices concerned with the disruptive potentials and risk for engineered inequities have called for reexamining ethical guidelines in the development and application of AI. This paper proposes a framework to incorporate ethical AI principles into the development process in ways that intentionally promote racial health equity and social justice. Without centering on equity, justice, and ethical AI, these tools may exacerbate structural inequities that can lead to disparate health outcomes.",0.8288842886686325
14,excluded,dff39cc6d10d6532711b0205e245750d47bb1d90,"Lernen, Wissen, Daten, Analysen",semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/dff39cc6d10d6532711b0205e245750d47bb1d90,2023.0,governance of artificial intelligence - a framework towards ethical ai applications,"Artificial intelligence (AI) has extensive potential in changing businesses. Various applications have been identified that are either already implemented, or under development. However, many – especially small and medium-sized – enterprises struggle with the potential problems that AI might cause. Leaders and managers are often willing to implement AI in their companies, but are looking for guidance, how they can ensure that the AI will have no negative impact on customers, employees or their business. To address this area of conflict, a governance framework is presented, which guides the development of AI solutions to address potential ethical challenges. The framework is rooted in the body of knowledge of the information systems discipline – especially in general IT governance frameworks and other proposed governance structures considering AI – and its content has been adapted specific to ethical issues in AI development and usage based on experts ’ insights.",0.8500882655382156
15,excluded,17aedaff08b4fd6f269c838ed8d7cb6698ea5f01,Workshop on Visual Analytics in Healthcare,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/17aedaff08b4fd6f269c838ed8d7cb6698ea5f01,2023.0,the iterative design process of an explainable ai application for non-invasive diagnosis of cns tumors: a user-centered approach,"Artificial Intelligence (AI) is well-suited to help support complex decision-making tasks within clinical medicine, including clinical imaging applications like radiographic differential diagnosis of central nervous system (CNS) tumors. So far, there have been numerous examples of theoretical AI solutions for this space, for example, large-scale corporate efforts like IBM’s Watson AI. However, clinical implementation remains limited due to factors related to the alignment of this technology in the clinical setting. User-Centered Design (UCD) is a design philosophy that focuses on developing tailored solutions for specific users or user groups. In this study, we applied UCD to develop an explainable AI tool to support clinicians in our use case. Through four design iterations, starting from basic functionality and visualizations, we progressed to functional prototypes in a realistic testing environment. We discuss our motivation and approach for each iteration, along with key insights gained. This UCD process has advanced our conceptual idea from feasibility testing to interactive functional AI interfaces designed for specific clinical and cognitive tasks. It has also provided us with directions to develop further an AI system for the non-invasive diagnosis of CNS tumors.",0.8040409088134766
16,excluded,dc0b8bde0efafffb609ea7da8bd9799688b050c2,arXiv.org,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/dc0b8bde0efafffb609ea7da8bd9799688b050c2,2024.0,ehazop: a proof of concept ethical hazard analysis of an assistive robot,"The use of assistive robots in domestic environments can raise significant ethical concerns, from the risk of individual ethical harm to wider societal ethical impacts including culture flattening and compromise of human dignity. It is therefore essential to ensure that technological development of these robots is informed by robust and inclusive techniques for mitigating ethical concerns. This paper presents EHAZOP, a method for conducting an ethical hazard analysis on an assistive robot. EHAZOP draws upon collaborative, creative and structured processes originating within safety engineering, using these to identify ethical concerns associated with the operation of a given assistive robot. We present the results of a proof of concept study of EHAZOP, demonstrating the potential for this process to identify diverse ethical hazards in these systems.",0.8116682052612305
17,excluded,6035386cbdadfd80ccae0b103bda9d04f65b44fb,2022 IEEE International Conference on Big Data (Big Data),semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/6035386cbdadfd80ccae0b103bda9d04f65b44fb,2022.0,towards implementing responsible ai,"As the deployment of artificial intelligence (AI) is changing many fields and industries, there are concerns about AI systems making decisions and recommendations without adequately considering various ethical aspects, such as accountability, reliability, transparency, explainability, contestability, privacy, and fairness. While many sets of AI ethics principles have been recently proposed that acknowledge these concerns, such principles are high-level and do not provide tangible advice on how to develop ethical and responsible AI systems. To gain insight on the possible implementation of the principles, we conducted an empirical investigation involving semi-structured interviews with a cohort of AI practitioners. The salient findings cover four aspects of AI system design and development, adapting processes used in software engineering: (i) high-level view, (ii) requirements engineering, (iii) design and implementation, (iv) deployment and operation.",0.8554961800575256
18,excluded,10.18500/1818-9601-2023-23-4-447-453,Izvestiya of Saratov University. Sociology. Politology,semantic_scholar,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',https://www.semanticscholar.org/paper/39a95e3f3124887f824d47586b917916466f5def,2000-01-01 00:00:00,strategic planning in public administration as a political system of institutional instruments and goal-setting mechanisms,"The article is devoted to the subject field of political science research within the scientific specialty “Public Administration and sectoral policies” and the analysis of the practice of state strategic planning as a political system of institutional instruments and goal-setting mechanisms in the Russian Federation. The article discusses public political and legal mechanisms for combining values and goals with the choice of ways and methods to achieve them. The constitutional exclusivity and independence of each branch of government as the basis of its strategic resource and strategic planning potential is highlighted as the fundamental public political and legal mechanism for combining values and goals with the choice of ways and methods to achieve them. A general description of the architecture of the unified system of public power in accordance with the amendments to the Constitution of Russia and the current configuration of the public administration system is given. The importance of political factor of the effectiveness of the administration of strategic management system and the implementation of sectoral policies in modern Russia on the basis of the mechanisms of coordinated functioning and interaction of public authorities and local self-government in a single system of public authority is shown. Fundamental changes are highlighted that make it possible to significantly increase the efficiency of the federal government in a unified system of public authority with an emphasis on the implementation of national development goals of both the Russian Federation as a whole and the regions based on the sectoral structure of the economy. Attention is focused on the political goals of the regional factor of socio-economic development of territories on the basis of mechanisms and tools of strategic planning, the directions of fundamental political, legal, administrative and managerial decisions on the implementation of the regional investment standard are determined. The role of federal institutions of innovative development and provision of infrastructure projects and programs for solving the tasks of ensuring sustainable economic growth and diversification of the modern Russian economy, which cannot be optimally implemented by market mechanisms, was demonstrated. Conclusions are drawn about the importance of the interpretative understanding of politics as a system of institutional tools and goal-setting mechanisms in the political and legal practice of state strategic planning for determining promising directions in the subject field of scientific research within the specialty “Public Administration and sectoral policies”, for solving problems of improving the quality of public administration through the introduction of a management model based on big data and artificial intelligence, the transition of the public authority system to a data-based management model using a platform approach.",0.8023776173591614
19,included,4f4a2f5c9e1519db0c64e8cd2fb015ea5e3e4d0b,Future Internet,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/4f4a2f5c9e1519db0c64e8cd2fb015ea5e3e4d0b,2025.0,high-risk ai systems—lie detection application,"Integrating artificial intelligence into border control systems may help to strengthen security and make operations more efficient. For example, the emerging application of artificial intelligence for lie detection when inspecting passengers presents significant opportunities for future implementation. However, as it makes use of technology that is associated with artificial intelligence, the system is classified as high risk, in accordance with the EU AI Act and, therefore, must adhere to rigorous regulatory requirements to mitigate potential risks. This manuscript distinctly amalgamates the technical, ethical, and legal aspects, thereby offering an extensive examination of the AI-based lie detection systems utilized in border security. This academic paper is uniquely set apart from others because it undertakes a thorough investigation into the categorization of these emerging technologies in terms of the regulatory framework established by the EU AI Act, which classifies them as high risk. It further makes an assessment of practical case studies, including notable examples such as iBorderCtrl and AVATAR. This in-depth analysis seeks to emphasize not only the enormous challenges ahead for practitioners but also the progress made in this emerging field of study. Furthermore, it seeks to investigate threats, vulnerabilities, and privacy concerns associated with AI, while providing security controls to address difficulties related to lie detection. Finally, we propose a framework that encompasses the EU AI Act’s principles and serves as a foundation for future approaches and research projects. By analyzing current methodologies and considering future directions, the paper aims to provide a comprehensive understanding of the viability and consequences of deploying AI lie detection capabilities in border control.",0.8043087244033813
20,excluded,6226c5ce995413df09025604bdff10f7ad48dc67,,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/6226c5ce995413df09025604bdff10f7ad48dc67,2020.0,artificial neural networks in public policy: towards an analytical framework,"ARTIFICIAL NEURAL NETWORKS IN PUBLIC POLICY: TOWARDS AN ANALYTICAL FRAMEWORK Joshua Lee, Ph.D. George Mason University, 2020 Committee Chair: Dr. Laurie Schintler This dissertation assesses how artificial neural networks (ANNs) and other machine learning systems should be devised, built, and implemented in US governmental organizations (i.e. public agencies). While it primarily focuses on ANNs given their current prevalence and accuracy, many of its conclusions are broadly applicable to other kinds of machine learning as well. It develops an analytical framework, drawn from diverse fields including law, behavioral psychology, public policy, and computer science, that public agency managers and analysts can utilize. The framework yields a series of principles based on my research methodology that I argue are the most relevant to public agencies. The qualitative methodology consists of an iterative approach based on archival research, peer review, expert interviews, and comparative analysis. Critically, this dissertation’s intent is not to provide the specific answers to all questions related to machine learning in public agencies. Given the speed at which this field changes, attempting to provide universally applicable answers would be difficult and short term at best. Rather, this framework focuses on principles which can help guide the user to the proper questions they need to ask for their particular use case. In that same vein, the normative principles it provides are procedurally focused in scope rather than focused on policy outcomes. In other words, this framework is meant to be equally applicable regardless of what one’s specific policy goals are.",0.8437141388654709
21,excluded,c1c851389073ff6d6030e53e36cdf1dff63609d5,Journal of Digital Economy Research,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/c1c851389073ff6d6030e53e36cdf1dff63609d5,2023.0,social-economic aspects of the implementation of natural intelligence technologies: part 2 - natural intelligence in government,"The purpose of this article is to investigate the application of artificial intelligence (AI) in public administration. Following up the research of M.V. Fedorov [38- 41], which provides an overview of the global effects of AI implementation, covering socio-ethical aspects, economic impact and regulatory framework for sustainable development strategies, the present paper focuses on the key factors that define the framework for the use of AI in public administration. The paper considers AI as part of the overall process of technological development and explores the links between AI and other areas such as computing technology and data collection techniques. Particular attention is paid to the analysis of international and Russian experience of implementing AI in public administration. The authors seek to develop recommendations for further development of this industry based on the experience gained. They also consider approaches that may lead to the development of strategic principles focused on long-term predictions of the effects of AI in optimising public administration, and the subsequent implementation of appropriate regulatory practices. Thus, this article seeks to provide an overview and analysis of the main aspects of the use of AI in public administration with a focus on international and Russian experience, and to offer recommendations for the further development of this field.",0.8133152216672898
22,excluded,b7b7f4331639d708d7881267101f54d92eefcf99,2023 5th International Conference of the Portuguese Society for Engineering Education (CISPEE),semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/b7b7f4331639d708d7881267101f54d92eefcf99,2023.0,the importance of ethical reasoning in next generation tech education,"Artificial intelligence (AI) is having a profound impact on human life, with both benefits and drawbacks in the societal, environmental, and technological realms. However, the ethical implications of AI are often not addressed in technology education, leaving future professionals with a lack of awareness in this area. This is concerning, as AI has the potential to greatly information delivery and affect human thinking, interaction, decision-making, and communication. To address these issues, there is a need for a framework to guide and help future AI developers make ethically responsible decisions. In this paper we propose a framework to foster ethical awareness and promote respect for human dignity and well-being, while also preventing harm. It is designed to be incorporated into technology education, ensuring that future professionals are equipped to navigate the ethical implications of AI. By prioritizing ethical reasoning in technology education, we can build a better and more responsible AI industry, ensuring that AI can provide benefits for society and does not cause harm. Additionally, a tech industry that values ethics and social responsibility will be better equipped to build technology that serves the public interest, rather than solely maximizing profits. Teaching ethical reasoning in technology education is a crucial step in preparing future professionals to make informed and ethical decisions in the development and use of AI systems. It will lead to a better and more responsible AI industry that benefits all of society.",0.8079065144062042
23,excluded,087ebdd2529399cca8394bf285dd92c6400ce33c,International Journal of Production Research,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/087ebdd2529399cca8394bf285dd92c6400ce33c,2022.0,human-centric artificial intelligence architecture for industry 5.0 applications,"Human-centricity is the core value behind the evolution of manufacturing towards Industry 5.0. Nevertheless, there is a lack of architecture that considers safety, trustworthiness, and human-centricity at its core. Therefore, we propose an architecture that integrates Artificial Intelligence (Active Learning, Forecasting, Explainable Artificial Intelligence), simulated reality, decision-making, and users' feedback, focussing on synergies between humans and machines. Furthermore, we align the proposed architecture with the Big Data Value Association Reference Architecture Model. Finally, we validate it on three use cases from real-world case studies.",0.8154128462076187
24,excluded,ae529b8b16c6eeb9f5a4dee7f8e39a6d0fa62ae0,arXiv.org,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/ae529b8b16c6eeb9f5a4dee7f8e39a6d0fa62ae0,2024.0,ethical and scalable automation: a governance and compliance framework for business applications,"The popularisation of applying AI in businesses poses significant challenges relating to ethical principles, governance, and legal compliance. Although businesses have embedded AI into their day-to-day processes, they lack a unified approach for mitigating its potential risks. This paper introduces a framework ensuring that AI must be ethical, controllable, viable, and desirable. Balancing these factors ensures the design of a framework that addresses its trade-offs, such as balancing performance against explainability. A successful framework provides practical advice for businesses to meet regulatory requirements in sectors such as finance and healthcare, where it is critical to comply with standards like GPDR and the EU AI Act. Different case studies validate this framework by integrating AI in both academic and practical environments. For instance, large language models are cost-effective alternatives for generating synthetic opinions that emulate attitudes to environmental issues. These case studies demonstrate how having a structured framework could enhance transparency and maintain performance levels as shown from the alignment between synthetic and expected distributions. This alignment is quantified using metrics like Chi-test scores, normalized mutual information, and Jaccard indexes. Future research should explore the framework's empirical validation in diverse industrial settings further, ensuring the model's scalability and adaptability.",0.826026663184166
25,excluded,53835288d4d42bb3a2abc44984613ec3e0b8e478,"Journal of Information, Communication and Ethics in Society",semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/53835288d4d42bb3a2abc44984613ec3e0b8e478,2021.0,"ai led ethical digital transformation: framework, research and managerial implications","
Purpose
Digital transformation (DT) leverages digital technologies to change current processes and introduce new processes in any organisation’s business model, customer/user experience and operational processes (DT pillars). Artificial intelligence (AI) plays a significant role in achieving DT. As DT is touching each sphere of humanity, AI led DT is raising many fundamental questions. These questions raise concerns for the systems deployed, how they should behave, what risks they carry, the monitoring and evaluation control we have in hand, etc. These issues call for the need to integrate ethics in AI led DT. The purpose of this study is to develop an “AI led ethical digital transformation framework”.


Design/methodology/approach
Based on the literature survey, various existing business ethics decision-making models were synthesised. The authors mapped essential characteristics such as intensity and the individual, organisational and opportunity factors of ethics models with the proposed AI led ethical DT. The DT framework is evaluated using a thematic analysis of 23 expert interviews with relevant AI ethics personas from industry and society. The qualitative data of the interviews and opinion data has been analysed using MAXQDA software.


Findings
The authors have explored how AI can drive the ethical DT framework and have identified the core constituents of developing an AI led ethical DT framework. Backed by established ethical theories, the paper presents how DT pillars are related and sequenced to ethical factors. This research provides the potential to examine theoretically sequenced ethical factors with practical DT pillars.


Originality/value
The study establishes deduced and induced ethical value codes based on thematic analysis to develop guidelines for the pursuit of ethical DT. The authors identify four unique induced themes, namely, corporate social responsibility, perceived value, standard benchmarking and learning willingness. The comprehensive findings of this research, supported by a robust theoretical background, have substantial implications for academic research and corporate applicability. The proposed AI led ethical DT framework is unique and can be used for integrated social, technological and economic ethical research.
",0.8418905466794968
26,excluded,1c12ca7f5f10181df25db869e04c9666fed69bca,Information and Computer Security,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/1c12ca7f5f10181df25db869e04c9666fed69bca,2023.0,european artificial intelligence act: an ai security approach,"
Purpose
The purpose of this paper is to highlight the key technical challenges that derive from the recently proposed European Artificial Intelligence Act and specifically, to investigate the applicability of the requirements that the AI Act mandates to high-risk AI systems from the perspective of AI security.


Design/methodology/approach
This paper presents the main points of the proposed AI Act, with emphasis on the compliance requirements of high-risk systems. It matches known AI security threats with the relevant technical requirements, it demonstrates the impact that these security threats can have to the AI Act technical requirements and evaluates the applicability of these requirements based on the effectiveness of the existing security protection measures. Finally, the paper highlights the necessity for an integrated framework for AI system evaluation.


Findings
The findings of the EU AI Act technical assessment highlight the gap between the proposed requirements and the available AI security countermeasures as well as the necessity for an AI security evaluation framework.


Originality/value
AI Act, high-risk AI systems, security threats, security countermeasures.
",0.8154128462076187
27,excluded,0bc528a9fbb004993ce1f5d13c4229b3ea078c34,,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/0bc528a9fbb004993ce1f5d13c4229b3ea078c34,2021.0,a toolkit to enable the design of trustworthy,"Technological progress in artificial intelligence (AI) and machine learning (ML) has an enormous impact on our society, economy and environment. And although the urgent need for creating sustainable and ethical AI technology is admitted, there exists a lack of design tools and expertise to facilitate this advancement. This study investigates how to help designers design for the value of trust in AI systems. A literature review unveiled a myriad of ethical AI principles as well as gathered existing tools addressing the research area. Iterative reviews together with an expert on trust in technology evaluated these guidelines and a toolkit prototype containing 29 design principles had been created. Through multiple participatory design workshops the next iteration of the toolkit was co-designed in collaboration with design professionals. The result is an iterated toolkit comprising 16 principles relevant in the design for trust in AI systems, and providing tool suggestions for each principle.",0.8235145151615143
28,included,37730b6bc3fe8c5655780efba083c8401808acaf,arXiv.org,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/37730b6bc3fe8c5655780efba083c8401808acaf,2022.0,putting ai ethics into practice: the hourglass model of organizational ai governance,"The organizational use of artificial intelligence (AI) has rapidly spread across various sectors. Alongside the awareness of the benefits brought by AI, there is a growing consensus on the necessity of tackling the risks and potential harms, such as bias and discrimination, brought about by advanced AI technologies. A multitude of AI ethics principles have been proposed to tackle these risks, but the outlines of organizational processes and practices for ensuring socially responsible AI development are in a nascent state. To address the paucity of comprehensive governance models, we present an AI governance framework, the hourglass model of organizational AI governance, which targets organizations that develop and use AI systems. The framework is designed to help organizations deploying AI systems translate ethical AI principles into practice and align their AI systems and processes with the forthcoming European AI Act. The hourglass framework includes governance requirements at the environmental, organizational, and AI system levels. At the AI system level, we connect governance requirements to AI system life cycles to ensure governance throughout the system's life span. The governance model highlights the systemic nature of AI governance and opens new research avenues into its practical implementation, the mechanisms that connect different AI governance layers, and the dynamics between the AI governance actors. The model also offers a starting point for organizational decision-makers to consider the governance components needed to ensure social acceptability, mitigate risks, and realize the potential of AI.",0.8546511858701706
29,excluded,fd8d42fa78292844749f999853ffb5fd1ff5b407,Frontiers in Human Dynamics,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/fd8d42fa78292844749f999853ffb5fd1ff5b407,2022.0,how should public administrations foster the ethical development and use of artificial intelligence? a review of proposals for developing governance of ai,"Recent advances in AI raise questions about its social impacts and implementation. In response, governments and public administrations seek to develop adequate governance frameworks to mitigate risks and maximize the potential of AI development and use. Such work largely deals with questions of how challenges and risks should be managed, which values and goals should be pursued, and through which institutional mechanisms and principles these goals could be achieved. In this paper, we conduct a systematic review of the existing literature on the development of AI governance for public administration. The article describes principles and means by which public administrations could guide and steer AI developers and users in adopting ethical and responsible practices. The reviewed literature indicates a need for public administrations to move away from top-down hierarchical governance principles and adopt forms of inclusive policy-making to ensure the actionability of ethical and responsibility principles in the successful governance of AI development and use. By combining the results, we propose a CIIA (Comprehensive, Inclusive, Institutionalized, and Actionable) framework that integrates the key aspects of the proposed development solutions into an ideal typical and comprehensive model for AI governance.",0.832381296157837
