id,status,doi,publisher,database,query_name,query_value,url,publication_date,title,abstract,semantic_score
1,included,10.48550/arxiv.2408.00965,arXiv.org,semantic_scholar,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',https://www.semanticscholar.org/paper/15b153e0f6342ad455bfb5e2b6090b89c8066340,2000-01-01 00:00:00,integrating esg and ai: a comprehensive responsible ai assessment framework,"Artificial Intelligence (AI) is a widely developed and adopted technology across entire industry sectors. Integrating environmental, social, and governance (ESG) considerations with AI investments is crucial for ensuring ethical and sustainable technological advancement. Particularly from an investor perspective, this integration not only mitigates risks but also enhances long-term value creation by aligning AI initiatives with broader societal goals. Yet, this area has been less explored in both academia and industry. To bridge the gap, we introduce a novel ESG-AI framework, which is developed based on insights from engagements with 28 companies and comprises three key components. The framework provides a structured approach to this integration, developed in collaboration with industry practitioners. The ESG-AI framework provides an overview of the environmental and social impacts of AI applications, helping users such as investors assess the materiality of AI use. Moreover, it enables investors to evaluate a company's commitment to responsible AI through structured engagements and thorough assessment of specific risk areas. We have publicly released the framework and toolkit in April 2024, which has received significant attention and positive feedback from the investment community. This paper details each component of the framework, demonstrating its applicability in real-world contexts and its potential to guide ethical AI investments.",0.8637049794197083
2,included,10.1007/s43681-023-00309-1,Springer,springer,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',http://dx.doi.org/10.1007/s43681-023-00309-1,2023-06-20 00:00:00,embedded ethics for responsible artificial intelligence systems (ee-rais) in disaster management: a conceptual model and its deployment,"In this paper, we argue that Responsible Artificial Intelligence Systems (RAIS) require a shift toward embedded ethics to address value-based challenges facing AI in disaster management; and we propose a model to achieve it. Disaster management requires Artificial Intelligence Systems (AIS) that would be sensitive to ethical, legal, and multi-dimensional values while being responsive and accountable in complex and acute disruptions that simultaneously call for fair, value-laden, and immediate decisions. Without such a necessary shift, AIS will be incapable of responding properly to major value-based challenges of axiological and hierarchical types, and might leave AIS vulnerable to meta-disasters, such as intelligent digital disasters. This study focuses on RAI in the context of disaster management and proposes a model of Embedded Ethics for Responsible Artificial Intelligence Systems (EE-RAIS), which is empowered by four platforms of embedded ethics—educational, cross-functional, developmental, and algorithmic embedded ethics—as well as four imperative metrics—ethical intelligence, legal intelligence, social-emotional competency, and artificial wisdom. The final section of the paper explores how EE-RAIS can be deployed for the purpose of disaster management and fair crisis informatics.",0.8110824227333069
3,included,10.1007/s00146-022-01458-3,Springer,springer,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',http://dx.doi.org/10.1007/s00146-022-01458-3,2023-04-01 00:00:00,"ethical artificial intelligence framework for a good ai society: principles, opportunities and perils","The justification and rationality of this paper is to present some fundamental principles, theories, and concepts that we believe moulds the nucleus of a good artificial intelligence (AI) society. The morally accepted significance and utilitarian concerns that stems from the inception and realisation of an AI’s structural foundation are displayed in this study. This paper scrutinises the structural foundation, fundamentals, and cardinal righteous remonstrations, as well as the gaps in mechanisms towards novel prospects and perils in determining resilient fundamentals, accountability, and AI’s convoluted and responsible implications. We outline a number of salient and practical benefits, in which to place moral norms within the mise en scène of AI, to delineate the rudimentary ethical dilemmas and decorous directions within the realms of AI.",0.8412108421325684
4,included,10.1016/j.giq.2022.101722,scopus,scopus,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132505364&origin=inward,2022-10-01,public ai canvas for ai-enabled public value: a design science approach,"
                  Public agencies have a strong interest in artificial intelligence (AI) systems. However, many public agencies lack tools and frameworks to articulate a viable business model and evaluate public value as they consider investing in AI systems. The business model canvas used extensively in the private sector offers us a foundation for designing a public AI canvas (PAIC). Employing a design science approach, this study reports on the design and evaluation of PAIC. The PAIC comprises three distinctive layers: (1) the public value-oriented AI-enablement layer; (2) the public value logic layer; and (3) the public value-oriented social guidance layer. PAIC offers guidance on innovating the business models of public agencies to create and capture AI-enabled value. For practitioners, PAIC presents a validated tool to guide AI deployment in public agencies.
               ",0.9426229000091552
5,included,10.18178/jaai.2023.1.2.103-116,Journal of Advances in Artificial Intelligence,semantic_scholar,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',https://www.semanticscholar.org/paper/87d110893ac357209247502dfdd2d5312ec44cfd,2000-01-01 00:00:00,use of domain engineering in hyperautomation applied to decision making in government,"This article presents the domain engineering process carried out to obtain the requirements for the implementation of an Artificial Intelligence (AI) compliance framework aimed at the public sector. Owing to the current competitive and fast economy, which generates huge demand for increasingly efficient, reliable, and transparent intelligent systems, decision-support architectures should also be developed under strong restrictions of cost and time. Such a context requires adequate structures, processes, and technologies for coping with the complexity of building such intelligent systems. Currently, many public organizations have adopted applications for process automation, with the aim of refraining from repetitive work and producing more efficient results. However, what is not so often observed is the development of intelligent engines to support complex public decision-making. Possible explanations are the plethora of available data sources and the number of legal norms to be abided by. Moreover, it is important to highlight the need to incorporate transparency, auditability, reusability, and flexibility into such systems. Thus, they can be safely utilized in various analogous situations, reducing the need to develop new applications from scratch. An architecture suitable for supporting public decision-making with so many features and increasingly unstructured data, as well as abundant regulation, needs well-crafted formal specifications. This article aims to analyze three existing frameworks and carry out domain engineering studies in three cases to produce some guidance for future public applications and services based on AI. Next, we provide a conceptual preliminary architectural definition for the public sector. The proposed architecture targets were identified in the three cases studied, namely, frequent tasks of process mining requirements, detection of anomalies, and extraction of rules and public policies for helping public servants. All these aim at expedient AI development for public decision-making.",0.8242796063423157
6,included,10.1007/s13162-024-00275-9,Springer,springer,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',http://dx.doi.org/10.1007/s13162-024-00275-9,2024-06-01 00:00:00,a theoretical framework to guide ai ethical decision making,"Artificial Intelligence (AI) ethics is needed to address the risks that are outpacing efforts to protect consumers and society. AI is becoming human-competitive with the ability to perform tasks, that without controls, can result in harmful or destructive actions. Principles are currently the most discussed ethical approach for pervasive boundaries for algorithmic rule-based intelligence. Principles, values, norms, and rules should be the foundation of an ethical corporate culture with all participants aware of and involved in developing AI ethics. To address these concerns, a theory-based decision framework is presented to incorporate ethical considerations into AI applications. With limited discussion on frameworks to manage AI ethics, we provide a modification of the Hunt–Vitell (H–V) ethical decision model to provide a supportive theoretical framework. This model considers the cultural, industry, organizational, and legal standards that shape AI ethical decision making. The model is based on individual decision making and parallels the decision process in autonomous AI system decision making. Topics for additional research are advanced to create expanded knowledge on this topic.",0.8691652417182922
7,included,10.1007/s43681-022-00178-0,Springer,springer,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',http://dx.doi.org/10.1007/s43681-022-00178-0,2023-02-01 00:00:00,"ethical assurance: a practical approach to the responsible design, development, and deployment of data-driven technologies","This article offers several contributions to the interdisciplinary project of responsible research and innovation in data science and AI. First, it provides a critical analysis of current efforts to establish practical mechanisms for algorithmic auditing and assessment to identify limitations and gaps with these approaches. Second, it provides a brief introduction to the methodology of argument-based assurance and explores how it is currently being applied in the development of safety cases for autonomous and intelligent systems. Third, it generalises this method to incorporate wider ethical, social, and legal considerations, in turn establishing a novel version of argument-based assurance that we call ‘ethical assurance.’ Ethical assurance is presented as a structured method for unifying the myriad practical mechanisms that have been proposed. It is built on a process-based form of project governance that enlists reflective innovation practices to operationalise normative principles, such as sustainability, accountability, transparency, fairness, and explainability. As a set of interlocutory governance mechanisms that span across the data science and AI lifecycle, ethical assurance supports inclusive and participatory ethical deliberation while also remaining grounded in social and technical realities. Finally, this article sets an agenda for ethical assurance, by detailing current challenges, open questions, and next steps, which serve as a springboard to build an active (and interdisciplinary) research programme as well as contribute to ongoing discussions in policy and governance.",0.8177997469902039
8,included,10.1007/s00146-024-01987-z,Springer,springer,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',http://dx.doi.org/10.1007/s00146-024-01987-z,2024-06-08 00:00:00,attitudes toward artificial intelligence: combining three theoretical perspectives on technology acceptance,"Evidence on AI acceptance comes from a diverse field comprising public opinion research and largely experimental studies from various disciplines. Differing theoretical approaches in this research, however, imply heterogeneous ways of studying AI acceptance. The present paper provides a framework for systematizing different uses. It identifies three families of theoretical perspectives informing research on AI acceptance—user acceptance, delegation acceptance, and societal adoption acceptance. These models differ in scope, each has elements specific to them, and the connotation of technology acceptance thus changes when shifting perspective. The discussion points to a need for combining the three perspectives as they have all become relevant for AI. A combined approach serves to systematically relate findings from different studies. And as AI systems affect people in different constellations and no single perspective can accommodate them all, building blocks from several perspectives are needed to comprehensively study how AI is perceived in society.",0.8313885927200317
9,included,10.1007/s00146-022-01452-9,Springer,springer,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',http://dx.doi.org/10.1007/s00146-022-01452-9,2023-04-01 00:00:00,cognitive architectures for artificial intelligence ethics,"As artificial intelligence (AI) thrives and propagates through modern life, a key question to ask is how to include humans in future AI? Despite human involvement at every stage of the production process from conception and design through to implementation, modern AI is still often criticized for its “black box” characteristics. Sometimes, we do not know what really goes on inside or how and why certain conclusions are met. Future AI will face many dilemmas and ethical issues unforeseen by their creators beyond those commonly discussed (e.g., trolley problems and variants of it) and to which solutions cannot be hard-coded and are often still up for debate. Given the sensitivity of such social and ethical dilemmas and the implications of these for human society at large, when and if our AI make the “wrong” choice we need to understand how they got there in order to make corrections and prevent recurrences. This is particularly true in situations where human livelihoods are at stake (e.g., health, well-being, finance, law) or when major individual or household decisions are taken. Doing so requires opening up the “black box” of AI; especially as they act, interact, and adapt in a human world and how they interact with other AI in this world. In this article, we argue for the application of cognitive architectures for ethical AI. In particular, for their potential contributions to AI transparency, explainability, and accountability. We need to understand how our AI get to the solutions they do, and we should seek to do this on a deeper level in terms of the machine-equivalents of motivations, attitudes, values, and so on. The path to future AI is long and winding but it could arrive faster than we think. In order to harness the positive potential outcomes of AI for humans and society (and avoid the negatives), we need to understand AI more fully in the first place and we expect this will simultaneously contribute towards greater understanding of their human counterparts also.",0.8579889535903931
10,included,10.1016/j.giq.2024.101962,scopus,scopus,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85199797451&origin=inward,2024-09-01,toward a person-environment fit framework for artificial intelligence implementation in the public sector,"
                  Using an embedded mixed method design, we compared a nationally representative sample of US adults and a sample of US-based emergency managers (EM) on their attitudes toward artificial intelligence (AI) and their intentions to rely on AI in a set of decision-making scenarios relevant to emergency management. Emergency managers reported significantly less positive attitudes toward AI and were less likely to rely on AI for decisions compared to the nationally representative sample. Our analysis of EMs' open-ended responses explaining their choices to use or not use AI-based solutions reflected specific concerns about implementation rather than wariness toward AI generally. These concerns included the complexity of the potential outcomes in the scenarios, the value they placed on human input and their own extensive experience, procedural concerns, collaborative decision-making, team-building, training, and the ethical implications of decisions, rather than a rejection of AI more generally. Managers' insights integrated with our quantitative findings led to a person-environment fit framework for AI implementation in the public sector. Our findings and framework have implications for how AI systems should be introduced and integrated in emergency managerial contexts and in public sector organizations more generally. Public managers' perceptions and intentions to use AI and organizational oversight processes are at least as important as technology design considerations when public sector organizations are considering the deployment of AI.
               ",0.8007835149765015
11,included,10.1007/s10676-021-09619-6,Springer,springer,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',http://dx.doi.org/10.1007/s10676-021-09619-6,2021-12-01 00:00:00,the ethical use of artificial intelligence in human resource management: a decision-making framework,"Artificial intelligence (AI) is increasingly inputting into various human resource management (HRM) functions, such as sourcing job applicants and selecting staff, allocating work, and offering personalized career coaching. While the use of AI for such tasks can offer many benefits, evidence suggests that without careful and deliberate implementation its use also has the potential to generate significant harms. This raises several ethical concerns regarding the appropriateness of AI deployment to domains such as HRM, which directly deal with managing sometimes sensitive aspects of individuals’ employment lifecycles. However, research at the intersection of HRM and technology continues to largely center on examining what AI can be used for, rather than focusing on the salient factors relevant to its ethical use and examining how to effectively engage human workers in its use. Conversely, the ethical AI literature offers excellent guiding principles for AI implementation broadly, but there remains much scope to explore how these principles can be enacted in specific contexts-of-use. By drawing on ethical AI and task-technology fit literature, this paper constructs a decision-making framework to support the ethical deployment of AI for HRM and guide determinations of the optimal mix of human and machine involvement for different HRM tasks. Doing so supports the deployment of AI for the betterment of work and workers and generates both scholarly and practical outcomes.",0.8131939768791199
12,included,10.1145/3657054.3657063,Digital Government Research,semantic_scholar,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',https://www.semanticscholar.org/paper/3488e7b86f16c0d93b7b151eca0d7e7526e6d44b,2000-01-01 00:00:00,beyond principles: embedding ethical ai risks in public sector risk management practice,"Artificial intelligence (AI) adoption by public sector organizations (PSOs) introduces various ethical risks stemming from a lack of integrating human values into AI design. Addressing these ethical risks is a complex collective responsibility among designers, developers, risk experts, and public sector managers. Embedding these risks in existing risk management practices is crucial for responsible AI adoption, as emphasized by the legal requirements of the EU AI Act. However, the responsibility for managing these ethical risks is often unclear. Public sector organizations face unique challenges due to the complex, uncertain, and rapidly evolving nature of AI technologies, further complicating the management of ethical risks. This paper explores using the Three Lines of Defense (TLoD) risk management model to understand and address these ethical risks in public sector AI adoption. The TLoD model structures risk management across three lines: operational management, risk oversight and compliance, and internal audit. This framework helps to distribute and integrate the collective responsibility for ethical AI risk management within public sector organizations, emphasizing alignment and collaboration among different actors. Through an exploratory study involving a survey and semi-structured interviews with professionals responsible for AI-related risk management in Dutch public sector organizations, we assess the TLoD model's usefulness in addressing ethical AI risks. The study examines the challenges and opportunities in applying the TLoD model to manage ethical risks and identifies the potential gaps in responsibility and oversight. The findings suggest that while the TLoD model offers a valuable lens for distributing risk management responsibilities, there are limitations in addressing the emergent and complex nature of ethical risks in AI adoption.",0.8306326270103455
13,included,10.3390/app14188259,Applied Sciences,semantic_scholar,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',https://www.semanticscholar.org/paper/282d499f97460e32efae0b0107480d75872bbc5d,2000-01-01 00:00:00,"enhancing e-government services through state-of-the-art, modular, and reproducible architecture over large language models","Integrating Large Language Models (LLMs) into e-government applications has the potential to improve public service delivery through advanced data processing and automation. This paper explores critical aspects of a modular and reproducible architecture based on Retrieval-Augmented Generation (RAG) for deploying LLM-based assistants within e-government systems. By examining current practices and challenges, we propose a framework ensuring that Artificial Intelligence (AI) systems are modular and reproducible, essential for maintaining scalability, transparency, and ethical standards. Our approach utilizing Haystack demonstrates a complete multi-agent Generative AI (GAI) virtual assistant that facilitates scalability and reproducibility by allowing individual components to be independently scaled. This research focuses on a comprehensive review of the existing literature and presents case study examples to demonstrate how such an architecture can enhance public service operations. This framework provides a valuable case study for researchers, policymakers, and practitioners interested in exploring the integration of advanced computational linguistics and LLMs into e-government services, although it could benefit from further empirical validation.",0.8260035514831543
14,included,10.1109/ecai46879.2019.9042157,European Conference on Artificial Intelligence,semantic_scholar,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',https://www.semanticscholar.org/paper/3d863678d53ef04773b3e6052995b85db1903e28,2019-01-01 00:00:00,intelligent solutions - based framework for digital public services. a case study for smart transportation,"Digital technology landscape is continuously improving, dragging along both the transformation of public services and new demands of citizens. Emerging new technologies like Artificial Intelligence, Machine Learning, Deep Learning or Internet of Things provide tremendous means to implement intelligent solutions for reshaping digital public services. This paper aims to disclose the most important features of several intelligent technologies and of these types of public services that can be integrated for providing new capabilities. An AI-based architecture for supporting digital public services in the smart transportation sector is presented in order to demonstrate the highlighted ideas and concepts.",0.8241714835166931
15,included,10.1007/s11023-022-09611-z,Springer,springer,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',http://dx.doi.org/10.1007/s11023-022-09611-z,2023-12-01 00:00:00,contestable ai by design: towards a framework,"As the use of AI systems continues to increase, so do concerns over their lack of fairness, legitimacy and accountability. Such harmful automated decision-making can be guarded against by ensuring AI systems are contestable by design: responsive to human intervention throughout the system lifecycle. Contestable AI by design is a small but growing field of research. However, most available knowledge requires a significant amount of translation to be applicable in practice. A proven way of conveying intermediate-level, generative design knowledge is in the form of frameworks. In this article we use qualitative-interpretative methods and visual mapping techniques to extract from the literature sociotechnical features and practices that contribute to contestable AI, and synthesize these into a design framework.",0.836030125617981
16,included,http://arxiv.org/abs/1906.05684v1,arxiv,arxiv,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',http://arxiv.org/abs/1906.05684v1,6/11/2019 0:00,understanding artificial intelligence ethics and safety,"A remarkable time of human promise has been ushered in by the convergence of
the ever-expanding availability of big data, the soaring speed and stretch of
cloud computing platforms, and the advancement of increasingly sophisticated
machine learning algorithms. Innovations in AI are already leaving a mark on
government by improving the provision of essential social goods and services
from healthcare, education, and transportation to food supply, energy, and
environmental management. These bounties are likely just the start. The
prospect that progress in AI will help government to confront some of its most
urgent challenges is exciting, but legitimate worries abound. As with any new
and rapidly evolving technology, a steep learning curve means that mistakes and
miscalculations will be made and that both unanticipated and harmful impacts
will occur.
  This guide, written for department and delivery leads in the UK public sector
and adopted by the British Government in its publication, 'Using AI in the
Public Sector,' identifies the potential harms caused by AI systems and
proposes concrete, operationalisable measures to counteract them. It stresses
that public sector organisations can anticipate and prevent these potential
harms by stewarding a culture of responsible innovation and by putting in place
governance processes that support the design and implementation of ethical,
fair, and safe AI systems. It also highlights the need for algorithmically
supported outcomes to be interpretable by their users and made understandable
to decision subjects in clear, non-technical, and accessible ways. Finally, it
builds out a vision of human-centred and context-sensitive implementation that
gives a central role to communication, evidence-based reasoning, situational
awareness, and moral justifiability.",0.873350501
17,included,10.1007/s10676-021-09593-z,Springer,springer,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',http://dx.doi.org/10.1007/s10676-021-09593-z,2021-09-01 00:00:00,artificial intelligence regulation: a framework for governance,"This article develops a conceptual framework for regulating Artificial Intelligence (AI) that encompasses all stages of modern public policy-making, from the basics to a sustainable governance. Based on a vast systematic review of the literature on Artificial Intelligence Regulation (AIR) published between 2010 and 2020, a dispersed body of knowledge loosely centred around the “framework” concept was organised, described, and pictured for better understanding. The resulting integrative framework encapsulates 21 prior depictions of the policy-making process, aiming to achieve gold-standard societal values, such as fairness, freedom and long-term sustainability. This challenge of integrating the AIR literature was matched by the identification of a structural common ground among different approaches. The AIR framework results from an effort to identify and later analytically deduce synthetic, and generic tool for a country-specific, stakeholder-aware analysis of AIR matters. Theories and principles as diverse as Agile and Ethics were combined in the “AIR framework”, which provides a conceptual lens for societies to think collectively and make informed policy decisions related to what, when, and how the uses and applications of AI should be regulated. Moreover, the AIR framework serves as a theoretically sound starting point for endeavours related to AI regulation, from legislation to research and development. As we know, the (potential) impacts of AI on society are immense, and therefore the discourses, social negotiations, and applications of this technology should be guided by common grounds based on contemporary governance techniques, and social values legitimated via dialogue and scientific research.",0.8591601252555847
18,included,10.1186/s41018-021-00096-6,Springer,springer,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',http://dx.doi.org/10.1186/s41018-021-00096-6,2021-10-06 00:00:00,explicability of humanitarian ai: a matter of principles,"In the debate on how to improve efficiencies in the humanitarian sector and better meet people’s needs, the argument for the use of artificial intelligence (AI) and automated decision-making (ADMs) systems has gained significant traction and ignited controversy for its ethical and human rights-related implications. Setting aside the implications of introducing unmanned and automated systems in warfare, we focus instead on the impact of the adoption of AI-based ADMs in humanitarian response. In order to maintain the status and protection conferred by the humanitarian mandate, aid organizations are called to abide by a broad set of rules condensed in the humanitarian principles and notably the principles of humanity, neutrality, impartiality, and independence. But how do these principles operate when decision-making is automated? This article opens with an overview of AI and ADMs in the humanitarian sector, with special attention to the concept of algorithmic opacity. It then explores the transformative potential of these systems on the complex power dynamics between humanitarians, principled assistance, and affected communities during acute crises. Our research confirms that the existing flaws in accountability and epistemic processes can be also found in the mathematical and statistical formulas and in the algorithms used for automation, artificial intelligence, predictive analytics, and other efficiency-gaining-related processes. In doing so, our analysis highlights the potential harm to people resulting from algorithmic opacity, either through removal or obfuscation of the causal connection between triggering events and humanitarian services through the so-called black box effect (algorithms are often described as black boxes, as their complexity and technical opacity hide and obfuscate their inner workings (Diakopoulos, Tow Center for Digital Journ, 2017 ). Recognizing the need for a humanitarian ethics dimension in the analysis of automation, AI, and ADMs used in humanitarian action, we endorse the concept of “explicability” as developed within the ethical framework of machine learning and human-computer interaction, together with a set of proxy metrics. Finally, we stress the need for developing auditable standards, as well as transparent guidelines and frameworks to rein in the risks of what has been defined as humanitarian experimentation (Sandvik, Jacobsen, and McDonald, Int. Rev. Red Cross 99(904), 319–344, 2017). This article concludes that accountability mechanisms for AI-based systems and ADMs used to respond to the needs of populations in situation of vulnerability should be an essential feature by default, in order to preserve the respect of the do no harm principle even in the digital dimension of aid. In conclusion, while we confirm existing concerns related to the adoption of AI-based systems and ADMs in humanitarian action, we also advocate for a roadmap towards humanitarian AI for the sector and introduce a tentative ethics framework as basis for future research.",0.8393886089324951
19,included,10.1007/s12525-021-00480-5,Springer,springer,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',http://dx.doi.org/10.1007/s12525-021-00480-5,2022-03-01 00:00:00,categorization and eccentricity of ai risks: a comparative study of the global ai guidelines,"Background Governments, enterprises, civil organizations, and academics are engaged to promote normative guidelines aimed at regulating the development and application of Artificial Intelligence (AI) in different fields such as judicial assistance, social governance, and business services. Aim Although more than 160 guidelines have been proposed globally, it remains uncertain whether they are sufficient to meet the governance challenges of AI. Given the absence of a holistic theoretical framework to analyze the potential risk of AI, it is difficult to determine what is overestimated and what is missing in the extant guidelines. Based on the classic theoretical model in the field of risk management, we developed a four-dimensional structure as a benchmark to analyze the risk of AI and its corresponding governance measures. The structure consists of four pairs of risks: specific-general, legal-ethical, individual-collective and generational-transgenerational. Method Using the framework, a comparative study of the extant guidelines is conducted by coding the 123 guidelines with 1023 articles. Result We find that the extant guidelines are eccentric, while collective risk and generational risk are largely underestimated by stakeholders. Based on this analysis, three gaps and conflicts are outlined for future improvements.",0.8365063667297363
20,included,10.1007/s43681-022-00143-x,Springer,springer,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',http://dx.doi.org/10.1007/s43681-022-00143-x,2022-11-01 00:00:00,defining organizational ai governance,"Artificial intelligence (AI) governance is required to reap the benefits and manage the risks brought by AI systems. This means that ethical principles, such as fairness, need to be translated into practicable AI governance processes. A concise AI governance definition would allow researchers and practitioners to identify the constituent parts of the complex problem of translating AI ethics into practice. However, there have been few efforts to define AI governance thus far. To bridge this gap, this paper defines AI governance at the organizational level. Moreover, we delineate how AI governance enters into a governance landscape with numerous governance areas, such as corporate governance, information technology (IT) governance, and data governance. Therefore, we position AI governance as part of an organization’s governance structure in relation to these existing governance areas. Our definition and positioning of organizational AI governance paves the way for crafting AI governance frameworks and offers a stepping stone on the pathway toward governed AI.",0.8767182230949402
21,included,0ef4f7572bb6a7321de2841245946f6151efb839,,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/0ef4f7572bb6a7321de2841245946f6151efb839,,managing ethical risks of artiﬁcial intelligence in business applications,"The introduction of artiﬁcial intelligence (AI) capabilities in business applications provides signiﬁcant beneﬁts but requires organizations to manage critical risks of AI ethical consequences. We survey a range of large organizations on their use of enterprise risk management (ERM) processes and toolsets to predict and control the ethical risks of AI. Four serious gaps in current ERM systems are identiﬁed from analyses of the survey results: (1) AI ethical principles do not translate eﬀectively to ethical practices; (2) Real-time monitoring of AI ethical risks is needed; (3) ERM systems emphasize economic not ethical risks; and (4) When ethical risks are identiﬁed, no solutions are readily at hand. To address these gaps, we propose a proactive approach to manage ethical risks by extending current ERM frameworks. An enhanced ERM (e-ERM) framework is designed and evaluated by subject matter expert focus groups. We conclude with observations and future research directions on the need for more aggressive pro-ethical management oversight as organizations move to ubiquitous use of AI-driven business applications.",0.8062968552112579
22,included,4f4a2f5c9e1519db0c64e8cd2fb015ea5e3e4d0b,Future Internet,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/4f4a2f5c9e1519db0c64e8cd2fb015ea5e3e4d0b,2025.0,high-risk ai systems—lie detection application,"Integrating artificial intelligence into border control systems may help to strengthen security and make operations more efficient. For example, the emerging application of artificial intelligence for lie detection when inspecting passengers presents significant opportunities for future implementation. However, as it makes use of technology that is associated with artificial intelligence, the system is classified as high risk, in accordance with the EU AI Act and, therefore, must adhere to rigorous regulatory requirements to mitigate potential risks. This manuscript distinctly amalgamates the technical, ethical, and legal aspects, thereby offering an extensive examination of the AI-based lie detection systems utilized in border security. This academic paper is uniquely set apart from others because it undertakes a thorough investigation into the categorization of these emerging technologies in terms of the regulatory framework established by the EU AI Act, which classifies them as high risk. It further makes an assessment of practical case studies, including notable examples such as iBorderCtrl and AVATAR. This in-depth analysis seeks to emphasize not only the enormous challenges ahead for practitioners but also the progress made in this emerging field of study. Furthermore, it seeks to investigate threats, vulnerabilities, and privacy concerns associated with AI, while providing security controls to address difficulties related to lie detection. Finally, we propose a framework that encompasses the EU AI Act’s principles and serves as a foundation for future approaches and research projects. By analyzing current methodologies and considering future directions, the paper aims to provide a comprehensive understanding of the viability and consequences of deploying AI lie detection capabilities in border control.",0.8043087244033813
23,included,37730b6bc3fe8c5655780efba083c8401808acaf,arXiv.org,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/37730b6bc3fe8c5655780efba083c8401808acaf,2022.0,putting ai ethics into practice: the hourglass model of organizational ai governance,"The organizational use of artificial intelligence (AI) has rapidly spread across various sectors. Alongside the awareness of the benefits brought by AI, there is a growing consensus on the necessity of tackling the risks and potential harms, such as bias and discrimination, brought about by advanced AI technologies. A multitude of AI ethics principles have been proposed to tackle these risks, but the outlines of organizational processes and practices for ensuring socially responsible AI development are in a nascent state. To address the paucity of comprehensive governance models, we present an AI governance framework, the hourglass model of organizational AI governance, which targets organizations that develop and use AI systems. The framework is designed to help organizations deploying AI systems translate ethical AI principles into practice and align their AI systems and processes with the forthcoming European AI Act. The hourglass framework includes governance requirements at the environmental, organizational, and AI system levels. At the AI system level, we connect governance requirements to AI system life cycles to ensure governance throughout the system's life span. The governance model highlights the systemic nature of AI governance and opens new research avenues into its practical implementation, the mechanisms that connect different AI governance layers, and the dynamics between the AI governance actors. The model also offers a starting point for organizational decision-makers to consider the governance components needed to ensure social acceptability, mitigate risks, and realize the potential of AI.",0.8546511858701706
