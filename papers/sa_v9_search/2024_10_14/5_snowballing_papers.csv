doi,type,query_name,query_value,publication,publisher,publication_date,database,title,url,abstract,status,semantic_score,id
http://arxiv.org/abs/1609.08765v1,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',arxiv,arxiv,2016-09-28 00:00:00,arxiv,"irescu - data for social good saving lives bridging the gaps in sudden
  cardiac arrest survival",http://arxiv.org/abs/1609.08765v1,"Currently every day in the USA 1000 people die of sudden cardiac arrest (SCA)
outside of hospitals or ambulances - before emergency medical help arrives - in
the streets, workplaces, schools and homes of our cities, adults and children.
Brain death commences in 3 minutes, and often the ambulance just can't be there
in time. Citizen cardiopulmonary resuscitation (CPR) and automated external
defibrillator (AED) use can save precious minutes and lives. Using public
access AED's saves lives in SCA- however AEDs are used in <2% of cardiac
arrests, though could save lives in 80% if available, findable, functioning,
and used. The systems problem to solve is that there is no comprehensive or
real time accessible database of the AED locations, and also it is not known
that they are actually being positioned where they are needed. The iRescU
project is designed to bridge this gap in SCA survival, by substantially
augmenting the AED database. Utilizing a combination of AED crowd sourcing and
geolocation integrated with existing 911 services and SCA events and projected
events based on machine learning data information to help make the nearest AED
accessible and available in the setting of a SCA emergency and to identify the
areas of greatest need for AEDs to be positioned in the community. Helping to
save lives and address preventable death with a social good approach and
applied big data.",not included,0.8043087244033813,1
10.2196/28858,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Medical Internet Research,2021-01-01 00:00:00,semantic_scholar,harnessing artificial intelligence for health message generation: the folic acid message engine,https://www.semanticscholar.org/paper/8e260e06c73f27f7a477021ce1b068099c2064bf,"Background Communication campaigns using social media can raise public awareness; however, they are difficult to sustain. A barrier is the need to generate and constantly post novel but on-topic messages, which creates a resource-intensive bottleneck. Objective In this study, we aim to harness the latest advances in artificial intelligence (AI) to build a pilot system that can generate many candidate messages, which could be used for a campaign to suggest novel, on-topic candidate messages. The issue of folic acid, a B-vitamin that helps prevent major birth defects, serves as an example; however, the system can work with other issues that could benefit from higher levels of public awareness. Methods We used the Generative Pretrained Transformer-2 architecture, a machine learning model trained on a large natural language corpus, and fine-tuned it using a data set of autodownloaded tweets about #folicacid. The fine-tuned model was then used as a message engine, that is, to create new messages about this topic. We conducted a web-based study to gauge how human raters evaluate AI-generated tweet messages compared with original, human-crafted messages. Results We found that the Folic Acid Message Engine can easily create several hundreds of new messages that appear natural to humans. Web-based raters evaluated the clarity and quality of a human-curated sample of AI-generated messages as on par with human-generated ones. Overall, these results showed that it is feasible to use such a message engine to suggest messages for web-based campaigns that focus on promoting awareness. Conclusions The message engine can serve as a starting point for more sophisticated AI-guided message creation systems for health communication. Beyond the practical potential of such systems for campaigns in the age of social media, they also hold great scientific potential for the quantitative analysis of message characteristics that promote successful communication. We discuss future developments and obvious ethical challenges that need to be addressed as AI technologies for health persuasion enter the stage.",not included,0.8351606994867324,2
10.3390/electronics10111223,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Electronics,2021-01-01 00:00:00,semantic_scholar,ai-enabled efficient and safe food supply chain,https://www.semanticscholar.org/paper/dad22e1cbdb21876ff1b4c10bb32f19fcc4a2a6a,"This paper provides a review of an emerging field in the food processing sector, referring to efficient and safe food supply chains, ’from farm to fork’, as enabled by Artificial Intelligence (AI). The field is of great significance from economic, food safety and public health points of views. The paper focuses on effective food production, food maintenance energy management and food retail packaging labeling control, using recent advances in machine learning. Appropriate deep neural architectures are adopted and used for this purpose, including Fully Convolutional Networks, Long Short-Term Memories and Recurrent Neural Networks, Auto-Encoders and Attention mechanisms, Latent Variable extraction and clustering, as well as Domain Adaptation. Three experimental studies are presented, illustrating the ability of these AI methodologies to produce state-of-the-art performance in the whole food supply chain. In particular, these concern: (i) predicting plant growth and tomato yield in greenhouses, thus matching food production to market needs and reducing food waste or food unavailability; (ii) optimizing energy consumption across large networks of food retail refrigeration systems, through optimal selection of systems that can be shut-down and through prediction of the respective food de-freezing times, during peaks of power demand load; (iii) optical recognition and verification of food consumption expiry date in automatic inspection of retail packaged food, thus ensuring safety of food and people’s health.",not included,0.838510024547577,3
10.1108/cr-09-2022-0137,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Competitiveness Review: An International Business Journal,2000-01-01 00:00:00,semantic_scholar,factors influencing artificial intelligence adoption in the accounting profession: the case of public sector in kuwait,https://www.semanticscholar.org/paper/c484ebb51b1b762867cb8367c9daf401d3db3e38,"
Purpose
This study aims to investigate the organizational and individual factors that influence the adoption of artificial intelligence (AI) in Kuwait's public accounting sector.


Design/methodology/approach
The methodology of this study is a cross-sectional survey of 393 experienced accounting professionals, using partial least square structural equation modeling to analyze the data.


Findings
The findings show that organizational culture, regulatory support, perceived usefulness and ease of use have a direct positive effect on AI adoption, while perceived usefulness and ease of use also have an indirect positive effect through accounting profit and behavioral intention. However, the availability of resources, effective communication channels and competition pressure have an insignificant impact on AI adoption.


Originality/value
This study pioneers a structural framework to elucidate the perceived enhancement of accounting quality through AI system integration. Further, this research adds to the literature on AI adoption in accounting. This study also offers empirical evidence regarding how organizations in Kuwait's public accounting sector view AI systems in accounting.
",not included,0.819400179386139,4
10.3390/knowledge3030032,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Knowledge,2000-01-01 00:00:00,semantic_scholar,chatgpt and the generation of digitally born “knowledge”: how does a generative ai language model interpret cultural heritage values?,https://www.semanticscholar.org/paper/c2b6f0cf3c76d314a8c1f46cd8c831e67f2e16bb,"The public release of ChatGPT, a generative artificial intelligence language model, caused wide-spread public interest in its abilities but also concern about the implications of the application on academia, depending on whether it was deemed benevolent (e.g., supporting analysis and simplification of tasks) or malevolent (e.g., assignment writing and academic misconduct). While ChatGPT has been shown to provide answers of sufficient quality to pass some university exams, its capacity to write essays that require an exploration of value concepts is unknown. This paper presents the results of a study where ChatGPT-4 (released May 2023) was tasked with writing a 1500-word essay to discuss the nature of values used in the assessment of cultural heritage significance. Based on an analysis of 36 iterations, ChatGPT wrote essays of limited length with about 50% of the stipulated word count being primarily descriptive and without any depth or complexity. The concepts, which are often flawed and suffer from inverted logic, are presented in an arbitrary sequence with limited coherence and without any defined line of argument. Given that it is a generative language model, ChatGPT often splits concepts and uses one or more words to develop tangential arguments. While ChatGPT provides references as tasked, many are fictitious, albeit with plausible authors and titles. At present, ChatGPT has the ability to critique its own work but seems unable to incorporate that critique in a meaningful way to improve a previous draft. Setting aside conceptual flaws such as inverted logic, several of the essays could possibly pass as a junior high school assignment but fall short of what would be expected in senior school, let alone at a college or university level.",not included,0.8052007794380188,5
10.1097/cin.0000000000001044,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"Computers, Informatics, Nursing",2000-01-01 00:00:00,semantic_scholar,the disruptive impacts of next generation generative artificial intelligence,https://www.semanticscholar.org/paper/b35e3ff63756648bf2b68f959f2f6e76b80a0290,"T he growing influence of generative artificial intelligence (GAI) on our personal and professional lives continues to give it the appearance of a truly disruptive innovation. Kivimaa et al noted the characteristics of disruptive innovations to include high-intensity disruption or deletion of entire job markets, resetting of process or business models, and a “technological substitution process.” Artificial intelligence (AI) applications have already been shown to be quite capable of acting as technological substitutions for human processes. Generative AI, though, moves beyond just the automation facets of AI into something more complex and curious that has captured the public's imagination. The impacts of GAI are continuing to unfold within healthcare delivery and education, with both value and cautions yet to be fully realized. Active engagement on the part of all nurses, particularly nurse informaticists, is required in order to shape the technology moving forward and to alleviate potential negative impacts and misuse.",not included,0.8033411532640458,6
10.1145/3582515.3609555,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Conference on Information Technology for Social Good,2000-01-01 00:00:00,semantic_scholar,the social impact of generative ai: an analysis on chatgpt,https://www.semanticscholar.org/paper/cea012c01f09ac382fe67ede5215a85175753487,"In recent months, the impact of Artificial Intelligence (AI) on citizens’ lives has gained considerable public interest, driven by the emergence of Generative AI models, ChatGPT in particular. The rapid development of these models has sparked heated discussions regarding their benefits, limitations, and associated risks. Generative models hold immense promise across multiple domains, such as healthcare, finance, and education, to cite a few, presenting diverse practical applications. Nevertheless, concerns about potential adverse effects have elicited divergent perspectives, ranging from privacy risks to escalating social inequality. This paper adopts a methodology to delve into the societal implications of Generative AI tools, focusing primarily on the case of ChatGPT. It evaluates the potential impact on several social sectors and illustrates the findings of a comprehensive literature review of both positive and negative effects, emerging trends, and areas of opportunity of Generative AI models. This analysis aims to facilitate an in-depth discussion by providing insights that can inspire policy, regulation, and responsible development practices to foster a citizen-centric AI.",not included,0.8294077664613724,7
10.14201/adcaij.31704,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Advances in Distributed Computing and Artificial Intelligence Journal,2000-01-01 00:00:00,semantic_scholar,generative artificial intelligence: fundamentals,https://www.semanticscholar.org/paper/75f37c5bd7c9c6389e7544807fbc45fd478d4242,"

Generative language models have witnessed substantial traction, notably with the introduction of refined models aimed at more coherent user-AI interactions—principally conversational models. The epitome of this public attention has arguably been the refinement of the GPT-3 model into ChatGPT and its subsequent integration with auxiliary capabilities such as search features in Microsoft Bing. Despite voluminous prior research devoted to its developmental trajectory, the model’s performance, and applicability to a myriad of quotidian tasks remained nebulous and task specific. In terms of technological implementation, the advent of models such as LLMv2 and ChatGPT-4 has elevated the discourse beyond mere textual coherence to nuanced contextual understanding and real-world task completion. Concurrently, emerging architectures that focus on interpreting latent spaces have offered more granular control over text generation, thereby amplifying the model’s applicability across various verticals. Within the purview of cyber defense, especially in the Swiss operational ecosystem, these models pose both unprecedented opportunities and challenges. Their capabilities in data analytics, intrusion detection, and even misinformation combatting is laudable; yet the ethical and security implications concerning data privacy, surveillance, and potential misuse warrant judicious scrutiny.
",not included,0.8252925395965576,8
10.1080/10790268.2023.2198926,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Spinal Cord Medicine (JSCM),2000-01-01 00:00:00,semantic_scholar,generative ai in spinal cord injury research and care: opportunities and challenges ahead,https://www.semanticscholar.org/paper/6afbce4fbf374b3b565ff1ea64354d57c7fe630c,"Generative artificial intelligence (AI) with its limitless potential is here to stay. Not since the advent of the digital age has the world faced a transformation of such magnitude. With the widely available user-friendly free and low-cost products of Open AI, the US AI research laboratory, the effects are being felt throughout multiple sectors, including scientific research and clinical care. Anyone with internet access can sign up at OpenAI.com to access a free research version of ChatGPT, a generative AI large-language model that interacts with the user in a natural language format (1). Powered by the world’s fifth largest supercomputer (2), ChatGPT gathers information using artificial neural network technology (3). Trained using large databases, it has an enormous vocabulary and is capable of understanding language and context, retrieving knowledge, generating new content in the forms of text and code, carrying on a conversation, and personalizing communications. While trained primarily in English, this chatbot supports 95 languages. While this tool became only recently available, ChatGPT gained traction immediately and gained followers at an unprecedented pace, attracting more than 100 million users in the two months following its public release in November 2022 (3). In March 2023, OpenAI released a more advanced multi-modal version, GPT-4, available through the paid service, ChatGPT Plus (4,5). To explore these new tools, we posed general queries to GPT-4 about the potential for generative AI to influence spinal cord research and clinical care. Selected responses are excerpted below: • Generative AI can be used to create virtual models of the human body, including the spinal cord, the musculoskeletal system, and the nervous system. These models can then be used to simulate spinal cord injury and test the effectiveness of different treatments in a more controlled and accurate environment. • AI algorithms can help researchers optimize the design of spinal stimulation protocols by analyzing large amounts of data, simulating the effects of stimulation, developing personalized treatment plans, and predicting outcomes. • Generative AI can be used to design new drugs that enhance the survival and integration of stem cells into the spinal cord. AI algorithms can analyze large amounts of data to identify potential drug candidates and simulate their effects • By optimizing the design and control algorithms of robotic exoskeletons, analyzing large amounts of data, and simulating different types of injuries, generative AI can help researchers improve the effectiveness of these devices. By analyzing medical history and imaging data, AI algorithms can recommend the most effective exoskeleton design and treatment protocol for that individual’s specific injury and needs. When asked about its potential role in recruiting research participants, it responded: • AI algorithms can analyze electronic medical records, patient data, and social media activity to identify potential candidates for spinal cord injury studies. Once potential candidates have been identified, AI algorithms can be used to create personalized messages and content tailored to individual patients. Generative AI can be used to develop predictive models that identify factors associated with patient enrollment and retention. Generative AI can be used to engage with patients, using chatbots, virtual assistants, or other AI-powered tools to provide patients with updates, answer questions, and address concerns. ChatGPT offered these potential applications when queried about clinical care: • In acute spinal cord injury care, generative AI can analyze vital signs, lab results, and other patient data to predict the likelihood of pressure sores, urinary tract infections, or other complications. This can help clinicians intervene early, which can improve patient outcomes and reduce healthcare costs,",not included,0.8133152216672898,9
10.21202/jdtl.2023.38,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Digital Technologies and Law,2000-01-01 00:00:00,semantic_scholar,towards legal regulations of generative ai in the creative industry,https://www.semanticscholar.org/paper/6f0329dfd1e45dbfbdfd95cd0b3e9f40752c984f,"Objective: this article aims to answer the following questions: 1. Can generative artificial intelligence be a subject of copyright law? 2. What risks the unregulated use of generative artificial intelligence systems can cause? 3. What legal gaps should be filled in to minimize such risks?Methods: comparative legal analysis, sociological method, concrete sociological method, quantitative data analysis, qualitative data analysis, statistical analysis, case study, induction, deduction.Results: the authors identified several risks of the unregulated usage of generative artificial intelligence in the creative industry, among which are: violation of copyright and labor law, violation of consumers rights and the rise of public distrust in government. They suggest that a prompt development of new legal norms can minimize these risks. In conclusion, the article constants that states have already begun to realize that the negative impact of generative artificial intelligence on the creative industry must not be ignored, hence the development of similar legal regulations in states with completely different regimes.Scientific novelty: the article provides a comprehensive study of the impact of generative artificial intelligence on the creative industry from two perspectives: the perspective of law and the perspective of the industry. The empirical basis of it consists of two international surveys and an expert opinion of a representative of the industry. This approach allowed the authors to improve the objectivity of their research and to obtain results that can be used for finding a practical solution for the identified risks. The problem of the ongoing development and popularization of generative artificial intelligence systems goes beyond the question “who is the author?” therefore, it needs to be solved by introduction of other than the already existing mechanisms and regulations - this point of view is supported not only by the results of the surveys but also by the analysis of current lawsuits against developers of generative artificial intelligence systems.Practical significance: the obtained results can be used to fasten the development of universal legal rules, regulations, instruments and standards, the current lack of which poses a threat not only to human rights, but also to several sectors within the creative industry and beyond.",not included,0.8154128462076187,10
10.52663/kcsr.2023.28.2.85,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,The Korea Association for Corruption Studies,2000-01-01 00:00:00,semantic_scholar,"study on artificial intelligence(ai) and chat gpt, corruption",https://www.semanticscholar.org/paper/7d2925856541169ac233ccd5a08eab8f36a87b64,"This study discusses corruption and moral hazard arising from the use of artificial intelligence(AI) and Chat GPT and presents implications. In this study, 1. The daily use of AI and Chat GPT is 1) Lack of situational awareness and thinking, 2) Absence of appropriate ethical norms and social codes, 3) Problems with shifting consciousness, 4) Concentration of economic and technological utility, 5) Deepening dependence on AI. 2. Problems in the information processing process of AI and Chat GPT were 1) lack of transparency and autonomy of AI, 2) value judgment and bias of Chat GPT algorithm, 3) intervention and distortion of decision-making, 4) possibility of copyright infringement, and 5) safety of use and services. 3. Problems in the information distribution process of AI and Chat GPT are 1) limitations of the Chat GPT algorithm and information distortion, 2) information asymmetry and centralization, 3) inaccuracy and uncertainty of information, 4) misuse of information or data, 5) Information security and information leakage occurred. The implications of this study are as follows. First, education and promotion of AI and Chat GPT include 1) Composition of public and private cooperation governance, 2) Development of national campaigns, 3) Found and service of AI and Chat GPT, 4) establishment of ethical norms and manners for AI and Chat GPT, 5) Social coding of AI ethics and manners. Second, the institutional devices for AI and Chat GPT are 1) Setting the regulatory scope and level to secure transparency and autonomy, 2) Monitoring of bias in information by public and private sectors, 3) Management and supervision of the information collection process, 4) Establishment of an institutional device to prevent AI from intervening in decision-making and preventing distortion, 5) Establishment of standards and systems to ensure the safety of information processing. Third, normative device for AI and Chat GPT as follows. The contents are: 1) Discussion and search for normative standards, 2) Exploring ways to resolve the centralization of information, 3) securing the clarity and reliability of information, 4) designing information security and leakage prevention programs, 5) Monitoring to prevent misuse and abuse of information.data.",not included,0.8546511858701706,11
10.48550/arxiv.2310.14651,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,arXiv.org,2000-01-01 00:00:00,semantic_scholar,$λ$-split: a privacy-preserving split computing framework for cloud-powered generative ai,https://www.semanticscholar.org/paper/980646c7dbd3689177a2fe5c27ffa116f042f4ee,"In the wake of the burgeoning expansion of generative artificial intelligence (AI) services, the computational demands inherent to these technologies frequently necessitate cloud-powered computational offloading, particularly for resource-constrained mobile devices. These services commonly employ prompts to steer the generative process, and both the prompts and the resultant content, such as text and images, may harbor privacy-sensitive or confidential information, thereby elevating security and privacy risks. To mitigate these concerns, we introduce $\Lambda$-Split, a split computing framework to facilitate computational offloading while simultaneously fortifying data privacy against risks such as eavesdropping and unauthorized access. In $\Lambda$-Split, a generative model, usually a deep neural network (DNN), is partitioned into three sub-models and distributed across the user's local device and a cloud server: the input-side and output-side sub-models are allocated to the local, while the intermediate, computationally-intensive sub-model resides on the cloud server. This architecture ensures that only the hidden layer outputs are transmitted, thereby preventing the external transmission of privacy-sensitive raw input and output data. Given the black-box nature of DNNs, estimating the original input or output from intercepted hidden layer outputs poses a significant challenge for malicious eavesdroppers. Moreover, $\Lambda$-Split is orthogonal to traditional encryption-based security mechanisms, offering enhanced security when deployed in conjunction. We empirically validate the efficacy of the $\Lambda$-Split framework using Llama 2 and Stable Diffusion XL, representative large language and diffusion models developed by Meta and Stability AI, respectively. Our $\Lambda$-Split implementation is publicly accessible at https://github.com/nishio-laboratory/lambda_split.",not included,0.8145611852407455,12
10.3390/asi7010006,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Applied System Innovation,2000-01-01 00:00:00,semantic_scholar,ai-powered academic guidance and counseling system based on student profile and interests,https://www.semanticscholar.org/paper/bba496787711047003c174009c034641bd84ab07,"Over the past few decades, the education sector has achieved impressive advancements by incorporating Artificial Intelligence (AI) into the educational environment. Nevertheless, specific educational processes, particularly educational counseling, still depend on traditional procedures. The current method of conducting group sessions between counselors and students does not offer personalized assistance or individual attention, which can cause stress to students and make it difficult for them to make informed decisions about their coursework and career path. This paper proposes a counseling solution designed to aid high school seniors in selecting appropriate academic paths at the tertiary level. The system utilizes a predictive model that considers academic history and student preferences to determine students’ likelihood of admission to their chosen university and recommends similar alternative universities to provide more opportunities. We developed the model based on data from 500 graduates from 12 public high schools in Morocco, as well as eligibility criteria from 31 institutions and colleges. The counseling system comprises two modules: a recommendation module that uses popularity-based and content-based recommendations and a prediction module that calculates the likelihood of admission using the Huber Regressor model. This model outperformed 13 other machine learning modules, with a low MSE of 0.0017, RMSE of 0.0422, and the highest R-squared value of 0.9306. Finally, the system is accessible through a user-friendly web interface.",not included,0.8040409088134766,13
10.46324/pmp2303307,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Postmodernism Problems,2000-01-01 00:00:00,semantic_scholar,revolutionizing education: the transformative power of ai technologies in pr,https://www.semanticscholar.org/paper/88a9d7421e4b36ca080be31d72f390736f3c4458,"In the coming years, Artificial Intelligence (AI) is set to make a profound impact on higher education, particularly within the field of Public Relations (PR). This surge in the adoption of AI technologies in academia is fuelled by compelling factors. AI is on the verge of transforming PR practices, elevating the efficiency and effectiveness of communication strategies. The data-driven prowess of AI equips PR professionals with enhanced capabilities to understand and engage with target audiences, tailor messages with precision, automate routine tasks, such as content creation, social media management, and data analysis, and liberate practitioners from administrative burdens. As educational institutions increasingly recognize the value of AI in PR, investments in its implementation are expected, heralding a paradigm shift in higher education communication and stakeholder engagement. The current article aims to delve into the European framework for AI applications in education, present the landscape of existing technology tools used in the communication area, and review recent publications describing the benefits and possible drawbacks of using generative AI technologies in the sphere of teaching.",not included,0.8405167400836945,14
10.26425/2309-3633-2023-11-2-103-113,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,UPRAVLENIE / MANAGEMENT (Russia),2000-01-01 00:00:00,semantic_scholar,value and risks of morphing technology into strategy and business model,https://www.semanticscholar.org/paper/4fd7a5a1abd0106b7d8567fba366ce868bba7150,"Digital technologies became the primary source of innovation in the private and public sectors. The Internet profoundly changed the way businesses are run catapulting “most digital” industries and companies to the top of the S&P500. Two innovations that drive digital transformation changing the nature of competition are cloud computing and artificial intelligence (AI) technologies. Cloud-native business models and strategies proved successful in various industries, while AI is being tested in vivo by management mainstream. The publication provides an analysis of a multidimensional impact cloud computing makes on strategies and business models of companies. We show that what made cloud computing special in the management context was the way it morphed into strategies and business models best suited for the uncertain future. We also noted that as the focus of digital transformation shifts towards cloud-based AI powered decision-making solutions, managing the human aspect of “more digital” business models and related risks, recently referred to as an existential threat, becomes a priority of management research.",not included,0.832381296157837,15
10.1145/3593013.3594032,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,"Conference on Fairness, Accountability and Transparency",2000-01-01 00:00:00,semantic_scholar,you sound depressed: a case study on sonde health’s diagnostic use of voice analysis ai,https://www.semanticscholar.org/paper/c25067c6078f3c6aa6478b6348c9d23a40cfc1c0,"There is growing interest within the medical sector about the diagnostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However, insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study of Sonde Health, a Boston-based startup that purports to offer “objective” depression detection and monitoring via its Mental Fitness app that extracts and analyzes the acoustic features of the user’s voice. Using a critical disability studies lens, we conducted a textual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizing Sonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychometrics can be meaningfully flattened into a single encompassing score, the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care.",not included,0.8418905466794968,16
10.1002/ctm2.1207,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Clinical and Translational Medicine,2000-01-01 00:00:00,semantic_scholar,the future of chatgpt in academic research and publishing: a commentary for clinical and translational medicine,https://www.semanticscholar.org/paper/ae17e17ed3a36ba3ffbadc9de2b10909aa13226c,"ChatGPT, an artificial intelligence (AI)-powered chatbot developed by OpenAI, is creating a buzz across all occupational sectors. Its name comes from its basis in the Generative Pretrained Transformer (GPT) language model. ChatGPT’s most promising feature is its ability to offer human-like responses to text input using deep learning techniques at a level far superior to any other AImodel. Its rapid integration in various industries signals the public’s burgeoning reliance on AI technology. Thus, it is essential to critically evaluate ChatGPT’s potential impacts on academic clinical and translational medicine research.",not included,0.8262364834547042,17
10.1109/access.2023.3297646,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,IEEE Access,2000-01-01 00:00:00,semantic_scholar,when ai meets information privacy: the adversarial role of ai in data sharing scenario,https://www.semanticscholar.org/paper/a90bef9cb05006aec434222f629499a325ba0252,"Artificial intelligence (AI) is a transformative technology with a substantial number of practical applications in commercial sectors such as healthcare, finance, aviation, and smart cities. AI also has strong synergy with the information privacy (IP) domain from two distinct aspects: as a protection tool (i.e., safeguarding privacy), and as a threat tool (i.e., compromising privacy). In the former case, AI techniques are amalgamated with the traditional anonymization techniques to improve various key components of the anonymity process, and therefore, privacy is safeguarded effectively. In the latter case, some adversarial knowledge is aggregated with the help of AI techniques and subsequently used to compromise the privacy of individuals. To the best of our knowledge, threats posed by AI-generated knowledge such as synthetic data (SD) to information privacy are often underestimated, and most of the existing anonymization methods do not consider/model this SD-based knowledge that can be available to the adversary, leading to privacy breaches in some cases. In this paper, we highlight the role of AI as a threat tool (i.e., AI used to compromise an individual’s privacy), with a special focus on SD that can serve as background knowledge leading to various kinds of privacy breaches. For instance, SD can encompass pertinent information (e.g., total # of attributes in data, distributions of sensitive information, category values of each attribute, minor and major values of some attributes, etc.) about real data that can offer a helpful hint to the adversary regarding the composition of anonymized data, that can subsequently lead to uncovering the identity or private information. We perform reasonable experiments on a real-life benchmark dataset to prove the pitfalls of AI in the data publishing scenario (when a database is either fully or partially released to public domains for conducting analytics).",not included,0.8531478404998779,18
10.1109/tim.2023.3317913,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,IEEE Transactions on Instrumentation and Measurement,2000-01-01 00:00:00,semantic_scholar,multiscale analysis domain interpretable deep neural network for detection of breast cancer using thermogram images,https://www.semanticscholar.org/paper/15161709d92a407730aecae04d4426accf0c49b6,"Breast cancer is the most prevalent cancer among women, with a high mortality rate. The early detection of breast cancer using medical imaging techniques helps reduce the number of deaths caused by this disease. Thermogram imaging is safer and less expensive than mammography for diagnosing breast cancer. The automated analysis of thermogram images using artificial intelligence (AI) methods is an interesting approach to detect breast cancer. This article proposes a novel multiscale analysis domain interpretable deep learning (MSADIDL) approach for automatically detecting breast cancer using thermogram images. The 2D empirical wavelet transform (2DEWT) with fixed boundary points (FBPs) is employed for the multiscale analysis of thermogram images and evaluation of modes or subbands. All the modes of the thermogram images are used as the input to the MSADIDL model for the automated detection of breast cancer. The MSADIDL architecture comprises seven individual deep neural networks (DNNs) connected in parallel. The outputs of the individual DNNs are concatenated and then used as the input to the dense layers, after which the output layer evaluates the probability score for the automated categorization of normal versus cancerous classes. A publicly available thermogram imaging dataset is utilized to evaluate the performance of the proposed MSADIDL approach. The results show that the proposed MSADIDL approach has obtained an accuracy value of 99.54% for both fivefold cross-validation (CV) and hold-out validation cases using all seven modes of thermogram images. The MSADIDL model has achieved an accuracy higher than all of the transfer learning-based breast cancer detection techniques using thermogram images. The suggested MSADIDL model has shown higher accuracy when compared with different existing methods to detect breast cancer using thermogram images.",not included,0.8011098295450211,19
10.48550/arxiv.2311.07326,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,arXiv.org,2000-01-01 00:00:00,semantic_scholar,metasymnet: a dynamic symbolic regression network capable of evolving into arbitrary formulations,https://www.semanticscholar.org/paper/4d233f8d623e01205e0c3327df2095f7746956da,"Mathematical formulas serve as the means of communication between humans and nature, encapsulating the operational laws governing natural phenomena. The concise formulation of these laws is a crucial objective in scientific research and an important challenge for artificial intelligence (AI). While traditional artificial neural networks (MLP) excel at data fitting, they often yield uninterpretable black box results that hinder our understanding of the relationship between variables x and predicted values y. Moreover, the fixed network architecture in MLP often gives rise to redundancy in both network structure and parameters. To address these issues, we propose MetaSymNet, a novel neural network that dynamically adjusts its structure in real-time, allowing for both expansion and contraction. This adaptive network employs the PANGU meta function as its activation function, which is a unique type capable of evolving into various basic functions during training to compose mathematical formulas tailored to specific needs. We then evolve the neural network into a concise, interpretable mathematical expression. To evaluate MetaSymNet's performance, we compare it with four state-of-the-art symbolic regression algorithms across more than 10 public datasets comprising 222 formulas. Our experimental results demonstrate that our algorithm outperforms others consistently regardless of noise presence or absence. Furthermore, we assess MetaSymNet against MLP and SVM regarding their fitting ability and extrapolation capability, these are two essential aspects of machine learning algorithms. The findings reveal that our algorithm excels in both areas. Finally, we compared MetaSymNet with MLP using iterative pruning in network structure complexity. The results show that MetaSymNet's network structure complexity is obviously less than MLP under the same goodness of fit.",not included,0.8079065144062042,20
10.18500/1818-9601-2023-23-4-447-453,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Izvestiya of Saratov University. Sociology. Politology,2000-01-01 00:00:00,semantic_scholar,strategic planning in public administration as a political system of institutional instruments and goal-setting mechanisms,https://www.semanticscholar.org/paper/39a95e3f3124887f824d47586b917916466f5def,"The article is devoted to the subject field of political science research within the scientific specialty “Public Administration and sectoral policies” and the analysis of the practice of state strategic planning as a political system of institutional instruments and goal-setting mechanisms in the Russian Federation. The article discusses public political and legal mechanisms for combining values and goals with the choice of ways and methods to achieve them. The constitutional exclusivity and independence of each branch of government as the basis of its strategic resource and strategic planning potential is highlighted as the fundamental public political and legal mechanism for combining values and goals with the choice of ways and methods to achieve them. A general description of the architecture of the unified system of public power in accordance with the amendments to the Constitution of Russia and the current configuration of the public administration system is given. The importance of political factor of the effectiveness of the administration of strategic management system and the implementation of sectoral policies in modern Russia on the basis of the mechanisms of coordinated functioning and interaction of public authorities and local self-government in a single system of public authority is shown. Fundamental changes are highlighted that make it possible to significantly increase the efficiency of the federal government in a unified system of public authority with an emphasis on the implementation of national development goals of both the Russian Federation as a whole and the regions based on the sectoral structure of the economy. Attention is focused on the political goals of the regional factor of socio-economic development of territories on the basis of mechanisms and tools of strategic planning, the directions of fundamental political, legal, administrative and managerial decisions on the implementation of the regional investment standard are determined. The role of federal institutions of innovative development and provision of infrastructure projects and programs for solving the tasks of ensuring sustainable economic growth and diversification of the modern Russian economy, which cannot be optimally implemented by market mechanisms, was demonstrated. Conclusions are drawn about the importance of the interpretative understanding of politics as a system of institutional tools and goal-setting mechanisms in the political and legal practice of state strategic planning for determining promising directions in the subject field of scientific research within the specialty “Public Administration and sectoral policies”, for solving problems of improving the quality of public administration through the introduction of a management model based on big data and artificial intelligence, the transition of the public authority system to a data-based management model using a platform approach.",included,0.8023776173591614,21
10.3390/brainsci13020348,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Brain Science,2000-01-01 00:00:00,semantic_scholar,tumor diagnosis against other brain diseases using t2 mri brain images and cnn binary classifier and dwt,https://www.semanticscholar.org/paper/60b5ffa73ea0852c6f20a625a78ef665f3b713e6,"Purpose: Brain tumors are diagnosed and classified manually and noninvasively by radiologists using Magnetic Resonance Imaging (MRI) data. The risk of misdiagnosis may exist due to human factors such as lack of time, fatigue, and relatively low experience. Deep learning methods have become increasingly important in MRI classification. To improve diagnostic accuracy, researchers emphasize the need to develop Computer-Aided Diagnosis (CAD) computational diagnostics based on artificial intelligence (AI) systems by using deep learning methods such as convolutional neural networks (CNN) and improving the performance of CNN by combining it with other data analysis tools such as wavelet transform. In this study, a novel diagnostic framework based on CNN and DWT data analysis is developed for the diagnosis of glioma tumors in the brain, among other tumors and other diseases, with T2-SWI MRI scans. It is a binary CNN classifier that treats the disease “glioma tumor” as positive and the other pathologies as negative, resulting in a very unbalanced binary problem. The study includes a comparative analysis of a CNN trained with wavelet transform data of MRIs instead of their pixel intensity values in order to demonstrate the increased performance of the CNN and DWT analysis in diagnosing brain gliomas. The results of the proposed CNN architecture are also compared with a deep CNN pre-trained on VGG16 transfer learning network and with the SVM machine learning method using DWT knowledge. Methods: To improve the accuracy of the CNN classifier, the proposed CNN model uses as knowledge the spatial and temporal features extracted by converting the original MRI images to the frequency domain by performing Discrete Wavelet Transformation (DWT), instead of the traditionally used original scans in the form of pixel intensities. Moreover, no pre-processing was applied to the original images. The images used are MRIs of type T2-SWI sequences parallel to the axial plane. Firstly, a compression step is applied for each MRI scan applying DWT up to three levels of decomposition. These data are used to train a 2D CNN in order to classify the scans as showing glioma or not. The proposed CNN model is trained on MRI slices originated from 382 various male and female adult patients, showing healthy and pathological images from a selection of diseases (showing glioma, meningioma, pituitary, necrosis, edema, non-enchasing tumor, hemorrhagic foci, edema, ischemic changes, cystic areas, etc.). The images are provided by the database of the Medical Image Computing and Computer-Assisted Intervention (MICCAI) and the Ischemic Stroke Lesion Segmentation (ISLES) challenges on Brain Tumor Segmentation (BraTS) challenges 2016 and 2017, as well as by the numerous records kept in the public general hospital of Chania, Crete, “Saint George”. Results: The proposed frameworks are experimentally evaluated by examining MRI slices originating from 190 different patients (not included in the training set), of which 56% are showing gliomas by the longest two axes less than 2 cm and 44% are showing other pathological effects or healthy cases. Results show convincing performance when using as information the spatial and temporal features extracted by the original scans. With the proposed CNN model and with data in DWT format, we achieved the following statistic percentages: accuracy 0.97, sensitivity (recall) 1, specificity 0.93, precision 0.95, FNR 0, and FPR 0.07. These numbers are higher for this data format (respectively: accuracy by 6% higher, recall by 11%, specificity by 7%, precision by 5%, FNR by 0.1%, and FPR is the same) than it would be, had we used as input data the intensity values of the MRIs (instead of the DWT analysis of the MRIs). Additionally, our study showed that when our CNN takes into account the TL of the existing network VGG, the performance values are lower, as follows: accuracy 0.87, sensitivity (recall) 0.91, specificity 0.84, precision 0.86, FNR of 0.08, and FPR 0.14. Conclusions: The experimental results show the outperformance of the CNN, which is not based on transfer learning, but is using as information the MRI brain scans decomposed into DWT information instead of the pixel intensity of the original scans. The results are promising for the proposed CNN based on DWT knowledge to serve for binary diagnosis of glioma tumors among other tumors and diseases. Moreover, the SVM learning model using DWT data analysis performs with higher accuracy and sensitivity than using pixel values.",not included,0.8071766257286072,22
10.1515/labmed-2023-0037,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',semantic_scholar,Journal of Laboratory Medicine,2000-01-01 00:00:00,semantic_scholar,statistical learning and big data applications,https://www.semanticscholar.org/paper/a076572adb65a0a810002d01a4831af1e82c9818,"Abstract The amount of data generated in the field of laboratory medicine has grown to an extent that conventional laboratory information systems (LISs) are struggling to manage and analyze this complex, entangled information (“Big Data”). Statistical learning, a generalized framework from machine learning (ML) and artificial intelligence (AI) is predestined for processing “Big Data” and holds the potential to revolutionize the field of laboratory medicine. Personalized medicine may in particular benefit from AI-based systems, especially when coupled with readily available wearables and smartphones which can collect health data from individual patients and offer new, cost-effective access routes to healthcare for patients worldwide. The amount of personal data collected, however, also raises concerns about patient-privacy and calls for clear ethical guidelines for “Big Data” research, including rigorous quality checks of data and algorithms to eliminate underlying bias and enable transparency. Likewise, novel federated privacy-preserving data processing approaches may reduce the need for centralized data storage. Generative AI-systems including large language models such as ChatGPT currently enter the stage to reshape clinical research, clinical decision-support systems, and healthcare delivery. In our opinion, AI-based systems have a tremendous potential to transform laboratory medicine, however, their opportunities should be weighed against the risks carefully. Despite all enthusiasm, we advocate for stringent added-value assessments, just as for any new drug or treatment. Human experts should carefully validate AI-based systems, including patient-privacy protection, to ensure quality, transparency, and public acceptance. In this opinion paper, data prerequisites, recent developments, chances, and limitations of statistical learning approaches are highlighted.",not included,0.8500882655382156,23
10.1007/s43681-022-00201-4,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',AI and Ethics,Springer,2023-08-01 00:00:00,springer,assessing the ethical and social concerns of artificial intelligence in neuroinformatics research: an empirical test of the european union assessment list for trustworthy ai (altai),http://dx.doi.org/10.1007/s43681-022-00201-4,"Ethical and social concerns are a key obstacle to the adoption of artificial intelligence (AI) in the life sciences and beyond. The discussion of these issues has intensified in recent years and led to a number of approaches, tools and initiatives. Key amongst them is the idea of ex-ante impact assessments that aim to identify issues at the early stages of development. One prominent example of such ex-ante impact assessment is the European Union's (EU) Assessment list for Trustworthy AI (ALTAI). This article uses the findings of a large-scale application of the ALTAI to a large neuro-informatics project as an exemplar to demonstrate the effectiveness and limitations of the ALTAI in practice. The article shows that ex-ante impact assessment has the potential to help identify and address ethical and social issues. However, they need to be understood as part of a broader socio-technical ecosystem of AI. For ALTAI and related approaches to be useful in bio-medical research, they should be interpreted from a systems theory perspective which allows for their integration into the rich set of tools, legislation and approaches. The paper argues that ex-ante impact assessments have the best chance of being successful if seen applied in conjunction with other approaches in the context of the overall AI ecosystem.",not included,0.8084449231624603,24
10.1007/s44206-022-00017-z,preprocessed,architecture,'architecture' AND ('generative ai' OR 'artificial intelligence') AND 'public sector' AND 'public value',Digital Society,Springer,2022-08-19 00:00:00,springer,algorithmic bias and risk assessments: lessons from practice,http://dx.doi.org/10.1007/s44206-022-00017-z,"In this paper, we distinguish between different sorts of assessments of algorithmic systems, describe our process of assessing such systems for ethical risk, and share some key challenges and lessons for future algorithm assessments and audits. Given the distinctive nature and function of a third-party audit, and the uncertain and shifting regulatory landscape, we suggest that second-party assessments are currently the primary mechanisms for analyzing the social impacts of systems that incorporate artificial intelligence. We then discuss two kinds of assessments: an ethical risk assessment and a narrower, technical algorithmic bias assessment. We explain how the two assessments depend on each other, highlight the importance of situating the algorithm within its particular socio-technical context, and discuss a number of lessons and challenges for algorithm assessments and, potentially, for algorithm audits. The discussion builds on our team’s experience of advising and conducting ethical risk assessments for clients across different industries in the last 4 years. Our main goal is to reflect on the key factors that are potentially ethically relevant in the use of algorithms and draw lessons for the nascent algorithm assessment and audit industry, in the hope of helping all parties minimize the risk of harm from their use.",not included,0.8046532601118088,25
4490d077087df12e47c6f8d77a217179980c7516,preprocessed,citation,citation,semantic_scholar,arXiv.org,2024.0,semantic_scholar,"navigating llm ethics: advancements, challenges, and future directions",https://www.semanticscholar.org/paper/4490d077087df12e47c6f8d77a217179980c7516,"This study addresses ethical issues surrounding Large Language Models (LLMs) within the field of artificial intelligence. It explores the common ethical challenges posed by both LLMs and other AI systems, such as privacy and fairness, as well as ethical challenges uniquely arising from LLMs. It highlights challenges such as hallucination, verifiable accountability, and decoding censorship complexity, which are unique to LLMs and distinct from those encountered in traditional AI systems. The study underscores the need to tackle these complexities to ensure accountability, reduce biases, and enhance transparency in the influential role that LLMs play in shaping information dissemination. It proposes mitigation strategies and future directions for LLM ethics, advocating for interdisciplinary collaboration. It recommends ethical frameworks tailored to specific domains and dynamic auditing systems adapted to diverse contexts. This roadmap aims to guide responsible development and integration of LLMs, envisioning a future where ethical considerations govern AI advancements in society.",not included,0.8411970287561417,26
2a0ac1e99d95ed64cc4ddb1bcc5ff40e21e706be,preprocessed,citation,citation,semantic_scholar,European Journal of Theoretical and Applied Sciences,2024.0,semantic_scholar,rise of the machines: exploring the emergence of machine consciousness,https://www.semanticscholar.org/paper/2a0ac1e99d95ed64cc4ddb1bcc5ff40e21e706be,"Inquiry into the field of artificial intelligence (machines) and its potential to develop consciousness is presented in this study. This investigation explores the complex issues surrounding machine consciousness at the nexus of AI, neuroscience, and philosophy as we delve into the fascinating world of artificial intelligence (AI) and investigate the intriguing question: are machines on the verge of becoming conscious beings? The study considers the likelihood of machines displaying self-awareness and the implications thereof through an analysis of the current state of AI and its limitations. However, with advancements in machine learning and cognitive computing, AI systems have made significant strides in emulating human-like behavior and decision-making. Furthermore, the emergence of machine consciousness raises questions about the blending of human and artificial intelligence, and ethical considerations are also considered. The study provides a glimpse into a multidisciplinary investigation that questions accepted theories of consciousness, tests the limits of what is possible with technology, and do these advancements signify a potential breakthrough in machine consciousness. ",not included,0.8065193235874176,27
7671665a287ed3fd6aae9f5b035a3fa05d55fdbe,preprocessed,citation,citation,semantic_scholar,Buildings,2024.0,semantic_scholar,artificial intelligence islamic architecture (aiia): what is islamic architecture in the age of artificial intelligence?,https://www.semanticscholar.org/paper/7671665a287ed3fd6aae9f5b035a3fa05d55fdbe,"Revisiting the long-debated question: “What is Islamic architecture?”, this research article aims to explore the identity of “Islamic architecture (IA)” in the context of artificial intelligence (AI) as well as the novel opportunities and cultural challenges associated with applying AI techniques, such as the machine learning of Midjourney in the context of IA. It investigates the impact factors of AI technologies on the understanding and interpretation of traditional Islamic architectural principles, especially architectural design processes. This article employs a quantitative research methodology, including the observation of works of artists and architectural designers appearing in the mass media in light of a literature review and critical analysis of scholarly debates on Islamic architecture, spanning from historical perspectives to contemporary discussions. The article argues for the emergence of a continuous paradigm shift from what is commonly known as “postmodern Islamic architecture” (PMIA) into “artificial intelligence Islamic architecture” (AIIA), as coined by the authors of this article. It identifies the following impact factors of AI on IA: (1) particular requirements and sensitivities, inaccuracies, and biases, (2) human touch, unique craftsmanship, and a deep understanding of cultural issues, (3) regional variation, (4) translation, (5) biases in sources, (6) previously used terms and expressions, and (7) intangible values. The significance of this research in digital heritage lies in the fact that there are no pre-existing theoretical publications on the topic of “Islamic architecture in the age of artificial intelligence”, although an extensive set of publications interpreting the question of the definition of Islamic architecture, in general, is found. This article is pivotal in analyzing this heritage-inspired design approach in light of former criticism of the definition of “Islamic architecture”, which could benefit both theorists and practitioners. This theoretical article is the first in a series of two sequential articles in the Buildings journal; the second (practical) article is an analytical evaluation of the Midjourney architectural virtual lab, defining major current limits in AI-generated representations of Islamic architectural heritage.",not included,0.826026663184166,28
d0c3fad605a29ab67e8befddcafac4f024e2def2,preprocessed,citation,citation,semantic_scholar,International Conference on Human Factors in Computing Systems,2023.0,semantic_scholar,what is human-centered about human-centered ai? a map of the research landscape,https://www.semanticscholar.org/paper/d0c3fad605a29ab67e8befddcafac4f024e2def2,"The application of Artificial Intelligence (AI) across a wide range of domains comes with both high expectations of its benefits and dire predictions of misuse. While AI systems have largely been driven by a technology-centered design approach, the potential societal consequences of AI have mobilized both HCI and AI researchers towards researching human-centered artificial intelligence (HCAI). However, there remains considerable ambiguity about what it means to frame, design and evaluate HCAI. This paper presents a critical review of the large corpus of peer-reviewed literature emerging on HCAI in order to characterize what the community is defining as HCAI. Our review contributes an overview and map of HCAI research based on work that explicitly mentions the terms ‘human-centered artificial intelligence’ or ‘human-centered machine learning’ or their variations, and suggests future challenges and research directions. The map reveals the breadth of research happening in HCAI, established clusters and the emerging areas of Interaction with AI and Ethical AI. The paper contributes a new definition of HCAI, and calls for greater collaboration between AI and HCI research, and new HCAI constructs.",not included,0.8288842886686325,29
315f2b989010b7857d5af77f3ac0bdb3cff55225,preprocessed,citation,citation,semantic_scholar,Spanish Journal of Agricultural Research,2024.0,semantic_scholar,selection of incentives for a business strategy based on crop diversification,https://www.semanticscholar.org/paper/315f2b989010b7857d5af77f3ac0bdb3cff55225,"Aim of study: This study proposes a crop diversification innovative business model based on stakeholder preferences towards different incentive alternatives. 
Area of study: South-east Spain. 
Material and methods: Citrus intercropping practices in south-east Spain has been used as case study. Stakeholders’ preferences for crop diversification incentives were investigated by using a multicriteria approach, and those results were integrated into the development of a business model canvas. 
Main results: Including crop diversification practices as environmental practices within the operational programmes of producer organizations is seen the most preferred incentive over which the business model canvas is developed. 
Research highlights: The establishment of business opportunities for crop diversification practices would facilitate the overcoming of adoption barriers along the agrifood value chain and would promote health and sustainable food systems.",not included,0.8043087244033813,30
11b9adb22e051a5bba7469a41f20714c93e8af17,preprocessed,citation,citation,semantic_scholar,Business and Politics,2024.0,semantic_scholar,"the future of ai politics, policy, and business",https://www.semanticscholar.org/paper/11b9adb22e051a5bba7469a41f20714c93e8af17,"
 Our aim with this special issue on the future of artificial intelligence (AI) politics, policy, and business is to give space to considering how the balalnce between risk and reward from AI technologies is and perhaps should be pursued by the public and private sectors. Ultimately, private firms and regulators will need to work collaboratively, given the complex networks of actors involved in AI development and deployment and the potential for the technology to alter existing policy regimes. We begin the introduction of this special issue of Business & Politics with a discussion of the growth in AI technology use and discussions of appropriate governance, followed by a consideration of how AI-related politics, policy, and business intersect. We then summarize the contributions of the authors in this issue and conclude with thoughts about how political science, public administration, and public policy scholars have much to offer, as well as much to study, the establishment of effective AI governance.",not included,0.8065193235874176,31
351409aaa58ce26ec060aa6c253500e64daa0291,preprocessed,citation,citation,semantic_scholar,Frontiers in Environmental Science,2023.0,semantic_scholar,"collaborating on esg consulting, reporting, and communicating education: using partner maps for capability building design",https://www.semanticscholar.org/paper/351409aaa58ce26ec060aa6c253500e64daa0291,"Given the rising demand for environmental, social, and governance (ESG) talents, this study aims to provide a multidisciplinary outlook of specific capability requirements for ESG talents, focusing on the use of ESG and carbon information, thereby providing a roadmap for ESG education. Following design science framework conventions and running design workshops that integrate design thinking of “how might we” design questions, literature analysis, and expert interviews across disciplines, this study presents findings regarding three main activities—consulting, reporting, and communicating. Based on the iterations of design workshops that adopt a circular economy-based partner map design canvas for stakeholder analysis with procedures such as expert interviews and literature analysis, three partner/capability maps were generated to map stakeholders and explore the capabilities needed. ESG and carbon information digital and data skills emerged as the core capability to complete all the three tasks. A conceptual framework—a Smart System of ESG and Carbon Information—is proposed to summarize planning, operating, and communicating with ESG and carbon information, along with high-level organizational actions and talent capabilities. It identifies the building blocks of an ESG operating system within an enterprise to engage various stakeholders for value-creation collaboration. Despite the limitation of a lack of comprehensive review and limited geographic and disciplinary representation, this study provides a roadmap for enterprises and universities to explore and define talent requirements and create specific education and training programs.",not included,0.8288842886686325,32
94ee5ffd76e46a2aab28744d655d30d3346c177d,preprocessed,citation,citation,semantic_scholar,Library Hi Tech News,2024.0,semantic_scholar,ai-powered libraries: enhancing user experience and efficiency in nigerian knowledge repositories,https://www.semanticscholar.org/paper/94ee5ffd76e46a2aab28744d655d30d3346c177d,"Purpose
This study explores the pivotal role of artificial intelligence (AI) in revolutionizing knowledge organization within Nigerian libraries. The purpose of this study is to assess the challenges faced by these libraries, propose strategic approaches for successful AI integration and highlight the potential benefits and future directions of this transformative journey.

Design/methodology/approach
This study uses a comprehensive review of existing literature, case studies and a qualitative analysis of challenges faced by Nigerian libraries. Strategies for AI integration are proposed based on targeted capacity building, collaborative partnerships and phased implementation approaches. The methodology also involves assessing the current landscape of AI in Nigerian academic libraries, examining applications and exploring the perceived impacts of AI on library services.

Findings
Nigerian libraries face challenges such as limited resources, outdated systems and diverse information that hinder traditional knowledge organization methods. The integration of AI offers dynamic solutions, streamlining administrative tasks, optimizing search algorithms and enhancing user engagement. The findings of this study emphasize the potential benefits of AI, including improved accessibility, searchability and long-term efficiency gains in library collections.

Originality/value
This research contributes to the existing literature by providing insights into the specific challenges faced by Nigerian libraries and proposing practical strategies for AI integration. This study emphasizes the transformative potential of AI in addressing immediate challenges and unlocking enduring benefits. The originality lies in the context-specific exploration of AI in Nigerian libraries, offering a roadmap for stakeholders to embrace technological advancements and position libraries as leaders in providing innovative knowledge services.
",not included,0.8411970287561417,33
bca16316e435349f7f793248813cac40b61083d2,preprocessed,citation,citation,semantic_scholar,2024 IEEE International Conference on Blockchain and Distributed Systems Security (ICBDS),2024.0,semantic_scholar,enhancing road safety: the role of intelligent driver drowsiness detection systems,https://www.semanticscholar.org/paper/bca16316e435349f7f793248813cac40b61083d2,"The analysis of this research paper is touching the road safety concerns by focusing on developing a system which catches the drowsy state of the person who is driving which emerges as a big reason that causes up to 30due to this. Throughout the paper, the study discusses about literature review and various methodologies that include physiological, behavioural, and multimodal fusion approaches showing the evolution and history of drowsiness detection technologies. The methodology includes using of various sensor technologies and algorithms to collect data from the driver and process it to analyse data in real time. Eye movement and facial expressions are extracted which helps the system to identify signs of drowsiness with greater accuracy. When the driver is detected as drowsy a mechanism is triggered to alert the driver, averting accidents. Even with some challenges, the system is a big step towards increasing road safety utilizing advanced technologies to reduce the risk of driver fatigueless.",not included,0.8062968552112579,34
c280ddd2030f51f9677d5451b972488251995650,preprocessed,citation,citation,semantic_scholar,Current Oncology,2024.0,semantic_scholar,"the role of artificial intelligence on tumor boards: perspectives from surgeons, medical oncologists and radiation oncologists",https://www.semanticscholar.org/paper/c280ddd2030f51f9677d5451b972488251995650,"The integration of multidisciplinary tumor boards (MTBs) is fundamental in delivering state-of-the-art cancer treatment, facilitating collaborative diagnosis and management by a diverse team of specialists. Despite the clear benefits in personalized patient care and improved outcomes, the increasing burden on MTBs due to rising cancer incidence and financial constraints necessitates innovative solutions. The advent of artificial intelligence (AI) in the medical field offers a promising avenue to support clinical decision-making. This review explores the perspectives of clinicians dedicated to the care of cancer patients—surgeons, medical oncologists, and radiation oncologists—on the application of AI within MTBs. Additionally, it examines the role of AI across various clinical specialties involved in cancer diagnosis and treatment. By analyzing both the potential and the challenges, this study underscores how AI can enhance multidisciplinary discussions and optimize treatment plans. The findings highlight the transformative role that AI may play in refining oncology care and sustaining the efficacy of MTBs amidst growing clinical demands.",not included,0.8065193235874176,35
2c98d009dba99c385c421fcfd99fc16e9198e818,preprocessed,citation,citation,semantic_scholar,Applied Psychology Research,2024.0,semantic_scholar,ethical deployment of cognitive biases in marketing a framework for responsible influence,https://www.semanticscholar.org/paper/2c98d009dba99c385c421fcfd99fc16e9198e818,"This paper examines the intersection of cognitive biases and ethical marketing practices, highlighting how psychological principles can influence consumer behavior within ethical boundaries. Cognitive biases such as scarcity bias, authority bias, the halo effect, and confirmation bias significantly shape consumer perceptions and decisions. However, their application in marketing raises complex ethical concerns, particularly regarding consumer autonomy and the potential for manipulation. This study proposes a conceptual framework that integrates ethical guidelines with marketing strategies that utilize cognitive biases. Through a comprehensive literature review and theoretical analysis, this paper outlines the implications of these biases in marketing, develops a set of ethical guidelines, and discusses the broader impacts on consumer trust and brand integrity. The findings give marketers practical insights for ethically harnessing cognitive biases, ensuring that marketing practices drive business success and maintain consumer respect and loyalty.",not included,0.826026663184166,36
2aee26dd83ba733b3bf3cfa6c89177abaa1f4904,preprocessed,citation,citation,semantic_scholar,The Florida AI Research Society,2024.0,semantic_scholar,sharing accountability of versatile ai systems: the role of developers and practitioners,https://www.semanticscholar.org/paper/2aee26dd83ba733b3bf3cfa6c89177abaa1f4904,"AI systems pose both opportunities and threats in various industries. To harness these opportunities and mitigate risks, accountability is crucial. Traditionally, developers bear the responsibility for auditing and modifying algorithms. How-ever, in the evolving landscape of versatile AI, developers may lack contextual understanding across diverse fields. This paper proposes a theoretical framework that distributes accountability to developers and practitioners according to their capabilities. This framework enhances systemic com-prehension of shared roles, empowering both groups to col-laboratively avert potential adverse impacts.",not included,0.8288842886686325,37
82164ac956e42b26ed64e9741f175d792c95b93f,preprocessed,citation,citation,semantic_scholar,arXiv.org,2023.0,semantic_scholar,"the ethics of ai value chains: an approach for integrating and expanding ai ethics research, practice, and governance",https://www.semanticscholar.org/paper/82164ac956e42b26ed64e9741f175d792c95b93f,"Researchers, practitioners, and policymakers with an interest in AI ethics need more integrative approaches for studying and intervening in AI systems across many contexts and scales of activity. This paper presents AI value chains as an integrative concept that satisfies that need. To more clearly theorize AI value chains and conceptually distinguish them from supply chains, we review theories of value chains and AI value chains from the strategic management, service science, economic geography, industry, government, and applied research literature. We then conduct an integrative review of a sample of 67 sources that cover the ethical concerns implicated in AI value chains. Building upon the findings of our integrative review, we recommend three future directions that researchers, practitioners, and policymakers can take to advance more ethical practices across AI value chains. We urge AI ethics researchers and practitioners to move toward value chain perspectives that situate actors in context, account for the many types of resources involved in co-creating AI systems, and integrate a wider range of ethical concerns across contexts and scales.",not included,0.8351606994867324,38
7c8a533175c24ceff2cdb82dfd340065cc93a1ce,preprocessed,citation,citation,semantic_scholar,Applied Sciences,2023.0,semantic_scholar,"re-thinking data strategy and integration for artificial intelligence: concepts, opportunities, and challenges",https://www.semanticscholar.org/paper/7c8a533175c24ceff2cdb82dfd340065cc93a1ce,"The use of artificial intelligence (AI) is becoming more prevalent across industries such as healthcare, finance, and transportation. Artificial intelligence is based on the analysis of large datasets and requires a continuous supply of high-quality data. However, using data for AI is not without challenges. This paper comprehensively reviews and critically examines the challenges of using data for AI, including data quality, data volume, privacy and security, bias and fairness, interpretability and explainability, ethical concerns, and technical expertise and skills. This paper examines these challenges in detail and offers recommendations on how companies and organizations can address them. By understanding and addressing these challenges, organizations can harness the power of AI to make smarter decisions and gain competitive advantage in the digital age. It is expected, since this review article provides and discusses various strategies for data challenges for AI over the last decade, that it will be very helpful to the scientific research community to create new and novel ideas to rethink our approaches to data strategies for AI.",included,0.8206197261810303,39
6a7e30d7ed98bc6cbd77d63931c45eb4b24f02a8,preprocessed,citation,citation,semantic_scholar,,2022.0,semantic_scholar,a principles-based ethical assurance argument pattern for ai and autonomous systems,https://www.semanticscholar.org/paper/6a7e30d7ed98bc6cbd77d63931c45eb4b24f02a8,"An assurance case presents a clear and defensible argument, supported by evidence, that a system will operate as intended in a particular context. Typically, an assurance case presents an argument that a system will be acceptably safe in its intended context. One emerging proposal within the Trustworthy AI research community is to extend and apply this methodology to provide assurance that the use of an AI system or an autonomous system (AI/AS) will be acceptably ethical in a particular context. In this paper, we advance this proposal further. We do so by presenting a principles-based ethical assurance (PBEA) argument pattern for AI/AS. The PBEA argument pattern offers a framework for reasoning about the overall ethical acceptability of the use of a given AI/AS and it could be an early prototype template for specific ethical assurance cases. The four core ethical principles that form the basis of the PBEA argument pattern are: justice; beneficence; non-maleficence; and respect for personal autonomy. Throughout, we connect stages of the argument pattern to examples of AI/AS applications. This helps to show its initial plausibility. The aim of this paper is to shape the debate around adapting the assurance case methodology to reason about ethically justifiable risk acceptance in the context of AI/AS. As a work in progress, we welcome comments to the corresponding author. 2",not included,0.8252739846706391,40
34779d3610536a4d0f1f6609cab11955704082bd,preprocessed,citation,citation,semantic_scholar,,,semantic_scholar,decentralized innovation: exploring the impact of blockchain technology in software development,https://www.semanticscholar.org/paper/34779d3610536a4d0f1f6609cab11955704082bd,",",not included,0.819400179386139,41
a40fe47a4cf069fb0a824778d374749370d2dc93,preprocessed,citation,citation,semantic_scholar,,,semantic_scholar,the confluence of big data and artificial intelligence: unraveling complex patterns and unveiling novel paradigms for transformative knowledge discovery,https://www.semanticscholar.org/paper/a40fe47a4cf069fb0a824778d374749370d2dc93,": The research paper embarks on an exploratory journey into the intricate intersection of big data and artificial intelligence (AI). This study aims to unravel complex patterns within massive datasets and illuminate novel paradigms, fostering transformative knowledge discovery in an era defined by unprecedented data volume and advanced AI capabilities. The abstract commences by acknowledging the synergy between big data and AI, positioning them as intertwined forces shaping the landscape of knowledge discovery. The paper underscores the need to unravel intricate patterns within vast datasets, emphasizing the transformative potential of this confluence.",not included,0.8052007794380188,42
60915efd60a37edaf553259d757ce994d5e3e430,preprocessed,citation,citation,semantic_scholar,European Conference on Information Systems,2024.0,semantic_scholar,unraveling the nuances of ai accountability: a synthesis of dimensions across disciplines,https://www.semanticscholar.org/paper/60915efd60a37edaf553259d757ce994d5e3e430,"The widespread diffusion of Artificial Intelligence (AI)-based systems offers many opportunities to contribute to the well-being of individuals and the advancement of economies and societies. This diffusion is, however, closely accompanied by public scandals causing harm to individuals, markets, or society, and leading to the increasing importance of accountability. AI accountability itself faces conceptual ambiguity, with research scattered across multiple disciplines. To address these issues, we review current research across multiple disciplines and identify key dimensions of accountability in the context of AI. We reveal six themes with 13 corresponding dimensions and additional accountability facilitators that future research can utilize to specify accountability scenarios in the context of AI-based systems.",not included,0.8411970287561417,43
59124702b213e415f95a1b938aa56981cd3553d1,preprocessed,citation,citation,semantic_scholar,2023 International Conference on Integrated Intelligence and Communication Systems (ICIICS),2023.0,semantic_scholar,analysis system of english translation on cloud platform based on artificial intelligence,https://www.semanticscholar.org/paper/59124702b213e415f95a1b938aa56981cd3553d1,"The rapid development of artificial intelligence has greatly affected various fields including language translation. This paper aims to explore the design and implementation of an AI cloud platform developed specifically for English translation analysis. This is a research work that uses artificial intelligence technology to improve the quality and efficiency of English translation. In this system, artificial intelligence is applied in every link, from data processing to model training and result generation, to achieve more accurate and smooth translation. This paper mainly applies the experimental method and comparative method, proposes machine translation from artificial intelligence, and tests the English translation analysis system on the cloud platform. The experimental results show that the fidelity and fluency of the optical translation system are the highest, respectively 59% and 59.8%, and its translation level is the highest in the experiment. But at the same time, there are still some problems in machine translation, so it is necessary to summarize from its error types and make improvements.",not included,0.8062968552112579,44
e0f63483b9daba4f0e81486651a6f768a9e8b9d8,preprocessed,citation,citation,semantic_scholar,Inf.,2023.0,semantic_scholar,ethics and trustworthiness of ai for predicting the risk of recidivism: a systematic literature review,https://www.semanticscholar.org/paper/e0f63483b9daba4f0e81486651a6f768a9e8b9d8,"Artificial Intelligence (AI) can be very beneficial in the criminal justice system for predicting the risk of recidivism. AI provides unrivalled high computing power, speed, and accuracy; all harnessed to strengthen the efficiency in predicting convicted individuals who may be on the verge of recommitting a crime. The application of AI models for predicting recidivism has brought positive effects by minimizing the possible re-occurrence of crime. However, the question remains of whether criminal justice system stakeholders can trust AI systems regarding fairness, transparency, privacy and data protection, consistency, societal well-being, and accountability when predicting convicted individuals’ possible risk of recidivism. These are all requirements for a trustworthy AI. This paper conducted a systematic literature review examining trust and the different requirements for trustworthy AI applied to predicting the risks of recidivism. Based on this review, we identified current challenges and future directions regarding applying AI models to predict the risk of recidivism. In addition, this paper provides a comprehensive framework of trustworthy AI for predicting the risk of recidivism.",not included,0.8065193235874176,45
e127da5ed504a9f3d143554242a93b7620743d25,preprocessed,citation,citation,semantic_scholar,Journal of Global Information Management,2024.0,semantic_scholar,optimizing digital market decision-making through artificial intelligence platforms,https://www.semanticscholar.org/paper/e127da5ed504a9f3d143554242a93b7620743d25,"As artificial intelligence rapidly advances, addressing the interplay of technical, ethical, and risk factors in optimizing digital market decision-making through AI platforms has become increasingly prominent. However, the impact of these factors on market performance, particularly in investment value, remains underexplored. The study, based on 412 validated responses from service industry professionals gathered through a carefully designed questionnaire, aims to predict the relationship among these factors and their influence on market performance. It also explores how cognitive engagement mediates the relationship between AI platforms and financial metrics. Key findings:(1) the interplay of technical, ethical, and risk factors optimizes market decision-making and guides AI investments; (2) cognitive engagement, especially in the services sector, is essential to maximize the impact of AI platforms on market performance. The study provides valuable insights into AI's role in shaping market dynamics within the services sector and relevant governance recommendations for policymakers.",not included,0.8411970287561417,46
73cdcbd57cadcaecbac76a0af9b75bf304e18011,preprocessed,citation,citation,semantic_scholar,Frontiers Artif. Intell.,2024.0,semantic_scholar,exploring how ai adoption in the workplace affects employees: a bibliometric and systematic review,https://www.semanticscholar.org/paper/73cdcbd57cadcaecbac76a0af9b75bf304e18011,"Introduction The adoption of artificial intelligence (AI) in the workplace is changing the way organizations function, and profoundly affecting employees. These organizational changes raise crucial questions about the employee’s future and well-being. Our study aims to explore the intersection between artificial intelligence and employee well-being through a bibliometric review and a contextual analysis. Methodology Carried out in May 2024, our study is divided into two phases. The first phase, dedicated to bibliometric review, was conducted using the PRISMA method, and explored the Scopus and Web of Science databases for the period from 2015 to 2024. A total of 92 articles were selected for quantitative analysis using VOSviewer software. The second phase is based on an in-depth systematic analysis of 25 articles selected from those previously identified. These articles were selected on the basis of their relevance to the research question, and were subjected to in-depth thematic analysis using NVivo software. Results The bibliometric analysis results reveal a significant increase in publications starting from the year 2020, highlighting advancements in research, primarily in the United States and China. The co-occurrence analysis identifies four main clusters: ethics, work autonomy, employee stress, and mental health, thus illustrating the dynamics created by artificial intelligence in the professional environment. Furthermore, the systematic analysis has brought to light theoretical gaps and under-explored areas, such as the need to conduct empirical studies in non-Western cultural contexts and among diverse target groups, including older adults, individuals of different sexes, people with low education levels, and participants from various sectors, including primary and secondary industries, small manufacturing businesses, call centers, as well as public and private healthcare sectors. Conclusion Existing literature emphasize the importance for organizations to implement supportive strategies aimed at mitigating the potential adverse effects of AI on employee well-being, while also leveraging its benefits to enhance workplace autonomy and satisfaction and promote AI-enabled innovation through employee creativity and self-efficacy.",not included,0.8062968552112579,47
a7b6f3fdf829ad0ec8b450758621938169b5206a,preprocessed,citation,citation,semantic_scholar,Economic Sciences,2024.0,semantic_scholar,a study on steps in building data infrastructure for data-driven hr practices in modern organizations (hr analytics perspective),https://www.semanticscholar.org/paper/a7b6f3fdf829ad0ec8b450758621938169b5206a,"HR analytics is the process of gathering and analyzing HR data to yield useful insights, enhance decision-making, and energize the workplace with data accuracy. Working dimensions are changing every minute in the world with advanced insights into the work environment.  As a part of the progression advanced technologies are adopted in organizations. New landscapes and horizons are made a pool of data as employees work 24/7, work from home, destination works, and various alternatives give rise to the use of the data. Now the data is new blood for the old organizations. So, in this parlance, the use of HR analytics is the need of the hour for HR Managers in developing organizations. A separate infrastructure is needed in the phase of modern organizations to become a rollercoaster in data-driven organizations. This paper focuses on the steps in creating data infrastructure for developing an infrastructure in modern organizations for data-driven HR practices.",not included,0.826026663184166,48
24869fc180b575e3eec292a467d7810e2f1e5bef,preprocessed,citation,citation,semantic_scholar,Human Resource Development Review,2024.0,semantic_scholar,exploring opportunities for artificial intelligence in organization development,https://www.semanticscholar.org/paper/24869fc180b575e3eec292a467d7810e2f1e5bef,"The purpose of this research was to examine the utilization of artificial intelligence (AI) in organization development (OD) through a comprehensive review of existing literature. We also propose potential avenues for future research on AI in OD. We conducted a systematic literature review of 68 studies on AI in OD based on Cummings and Worley’s four OD categories (i.e., human process, human resource, strategic change, and technostructural interventions). We first summarized and analyzed key information about how AI is implemented in OD contexts, and then examined the underlying theories or theoretical frameworks utilized in OD studies focusing on AI. We examined the application of AI in OD, potential ethical concerns, and recommendations for future research and practice using AI in OD. The paper concludes with discussion and implications for research and practice.",not included,0.8288842886686325,49
e1936e4c5e2ef07fe5e8683f5bf3f67ee77ba8ad,preprocessed,citation,citation,semantic_scholar,Mednarodno inovativno poslovanje = Journal of Innovative Business and Management,2024.0,semantic_scholar,the transformative role of artificial intelligence in human resources,https://www.semanticscholar.org/paper/e1936e4c5e2ef07fe5e8683f5bf3f67ee77ba8ad,"The article explores the landscape of Artificial Intelligence (AI) applications in Human Resources (HR) by highlighting current trends and providing some anticipations about future trends and developments. AI is revolutionary reshaping basic HR processes – from workforce planning, recruitment, to employee’s development and fostering diversity and inclusion. AI plays important role in addressing bias in recruitment, enhances objectivity and promotes equal opportunities. AI-driven tools (like chatbots and virtual assistants etc.) integration in HR processes enables seamless communication and propellers HR practices towards enhanced efficiency and strategic decision-making. Furthermore, the article provides a short analysis of some software solutions that serve organizations as AI HR tools. 
By taking a look towards the future, we can predicts that tools like predictive analytics, monitoring of employee well-being, and convergence of AI with augmented reality (AR) and virtual reality (VR) can be projected as some of the future key developments. 
In the conclusion we can stress out the transformative synergy between AI and HR, which allows organizations and HR professionals to embrace innovation in order to deliver better results for all of the stakeholders: employees, management, owners and broader society.",not included,0.8351606994867324,50
b24cd6730256ced735b6fab7ef3ad637fcf9a0b0,preprocessed,citation,citation,semantic_scholar,International Journal For Multidisciplinary Research,2024.0,semantic_scholar,impact of artificial intelligence (ai) on human resource management (hrm),https://www.semanticscholar.org/paper/b24cd6730256ced735b6fab7ef3ad637fcf9a0b0,"Incorporating Artificial Intelligence (AI) into Human Resource Management (HRM) has become a significant driving force in shaping contemporary workplaces. This paper comprehensively examines AI's influence on HRM, from its foundational concepts to its practical applications, advantages, challenges, ethical considerations, legal ramifications, anticipated trends, and actionable recommendations. Commencing with an introductory framework, the paper navigates the intricate facets of AI within HRM, elucidating its diverse components and functionalities. It further scrutinizes AI's specific roles in recruitment, training, performance management, and employee engagement, emphasizing its transformative potential. Additionally, the paper articulates the manifold benefits AI affords HRM, such as process optimization, informed decision-making, and enhanced employee engagement, juxtaposed against the inherent challenges, including data integrity, privacy concerns, biases, and algorithmic transparency issues. Addressing AI's ethical and legal dimensions in HRM, the paper underscores the imperative of conscientious AI integration and governance. Furthermore, it anticipates forthcoming AI trends and furnishes strategic guidance for organizations navigating this evolving landscape. Ultimately, the paper advocates for ethical, transparent, and human-centric approaches to AI adoption, underscoring its profound impact on HRM practices and workplace dynamics.",not included,0.838510024547577,51
b644a9594eb96450dd6f5e9c7e1056eccd24ded4,preprocessed,citation,citation,semantic_scholar,Frontiers in Management Science,2024.0,semantic_scholar,unlocking the potential: literature review on the evolving role of ai in hrm,https://www.semanticscholar.org/paper/b644a9594eb96450dd6f5e9c7e1056eccd24ded4,"This paper explores the evolution and strategic significance of Artificial Intelligence (AI) in Human Resource Management (HRM). Tracing AI’s journey from rudimentary automation to sophisticated systems, it focuses on the impact of AI, particularly in Natural Language Processing (NLP) for recruitment and AI-driven employee engagement solutions. The study delves into challenges, opportunities, and ethical considerations, aiming to identify gaps in existing knowledge. The research aims to provide a comprehensive understanding of the current state of AI integration in HRM, with key objectives including defining scope, exploring technologies, presenting case studies, and unravelling regulatory and ethical dimensions. This abstract sets the stage for a nuanced exploration of AI-HRM dynamics.",not included,0.8116682052612305,52
f8b16298b126c7341e22aea19c0d257417ed2a04,preprocessed,citation,citation,semantic_scholar,International Journal of Science and Research Archive,2023.0,semantic_scholar,ethical decision-making in it governance: a review of models and frameworks,https://www.semanticscholar.org/paper/f8b16298b126c7341e22aea19c0d257417ed2a04,"Ethical decision-making within the realm of Information Technology (IT) governance is of paramount importance due to its far-reaching implications on organizational integrity, stakeholder trust, and societal welfare. This review presents a comprehensive review of various models and frameworks aimed at guiding ethical decision-making processes within IT governance contexts. The review begins by elucidating the fundamental principles underlying ethical decision-making, emphasizing the significance of moral reasoning, accountability, and transparency in IT governance. It then proceeds to examine prominent models and frameworks, categorizing them based on their theoretical foundations, applicability, and intended outcomes. Firstly, traditional normative ethical theories such as utilitarianism, deontology, and virtue ethics are discussed in the context of their application to IT governance dilemmas. These theories provide overarching ethical frameworks within which IT decision-makers can evaluate actions and policies. Secondly, the review delves into contemporary approaches specifically tailored for IT governance, including the Ethical Decision-Making Framework (EDMF), the Responsible Decision-Making Model (RDM), and the Ethical Governance Framework (EGF). These models offer systematic processes for identifying, analyzing, and resolving ethical dilemmas inherent in IT decision-making, considering factors such as privacy, security, intellectual property rights, and social responsibility. Furthermore, the review highlights the importance of integrating ethical considerations into existing IT governance frameworks, such as COBIT (Control Objectives for Information and Related Technologies) and ITIL (Information Technology Infrastructure Library), to ensure comprehensive governance practices. Lastly, the review examines emerging trends in ethical decision-making within IT governance, including the utilization of artificial intelligence and machine learning algorithms for ethical decision support, the incorporation of ethical design principles in software development processes, and the role of organizational culture in fostering ethical behavior among IT professionals. This review underscores the critical need for robust models and frameworks to guide ethical decision-making in IT governance, providing a foundation for organizations to navigate complex ethical dilemmas while upholding principles of integrity, accountability, and societal welfare.",not included,0.819400179386139,53
c10ce09d53f66868fa46824793e261e4e70b280a,preprocessed,citation,citation,semantic_scholar,Frontiers in Environmental Science,2023.0,semantic_scholar,optimization of the environmental protection tax system design based on artificial intelligence,https://www.semanticscholar.org/paper/c10ce09d53f66868fa46824793e261e4e70b280a,"Introduction: China achieved significant economic growth in the past two decades, and the sustained economic growth also brings negative implications for the environment. The Chinese government has introduced various fiscal reforms to mitigate the negative implication of the environment in the economy. Modernization of China's governance system and improvement of social development were the main goals of the 14th Five-Year Plan. Methods: Literature combing method and Chart analysis method. Result: Artificial intelligence promotes the efficiency of bonded governance environment and boosts national management modernization. Discussion: This paper suggests that the artificial intelligence construction of the environmental protection tax system improves tax collection and management, tax payment service, and tax management. In addition, the government should adopt other strategies to promote a clean environment, such as tax exemption for green and cleaner production. Easy loans should be provided to the exports, especially those contributing to clean energy production.",not included,0.8052007794380188,54
6136274ca9bd6a1dd2b91e2e96e27f660e1116f1,preprocessed,citation,citation,semantic_scholar,Behavior and Information Technology,2023.0,semantic_scholar,dignity and use of algorithm in performance evaluation,https://www.semanticscholar.org/paper/6136274ca9bd6a1dd2b91e2e96e27f660e1116f1,"ABSTRACT Algorithms are increasingly used by human resource departments to evaluate employee performance. While the algorithms are perceived to be objective and neutral by removing human biases, they are often perceived to be less fair than human managers. This research proposes dignity as an important construct in explaining the discrepancy in perceived fairness and investigates remedial steps for improving dignity and fairness for algorithm-based employee evaluations. Three experiments’ results show that those evaluated by algorithms perceive lower levels of dignity, leading them to believe the process is less fair. In addition, we find that providing justifications for algorithm usage in employee evaluations improves perceived dignity. However, human-algorithm collaboration does not enhance perceived dignity.",not included,0.8033411532640458,55
e4a2baa92dcdfbcd82621e9915d0a433704a8516,preprocessed,citation,citation,semantic_scholar,Frontiers in Artificial Intelligence,2022.0,semantic_scholar,politics by automatic means? a critique of artificial intelligence ethics at work,https://www.semanticscholar.org/paper/e4a2baa92dcdfbcd82621e9915d0a433704a8516,"Calls for “ethical Artificial Intelligence” are legion, with a recent proliferation of government and industry guidelines attempting to establish ethical rules and boundaries for this new technology. With few exceptions, they interpret Artificial Intelligence (AI) ethics narrowly in a liberal political framework of privacy concerns, transparency, governance and non-discrimination. One of the main hurdles to establishing “ethical AI” remains how to operationalize high-level principles such that they translate to technology design, development and use in the labor process. This is because organizations can end up interpreting ethics in an ad-hoc way with no oversight, treating ethics as simply another technological problem with technological solutions, and regulations have been largely detached from the issues AI presents for workers. There is a distinct lack of supra-national standards for fair, decent, or just AI in contexts where people depend on and work in tandem with it. Topics such as discrimination and bias in job allocation, surveillance and control in the labor process, and quantification of work have received significant attention, yet questions around AI and job quality and working conditions have not. This has left workers exposed to potential risks and harms of AI. In this paper, we provide a critique of relevant academic literature and policies related to AI ethics. We then identify a set of principles that could facilitate fairer working conditions with AI. As part of a broader research initiative with the Global Partnership on Artificial Intelligence, we propose a set of accountability mechanisms to ensure AI systems foster fairer working conditions. Such processes are aimed at reshaping the social impact of technology from the point of inception to set a research agenda for the future. As such, the key contribution of the paper is how to bridge from abstract ethical principles to operationalizable processes in the vast field of AI and new technology at work.",not included,0.8437141388654709,56
ffada086cca0ba01ab7d72845bafcab035f8618d,preprocessed,citation,citation,semantic_scholar,ITM Web of Conferences,2024.0,semantic_scholar,role of artificial intelligence in human resource management for optimizing employee productivity,https://www.semanticscholar.org/paper/ffada086cca0ba01ab7d72845bafcab035f8618d,"Artificial intelligence (AI) has significantly transformed various industries, including human resource management, by enhancing efficiency, decision-making, and employee productivity. Recruitments can be modernized by using catboats, predictive analysis helps in offering data-driven insights that can be used to find skill gaps and people management planning. AI’s advancements have made it easy to integrate AI with HRM for increasing efficiency, despite this a lot of ethical concerns, biases, and privacy issue makes it difficult to implement AI completely in the decision-making process. This paper is a bibliometric study focusing on the evolution of AI with HRM to enhance employee productivity and identify key trends and research gaps. This study considered publications for 10 years from 2014 to 2024 through various databases such as Scopus, Web of Science, and IEEE, the study further divides the literature to highlight the most cited authors, countries contributing to the field, and year-wise contribution. The paper focuses on studying the role of AI in various functional areas of HR such as recruitment, performance, and employee productivity. The findings highlight the increasing role of AI across multiple HR practices. This bibliometric investigation offers valuable findings for researchers and practitioners aiming to use AI to enhance HR jobs.",not included,0.8294077664613724,57
0072959e028aadf81e55353d3be7bd4f95e447e4,preprocessed,citation,citation,semantic_scholar,ACM Journal on Responsible Computing,2024.0,semantic_scholar,"a systematic review on fostering appropriate trust in human-ai interaction: trends, opportunities and challenges",https://www.semanticscholar.org/paper/0072959e028aadf81e55353d3be7bd4f95e447e4,"Appropriate Trust in Artificial Intelligence (AI) systems has rapidly become an important area of focus for both researchers and practitioners. Various approaches have been used to achieve it, such as confidence scores, explanations, trustworthiness cues, or uncertainty communication. However, a comprehensive understanding of the field is lacking due to the diversity of perspectives arising from various backgrounds that influence it and the lack of a single definition for appropriate trust. To investigate this topic, this paper presents a systematic review to identify current practices in building appropriate trust, different ways to measure it, types of tasks used, and potential challenges associated with it. We also propose a Belief, Intentions, and Actions (BIA) mapping to study commonalities and differences in the concepts related to appropriate trust by (a) describing the existing disagreements on defining appropriate trust, and (b) providing an overview of the concepts and definitions related to appropriate trust in AI from the existing literature. Finally, the challenges identified in studying appropriate trust are discussed, and observations are summarized as current trends, potential gaps, and research opportunities for future work. Overall, the paper provides insights into the complex concept of appropriate trust in human-AI interaction and presents research opportunities to advance our understanding on this topic.",not included,0.8411970287561417,58
b9fbdfcd81091111b2f2263cbde8c7df67c7f591,preprocessed,citation,citation,semantic_scholar,CSCW Companion,2024.0,semantic_scholar,from stem to stern: contestability along ai value chains,https://www.semanticscholar.org/paper/b9fbdfcd81091111b2f2263cbde8c7df67c7f591,"This workshop will grow and consolidate a community of interdisciplinary CSCW researchers focusing on the topic of contestable AI. As an outcome of the workshop, we will synthesize the most pressing opportunities and challenges for contestability along AI value chains in the form of a research roadmap. This roadmap will help shape and inspire imminent work in this field. Considering the length and depth of AI value chains, it will especially spur discussions around the contestability of AI systems along various sites of such chains. The workshop will serve as a platform for dialogue and demonstrations of concrete, successful, and unsuccessful examples of AI systems that (could or should) have been contested, to identify requirements, obstacles, and opportunities for designing and deploying contestable AI in various contexts. This will be held primarily as an in-person workshop, with some hybrid accommodation. The day will consist of individual presentations and group activities to stimulate ideation and inspire broad reflections on the field of contestable AI. Our aim is to facilitate interdisciplinary dialogue by bringing together researchers, practitioners, and stakeholders to foster the design and deployment of contestable AI.",not included,0.8062968552112579,59
a78d29edbfe9d7e2a5075888fd989d6b20657121,preprocessed,citation,citation,semantic_scholar,Big Data & Society,2024.0,semantic_scholar,ai as super-controversy: eliciting ai and society controversies with an extended expert community in the uk,https://www.semanticscholar.org/paper/a78d29edbfe9d7e2a5075888fd989d6b20657121,"Following the release of large language models in the late 2010s, the backers of this new type of artificial intelligence (AI) publicly affirmed that the technology is controversial and harmful to society. This situation sets contemporary AI apart from 20th-century controversies about technnoscience, such as nuclear power and genetically modified (GM) foods, and disrupts established assumptions concerning public controversies as occasions for technological democracy. In particular, it challenges the idea that such controversies enable inclusion and collective processes of problem definition (‘problematisation’) across societal domains. In this paper, we show how social research can contribute to addressing this challenge of AI controversies by adopting a distinctive methodology of controversy analysis: controversy elicitation. This approach actively selects, qualifies and evaluates controversies in terms of their capacity to problematise AI across the science and non-science binary. We describe our implementation of this approach in a participatory study of recent AI controversies, conducted through consultation with UK experts in AI and society. Combining an online questionnaire, social media analysis and a participatory workshop, our study suggests that civil society actors have developed distinctive strategies of problematisation that counter the strategic affirmation of AI’s controversiality by its proponents and which centre on the public mobilisation of AI-related incidents: demonstrations of bias, accidents and walkouts. Crucially, this emphasis on ‘AI frictions’ does not result in the fragmentation of AI controversies, but rather enables the articulation of AI as a ‘super-controversy’: the explication of connections between technical propositions, situated troubles and structural problems in society (discrimination, inequalities and corporate power).",not included,0.826026663184166,60
a4c91a99f5ae7f5b40649c47a9becebfd47ca8e4,preprocessed,citation,citation,semantic_scholar,Open international journal of informatics,2023.0,semantic_scholar,"review of the governance, risk and compliance approaches for artificial intelligence",https://www.semanticscholar.org/paper/a4c91a99f5ae7f5b40649c47a9becebfd47ca8e4,"Advancement in the domain of big data, computing power and internet of things continue to spur the development of algorithmic models that morphed into artificial intelligence. Notable achievements have been made in the application of artificial intelligence in image recognition, natural language processing, smart farming, personal learning assistance, and autonomous systems. As its adoption increases and proliferates into every sphere of activities, governments, businesses and organizations begin to formulate strategies and measures to facilitate its adoption even as it is still rapidly progressing. Meanwhile, artificial intelligence’s impact to the individual, organizational and societal levels as well as the mechanisms to ensure realization of its benefits and minimization of its drawbacks are actively being pursued by the academic communities. This study endeavours to aggregate the perspectives from multiple review studies to shed light on the approaches pertaining to the governance, risk management and compliance of artificial intelligence. The concepts, elements and practices relevant to the three aspects are presented together with the proposed way forward to facilitate artificial intelligence adoption by the organizations.",not included,0.8351606994867324,61
02e8bae4e4a73f4c622b4bde1ae154504564ff93,preprocessed,citation,citation,semantic_scholar,IASDR 2023: Life-Changing Design,2023.0,semantic_scholar,when 'doing ethics' meets public procurement of smart city technology – an amsterdam case study,https://www.semanticscholar.org/paper/02e8bae4e4a73f4c622b4bde1ae154504564ff93,"City governments increasingly experiment with civic participation in the procurement and the realization of smart city technologies in order to improve the incorporation of human values. In this paper, a model is proposed with the level of participation, the continuity of participation and the extent of institutional embedding to illustrate how challenging these experiments are. The City of Amsterdam also experiments with its procurement approach for a new camera car service that ensures an ethically responsible, privacy-friendly and secure collection of images from public space. Two starting points drive this change: 1) in order to have more control over the data, the municipality develops its own machine learning models for processing the images and 2) a multi-stakeholder co-design project – including a citizen panel – is an integral part of the process in which the service is designed and realized. To support this new procurement process, a group of design-researchers were involved in a collaborative case study to identify requirements relevant for the tender. An analysis of the case study findings along the three dimensions brings us to the conclusion that the approach developed by the City of Amsterdam is a fruitful encounter between ‘doing ethics’ and procurement. The lessons of this procurement approach for ‘doing ethics’ are claimed to be of value for other practical contexts and further research.",not included,0.838510024547577,62
e130c5aa440815105edbf14234f4e3aa4d188ec4,preprocessed,citation,citation,semantic_scholar,International Conference on Human Factors in Computing Systems,2023.0,semantic_scholar,"disentangling fairness perceptions in algorithmic decision-making: the effects of explanations, human oversight, and contestability",https://www.semanticscholar.org/paper/e130c5aa440815105edbf14234f4e3aa4d188ec4,"Recent research claims that information cues and system attributes of algorithmic decision-making processes affect decision subjects’ fairness perceptions. However, little is still known about how these factors interact. This paper presents a user study (N = 267) investigating the individual and combined effects of explanations, human oversight, and contestability on informational and procedural fairness perceptions for high- and low-stakes decisions in a loan approval scenario. We find that explanations and contestability contribute to informational and procedural fairness perceptions, respectively, but we find no evidence for an effect of human oversight. Our results further show that both informational and procedural fairness perceptions contribute positively to overall fairness perceptions but we do not find an interaction effect between them. A qualitative analysis exposes tensions between information overload and understanding, human involvement and timely decision-making, and accounting for personal circumstances while maintaining procedural consistency. Our results have important design implications for algorithmic decision-making processes that meet decision subjects’ standards of justice.",not included,0.8116682052612305,63
e65f90e475939c754571aab422cbc24fe01aa3ff,preprocessed,citation,citation,semantic_scholar,Central Asian Journal of Medical Hypotheses and Ethics,2024.0,semantic_scholar,artificial intelligence in writing and research: ethical implications and best practices,https://www.semanticscholar.org/paper/e65f90e475939c754571aab422cbc24fe01aa3ff,"Artificial Intelligence (AI) is a field that utilizes computer technology to imitate, improve, and expand human intelligence. The concept of AI was originally proposed in the mid-twentieth century, and it has evolved into a technology that serves different purposes, ranging from simple automation to complex decision-making processes. AI encompasses Artificial Narrow Intelligence, General Intelligence, and Super Intelligence. AI is transforming data analysis, language checks, and literature reviews in research. In many fields of AI applications, ethical considerations, including plagiarism, bias, privacy, responsibility, and transparency, need precise norms and human oversight. By promoting understanding and adherence to ethical principles, the research community may successfully utilize the advantages of AI while upholding academic accountability and integrity. It takes teamwork from all stakeholders to improve human knowledge and creativity, and ethical AI use in research is essential.",not included,0.8411970287561417,64
df20988ed9bd2c8e0257b52b40150afec05c3d5d,preprocessed,citation,citation,semantic_scholar,Communications of the ACM,2024.0,semantic_scholar,the eu ai act and the wager on trustworthy ai,https://www.semanticscholar.org/paper/df20988ed9bd2c8e0257b52b40150afec05c3d5d,"As the impact of AI is difficult to assess by a single group, policymakers should prioritize societal and environmental well being and seek advice from interdisciplinary groups focusing on ethical aspects, responsibility, and transparency in the development of algorithms.",not included,0.8062968552112579,65
5702623771090f3e7a40be4bd454f461ffb178be,preprocessed,citation,citation,semantic_scholar,Journal of Artificial Intelligence &amp; Cloud Computing,2024.0,semantic_scholar,ai with integrity: the necessity of responsible ai governance,https://www.semanticscholar.org/paper/5702623771090f3e7a40be4bd454f461ffb178be,"Responsible AI Governance has emerged as a critical framework for ensuring the ethical development and deployment of artificial intelligence systems. As AI technologies continue to advance and permeate various sectors of society, the need for robust governance structures becomes increasingly apparent. This document explores the key principles, challenges, and best practices in Responsible AI Governance, highlighting the importance of transparency, accountability, and fairness in AI systems. By examining current initiatives, regulatory landscapes, and industry standards, we aim to provide a comprehensive overview of the strategies organizations can employ to navigate the complex ethical terrain of AI development and implementation.",not included,0.8065193235874176,66
ae529b8b16c6eeb9f5a4dee7f8e39a6d0fa62ae0,preprocessed,citation,citation,semantic_scholar,arXiv.org,2024.0,semantic_scholar,ethical and scalable automation: a governance and compliance framework for business applications,https://www.semanticscholar.org/paper/ae529b8b16c6eeb9f5a4dee7f8e39a6d0fa62ae0,"The popularisation of applying AI in businesses poses significant challenges relating to ethical principles, governance, and legal compliance. Although businesses have embedded AI into their day-to-day processes, they lack a unified approach for mitigating its potential risks. This paper introduces a framework ensuring that AI must be ethical, controllable, viable, and desirable. Balancing these factors ensures the design of a framework that addresses its trade-offs, such as balancing performance against explainability. A successful framework provides practical advice for businesses to meet regulatory requirements in sectors such as finance and healthcare, where it is critical to comply with standards like GPDR and the EU AI Act. Different case studies validate this framework by integrating AI in both academic and practical environments. For instance, large language models are cost-effective alternatives for generating synthetic opinions that emulate attitudes to environmental issues. These case studies demonstrate how having a structured framework could enhance transparency and maintain performance levels as shown from the alignment between synthetic and expected distributions. This alignment is quantified using metrics like Chi-test scores, normalized mutual information, and Jaccard indexes. Future research should explore the framework's empirical validation in diverse industrial settings further, ensuring the model's scalability and adaptability.",included,0.826026663184166,67
3fe403df31a0dbd950a4e94a1913f2eeb89d9de9,preprocessed,citation,citation,semantic_scholar,IJIE (Indonesian Journal of Informatics Education),2024.0,semantic_scholar,navigating the grey area: students' ethical dilemmas in using ai tools for coding assignments,https://www.semanticscholar.org/paper/3fe403df31a0dbd950a4e94a1913f2eeb89d9de9,"Integrating artificial intelligence (AI) in higher education, particularly in coding assignments for Information Technology (IT) students, represents a rapidly evolving research area with significant implications for academic practices and integrity. This study focuses on the ethical challenges faced by IT students when using AI tools like ChatGPT for coding assignments. Despite the growing use of AI in education, there is a notable gap in understanding how students perceive and navigate the ethical dilemmas associated with these technologies. To address this gap, this study employed a thematic analysis of qualitative data collected from interviews with IT students. The results reveal a complex landscape of ethical considerations, including issues of originality, academic integrity, and the potential for misuse of AI tools. Students reported challenges in balancing the benefits of AI assistance with the need to maintain independent learning and adhere to ethical standards. The implications of this research are significant for educators, institutions, and policymakers. Understanding the ethical challenges students face can inform the development of more effective teaching strategies, assessment methods, and institutional policies. This study contributes to the ongoing dialogue about AI ethics in academia, providing valuable insights for creating an educational environment that leverages the power of AI while upholding the principles of academic integrity and meaningful learning.",not included,0.8288842886686325,68
b2ad5b47615e6a1e7f33f9f836fe21503bba57e0,preprocessed,citation,citation,semantic_scholar,arXiv.org,2024.0,semantic_scholar,"a collaborative, human-centred taxonomy of ai, algorithmic, and automation harms",https://www.semanticscholar.org/paper/b2ad5b47615e6a1e7f33f9f836fe21503bba57e0,"This paper introduces a collaborative, human-centred taxonomy of AI, algorithmic and automation harms. We argue that existing taxonomies, while valuable, can be narrow, unclear, typically cater to practitioners and government, and often overlook the needs of the wider public. Drawing on existing taxonomies and a large repository of documented incidents, we propose a taxonomy that is clear and understandable to a broad set of audiences, as well as being flexible, extensible, and interoperable. Through iterative refinement with topic experts and crowdsourced annotation testing, we propose a taxonomy that can serve as a powerful tool for civil society organisations, educators, policymakers, product teams and the general public. By fostering a greater understanding of the real-world harms of AI and related technologies, we aim to increase understanding, empower NGOs and individuals to identify and report violations, inform policy discussions, and encourage responsible technology development and deployment.",not included,0.8351606994867324,69
15b2147eec0a45dcd88ac9e2731dd8ab81597d79,preprocessed,citation,citation,semantic_scholar,Indonesian Journal of Computer Science,2024.0,semantic_scholar,transformation of students' career orientation in the era of artificial intelligence: a systematic literature review,https://www.semanticscholar.org/paper/15b2147eec0a45dcd88ac9e2731dd8ab81597d79,"This research revolves around the challenges faced by students in aligning their career orientation with demands and changes, as many traditional jobs are threatened by AI technology. The aim of this study is to identify trends in the transformation of students' career orientation in the era of Artificial Intelligence (AI), map the career challenges for students in the AI era, analyze the skills and competencies required, and assess the role of educational institutions in supporting this career transformation. The research method applied in this study is a systematic literature review. The initial stages involve collecting literature sources from scholarly databases and proceeding with a screening process to select literature relevant to the research focus. Finally, in-depth analysis of selected literature is conducted to identify patterns, trends, and key points related to the research topic. The results of the study describe that the development of AI technology has a significant impact on students' career orientation in higher education. Furthermore, students also face career challenges such as competition with technology, uncertainty about future employment, and skills gaps. To address these challenges, students need to develop technical AI skills, ethical AI understanding, problem-solving abilities, and continuous learning skills to succeed in the AI job market. On the other hand, higher education institutions should play a proactive role in addressing these challenges by developing relevant curricula, organizing training sessions, collaborating with industries, and enhancing AI learning facilities. Further research is suggested to focus on the implementation of these strategies in the specific context of higher education institutions and evaluate their impact on student career preparedness in the evolving AI era.",not included,0.8206197261810303,70
fa1e161cb53445cc5266427a75b5a3f558c522ed,preprocessed,citation,citation,semantic_scholar,Education sciences,2024.0,semantic_scholar,understanding researchers’ ai readiness in a higher education context: q methodology research,https://www.semanticscholar.org/paper/fa1e161cb53445cc5266427a75b5a3f558c522ed,"Taking a human-centered socio-cultural perspective, this study explored the manifold individual and structural processes that contribute to researchers’ AI readiness. Forty-three graduate students and faculty at one university in Qatar took part in this Q methodology study. The results represented participants’ collective perspectives on what they considered relevant to their AI readiness. A 5 + 1-factor solution was accepted, illustrating diverse perspectives and no consensus. The factors were termed based on their main foci, as follows, (F-1) how technical skills are acquired, (F-2) when it is all about ethics, (F-3) when technical skills meet ethical considerations, (F-4a and F-4b) when opposites concede, and (F-5) how collaborations reflect AI readiness. The results revealed the diversity of viewpoints among participants, and the interrelations among some factors. This study recommended a holistic approach to enhance AI readiness. It suggested integrating targeted educational initiatives and developing localized ethical frameworks to promote responsible AI use across various research disciplines.",not included,0.838510024547577,71
dc0b8bde0efafffb609ea7da8bd9799688b050c2,preprocessed,citation,citation,semantic_scholar,arXiv.org,2024.0,semantic_scholar,ehazop: a proof of concept ethical hazard analysis of an assistive robot,https://www.semanticscholar.org/paper/dc0b8bde0efafffb609ea7da8bd9799688b050c2,"The use of assistive robots in domestic environments can raise significant ethical concerns, from the risk of individual ethical harm to wider societal ethical impacts including culture flattening and compromise of human dignity. It is therefore essential to ensure that technological development of these robots is informed by robust and inclusive techniques for mitigating ethical concerns. This paper presents EHAZOP, a method for conducting an ethical hazard analysis on an assistive robot. EHAZOP draws upon collaborative, creative and structured processes originating within safety engineering, using these to identify ethical concerns associated with the operation of a given assistive robot. We present the results of a proof of concept study of EHAZOP, demonstrating the potential for this process to identify diverse ethical hazards in these systems.",included,0.8116682052612305,72
083a4a960fcf317d9639c8b7bc9140c92b63c872,preprocessed,citation,citation,semantic_scholar,International Journal of Science and Research (IJSR),2024.0,semantic_scholar,harnessing the power of ai in banking,https://www.semanticscholar.org/paper/083a4a960fcf317d9639c8b7bc9140c92b63c872,": Modern banking, accompanied by rapid technological development, is evolving towards the comprehensive implementation of Artificial Intelligence. Throughout this article, the ongoing integration of AI into most banking procedures will be discussed, concentrating on the implementation of AI to enhance the optimal customer experience, affordable data privacy, effective bank performance, and adherence to all existing rules and restrictions. Contrary to widespread belief, AI is no longer limited to the automation of routine activities: it has the potential to conduct cybersecurity, risk evaluation, and real-time transaction oversight. Not only do testing methodologies evolve through automated test creation and automation of intelligent data integrity, but even security measures and regulation are bolstered, minimizing human negligence and overheads. Developments in the work of banks imply a further utilization of AI in the creation of smart systems that enhance risk assessment, customer relationships, and court proceedings, according to the survey. This research examines current banking trends and provides recommendations for utilizing AI to improve cost savings and protection. It argues that shortly, AI will be a key focus of banking industry innovation and advancement.",not included,0.8252739846706391,73
bd09bc4fe7eb614a9dc161dc901e9be8b88cea77,preprocessed,citation,citation,semantic_scholar,Journal of Electrical Systems,2024.0,semantic_scholar,roadmap to smart cities in saudi arabia: key challenges and opportunities,https://www.semanticscholar.org/paper/bd09bc4fe7eb614a9dc161dc901e9be8b88cea77,"Technologies are constantly evolving and finding applications in both business and government organizations for various purposes such as improving operational efficiency, increasing customer satisfaction, and reducing the cost of transactions. Technology adoption is a well-researched phenomenon in the academic world. Based on the current state of e-governance in Saudi cities and the potential of Artificial Intelligence (AI), Internet of Things and Blockchain in e-governance, this research study aims to study factors influencing the adoption intention of AI and Blockchain technologies in citizen services delivery within a smart city in Saudi. This article explores AI, IoT and Blockchain technologies adoption intention in the context of smart cities within a developing country like Saudi. Finally, the study provides a set of practical recommendations for smart city officials while planning to launch service applications using AI and Blockchain technologies. More specifically, the study emphasizes the importance of ethics, data privacy and security, and transparency in building trust between smart city systems.",not included,0.819400179386139,74
5980ba3bd677427b63bde13e9f6902c9678e378e,preprocessed,citation,citation,semantic_scholar,"Conference on Fairness, Accountability and Transparency",2024.0,semantic_scholar,auditing work: exploring the new york city algorithmic bias audit regime,https://www.semanticscholar.org/paper/5980ba3bd677427b63bde13e9f6902c9678e378e,"In July 2023, New York City (NYC) implemented the first attempt to create an algorithm auditing regime for commercial machine-learning systems. Local Law 144 (LL 144), requires NYC-based employers using automated employment decision-making tools (AEDTs) in hiring to be subject to annual bias audits by an independent auditor. In this paper, we analyse what lessons can be learned from LL 144 for other national attempts to create algorithm auditing regimes. Using qualitative interviews with 17 experts and practitioners working within the regime, we find LL 144 has failed to create an effective auditing regime: the law fails to clearly define key aspects like AEDTs and what constitutes an independent auditor, leaving auditors, vendors who create AEDTs, and companies using AEDTs to define the law’s practical implementation in ways that failed to protect job applicants. Several factors contribute to this: first, the law was premised on a faulty transparency-driven theory of change that fails to stop biased AEDTs from being used by employers. Second, industry lobbying led to the definition of what constitutes an AEDT being narrowed to the point where most companies considered their tools exempt. Third, we find auditors face enormous practical and cultural challenges gaining access to data from employers and vendors building these tools. Fourth, we find wide disagreement over what constitutes a legitimate auditor and identify four different kinds of ‘auditor roles’ that serve different functions and offer different kinds of services. We conclude with four recommendations for policymakers seeking to create similar bias auditing regimes that use clearer definitions and metrics and more accountability. By exploring LL 144 through the lens of auditors, our paper advances the evidence base around audit as an accountability mechanism, and can provide guidance for policymakers seeking to create similar regimes.",not included,0.8294077664613724,75
8b4124dcae7e6c75a0965d82e7a042d70e5c108c,preprocessed,citation,citation,semantic_scholar,JMIR Medical Education,2023.0,semantic_scholar,proposing a principle-based approach for teaching ai ethics in medical education,https://www.semanticscholar.org/paper/8b4124dcae7e6c75a0965d82e7a042d70e5c108c,"The use of artificial intelligence (AI) in medicine, potentially leading to substantial advancements such as improved diagnostics, has been of increasing scientific and societal interest in recent years. However, the use of AI raises new ethical challenges, such as an increased risk of bias and potential discrimination against patients, as well as misdiagnoses potentially leading to over- or underdiagnosis with substantial consequences for patients. Recognizing these challenges, current research underscores the importance of integrating AI ethics into medical education. This viewpoint paper aims to introduce a comprehensive set of ethical principles for teaching AI ethics in medical education. This dynamic and principle-based approach is designed to be adaptive and comprehensive, addressing not only the current but also emerging ethical challenges associated with the use of AI in medicine. This study conducts a theoretical analysis of the current academic discourse on AI ethics in medical education, identifying potential gaps and limitations. The inherent interconnectivity and interdisciplinary nature of these anticipated challenges are illustrated through a focused discussion on “informed consent” in the context of AI in medicine and medical education. This paper proposes a principle-based approach to AI ethics education, building on the 4 principles of medical ethics—autonomy, beneficence, nonmaleficence, and justice—and extending them by integrating 3 public health ethics principles—efficiency, common good orientation, and proportionality. The principle-based approach to teaching AI ethics in medical education proposed in this study offers a foundational framework for addressing the anticipated ethical challenges of using AI in medicine, recommended in the current academic discourse. By incorporating the 3 principles of public health ethics, this principle-based approach ensures that medical ethics education remains relevant and responsive to the dynamic landscape of AI integration in medicine. As the advancement of AI technologies in medicine is expected to increase, medical ethics education must adapt and evolve accordingly. The proposed principle-based approach for teaching AI ethics in medical education provides an important foundation to ensure that future medical professionals are not only aware of the ethical dimensions of AI in medicine but also equipped to make informed ethical decisions in their practice. Future research is required to develop problem-based and competency-oriented learning objectives and educational content for the proposed principle-based approach to teaching AI ethics in medical education.",not included,0.8252925395965576,76
c1c851389073ff6d6030e53e36cdf1dff63609d5,preprocessed,citation,citation,semantic_scholar,Journal of Digital Economy Research,2023.0,semantic_scholar,social-economic aspects of the implementation of natural intelligence technologies: part 2 - natural intelligence in government,https://www.semanticscholar.org/paper/c1c851389073ff6d6030e53e36cdf1dff63609d5,"The purpose of this article is to investigate the application of artificial intelligence (AI) in public administration. Following up the research of M.V. Fedorov [38- 41], which provides an overview of the global effects of AI implementation, covering socio-ethical aspects, economic impact and regulatory framework for sustainable development strategies, the present paper focuses on the key factors that define the framework for the use of AI in public administration. The paper considers AI as part of the overall process of technological development and explores the links between AI and other areas such as computing technology and data collection techniques. Particular attention is paid to the analysis of international and Russian experience of implementing AI in public administration. The authors seek to develop recommendations for further development of this industry based on the experience gained. They also consider approaches that may lead to the development of strategic principles focused on long-term predictions of the effects of AI in optimising public administration, and the subsequent implementation of appropriate regulatory practices. Thus, this article seeks to provide an overview and analysis of the main aspects of the use of AI in public administration with a focus on international and Russian experience, and to offer recommendations for the further development of this field.",included,0.8133152216672898,77
1c12ca7f5f10181df25db869e04c9666fed69bca,preprocessed,citation,citation,semantic_scholar,Information and Computer Security,2023.0,semantic_scholar,european artificial intelligence act: an ai security approach,https://www.semanticscholar.org/paper/1c12ca7f5f10181df25db869e04c9666fed69bca,"
Purpose
The purpose of this paper is to highlight the key technical challenges that derive from the recently proposed European Artificial Intelligence Act and specifically, to investigate the applicability of the requirements that the AI Act mandates to high-risk AI systems from the perspective of AI security.


Design/methodology/approach
This paper presents the main points of the proposed AI Act, with emphasis on the compliance requirements of high-risk systems. It matches known AI security threats with the relevant technical requirements, it demonstrates the impact that these security threats can have to the AI Act technical requirements and evaluates the applicability of these requirements based on the effectiveness of the existing security protection measures. Finally, the paper highlights the necessity for an integrated framework for AI system evaluation.


Findings
The findings of the EU AI Act technical assessment highlight the gap between the proposed requirements and the available AI security countermeasures as well as the necessity for an AI security evaluation framework.


Originality/value
AI Act, high-risk AI systems, security threats, security countermeasures.
",included,0.8154128462076187,78
0ef63efe100b39c691a7db6b77b16d3f3f72c267,preprocessed,citation,citation,semantic_scholar,Global Business and Organizational Excellence,2023.0,semantic_scholar,mitigating the adverse effects of ai with the european union's artificial intelligence act: hype or hope?,https://www.semanticscholar.org/paper/0ef63efe100b39c691a7db6b77b16d3f3f72c267,"In light of the rise of generative AI and recent debates about the socio‐political implications of large‐language models, chatbots, and the like, this paper analyzes the E.U.’s Artificial Intelligence Act (AIA), the world's first comprehensive attempt by a government body to address and mitigate the potentially negative impacts of AI technologies. The paper critically analyzes the AIA from a business and computer ethics point of view—a perspective currently lacking in the academic (e.g., GBOE‐related) literature. It evaluates, in particular, the AIA's strengths and weaknesses and proposes reform measures that could help to strengthen the AIA. Among the AIA's strengths are its legally binding character, extra‐territoriality, ability to address data quality and discrimination risks, and institutional innovations such as the AI Board and publicly accessible logs and database for AI systems. Among its main weaknesses are its lack of effective enforcement, oversight, and control, absence of procedural rights and remedy mechanisms, inadequate worker protection, institutional ambiguities, insufficient funding and staffing, and inadequate consideration of sustainability issues. Reform suggestions include establishing independent conformity assessment procedures, strengthening democratic accountability and judicial oversight, introducing redress and complaint mechanisms, ensuring the participation and inclusion of workers, guaranteeing political independence of the AI Board, providing enhanced funding and staffing of market surveillance authorities, and mandating “green AI.”",not included,0.8546511858701706,79
81d7522fe631da5299064e887faff1b94e00b036,preprocessed,citation,citation,semantic_scholar,2023 3rd International Conference on Technological Advancements in Computational Sciences (ICTACS),2023.0,semantic_scholar,disease prediction models using machine learning algorithms,https://www.semanticscholar.org/paper/81d7522fe631da5299064e887faff1b94e00b036,"Disease Prediction Models hold paramount importance in healthcare for enabling early intervention and improving patient outcomes. Machine Learning Algorithms, with their predictive capabilities, offer promising avenues for enhancing disease prediction accuracy. In this research, we explore the application of various machine learning techniques for disease prediction. We begin by introducing the context of disease prediction and the significance of machine learning algorithms. Our study involves preprocessing data, selecting relevant features, and implementing diverse algorithms. Through rigorous experimentation and evaluation, we present comparative insights into algorithm performance. The outcomes underscore the potential of machine learning in revolutionizing disease diagnosis and prognosis. Ethical considerations and potential biases are discussed, highlighting the responsible integration of predictive models in healthcare practices. This research contributes to the growing body of knowledge, paving the way for future advancements in disease prediction and personalized medical interventions.",not included,0.8554961800575256,80
a9b5c98d96cb77b4cdc8a2ee833288f205527795,preprocessed,citation,citation,semantic_scholar,Management &amp; Avenir,2023.0,semantic_scholar,développement de l’ia et questions éthiques : passage d’une perspective statique à une perspective dynamique,https://www.semanticscholar.org/paper/a9b5c98d96cb77b4cdc8a2ee833288f205527795,"L’utilisation éthique de l’IA dans les organisations pose de nombreux défis qui nécessitent que les parties prenantes débattent et expriment les valeurs relatives à leur position éthique. Ces débats donnent lieu à des tensions entre l’idéal de délibération inclusive issu de l’éthique du discours de Habermas, et la conception bourdieusienne de débats se déroulant dans des champs structurés avec des relations de pouvoir entre des parties prenantes concurrentes et inégales. En analysant 137 documents provenant de 21 pays, nous avons pu cartographier les facteurs influençant la prise en compte des questions d’éthique commerciale qui se posent au cours des deux principales étapes du développement d’un système d’IA, à savoir l’étape de sa conception et l’étape de son utilisation. Ensuite, nous élaborons trois dispositifs techniques pour résoudre les questions éthiques et les discutons à la lumière de la tension entre les conceptions idéales et pratiques des débats requis pour une utilisation éthique de l’IA.",not included,0.8145611852407455,81
ec3f19633a9cbf913ae9f418c80d59857e966afb,preprocessed,citation,citation,semantic_scholar,medRxiv,2023.0,semantic_scholar,artificial intelligence and health equity in primary care: a qualitative study with key stakeholders,https://www.semanticscholar.org/paper/ec3f19633a9cbf913ae9f418c80d59857e966afb,"Artificial Intelligence (AI)-augmented interventions are currently being rolled out across primary care, but how it affects health equity remains insufficiently understood. This qualitative study addresses this gap through an ethnographical inquiry based on 32 interviews and focus groups with stakeholders including commissioners, decision makers, AI developers, researchers, GPs and patient groups involved in the implementation of AI in English primary care. We took a sociotechnical perspective in order to assess how the stakeholders can improve health equity through the implementation process of AI within the wider system. We found that regulation and policy alone cannot guarantee equitable implementation of AI but can provide a framework to enable other stakeholders to take measures to promote equity: fostering a shared understanding of the causal mechanisms of AI and health equity, how to measure health equity, and how to share data necessary for equity promotion. Further, all stakeholders need to be on board for equitable implementation, and currently innovation leaves clinicians and patients behind. Capacity building is needed to achieve this, in particular at local commissioning and clinician level. Careful implementation and pragmatically focused research are needed to make AI in primary care capable of advancing health equity.",not included,0.8303640842437744,82
8b28e758be31ac18afd138cb5d6cb0709e45e655,preprocessed,citation,citation,semantic_scholar,Perspectives on Medical Education,2023.0,semantic_scholar,teaching ai ethics in medical education: a scoping review of current literature and practices,https://www.semanticscholar.org/paper/8b28e758be31ac18afd138cb5d6cb0709e45e655,"Introduction: The increasing use of Artificial Intelligence (AI) in medicine has raised ethical concerns, such as patient autonomy, bias, and transparency. Recent studies suggest a need for teaching AI ethics as part of medical curricula. This scoping review aimed to represent and synthesize the literature on teaching AI ethics as part of medical education. Methods: The PRISMA-SCR guidelines and JBI methodology guided a literature search in four databases (PubMed, Embase, Scopus, and Web of Science) for the past 22 years (2000–2022). To account for the release of AI-based chat applications, such as ChatGPT, the literature search was updated to include publications until the end of June 2023. Results: 1384 publications were originally identified and, after screening titles and abstracts, the full text of 87 publications was assessed. Following the assessment of the full text, 10 publications were included for further analysis. The updated literature search identified two additional relevant publications from 2023 were identified and included in the analysis. All 12 publications recommended teaching AI ethics in medical curricula due to the potential implications of AI in medicine. Anticipated ethical challenges such as bias were identified as the recommended basis for teaching content in addition to basic principles of medical ethics. Case-based teaching using real-world examples in interactive seminars and small groups was recommended as a teaching modality. Conclusion: This scoping review reveals a scarcity of literature on teaching AI ethics in medical education, with most of the available literature being recent and theoretical. These findings emphasize the importance of more empirical studies and foundational definitions of AI ethics to guide the development of teaching content and modalities. Recognizing AI’s significant impact of AI on medicine, additional research on the teaching of AI ethics in medical education is needed to best prepare medical students for future ethical challenges.",not included,0.828378900885582,83
8ff4eda4fbde5531cbc01cb2b8d1bae7aa97c265,preprocessed,citation,citation,semantic_scholar,Applied Informatics,2023.0,semantic_scholar,anthropocentrism and environmental wellbeing in ai ethics standards: a scoping review and discussion,https://www.semanticscholar.org/paper/8ff4eda4fbde5531cbc01cb2b8d1bae7aa97c265,"As AI deployment has broadened, so too has an awareness for the ethical implications and problems that may ensue from this deployment. In response, groups across multiple domains have issued AI ethics standards that rely on vague, high-level principles to find consensus. One such high-level principle that is common across the AI landscape is ‘human-centredness’, though oftentimes it is applied without due investigation into its merits and limitations and without a clear, common definition. This paper undertakes a scoping review of AI ethics standards to examine the commitment to ‘human-centredness’ and how this commitment interacts with other ethical concerns, namely, concerns for nonhumans animals and environmental wellbeing. We found that human-centred AI ethics standards tend to prioritise humans over nonhumans more so than nonhuman-centred standards. A critical analysis of our findings suggests that a commitment to human-centredness within AI ethics standards accords with the definition of anthropocentrism in moral philosophy: that humans have, at least, more intrinsic moral value than nonhumans. We consider some of the limitations of anthropocentric AI ethics, which include permitting harm to the environment and animals and undermining the stability of ecosystems.",not included,0.8474408477544785,84
17aedaff08b4fd6f269c838ed8d7cb6698ea5f01,preprocessed,citation,citation,semantic_scholar,Workshop on Visual Analytics in Healthcare,2023.0,semantic_scholar,the iterative design process of an explainable ai application for non-invasive diagnosis of cns tumors: a user-centered approach,https://www.semanticscholar.org/paper/17aedaff08b4fd6f269c838ed8d7cb6698ea5f01,"Artificial Intelligence (AI) is well-suited to help support complex decision-making tasks within clinical medicine, including clinical imaging applications like radiographic differential diagnosis of central nervous system (CNS) tumors. So far, there have been numerous examples of theoretical AI solutions for this space, for example, large-scale corporate efforts like IBM’s Watson AI. However, clinical implementation remains limited due to factors related to the alignment of this technology in the clinical setting. User-Centered Design (UCD) is a design philosophy that focuses on developing tailored solutions for specific users or user groups. In this study, we applied UCD to develop an explainable AI tool to support clinicians in our use case. Through four design iterations, starting from basic functionality and visualizations, we progressed to functional prototypes in a realistic testing environment. We discuss our motivation and approach for each iteration, along with key insights gained. This UCD process has advanced our conceptual idea from feasibility testing to interactive functional AI interfaces designed for specific clinical and cognitive tasks. It has also provided us with directions to develop further an AI system for the non-invasive diagnosis of CNS tumors.",included,0.8040409088134766,85
7d36034e81b7952bbbb1bdb63149df7a06fa709c,preprocessed,citation,citation,semantic_scholar,Neural Information Processing Systems,2023.0,semantic_scholar,how to data in datathons,https://www.semanticscholar.org/paper/7d36034e81b7952bbbb1bdb63149df7a06fa709c,"The rise of datathons, also known as data or data science hackathons, has provided a platform to collaborate, learn, and innovate in a short timeframe. Despite their significant potential benefits, organizations often struggle to effectively work with data due to a lack of clear guidelines and best practices for potential issues that might arise. Drawing on our own experiences and insights from organizing>80 datathon challenges with>60 partnership organizations since 2016, we provide guidelines and recommendations that serve as a resource for organizers to navigate the data-related complexities of datathons. We apply our proposed framework to 10 case studies.",not included,0.832381296157837,86
8b34eee10e3ec5c65ca7e1f5776d1339dded829e,preprocessed,citation,citation,semantic_scholar,Frontiers in Medicine,2023.0,semantic_scholar,doctor-patient interactions in the age of ai: navigating innovation and expertise,https://www.semanticscholar.org/paper/8b34eee10e3ec5c65ca7e1f5776d1339dded829e,"The integration of artificial intelligence (AI) in healthcare has the capacity to transform medical practice. Despite its revolutionary potential, the influence of AI may affect the physician-patient interaction and presents ethical challenges that will need to be carefully considered. This article discusses how patients may interact with this technology, considers how emerging technologies may alter the dynamics of the physician-patient relationship, and reviews some of the limitations that continue to exist. We identify potential challenges that may arise with the integration of AI into medical settings and propose solutions to help mitigate these issues.",not included,0.8262364834547042,87
38c4cfc9dc53f18e57c9c185edac3c085653d628,preprocessed,citation,citation,semantic_scholar,IEEE Transactions on Artificial Intelligence,2023.0,semantic_scholar,the different faces of ai ethics across the world: a principle-to-practice gap analysis,https://www.semanticscholar.org/paper/38c4cfc9dc53f18e57c9c185edac3c085653d628,"Artificial Intelligence (AI) is transforming our daily life with many applications in healthcare, space exploration, banking, and finance. This rapid progress in AI has brought increasing attention to the potential impacts of AI technologies on society, with ethically questionable consequences. In recent years, several ethical principles have been released by governments, national organizations, and international organizations. These principles outline high-level precepts to guide the ethical development, deployment, and governance of AI. However, the abstract nature, diversity, and context-dependence of these principles make them difficult to implement and operationalize, resulting in gaps between principles and their execution. Most recent work analyzed and summarized existing AI principles and guidelines but did not provide findings on principle-to-practice gaps nor how to mitigate them. These findings are particularly important to ensure that AI practical guidances are aligned with ethical principles and values. In this article, we provide a contextual and global evaluation of current ethical AI principles for all continents, with the aim to identify potential principle characteristics tailored to specific countries or applicable across countries. Next, we analyze the current level of AI readiness and current practical guidances of ethical AI principles in different countries, to identify gaps in the practical guidance of AI principles and their causes. Finally, we propose recommendations to mitigate the principle-to-practice gaps.",not included,0.8531478404998779,88
9b8b78b5bee1fed9dd04a00a9e89acb93c64fbba,preprocessed,citation,citation,semantic_scholar,Conference on Designing Interactive Systems,2023.0,semantic_scholar,participatory noticing through photovoice: engaging arts- and community-based approaches in design research,https://www.semanticscholar.org/paper/9b8b78b5bee1fed9dd04a00a9e89acb93c64fbba,"Noticing differently commits to stepping out of familiar reference frameworks while attending to oft-neglected actors, relations, and ways of knowing for design. Photovoice is an arts- and community-based participatory approach allowing individuals to communicate their lives and stories about pressing community concerns through photography. This paper bridges photovoice and the commitment to noticing in HCI and design through a photovoice project with Detroit residents on safety and surveillance. The photovoice process—alongside the production, reflection, and dissemination of photographs—makes residents’ everyday situations legible and sensible, allowing both community members and researchers to orient to and engage with multiple viewpoints, sensibilities, and temporal trajectories. This process confronts the invisibility of both the sociotechnical infrastructures (in our case, surveillance infrastructures) and minoritized communities’ relational ontologies. By advocating participatory noticing in design research, we show the opportunities for adopting arts- and community-based participatory approaches in decentering dominant ways of knowing and seeing, while at the same time fostering community capacity and relations for future potentialities.",not included,0.8011098295450211,89
b7b7f4331639d708d7881267101f54d92eefcf99,preprocessed,citation,citation,semantic_scholar,2023 5th International Conference of the Portuguese Society for Engineering Education (CISPEE),2023.0,semantic_scholar,the importance of ethical reasoning in next generation tech education,https://www.semanticscholar.org/paper/b7b7f4331639d708d7881267101f54d92eefcf99,"Artificial intelligence (AI) is having a profound impact on human life, with both benefits and drawbacks in the societal, environmental, and technological realms. However, the ethical implications of AI are often not addressed in technology education, leaving future professionals with a lack of awareness in this area. This is concerning, as AI has the potential to greatly information delivery and affect human thinking, interaction, decision-making, and communication. To address these issues, there is a need for a framework to guide and help future AI developers make ethically responsible decisions. In this paper we propose a framework to foster ethical awareness and promote respect for human dignity and well-being, while also preventing harm. It is designed to be incorporated into technology education, ensuring that future professionals are equipped to navigate the ethical implications of AI. By prioritizing ethical reasoning in technology education, we can build a better and more responsible AI industry, ensuring that AI can provide benefits for society and does not cause harm. Additionally, a tech industry that values ethics and social responsibility will be better equipped to build technology that serves the public interest, rather than solely maximizing profits. Teaching ethical reasoning in technology education is a crucial step in preparing future professionals to make informed and ethical decisions in the development and use of AI systems. It will lead to a better and more responsible AI industry that benefits all of society.",included,0.8079065144062042,90
40d478b580827c4ef054ed85d31c5ffd6a0a3590,preprocessed,citation,citation,semantic_scholar,Studies in Christian Ethics,2023.0,semantic_scholar,proximate and ultimate concerns in christian ethical responses to artificial intelligence,https://www.semanticscholar.org/paper/40d478b580827c4ef054ed85d31c5ffd6a0a3590,"I argue here that Christian ethical responses to Artificial Intelligence (AI) ought to take on, largely, two different approaches. The first considers proximate ethical concerns related to AI. This ethical approach most often considers more immediate personal and socio-political repercussions and the kind of impact that is occurring now or in the very near future. Proximate ethics of this type includes discussion about fairness, accountability, sustainability and transparency. The second concerns ultimate ethics which focuses on the longer-term impact and implications of AI. Examples of this type might include issues of uniqueness, deep societal transformation and inequality, changes to personal character and even the role AI might have in God's ultimate economy of creation and grace. My contention is that the Christian church needs to attend to both approaches to AI and that when it focuses too myopically on one at the expense of the other it often eclipses the entire witness of the church in our technological society.",not included,0.8023776173591614,91
35ad707180a312ada17aaea15ea59169d1fd876d,preprocessed,citation,citation,semantic_scholar,2023 3rd International Conference on Technology Enhanced Learning in Higher Education (TELE),2023.0,semantic_scholar,ethical problems of applying artificial intelligence: medical intelligent systems and autonomous vehicles,https://www.semanticscholar.org/paper/35ad707180a312ada17aaea15ea59169d1fd876d,"It is impossible to imagine modern life without intelligent systems that facilitate daily routine actions and save time. But how ethical does it become to use them in terms of improving the quality of their work? The paper analyzes the development of artificial intelligence methods, particularly in healthcare and unmanned transportation, and provides a discussion of the ethical aspects of the use of such systems.",not included,0.8071766257286072,92
9d7e76d1132eb4078d2bd1e786aee69a00419db5,preprocessed,citation,citation,semantic_scholar,arXiv.org,2023.0,semantic_scholar,aha!: facilitating ai impact assessment by generating examples of harms,https://www.semanticscholar.org/paper/9d7e76d1132eb4078d2bd1e786aee69a00419db5,"While demands for change and accountability for harmful AI consequences mount, foreseeing the downstream effects of deploying AI systems remains a challenging task. We developed AHA! (Anticipating Harms of AI), a generative framework to assist AI practitioners and decision-makers in anticipating potential harms and unintended consequences of AI systems prior to development or deployment. Given an AI deployment scenario, AHA! generates descriptions of possible harms for different stakeholders. To do so, AHA! systematically considers the interplay between common problematic AI behaviors as well as their potential impacts on different stakeholders, and narrates these conditions through vignettes. These vignettes are then filled in with descriptions of possible harms by prompting crowd workers and large language models. By examining 4113 harms surfaced by AHA! for five different AI deployment scenarios, we found that AHA! generates meaningful examples of harms, with different problematic AI behaviors resulting in different types of harms. Prompting both crowds and a large language model with the vignettes resulted in more diverse examples of harms than those generated by either the crowd or the model alone. To gauge AHA!'s potential practical utility, we also conducted semi-structured interviews with responsible AI professionals (N=9). Participants found AHA!'s systematic approach to surfacing harms important for ethical reflection and discovered meaningful stakeholders and harms they believed they would not have thought of otherwise. Participants, however, differed in their opinions about whether AHA! should be used upfront or as a secondary-check and noted that AHA! may shift harm anticipation from an ideation problem to a potentially demanding review problem. Drawing on our results, we discuss design implications of building tools to help practitioners envision possible harms.",not included,0.8500882655382156,93
c67f9c6a31bb7087a86e5d58c87cea13ce1eef52,preprocessed,citation,citation,semantic_scholar,Gut,2023.0,semantic_scholar,screening of normal endoscopic large bowel biopsies with interpretable graph learning: a retrospective study,https://www.semanticscholar.org/paper/c67f9c6a31bb7087a86e5d58c87cea13ce1eef52,"Objective To develop an interpretable artificial intelligence algorithm to rule out normal large bowel endoscopic biopsies, saving pathologist resources and helping with early diagnosis. Design A graph neural network was developed incorporating pathologist domain knowledge to classify 6591 whole-slides images (WSIs) of endoscopic large bowel biopsies from 3291 patients (approximately 54% female, 46% male) as normal or abnormal (non-neoplastic and neoplastic) using clinically driven interpretable features. One UK National Health Service (NHS) site was used for model training and internal validation. External validation was conducted on data from two other NHS sites and one Portuguese site. Results Model training and internal validation were performed on 5054 WSIs of 2080 patients resulting in an area under the curve-receiver operating characteristic (AUC-ROC) of 0.98 (SD=0.004) and AUC-precision-recall (PR) of 0.98 (SD=0.003). The performance of the model, named Interpretable Gland-Graphs using a Neural Aggregator (IGUANA), was consistent in testing over 1537 WSIs of 1211 patients from three independent external datasets with mean AUC-ROC=0.97 (SD=0.007) and AUC-PR=0.97 (SD=0.005). At a high sensitivity threshold of 99%, the proposed model can reduce the number of normal slides to be reviewed by a pathologist by approximately 55%. IGUANA also provides an explainable output highlighting potential abnormalities in a WSI in the form of a heatmap as well as numerical values associating the model prediction with various histological features. Conclusion The model achieved consistently high accuracy showing its potential in optimising increasingly scarce pathologist resources. Explainable predictions can guide pathologists in their diagnostic decision-making and help boost their confidence in the algorithm, paving the way for its future clinical adoption.",not included,0.8215756505727768,94
16d83e930a4dab2d49f5d276838ddce79df3f787,preprocessed,citation,citation,semantic_scholar,First Monday,2023.0,semantic_scholar,should chatgpt be biased? challenges and risks of bias in large language models,https://www.semanticscholar.org/paper/16d83e930a4dab2d49f5d276838ddce79df3f787,"As generative language models, exemplified by ChatGPT, continue to advance in their capabilities, the spotlight on biases inherent in these models intensifies. This paper delves into the distinctive challenges and risks associated with biases specifically in large-scale language models. We explore the origins of biases, stemming from factors such as training data, model specifications, algorithmic constraints, product design, and policy decisions. Our examination extends to the ethical implications arising from the unintended consequences of biased model outputs. In addition, we analyze the intricacies of mitigating biases, acknowledging the inevitable persistence of some biases, and consider the consequences of deploying these models across diverse applications, including virtual assistants, content generation, and chatbots. Finally, we provide an overview of current approaches for identifying, quantifying, and mitigating biases in language models, underscoring the need for a collaborative, multidisciplinary effort to craft AI systems that embody equity, transparency, and responsibility. This article aims to catalyze a thoughtful discourse within the AI community, prompting researchers and developers to consider the unique role of biases in the domain of generative language models and the ongoing quest for ethical AI.",not included,0.8053445428609848,95
01906e795d47e34814a2ffe139beb8c5e81b685f,preprocessed,citation,citation,semantic_scholar,arXiv.org,2023.0,semantic_scholar,the representational status of deep learning models,https://www.semanticscholar.org/paper/01906e795d47e34814a2ffe139beb8c5e81b685f,"This paper aims to clarify the representational status of Deep Learning Models (DLMs). While commonly referred to as 'representations', what this entails is ambiguous due to a conflation of functional and relational conceptions of representation. This paper argues that while DLMs represent their targets in a relational sense, they are best understood as highly idealized models. This result has immediate implications for explainable AI (XAI) and directs philosophical attention toward examining the idealized nature of DLM representations and their role in future scientific investigation.",not included,0.8235145151615143,96
14ddc00b04f466fb70d97ab920bb29988fd37d4b,preprocessed,citation,citation,semantic_scholar,Frontiers in Artificial Intelligence,2023.0,semantic_scholar,the assessment list for trustworthy artificial intelligence: a review and recommendations,https://www.semanticscholar.org/paper/14ddc00b04f466fb70d97ab920bb29988fd37d4b,"In July 2020, the European Commission's High-Level Expert Group on AI (HLEG-AI) published the Assessment List for Trustworthy Artificial Intelligence (ALTAI) tool, enabling organizations to perform self-assessments of the fit of their AI systems and surrounding governance to the “7 Principles for Trustworthy AI.” Prior research on ALTAI has focused primarily on specific application areas, but there has yet to be a comprehensive analysis and broader recommendations aimed at proto-regulators and industry practitioners. This paper therefore starts with an overview of this tool, including an assessment of its strengths and limitations. The authors then consider the success by which the ALTAI tool is likely to be of utility to industry in improving understanding of the risks inherent in AI systems and best practices to mitigate such risks. It is highlighted how research and practices from fields such as Environmental Sustainability, Social Justice, and Corporate Governance (ESG) can be of benefit for addressing similar challenges in ethical AI development and deployment. Also explored is the extent to which the tool is likely to be successful in being taken up by industry, considering various factors pertaining to its likely adoption. Finally, the authors also propose recommendations applicable internationally to similar bodies to the HLEG-AI regarding the gaps needing to be addressed between high-level principles and practical support for those on the front-line developing or commercializing AI tools. In all, this work provides a comprehensive analysis of the ALTAI tool, as well as recommendations to relevant stakeholders, with the broader aim of promoting more widespread adoption of such a tool in industry.",not included,0.8043087244033813,97
bdd1d47e179e7c00dfa78ad0d62be967575e0d1e,preprocessed,citation,citation,semantic_scholar,Journal of Hand Surgery (European Volume),2023.0,semantic_scholar,insights and trends review: artificial intelligence in hand surgery,https://www.semanticscholar.org/paper/bdd1d47e179e7c00dfa78ad0d62be967575e0d1e,"Artificial intelligence (AI) in hand surgery is an emerging and evolving field that will likely play a large role in the future care of our patients. However, there remain several challenges to makes this technology meaningful, acceptable and usable at scale. In this review article, we discuss basic concepts in AI, including challenges and key considerations, provide an update on how AI is being used in hand and wrist surgery and propose potential future applications. The aims are to equip clinicians and researchers with the basic knowledge needed to understand and explore the incorporation of AI in hand surgery within their own practice and recommends further reading to develop knowledge in this emerging field.",not included,0.8062968552112579,98
cd07edfae3ba7064e4e7c222b6d18e39d7983abc,preprocessed,citation,citation,semantic_scholar,Big Data & Society,2023.0,semantic_scholar,ethical assessments and mitigation strategies for biases in ai-systems used during the covid-19 pandemic,https://www.semanticscholar.org/paper/cd07edfae3ba7064e4e7c222b6d18e39d7983abc,"The main aim of this article is to reflect on the impact of biases related to artificial intelligence (AI) systems developed to tackle issues arising from the COVID-19 pandemic, with special focus on those developed for triage and risk prediction. A secondary aim is to review assessment tools that have been developed to prevent biases in AI systems. In addition, we provide a conceptual clarification for some terms related to biases in this particular context. We focus mainly on non-racial biases that may be less considered when addressing biases in AI systems in the existing literature. In the manuscript, we found that the existence of bias in AI systems used for COVID-19 can result in algorithmic justice and that the legal frameworks and strategies developed to prevent the apparition of bias have failed to adequately consider social determinants of health. Finally, we make some recommendations on how to include more diverse professional profiles in order to develop AI systems that increase the epistemic diversity needed to tackle AI biases during the COVID-19 pandemic and beyond.",not included,0.826026663184166,99
b5b4eff9962c1fdcdc11fc7c4861d579f4c34f3a,preprocessed,citation,citation,semantic_scholar,medRxiv,2022.0,semantic_scholar,screening of normal endoscopic large bowel biopsies with artificial intelligence: a retrospective study,https://www.semanticscholar.org/paper/b5b4eff9962c1fdcdc11fc7c4861d579f4c34f3a,"Objectives: Develop an interpretable AI algorithm to rule out normal large bowel endoscopic biopsies saving pathologist resources. Design: Retrospective study. Setting: One UK NHS site was used for model training and internal validation. External validation conducted on data from two other NHS sites and one site in Portugal. Participants: 6,591 whole-slides images of endoscopic large bowel biopsies from 3,291 patients (54% Female, 46% Male). Main outcome measures: Area under the receiver operating characteristic and precision recall curves (AUC-ROC and AUC-PR), measuring agreement between consensus pathologist diagnosis and AI generated classification of normal versus abnormal biopsies. Results: A graph neural network was developed incorporating pathologist domain knowledge to classify the biopsies as normal or abnormal using clinically driven interpretable features. Model training and internal validation were performed on 5,054 whole slide images of 2,080 patients from a single NHS site resulting in an AUC-ROC of 0.98 (SD=0.004) and AUC-PR of 0.98 (SD=0.003). The predictive performance of the model was consistent in testing over 1,537 whole slide images of 1,211 patients from three independent external datasets with mean AUC-ROC = 0.97 (SD=0.007) and AUC-PR = 0.97 (SD=0.005). Our analysis shows that at a high sensitivity threshold of 99%, the proposed model can, on average, reduce the number of normal slides to be reviewed by a pathologist by 55%. A key advantage of IGUANA is its ability to provide an explainable output highlighting potential abnormalities in a whole slide image as a heatmap overlay in addition to numerical values associating model prediction with various histological features. Example results with interpretable features can be viewed online at https://iguana.dcs.warwick.ac.uk/. Conclusions: An interpretable AI model was developed to screen abnormal cases for review by pathologists. The model achieved consistently high predictive accuracy on independent cohorts showing its potential in optimising increasingly scarce pathologist resources and for achieving faster time to diagnosis. Explainable predictions of IGUANA can guide pathologists in their diagnostic decision making and help boost their confidence in the algorithm, paving the way for future clinical adoption.",not included,0.838510024547577,100
16728948ad67e35a1b7193cf91bfe09e5fbfa6ba,preprocessed,citation,citation,semantic_scholar,Frontiers in Artificial Intelligence,2022.0,semantic_scholar,overview and commentary of the cdei's extended roadmap to an effective ai assurance ecosystem,https://www.semanticscholar.org/paper/16728948ad67e35a1b7193cf91bfe09e5fbfa6ba,"In recent years, the field of ethical artificial intelligence (AI), or AI ethics, has gained traction and aims to develop guidelines and best practices for the responsible and ethical use of AI across sectors. As part of this, nations have proposed AI strategies, with the UK releasing both national AI and data strategies, as well as a transparency standard. Extending these efforts, the Centre for Data Ethics and Innovation (CDEI) has published an AI Assurance Roadmap, which is the first of its kind and provides guidance on how to manage the risks that come from the use of AI. In this article, we provide an overview of the document's vision for a “mature AI assurance ecosystem” and how the CDEI will work with other organizations for the development of regulation, industry standards, and the creation of AI assurance practitioners. We also provide a commentary of some key themes identified in the CDEI's roadmap in relation to (i) the complexities of building “justified trust”, (ii) the role of research in AI assurance, (iii) the current developments in the AI assurance industry, and (iv) convergence with international regulation.",not included,0.8116682052612305,101
f693bcdc08b69a66ae93122259db6f8393546253,preprocessed,citation,citation,semantic_scholar,Social Science Research Network,2022.0,semantic_scholar,"don't ""research fast and break things"": on the ethics of computational social science",https://www.semanticscholar.org/paper/f693bcdc08b69a66ae93122259db6f8393546253,"As a quintessential social impact science, Computational Social Science (CSS) holds great promise to advance social justice, human flourishing, and biospheric sustainability. However, CSS is also an all-too-human science—conceived in particular social, cultural, and historical contexts and pursued amidst intractable power imbalances, structural inequities, and potential conflicts of interest. Its proponents must thus remain continuously self-critical about the role that values, interests, and power dynamics play in shaping mission-driven research. Likewise, they must take heed of the complicated social and historical conditions surrounding the generation and construction of data as well as the way that the activities and theories of CSS researchers can function to restructure and shape the phenomena that they purport only to measure and analyse. This article is concerned with setting up practical guardrails within the research activities and environments of CSS in response to these dilemmas. It aims to provide CSS scholars, as well as policymakers and other stakeholders who apply CSS methods, with the critical and constructive means needed to ensure that their practices are ethical, trustworthy, and responsible. It begins by providing a taxonomy of the ethical challenges faced by researchers in the field of CSS. These are challenges related to (1) the treatment of research subjects, (2) the impacts of CSS research on affected individuals and communities, (3) the quality of CSS research and to its epistemological status, (4) research integrity, and (5) research equity. Taking these challenges as a motivation for cultural transformation, it then argues for the end-to-end incorporation of habits of responsible research and innovation (RRI) into CSS practices, focusing on the role that contextual considerations, anticipatory reflection, impact assessment, public engagement, and justifiable and well-documented action should play across the research lifecycle. In proposing the inclusion of habits of RRI in CSS practices, the chapter lays out several practical steps needed for ethical, trustworthy, and responsible CSS research activities. These include stakeholder engagement processes, research impact assessments, data lifecycle documentation, bias self-assessments, and transparent research reporting protocols. INTRODUCTION: COMBATTING THE LURES OF SCIENTISM IN CSS ............................................ 2 ETHICAL CHALLENGES FACED BY CSS ................................................................................................. 5 CHALLENGES RELATED TO THE TREATMENT OF RESEARCH SUBJECTS. ........................................................................... 6 CHALLENGES RELATED TO THE IMPACTS OF CSS RESEARCH ON AFFECTED INDIVIDUALS AND COMMUNITIES ..... 9 Adverse impacts at the individual level ..................................................................................................................................... 10 Adverse impacts at the social level ............................................................................................................................................ 11 Adverse impacts at the biospheric level ..................................................................................................................................... 12 CHALLENGES RELATED TO THE QUALITY OF CSS RESEARCH AND TO ITS EPISTEMOLOGICAL STATUS ................... 13 CHALLENGES RELATED TO RESEARCH INTEGRITY ........................................................................................................... 16 CHALLENGES RELATED TO RESEARCH EQUITY .................................................................................................................. 17 INCORPORATING HABITS OF RESPONSIBLE RESEARCH AND INNOVATION INTO CSS PRACTICES ................................................................................................................................................... 18 CONSIDER CONTEXT ............................................................................................................................................................... 19 ANTICIPATE IMPACTS .............................................................................................................................................................. 20 Stakeholder analysis ................................................................................................................................................................ 21 Establishment of clear normative criteria for impact assessment ................................................................................................. 22 Methodical evaluation of potential impacts and impact mitigation planning ............................................................................... 23 Establishment of protocols for re-visitation and re-evaluation of the research impact assessment .................................................. 26 * This paper is an unabridged pre-print of a chapter written for the European Commission’s Joint Research Centre, Scientific Development Centre for Advanced Studies, to be published in Handbook of Computational Social Science for Policy (2022) by Springer. In addition to the JRC’s support, the author would like to acknowledge the support of a grant from ESRC (ES/T007354/1), Wave 1 of The UKRI Strategic Priorities Fund under the EPSRC Grant EP/W006022/1, Towards Turing 2.0 under the EPSRC Grant EP/W037211/1, and the public funds that make the Turing's Public Policy Programme possible. The author would additionally like to thank Serena Signorelli, Claudia Fischer, and Morgan Briggs for their invaluable editorial assistance.",not included,0.8252739846706391,102
3c4b04b68d5e14c60d597ecc5f58ed09e60c9dc4,preprocessed,citation,citation,semantic_scholar,KnE Social Sciences,2022.0,semantic_scholar,implementation of investment facility services in the ministry of investment,https://www.semanticscholar.org/paper/3c4b04b68d5e14c60d597ecc5f58ed09e60c9dc4,"This study aimed to examine the implementation of policies concerning the investment facility services of the Online Single Submission system (OSS) version 1.1 which was launched in January 2020. According to the results of a previous survey of facilities in 2020, there had been a decrease in the level of satisfaction concerning the effectiveness and efficiency of facility services, and also a decrease in the processing targets for the import facility services. The researchers of this study used a qualitative approach and collected data through interviews and document analysis. Edward’s theory was employed, which focuses on four aspects: communication, disposition, resources, and bureaucracy. This was combined with three perspectives of performance indicators from the Ministry of Investment/BKPM: customers, internal processes, and learning. The results indicated that the investment facility services of the OSS version 1.1 have not been effective due to several factors, including: (1) lack of communication and coordination, and policy inconsistencies; (2) unclear service information, and a lack of technical guidance and socialization; (3) a lack of support for resources through the budget, human resources and infrastructure; and (4) a lack of periodic structured evaluation. The recommendations are thus as follows: first, the government needs to make informative and detailed guidelines for facility services and conduct socialization with stakeholders before a policy is implemented. Second, there is a need for periodic internal evaluations every month; the results of the evaluation should be recorded; follow-up improvements should be made; and the obstacles found should be submitted to the Management Review Meeting. Third, the facility unit should detail its training needs and training should be carried out regularly to improve the capabilities of its employees. Fourth, the government needs to create a fully integrated facility service system, and when building this, it is necessary to apply the principles of clarity, convenience, acceleration, transparency, legality, and measurability. 
Keywords: communication, information, coordination, consistency, evaluation, socialization",not included,0.8033411532640458,103
43d9b2c31f5e80ed0948ee460fe452cf223b6ab4,preprocessed,citation,citation,semantic_scholar,International Conference on Human Factors in Computing Systems,2022.0,semantic_scholar,informing age-appropriate ai: examining principles and practices of ai for children,https://www.semanticscholar.org/paper/43d9b2c31f5e80ed0948ee460fe452cf223b6ab4,"AI systems are becoming increasingly pervasive within children’s devices, apps, and services. However, it is not yet well-understood how risks and ethical considerations of AI relate to children. This paper makes three contributions to this area: first, it identifies ten areas of alignment between general AI frameworks and codes for age-appropriate design for children. Then, to understand how such principles relate to real application contexts, we conducted a landscape analysis of children’s AI systems, via a systematic literature review including 188 papers. This analysis revealed a wide assortment of applications, and that most systems’ designs addressed only a small subset of principles among those we identified. Finally, we synthesised our findings in a framework to inform a new “Code for Age-Appropriate AI”, which aims to provide timely input to emerging policies and standards, and inspire increased interactions between the AI and child-computer interaction communities.",not included,0.8294077664613724,104
087ebdd2529399cca8394bf285dd92c6400ce33c,preprocessed,citation,citation,semantic_scholar,International Journal of Production Research,2022.0,semantic_scholar,human-centric artificial intelligence architecture for industry 5.0 applications,https://www.semanticscholar.org/paper/087ebdd2529399cca8394bf285dd92c6400ce33c,"Human-centricity is the core value behind the evolution of manufacturing towards Industry 5.0. Nevertheless, there is a lack of architecture that considers safety, trustworthiness, and human-centricity at its core. Therefore, we propose an architecture that integrates Artificial Intelligence (Active Learning, Forecasting, Explainable Artificial Intelligence), simulated reality, decision-making, and users' feedback, focussing on synergies between humans and machines. Furthermore, we align the proposed architecture with the Big Data Value Association Reference Architecture Model. Finally, we validate it on three use cases from real-world case studies.",included,0.8154128462076187,105
88ac4c730ff83502a1e3a559b9a1cc13eb2c0004,preprocessed,citation,citation,semantic_scholar,Frontiers in Artificial Intelligence,2022.0,semantic_scholar,review of multi-criteria decision-making methods in finance using explainable artificial intelligence,https://www.semanticscholar.org/paper/88ac4c730ff83502a1e3a559b9a1cc13eb2c0004,"The influence of Artificial Intelligence is growing, as is the need to make it as explainable as possible. Explainability is one of the main obstacles that AI faces today on the way to more practical implementation. In practise, companies need to use models that balance interpretability and accuracy to make more effective decisions, especially in the field of finance. The main advantages of the multi-criteria decision-making principle (MCDM) in financial decision-making are the ability to structure complex evaluation tasks that allow for well-founded financial decisions, the application of quantitative and qualitative criteria in the analysis process, the possibility of transparency of evaluation and the introduction of improved, universal and practical academic methods to the financial decision-making process. This article presents a review and classification of multi-criteria decision-making methods that help to achieve the goal of forthcoming research: to create artificial intelligence-based methods that are explainable, transparent, and interpretable for most investment decision-makers.",not included,0.8546511858701706,106
fe0dd6c6f09d9efe49148205237d66602fe9f06d,preprocessed,citation,citation,semantic_scholar,arXiv.org,2022.0,semantic_scholar,explainability for identification of vulnerable groups in machine learning models,https://www.semanticscholar.org/paper/fe0dd6c6f09d9efe49148205237d66602fe9f06d,"If a prediction model identifies vulnerable individuals or groups, the use of that model may become an ethical issue. But can we know that this is what a model does? Machine learning fairness as a field is focused on the just treatment of individuals and groups under information processing with machine learning methods. While considerable attention has been given to mitigating discrimination of protected groups, vulnerable groups have not received the same attention. Unlike protected groups, which can be regarded as always vulnerable, a vulnerable group may be vulnerable in one context but not in another. This raises new challenges on how and when to protect vulnerable individuals and groups under machine learning. Methods from explainable artificial intelligence (XAI), in contrast, do consider more contextual issues and are concerned with answering the question""why was this decision made?"". Neither existing fairness nor existing explainability methods allow us to ascertain if a prediction model identifies vulnerability. We discuss this problem and propose approaches for analysing prediction models in this respect.",included,0.8554961800575256,107
3fe4f0e313f811992b3d45bd2a72a374d1f5572d,preprocessed,citation,citation,semantic_scholar,arXiv.org,2022.0,semantic_scholar,towards an accountable and reproducible federated learning: a factsheets approach,https://www.semanticscholar.org/paper/3fe4f0e313f811992b3d45bd2a72a374d1f5572d,"Federated Learning (FL) is a novel paradigm for the shared training of models based on decentralized and private data. With respect to ethical guidelines, FL is promising regarding privacy, but needs to excel vis-\`a-vis transparency and trustworthiness. In particular, FL has to address the accountability of the parties involved and their adherence to rules, law and principles. We introduce AF^2 Framework, where we instrument FL with accountability by fusing verifiable claims with tamper-evident facts, into reproducible arguments. We build on AI FactSheets for instilling transparency and trustworthiness into the AI lifecycle and expand it to incorporate dynamic and nested facts, as well as complex model compositions in FL. Based on our approach, an auditor can validate, reproduce and certify a FL process. This can be directly applied in practice to address the challenges of AI engineering and ethics.",not included,0.8145611852407455,108
bac66654b5fff0e0fc75d477c60b365f31c4f057,preprocessed,citation,citation,semantic_scholar,The Journal of Pathology: Clinical Research,2022.0,semantic_scholar,the ethical challenges of artificial intelligence‐driven digital pathology,https://www.semanticscholar.org/paper/bac66654b5fff0e0fc75d477c60b365f31c4f057,"Digital pathology – the digitalisation of clinical histopathology services through the scanning and storage of pathology slides – has opened up new possibilities for health care in recent years, particularly in the opportunities it brings for artificial intelligence (AI)‐driven research. Recognising, however, that there is little scholarly debate on the ethics of digital pathology when used for AI research, this paper summarises what it sees as four key ethical issues to consider when deploying AI infrastructures in pathology, namely, privacy, choice, equity, and trust. The themes are inspired from the authors' experience grappling with the challenge of deploying an ethical digital pathology infrastructure to support AI research as part of the National Pathology Imaging Cooperative (NPIC), a collaborative of universities, hospital trusts, and industry partners largely located across the North of England. Though focusing on the UK case, internationally, few pathology departments have gone fully digital, and so the themes developed here offer a heuristic for ethical reflection for other departments currently making a similar transition or planning to do so in the future. We conclude by promoting the need for robust public governance mechanisms in AI‐driven digital pathology.",not included,0.8303640842437744,109
71d49da4bfafdd0db18a7be44fdeee45ca04ecda,preprocessed,citation,citation,semantic_scholar,Italian National Conference on Sensors,2022.0,semantic_scholar,a fairness of data combination in wireless packet scheduling,https://www.semanticscholar.org/paper/71d49da4bfafdd0db18a7be44fdeee45ca04ecda,"With the proliferation of artificial intelligence (AI) technology, the function of AI in a sixth generation (6G) environment is likely to come into play on a large scale. Moreover, in recent years, with the rapid advancement in AI technology, the ethical issues of AI have become a hot topic. In this paper, the ethical concern of AI in wireless networks is studied from the perspective of fairness in data. To make the dataset fairer, novel dataset categorization and dataset combination schemes are proposed. For the dataset categorization scheme, a deep-learning-based dataset categorization (DLDC) model is proposed. Based on the results of the DLDC model, the input dataset is categorized based on the group index. The datasets based on the group index are combined using various combination schemes. Through simulations, the results of each dataset combination method and their performance are compared, and the advantages and disadvantages of fairness and performance according to the dataset configuration are analyzed.",not included,0.828378900885582,110
93df514d1873b4bc7c11f147c297746efe223ee1,preprocessed,citation,citation,semantic_scholar,Philosophia Scientiæ,2022.0,semantic_scholar,explaining machine learning decisions,https://www.semanticscholar.org/paper/93df514d1873b4bc7c11f147c297746efe223ee1,"Abstract The operations of deep networks are widely acknowledged to be inscrutable. The growing field of Explainable AI (XAI) has emerged in direct response to this problem. However, owing to the nature of the opacity in question, XAI has been forced to prioritise interpretability at the expense of completeness, and even realism, so that its explanations are frequently interpretable without being underpinned by more comprehensive explanations faithful to the way a network computes its predictions. While this has been taken to be a shortcoming of the field of XAI, I argue that it is broadly the right approach to the problem.",not included,0.8040409088134766,111
53835288d4d42bb3a2abc44984613ec3e0b8e478,preprocessed,citation,citation,semantic_scholar,"Journal of Information, Communication and Ethics in Society",2021.0,semantic_scholar,"ai led ethical digital transformation: framework, research and managerial implications",https://www.semanticscholar.org/paper/53835288d4d42bb3a2abc44984613ec3e0b8e478,"
Purpose
Digital transformation (DT) leverages digital technologies to change current processes and introduce new processes in any organisation’s business model, customer/user experience and operational processes (DT pillars). Artificial intelligence (AI) plays a significant role in achieving DT. As DT is touching each sphere of humanity, AI led DT is raising many fundamental questions. These questions raise concerns for the systems deployed, how they should behave, what risks they carry, the monitoring and evaluation control we have in hand, etc. These issues call for the need to integrate ethics in AI led DT. The purpose of this study is to develop an “AI led ethical digital transformation framework”.


Design/methodology/approach
Based on the literature survey, various existing business ethics decision-making models were synthesised. The authors mapped essential characteristics such as intensity and the individual, organisational and opportunity factors of ethics models with the proposed AI led ethical DT. The DT framework is evaluated using a thematic analysis of 23 expert interviews with relevant AI ethics personas from industry and society. The qualitative data of the interviews and opinion data has been analysed using MAXQDA software.


Findings
The authors have explored how AI can drive the ethical DT framework and have identified the core constituents of developing an AI led ethical DT framework. Backed by established ethical theories, the paper presents how DT pillars are related and sequenced to ethical factors. This research provides the potential to examine theoretically sequenced ethical factors with practical DT pillars.


Originality/value
The study establishes deduced and induced ethical value codes based on thematic analysis to develop guidelines for the pursuit of ethical DT. The authors identify four unique induced themes, namely, corporate social responsibility, perceived value, standard benchmarking and learning willingness. The comprehensive findings of this research, supported by a robust theoretical background, have substantial implications for academic research and corporate applicability. The proposed AI led ethical DT framework is unique and can be used for integrated social, technological and economic ethical research.
",included,0.8418905466794968,112
bcc82ce554942880814243fc8c08a88b9d2aad09,preprocessed,citation,citation,semantic_scholar,Technology Analysis & Strategic Management,2021.0,semantic_scholar,reading the road: challenges and opportunities on the path to responsible innovation in quantum computing,https://www.semanticscholar.org/paper/bcc82ce554942880814243fc8c08a88b9d2aad09,"ABSTRACT
 Novel technologies such as quantum computing present new opportunities to support societal needs, but societal engagement is vital to secure public trust. Quantum computing technologies are at a pivotal point in their journey from foundational research to deployment, creating a moment for society to investigate, reflect, and consult on their implications. Responsible Innovation (RI) is one method for considering impacts, engaging with societal needs, reflecting on any concerns, and influencing the trajectory of the innovation in response. This paper draws on the empirical work of the RI team embedded in the Networked Quantum Information Technologies Hub. The team investigated researchers’ perceptions of RI and their understanding of societal impacts of quantum technologies, and sought to gauge the challenges of embedding RI across a multi-disciplinary, large-scale enterprise such as the UK quantum programme. The work demonstrated some of the difficulties involved in embedding RI approaches, and in creating a dialogue between innovators and societies. Finally, the authors offer recommendations to policymakers, researchers, and industrial organisations, for better practice in responsible quantum computing, and to ensure that societal considerations are discussed alongside commercial motivations. Applying RI to quantum computing at this pivotal point has implications for RI in other emerging technologies.",not included,0.8079065144062042,113
70043a0b612b6253b37df7d363b3bf2ec3d581c7,preprocessed,citation,citation,semantic_scholar,ACM Computing Surveys,2021.0,semantic_scholar,trustworthy ai: from principles to practices,https://www.semanticscholar.org/paper/70043a0b612b6253b37df7d363b3bf2ec3d581c7,"The rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people’s trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, and accountability. To unify currently available but fragmented approaches toward trustworthy AI, we organize them in a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to system development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items for practitioners and societal stakeholders (e.g., researchers, engineers, and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges for the future development of trustworthy AI systems, where we identify the need for a paradigm shift toward comprehensively trustworthy AI systems.",included,0.8023776173591614,114
50b03c76c31c015b28d85594788fc939f68eeada,preprocessed,citation,citation,semantic_scholar,Sustainability,2021.0,semantic_scholar,"engaging with artificial intelligence (ai) with a bottom-up approach for the purpose of sustainability: victorian farmers market association, melbourne australia",https://www.semanticscholar.org/paper/50b03c76c31c015b28d85594788fc939f68eeada,"Artificial intelligence (AI) is impacting all aspects of food systems, including production, food processing, distribution, and consumption. AI, if implemented ethically for sustainability, can enhance biodiversity, conserve water and energy resources, provide land-related services, power smart cities, and help mitigate climate change. However, there are significant issues in using AI to transition to sustainable food systems. AI’s own carbon footprint could cancel out any sustainability benefits that it creates. Additionally, the technology could further entrench inequalities between and within countries, and bias against minorities or less powerful groups. This paper draws on findings from a study of the Victorian Farmers’ Markets Association (VFMA) that investigated the complexity of designing AI tools to enhance sustainability and resilience for the benefit of the organisation and its members. Codesign workshops, both synchronous and asynchronous, semi-structured interviews, and design innovation methods led the VFMA to experiment with an AI tool to link sustainable soil practices, nutrient rich produce, and human health. The analysis shows that the codesign process and an agile approach created a co-learning environment where sustainability and ethical questions could be considered iteratively within transdisciplinary engagement. The bottom-up approach developed through this study supports organisations who want to engage with AI while reinforcing fairness, transparency, and sustainability.",not included,0.8053445428609848,115
e82457782f2ef23576d6b04ed887ba1b35fa3df3,preprocessed,citation,citation,semantic_scholar,"Journal of Leadership, Accountability and Ethics",2021.0,semantic_scholar,data ethics in practice: challenges and opportunities for a data ethics policy function in the public sector,https://www.semanticscholar.org/paper/e82457782f2ef23576d6b04ed887ba1b35fa3df3,"The overarching aim of data ethics is to promote responsible and sustainable use of data and its products for the benefit of people and society. Through an analysis of various approaches to data and AI ethics in the public sector, this paper aims to identify key goals and challenges for a data ethics policy function in the public sector and provide a set of recommendations. The paper connects the practical experience of data ethics professionals and the emerging theory and research on data ethics through combining a literature review with primary data gathered during workshops with data ethics practitioners in the public sector in the United Kingdom. The research finds that key challenges for data ethics in the public sector include the lack of accountability of data ethics tools, lack of skills and awareness, the saturated landscape of ethical guidance, and insufficient diversity in the field. Opportunities and recommendations identified in the paper include: embedding data ethics in data science processes; providing a platform that collates the available ethical guidance; developing data ethics courses for public sector data scientists; establishing a data ethics community; increasing transparency in hiring and promotion and introducing extensive diversity measures to attract candidates from under-represented groups for data and AI ethics positions within the public sector.",not included,0.8411970287561417,116
826a19bda59aa7ce8a33235b35c0480aac827ea1,preprocessed,citation,citation,semantic_scholar,Social Science Research Network,2021.0,semantic_scholar,"the role of social movements, coalitions, and workers in resisting harmful artificial intelligence and contributing to the development of responsible ai",https://www.semanticscholar.org/paper/826a19bda59aa7ce8a33235b35c0480aac827ea1,"The rapid spread of artificial intelligence (AI) systems has precipitated a rise in ethical and rights-based frameworks intended to guide the development and use of these technologies. Despite the proliferation of these ""AI principles"", there is mounting public concern over the influence that the AI systems have in our society, and coalitions in all sectors are organizing to resist harmful applications of AI worldwide. Responses from peoples everywhere, from workers protesting unethical conduct and applications of AI, to student's protesting MIT's relationships with donor, sex trafficker, and pedophile Jeffery Epstein, to the healthcare community, to indigenous people addressing “the twin problems of a lack of reliable data and information on indigenous peoples and biopiracy and misuse of their traditional knowledge and cultural heritage”, to smart city stakeholders, to many others. Like corporations, governments around the world have adopted strategies for becoming leaders in the development and use of Artificial Intelligence, fostering environments congenial to AI innovators. Neither corporations nor policymakers have sufficiently addressed how the rights of children fit into their AI strategies or products. The role of artificial intelligence in children’s lives—from how children play, to how they are educated, to how they consume information and learn about the world—is expected to increase exponentially over the coming years. Thus, it’s imperative that stakeholders evaluate the risks and assess opportunities to use artificial intelligence to maximize children’s wellbeing in a thoughtful and systematic manner. This paper discusses AI and children's rights in the context of social media platforms such as YouTube, smart toys, and AI education applications. The Hello Barbie, Cloud Pets, and Cayla smart toys case studies are analyzed, as well as the ElsaGate social media hacks and education's new Intelligent Tutoring Systems and surveillance of students apps. Though AI has valuable benefits for children, it presents some particular challenges around important issues including child safety, privacy, data privacy, device security and consent. Technology giants, all of whom are heavily investing in and profiting from AI, must not dominate the public discourse on responsible use of AI. We all need to shape the future of our core values and democratic institutions. As artificial intelligence continues to find its way into our daily lives, its propensity to interfere with our rights only gets more severe. Many of the issues mentioned in this examination of harmful AI are not new, but they are greatly exacerbated and threatened by the scale, proliferation, and real-life impact that artificial intelligence facilitates. The potential of artificial intelligence to both help and harm people is much greater than earlier technologies. Continuing to examine what safeguards and structures can address AI’s problems and harms, including those that disproportionately impact marginalized people, is a critical activity. There are assumptions embedded in the AI algorithms that will shape how our world is realized. Many of these algorithms are wrongful and biased, they must get locked-in. Our best human judgment is needed to contain AI's harmful impacts. Perhaps one of the greatest contributions of AI will be to make us ultimately understand how important human wisdom truly is in life on earth.",not included,0.8043087244033813,117
65f0e1debbc22274681b3169822f6cc08e28a47e,preprocessed,citation,citation,semantic_scholar,XVII Brazilian Symposium on Information Systems,2021.0,semantic_scholar,ethical guidelines and principles in the context of artificial intelligence,https://www.semanticscholar.org/paper/65f0e1debbc22274681b3169822f6cc08e28a47e,"The interest in Artificial Intelligence (AI) based systems has been gaining momentum at a fast pace, both for software development teams and for society as a whole. This work aims to identify the guidelines and ethical principles for systems based on Artificial Intelligence. Design Science Research methodology was adopted in order to understand the various guidelines and principles existing in the literature. From the current landscape, a body of knowledge in the field of AI ethics is presented, with the purpose of supporting developers and Product Owners in identifying the guidelines and ethical principles in the literature so that they can be used during the software development process. Thus, this work will contribute to the various stakeholders in the development of ethical systems in the context of AI, such as: policy makers, ethicists, users, organizations, data scientists, development teams, among others.",included,0.8062968552112579,118
746dd0ed2004e8ac6d7084e1e671eccefec3fb85,preprocessed,citation,citation,semantic_scholar,Research Ethics,2021.0,semantic_scholar,evaluating the prospects for university-based ethical governance in artificial intelligence and data-driven innovation,https://www.semanticscholar.org/paper/746dd0ed2004e8ac6d7084e1e671eccefec3fb85,"There has been considerable debate around the ethical issues raised by data-driven technologies such as artificial intelligence. Ethical principles for the field have focused on the need to ensure that such technologies are used for good rather than harm, that they enshrine principles of social justice and fairness, that they protect privacy, respect human autonomy and are open to scrutiny. While development of such principles is well advanced, there is as yet little consensus on the mechanisms appropriate for ethical governance in this field. This paper examines the prospects for the university ethics committee to undertake effective review of research conducted on data-driven technologies in the university context. Challenges identified include: the relatively narrow focus of university-based ethical review on the human subjects research process and lack of capacity to anticipate downstream impacts; the difficulties of accommodating the complex interplay of academic and commercial interests in the field; and the need to ensure appropriate expertise from both specialists and lay voices. Overall, the challenges identified sharpen appreciation of the need to encourage a joined-up and effective system of ethical governance that fosters an ethical culture rather than replacing ethical reflection with bureaucracy.",not included,0.8065193235874176,119
f703bcbef46fb7ddad9c06261f40376ad4f96749,preprocessed,citation,citation,semantic_scholar,Journal of Medical Ethics,2021.0,semantic_scholar,machine learning in medicine: should the pursuit of enhanced interpretability be abandoned?,https://www.semanticscholar.org/paper/f703bcbef46fb7ddad9c06261f40376ad4f96749,"We argue why interpretability should have primacy alongside empiricism for several reasons: first, if machine learning (ML) models are beginning to render some of the high-risk healthcare decisions instead of clinicians, these models pose a novel medicolegal and ethical frontier that is incompletely addressed by current methods of appraising medical interventions like pharmacological therapies; second, a number of judicial precedents underpinning medical liability and negligence are compromised when ‘autonomous’ ML recommendations are considered to be en par with human instruction in specific contexts; third, explainable algorithms may be more amenable to the ascertainment and minimisation of biases, with repercussions for racial equity as well as scientific reproducibility and generalisability. We conclude with some reasons for the ineludible importance of interpretability, such as the establishment of trust, in overcoming perhaps the most difficult challenge ML will face in a high-stakes environment like healthcare: professional and public acceptance.",not included,0.826026663184166,120
367b89b469cf901005f6eb35c67d9ff6b24291fc,preprocessed,citation,citation,semantic_scholar,Journal of health care for the poor and underserved,2021.0,semantic_scholar,a proposed framework on integrating health equity and racial justice into the artificial intelligence development lifecycle,https://www.semanticscholar.org/paper/367b89b469cf901005f6eb35c67d9ff6b24291fc,"Abstract:The COVID-19 pandemic has created multiple opportunities to deploy artificial intelligence (AI)-driven tools and applied interventions to understand, mitigate, and manage the pandemic and its consequences. The disproportionate impact of COVID-19 on racial/ethnic minority and socially disadvantaged populations underscores the need to anticipate and address social inequalities and health disparities in AI development and application. Before the pandemic, there was growing optimism about AI's role in addressing inequities and enhancing personalized care. Unfortunately, ethical and social issues that are encountered in scaling, developing, and applying advanced technologies in health care settings have intensified during the rapidly evolving public health crisis. Critical voices concerned with the disruptive potentials and risk for engineered inequities have called for reexamining ethical guidelines in the development and application of AI. This paper proposes a framework to incorporate ethical AI principles into the development process in ways that intentionally promote racial health equity and social justice. Without centering on equity, justice, and ethical AI, these tools may exacerbate structural inequities that can lead to disparate health outcomes.",included,0.8288842886686325,121
0065be6d1ad6a14c3bf69c7f1443cf1be6c65705,preprocessed,citation,citation,semantic_scholar,"Conference on Fairness, Accountability and Transparency",2021.0,semantic_scholar,an action-oriented ai policy toolkit for technology audits by community advocates and activists,https://www.semanticscholar.org/paper/0065be6d1ad6a14c3bf69c7f1443cf1be6c65705,"Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.",included,0.8351606994867324,122
15eb623293cf1cad1b50430e37dc361bc6c86cee,preprocessed,citation,citation,semantic_scholar,IEEE technology & society magazine,2021.0,semantic_scholar,to be fair or not to be: using ai for the good of citizens,https://www.semanticscholar.org/paper/15eb623293cf1cad1b50430e37dc361bc6c86cee,"In the last decade, there has been an explosion in the progress and applications of artificial intelligence (AI) in our society. For the first time, the applications of AI have left the laboratory to reach society in a broad, visible, and relevant way. This fact has raised numerous questions about the potential of AI in the future and its implications in our lives. The benefits of AI do not come alone; they also bring with them responsibilities that if not considered properly can become misuses, intentional or not.",not included,0.8206197261810303,123
31c6e226d88ea6611dd6e7aa03febfcf8c1beb36,preprocessed,citation,citation,semantic_scholar,IEEE technology & society magazine,2021.0,semantic_scholar,ai4eq: for a true global village not for global pillage,https://www.semanticscholar.org/paper/31c6e226d88ea6611dd6e7aa03febfcf8c1beb36,"<bold>The last few</bold> years have seen a large number of initiatives on artificial intelligence (AI) ethics: intergovernmental-institution initiatives such as “Ethics Guidelines for Trustworthy AI” from the high-level expert group on AI of the European Commission <xref ref-type=""bibr"" rid=""ref1"">[1]</xref> or the Organisation for Economic Cooperation and Development (OECD) Council Recommendation on Artificial Intelligence <xref ref-type=""bibr"" rid=""ref2"">[2]</xref>, government initiatives such as that of the U.K. Parliament Select Committee on Artificial Intelligence <xref ref-type=""bibr"" rid=""ref3"">[3]</xref>, industry initiatives on AI ethical codes such as those of Google, IBM, Microsoft, and Intel, academic initiatives such as the Montreal declaration for the responsible development of AI <xref ref-type=""bibr"" rid=""ref4"">[4]</xref>, the Stanford University 100 Year Study on AI <xref ref-type=""bibr"" rid=""ref5"">[5]</xref> or the Alan Turing Institute’s “Understanding Artificial Intelligence Ethics and Safety” <xref ref-type=""bibr"" rid=""ref6"">[6]</xref>, and finally professional body initiatives such as the IEEE Global Initiative on Ethics of Autonomous/Intelligent Systems (A/IS) <xref ref-type=""bibr"" rid=""ref7"">[7]</xref>. These initiatives, while acknowledging the potential of A/IS technologies to contribute to global socioeconomic solutions, highlight the increasing challenges posed by these technologies in the ethical, moral, legal, humanitarian, and sociopolitical domains.",included,0.838510024547577,124
45f76ad0bd54e00687ea58d4e2c2a38477372fe6,preprocessed,citation,citation,semantic_scholar,Italian National Conference on Sensors,2020.0,semantic_scholar,individualised responsible artificial intelligence for home-based rehabilitation,https://www.semanticscholar.org/paper/45f76ad0bd54e00687ea58d4e2c2a38477372fe6,"Socioeconomic reasons post-COVID-19 demand unsupervised home-based rehabilitation and, specifically, artificial ambient intelligence with individualisation to support engagement and motivation. Artificial intelligence must also comply with accountability, responsibility, and transparency (ART) requirements for wider acceptability. This paper presents such a patient-centric individualised home-based rehabilitation support system. To this end, the Timed Up and Go (TUG) and Five Time Sit To Stand (FTSTS) tests evaluate daily living activity performance in the presence or development of comorbidities. We present a method for generating synthetic datasets complementing experimental observations and mitigating bias. We present an incremental hybrid machine learning algorithm combining ensemble learning and hybrid stacking using extreme gradient boosted decision trees and k-nearest neighbours to meet individualisation, interpretability, and ART design requirements while maintaining low computation footprint. The model reaches up to 100% accuracy for both FTSTS and TUG in predicting associated patient medical condition, and 100% or 83.13%, respectively, in predicting area of difficulty in the segments of the test. Our results show an improvement of 5% and 15% for FTSTS and TUG tests, respectively, over previous approaches that use intrusive means of monitoring such as cameras.",not included,0.8116682052612305,125
3de9c3db7acc9e883564d3a8b70624b820e2eb7a,preprocessed,citation,citation,semantic_scholar,Frontiers in Neuroergonomics,2020.0,semantic_scholar,brain at work and in everyday life as the next frontier: grand field challenges for neuroergonomics,https://www.semanticscholar.org/paper/3de9c3db7acc9e883564d3a8b70624b820e2eb7a,"1 ISAE-SUPAERO, Université de Toulouse, Toulouse, France, 2 School of Biomedical Engineering, Science and Health Systems, Drexel University, Philadelphia, PA, United States, Computational Neuroergonomics Laboratory, Department of Industrial Engineering and Management Systems, University of Central Florida, Orlando, FL, United States, Drexel Solutions Institute, Drexel University, Philadelphia, PA, United States, Department of Psychology, College of Arts and Sciences, Drexel University, Philadelphia, PA, United States, Department of Family and Community Health, University of Pennsylvania, Philadelphia, PA, United States, Center for Injury Research and Prevention, Children’s Hospital of Philadelphia, Philadelphia,",not included,0.8252739846706391,126
29cb846512d2b8920e93233b613f62cdb53b3256,preprocessed,citation,citation,semantic_scholar,Social Science Research Network,2020.0,semantic_scholar,understanding bias in facial recognition technologies,https://www.semanticscholar.org/paper/29cb846512d2b8920e93233b613f62cdb53b3256,"Over the past couple of years, the growing debate around automated facial recognition has reached a boiling point. As developers have continued to swiftly expand the scope of these kinds of technologies into an almost unbounded range of applications, an increasingly strident chorus of critical voices has sounded concerns about the injurious effects of the proliferation of such systems. Opponents argue that the irresponsible design and use of facial detection and recognition technologies (FDRTs) threatens to violate civil liberties, infringe on basic human rights and further entrench structural racism and systemic marginalisation. They also caution that the gradual creep of face surveillance infrastructures into every domain of lived experience may eventually eradicate the modern democratic forms of life that have long provided cherished means to individual flourishing, social solidarity and human self-creation. Defenders, by contrast, emphasise the gains in public safety, security and efficiency that digitally streamlined capacities for facial identification, identity verification and trait characterisation may bring. In this explainer, I focus on one central aspect of this debate: the role that dynamics of bias and discrimination play in the development and deployment of FDRTs. I examine how historical patterns of discrimination have made inroads into the design and implementation of FDRTs from their very earliest moments. And, I explain the ways in which the use of biased FDRTs can lead distributional and recognitional injustices. The explainer concludes with an exploration of broader ethical questions around the potential proliferation of pervasive face-based surveillance infrastructures and makes some recommendations for cultivating more responsible approaches to the development and governance of these technologies.",not included,0.819400179386139,127
0f6fd7c0d31222cca9f33f47c4a32af49a9c4423,preprocessed,citation,citation,semantic_scholar,Science Technology & Society,2020.0,semantic_scholar,"suspect ai: vibraimage, emotion recognition technology and algorithmic opacity",https://www.semanticscholar.org/paper/0f6fd7c0d31222cca9f33f47c4a32af49a9c4423,"Vibraimage is a digital system that quantifies a subject’s mental and emotional state by analysing video footage of the movements of their head. Vibraimage is used by police, nuclear power station operators, airport security and psychiatrists in Russia, China, Japan and South Korea, and has been deployed at two Olympic Games, a FIFA World Cup and a G7 Summit. Yet there is no reliable empirical evidence for its efficacy; indeed, many claims made about its effects seem unprovable. What exactly does vibraimage measure and how has it acquired the power to penetrate the highest profile and most sensitive security infrastructure across Russia and Asia? I first trace the development of the emotion recognition industry, before examining attempts by vibraimage’s developers and affiliates scientifically to legitimate the technology, concluding that the disciplining power and corporate value of vibraimage are generated through its very opacity, in contrast to increasing demands across the social sciences for transparency. I propose the term ‘suspect artificial intelligence (AI)’ to describe the growing number of systems like vibraimage that algorithmically classify suspects/non-suspects, yet are themselves deeply suspect. Popularising this term may help resist such technologies’ reductivist approaches to ‘reading’—and exerting authority over—emotion, intentionality and agency.",not included,0.8052007794380188,128
6226c5ce995413df09025604bdff10f7ad48dc67,preprocessed,citation,citation,semantic_scholar,,2020.0,semantic_scholar,artificial neural networks in public policy: towards an analytical framework,https://www.semanticscholar.org/paper/6226c5ce995413df09025604bdff10f7ad48dc67,"ARTIFICIAL NEURAL NETWORKS IN PUBLIC POLICY: TOWARDS AN ANALYTICAL FRAMEWORK Joshua Lee, Ph.D. George Mason University, 2020 Committee Chair: Dr. Laurie Schintler This dissertation assesses how artificial neural networks (ANNs) and other machine learning systems should be devised, built, and implemented in US governmental organizations (i.e. public agencies). While it primarily focuses on ANNs given their current prevalence and accuracy, many of its conclusions are broadly applicable to other kinds of machine learning as well. It develops an analytical framework, drawn from diverse fields including law, behavioral psychology, public policy, and computer science, that public agency managers and analysts can utilize. The framework yields a series of principles based on my research methodology that I argue are the most relevant to public agencies. The qualitative methodology consists of an iterative approach based on archival research, peer review, expert interviews, and comparative analysis. Critically, this dissertation’s intent is not to provide the specific answers to all questions related to machine learning in public agencies. Given the speed at which this field changes, attempting to provide universally applicable answers would be difficult and short term at best. Rather, this framework focuses on principles which can help guide the user to the proper questions they need to ask for their particular use case. In that same vein, the normative principles it provides are procedurally focused in scope rather than focused on policy outcomes. In other words, this framework is meant to be equally applicable regardless of what one’s specific policy goals are.",included,0.8437141388654709,129
6a32519eff74d85d90318ec4e8e1453b07fa808d,preprocessed,citation,citation,semantic_scholar,AI and Ethics,2020.0,semantic_scholar,the interrelation between data and ai ethics in the context of impact assessments,https://www.semanticscholar.org/paper/6a32519eff74d85d90318ec4e8e1453b07fa808d,"In the growing literature on artificial intelligence (AI) impact assessments, the literature on data protection impact assessments is heavily referenced. Given the relative maturity of the data protection debate and that it has translated into legal codification, it is indeed a natural place to start for AI. In this article, we anticipate directions in what we believe will become a dominant and impactful forthcoming debate, namely, how to conceptualise the relationship between data protection and AI impact. We begin by discussing the value canvas i.e. the ethical principles that underpin data and AI ethics, and discuss how these are instantiated in the context of value trade-offs when the ethics are applied. Following this, we map three kinds of relationships that can be envisioned between data and AI ethics, and then close with a discussion of asymmetry in value trade-offs when privacy and fairness are concerned.",not included,0.8294077664613724,130
4ae7b75e7b0fdf00a687c8f308ceccbe26c24e8d,preprocessed,citation,citation,semantic_scholar,Business Information Review,2020.0,semantic_scholar,regulation and ethics in artificial intelligence and machine learning technologies: where are we now? who is responsible? can the information professional play a role?,https://www.semanticscholar.org/paper/4ae7b75e7b0fdf00a687c8f308ceccbe26c24e8d,"Artificial intelligence (AI) and machine learning (ML) technologies are rapidly maturing and proliferating through all public and private sectors. The potential for these technologies to do good and to help us in our everyday lives is immense. But there is a risk that unless managed and controlled AI can also cause us harm. Questions about regulation, what form it takes and who is responsible for governance are only just beginning to be answered. In May 2019, 42 countries came together to support a global governance framework for AI. The Organisation for Economic Co-operation and Development (OECD) Principles on Artificial Intelligence (OECD (2019) OECD principles on AI. Available at: https://www.oecd.org/going-digital/ai/principles/ (accessed 2 March 2020)) saw like-minded democracies of the world commit to common AI values of trust and respect. In Europe, the European Commission’s (EC) new president, Ursula von der Leyen has made calls for a General Data Protection Regulation style. As a first step the EC has published a white paper: ‘On Artificial Intelligence – A European Approach to Excellence and Trust’ (European Commission (2020) Report, Europa, February). In February 2020, the UK government has published a report on ‘Artificial Intelligence in the Public Sector’ (The Committee on Standards in Public Life (2020) Artificial intelligence and public standards. Report, UK Government, February). This article discusses some of the potential threats AI may hold if left unregulated. It provides a brief overview of the regulatory activities for AI worldwide, and in more detail the current UK AI regulatory landscape. Finally, the article looks at the role that the information professional might play in AI and ML.",not included,0.8252925395965576,131
a2cb1bcc2632b2909aca42b694bd78d4502ad31d,preprocessed,citation,citation,semantic_scholar,Policy Quarterly,2020.0,semantic_scholar,catastrophic risk from rapid developments in artificial intelligence,https://www.semanticscholar.org/paper/a2cb1bcc2632b2909aca42b694bd78d4502ad31d,"This article describes important possible scenarios in which rapid advances in artificial intelligence (AI) pose multiple risks, including to democracy and for inter-state conflict. In parallel with other countries, New Zealand needs policies to monitor, anticipate and mitigate global catastrophic and existential risks from advanced new technologies. A dedicated policy capacity could translate emerging research and policy options into the New Zealand context. It could also identify how New Zealand could best contribute to global solutions. It is desirable that the potential benefits of AI are realised, while the risks are also mitigated to the greatest extent possible.",not included,0.8133152216672898,132
b190ec06738b863b207436a5be3e926598608211,preprocessed,citation,citation,semantic_scholar,arXiv.org,2020.0,semantic_scholar,fundamental issues regarding uncertainties in artificial neural networks,https://www.semanticscholar.org/paper/b190ec06738b863b207436a5be3e926598608211,"Artificial Neural Networks (ANNs) implement a specific form of multi-variate extrapolation and will generate an output for any input pattern, even when there is no similar training pattern. Extrapolations are not necessarily to be trusted, and in order to support safety critical systems, we require such systems to give an indication of the training sample related uncertainty associated with their output. Some readers may think that this is a well known issue which is already covered by the basic principles of pattern recognition. We will explain below how this is not the case and how the conventional (Likelihood estimate of) conditional probability of classification does not correctly assess this uncertainty. We provide a discussion of the standard interpretations of this problem and show how a quantitative approach based upon long standing methods can be practically applied. The methods are illustrated on the task of early diagnosis of dementing diseases using Magnetic Resonance Imaging.",not included,0.8154128462076187,133
de3be210ad49b8f577c84d6558c459afd6182a30,preprocessed,citation,citation,semantic_scholar,Inroads,2020.0,semantic_scholar,broadening artificial intelligence education in k-12,https://www.semanticscholar.org/paper/de3be210ad49b8f577c84d6558c459afd6182a30,"M from steam and waterpower, electricity and assembly lines, and computerization, to the adoption of cyber-physical systems, the Internet of Things and the Internet of Systems executed as intelligent machines, the world now dives into a new era called by some the Fourth Industrial Revolution. New discussions are conducted regarding what implications this has for the education of today’s generation—and that of those in the decades to come. As artificial intelligence (AI) branching from computer science (CS) is becoming more ubiquitous and seamless behind the scene in our daily life, many countries have dedicated significant amounts of resources in order to fuel research to maximize their leverage of this continually developing technology. This renewed interest in AI has sparked discourse about the importance of AI knowledge, concepts, and computational skills for young people, and about the introduction of CS education in K-12. This article aims to reshape the concepts of AI through the lens of historical development in computing industry and education, and to uncover a new direction for AI education in K-12 around the globe.",not included,0.8546511858701706,134
e2ad29298bb06631725ae2436c3a9fd5bc2a78be,preprocessed,citation,citation,semantic_scholar,BMJ Open,2020.0,semantic_scholar,enablers and barriers to the implementation of socially assistive humanoid robots in health and social care: a systematic review,https://www.semanticscholar.org/paper/e2ad29298bb06631725ae2436c3a9fd5bc2a78be,"Objectives Socially assistive humanoid robots are considered a promising technology to tackle the challenges in health and social care posed by the growth of the ageing population. The purpose of our study was to explore the current evidence on barriers and enablers for the implementation of humanoid robots in health and social care. Design Systematic review of studies entailing hands-on interactions with a humanoid robot. Setting From April 2018 to June 2018, databases were searched using a combination of the same search terms for articles published during the last decade. Data collection was conducted by using the Rayyan software, a standardised predefined grid, and a risk of bias and a quality assessment tool. Participants Post-experimental data were collected and analysed for a total of 420 participants. Participants comprised: older adults (n=307) aged ≥60 years, with no or some degree of age-related cognitive impairment, residing either in residential care facilities or at their home; care home staff (n=106); and informal caregivers (n=7). Primary outcomes Identification of enablers and barriers to the implementation of socially assistive humanoid robots in health and social care, and consequent insights and impact. Future developments to inform further research. Results Twelve studies met the eligibility criteria and were included. None of the selected studies had an experimental design; hence overall quality was low, with high risks of biases. Several studies had no comparator, no baseline, small samples, and self-reported measures only. Within this limited evidence base, the enablers found were enjoyment, usability, personalisation and familiarisation. Barriers were related to technical problems, to the robots’ limited capabilities and the negative preconceptions towards the use of robots in healthcare. Factors which produced mixed results were the robot’s human-like attributes, previous experience with technology and views of formal and informal carers. Conclusions The available evidence related to implementation factors of socially assistive humanoid robots for older adults is limited, mainly focusing on aspects at individual level, and exploring acceptance of this technology. Investigation of elements linked to the environment, organisation, societal and cultural milieu, policy and legal framework is necessary. PROSPERO registration number CRD42018092866.",not included,0.8554961800575256,135
9847861d1fa4a81ce1c94ebcd4f40e4a3eb4d04f,preprocessed,citation,citation,semantic_scholar,Indian Journal of Law and Technology,2020.0,semantic_scholar,"recommender systems and autonomy: a role for regulation of design, rights, and transparency",https://www.semanticscholar.org/paper/9847861d1fa4a81ce1c94ebcd4f40e4a3eb4d04f,"Recommender systems are now widely deployed across multiple dimensions of the digital reality that increasingly shapes our lives. In doing so, they mould individual thoughts and actions and can affect individual and collective autonomy. In this paper we first discuss how the ubiquitous exercise of ‘soft’ power by recommender systems on individual users presents interference into individual autonomy and its legal dimensions, expressed through collective and individual self-determination, democratic values and institutions, as well as individual human rights and freedoms. We then argue that this exercise of power over individual and collective destinies necessitates regulatory action to establish an appropriate system of checks and balances on recommender systems and their creators. Utilising a bottom-up approach, we look at the fundamental aspects of a recommender system’s design and functioning that shape the impact these algorithms have on individual autonomy. On the basis of this, we identify three key areas where regulation can be targeted in order to empower users and address current power imbalances - (1) algorithmic design, (2) data protection rights, and (3) transparency and oversight. We map the key questions and options for future regulatory action in each of these domains, highlighting the decisions and competing interests that regulators will need to consider. We conclude by discussing the policy implications of this mapping of the debate and the relevance they have for the future of recommender systems regulation.",not included,0.8145611852407455,136
4d5f145f5cdf2f01f2b3a422a6ff4558fd7ebf98,preprocessed,citation,citation,semantic_scholar,OECD Working Papers on Public Governance,2019.0,semantic_scholar,"hello, world",https://www.semanticscholar.org/paper/4d5f145f5cdf2f01f2b3a422a6ff4558fd7ebf98,"Artificial Intelligence (AI) is an area of research and technology application that can have a significant impact on public policies and services in many ways. In just a few years, it is expected that the potential will exist to free up nearly one-third of public servants’ time, allowing them to shift from mundane tasks to high-value work. Governments can also use AI to design better policies and make better decisions, improve communication and engagement with citizens and residents, and improve the speed and quality of public services. While the potential benefits of AI are significant, attaining them is not an easy task. Government use of AI trails that of the private sector; the field is complex and has a steep learning curve; and the purpose of, and context within, government are unique and present a number of challenges.",not included,0.8303640842437744,137
d45ef2d32b09387970a2712de992b5fa531938a0,preprocessed,citation,citation,semantic_scholar,DELITE,2024.0,semantic_scholar,a hybrid human-ai approach for argument map creation from transcripts,https://www.semanticscholar.org/paper/d45ef2d32b09387970a2712de992b5fa531938a0,"In order to overcome challenges of traditional deliberation approaches that often silo information exchange between synchronous and asynchronous modes therefore hindering effective deliberation, we present a hybrid framework combining Large Language Models (LLMs) and human-in-the-loop curation to generate argument maps from deliberation transcripts. This approach aims to enhance the efficiency and quality of the generated argument maps, promote transparency, and connect the asynchronous and synchronous deliberation modes. Finally, we outline a realistic deliberation scenario where this process can be successfully integrated.",not included,0.8474408477544785,138
484b1efe7a4b9c0bb946c50c69ed9aad111469f3,preprocessed,citation,citation,semantic_scholar,,2023.0,semantic_scholar,bsa comments on supporting safe and responsible artificial intelligence in australia,https://www.semanticscholar.org/paper/484b1efe7a4b9c0bb946c50c69ed9aad111469f3,"BSA is the leading advocate for the global software industry. BSA members create technology solutions that power other businesses, including cloud storage services, customer relationship management software, human resources management programs, identity management services, security solutions, and collaboration software. Our members are on the leading edge of providing AIenabled products and services, and tools used by others in the development of AI systems and applications. As a result, they have unique insights into the technology’s tremendous potential to spur digital transformation and the policies that can best support the responsible use of AI.",not included,0.832381296157837,139
d4435fd7abcdd9830599079867c94e826251eb36,preprocessed,citation,citation,semantic_scholar,,2022.0,semantic_scholar,psychosocial intervention,https://www.semanticscholar.org/paper/d4435fd7abcdd9830599079867c94e826251eb36,"Domestic abuse victim risk assessment is crucial for providing victims with the correct level of support. However, it has been shown that the approach currently taken by most UK police forces, the Domestic Abuse, Stalking",not included,0.8011098295450211,140
dbf09a02b23ecc2fcfcd4628713b73aeec98f451,preprocessed,citation,citation,semantic_scholar,Annals of Robotics and Automation,2021.0,semantic_scholar,"artificial intelligence: explainability, ethical issues and bias",https://www.semanticscholar.org/paper/dbf09a02b23ecc2fcfcd4628713b73aeec98f451,"Artifi cial Intelligence (AI) is a topic of growing signifi cance for businesses as well as academic researchers. Its applications encompass many domains such as healthcare [1], fi nance [2] and manufacturing [3]. Artifi cial intelligence is represented in general purpose smart technologies that give the machines the ability to imitate human intelligence and perform complex tasks. Communicating with the machine using natural language, operating autonomous and adaptive assembly lines, and predicting supply chain demand stock market fl uctuations are all examples of AI implementation in industrial contexts. Investigating the literature and despite AI’s broad range of applications in several industry domains, till now, there is no agreement on a unifi ed defi nition of artifi cial intelligence. For instance Simmons and Chappell [4], defi ne AI as the term that “denotes behaviour of a machine which, if a human behaves in the same way, is considered intelligent”. Also Kumar, et al. [5], describe AI as the “A system’s ability to interpret external data correctly, to learn from such data, and to use those learnings to achieve specifi c goals and tasks through fl exible adaptation”. Consequently, it can be inferred that AI is not confi ned to limited number of applications, but rather it is considered as a pervasive economic, societal, and organizational phenomenon.",not included,0.8023776173591614,141
a01495afe37b54088a734891db2e9d723bccafb7,preprocessed,citation,citation,semantic_scholar,E3S Web of Conferences,2021.0,semantic_scholar,assessment of risks and threats to the development of artificial intelligence technologies,https://www.semanticscholar.org/paper/a01495afe37b54088a734891db2e9d723bccafb7,"The paper analyses the risks and threats caused by the development and implementation of artificial intelligence technologies. We believe that these risks and threats need research in the long term. Importantly, the use of intelligent information systems has a twofold effect: it can lead to both positive and negative results. The paper also considers the influence of artificial intelligence technologies on the various activities. It proposes a classification of risks and threats caused by the development and implementation of artificial intelligence technologies by the main spheres of human activity.",not included,0.8071766257286072,142
07b6b6a349a9644ffb64d09fd442d97bbc943517,preprocessed,citation,citation,semantic_scholar,IEEE technology & society magazine,2021.0,semantic_scholar,"artificial intelligence for a fair, just, and equitable world",https://www.semanticscholar.org/paper/07b6b6a349a9644ffb64d09fd442d97bbc943517,"From the 1970s onward, we started to dream of the leisure society in which, thanks to technological progress and consequent increase in productivity, working hours would be minimized and we would all live in abundance We all could devote our time almost exclusively to personal relationships, contact with nature, sciences, the arts, playful activities, and so on Today, this utopia seems more unattainable than it did then Since the 21st century, we have seen inequalities increasingly accentuated: of the increase in wealth in the United States between 2006 and 2018, adjusted for inflation and population growth, more than 87% went to the richest 10% of the population, and the poorest 50% lost wealth [1] Following the crisis of 2008, social inequalities, rights violations, planetary degradation, and the climate emergency worsened and increased (see [2] ) In 2019, the world’s 2153 billionaires had more wealth than 4 6 billion people [3] The World Bank estimates that COVID-19 will push up to 150 million people into extreme poverty [4]",not included,0.8053445428609848,143
0bc528a9fbb004993ce1f5d13c4229b3ea078c34,preprocessed,citation,citation,semantic_scholar,,2021.0,semantic_scholar,a toolkit to enable the design of trustworthy,https://www.semanticscholar.org/paper/0bc528a9fbb004993ce1f5d13c4229b3ea078c34,"Technological progress in artificial intelligence (AI) and machine learning (ML) has an enormous impact on our society, economy and environment. And although the urgent need for creating sustainable and ethical AI technology is admitted, there exists a lack of design tools and expertise to facilitate this advancement. This study investigates how to help designers design for the value of trust in AI systems. A literature review unveiled a myriad of ethical AI principles as well as gathered existing tools addressing the research area. Iterative reviews together with an expert on trust in technology evaluated these guidelines and a toolkit prototype containing 29 design principles had been created. Through multiple participatory design workshops the next iteration of the toolkit was co-designed in collaboration with design professionals. The result is an iterated toolkit comprising 16 principles relevant in the design for trust in AI systems, and providing tool suggestions for each principle.",included,0.8235145151615143,144
8dab76f134da8ddd2c7d3f66d0e639ea46f793b8,preprocessed,citation,citation,semantic_scholar,,2020.0,semantic_scholar,exploitation of expert system in identifying organizational ethics through controlling decision making process,https://www.semanticscholar.org/paper/8dab76f134da8ddd2c7d3f66d0e639ea46f793b8,"Article history: Received: October 24, 2019 Received in revised format: November 28 2019 Accepted: December 22, 2019 Available online: December 22, 2019 This paper examines how expert system may negatively or positively influence ethical based decision making process in an organization. Expert system ethical characteristics are chosen; including lack of human intelligence, lack of emotions, accidental bias and lack of values. Depending on quantitative approach; and through distributing a questionnaire on (132) GM, Deputy GM, project manager and officer, it appeared that expert systems' characteristics negatively influenced on the degree of ethics within the organizational setting. According to analysis, it appears that lack of values and human intelligence were among the characteristics that hinder the ethical stream adoption within an organization leading to ethics problems and malfunctioning on the decision making process. On the other hand, lack of emotions appeared to have good impact on the ethical efforts within an organization. Researchers recommended taking extra measures of surveillance in terms of ethics for individuals who are supposed to develop and monitor expert systems. © 2020 by the authors; licensee Growing Science, Canada",not included,0.826026663184166,145
4f4a2f5c9e1519db0c64e8cd2fb015ea5e3e4d0b,preprocessed,citation,citation,semantic_scholar,Future Internet,2025.0,semantic_scholar,high-risk ai systems—lie detection application,https://www.semanticscholar.org/paper/4f4a2f5c9e1519db0c64e8cd2fb015ea5e3e4d0b,"Integrating artificial intelligence into border control systems may help to strengthen security and make operations more efficient. For example, the emerging application of artificial intelligence for lie detection when inspecting passengers presents significant opportunities for future implementation. However, as it makes use of technology that is associated with artificial intelligence, the system is classified as high risk, in accordance with the EU AI Act and, therefore, must adhere to rigorous regulatory requirements to mitigate potential risks. This manuscript distinctly amalgamates the technical, ethical, and legal aspects, thereby offering an extensive examination of the AI-based lie detection systems utilized in border security. This academic paper is uniquely set apart from others because it undertakes a thorough investigation into the categorization of these emerging technologies in terms of the regulatory framework established by the EU AI Act, which classifies them as high risk. It further makes an assessment of practical case studies, including notable examples such as iBorderCtrl and AVATAR. This in-depth analysis seeks to emphasize not only the enormous challenges ahead for practitioners but also the progress made in this emerging field of study. Furthermore, it seeks to investigate threats, vulnerabilities, and privacy concerns associated with AI, while providing security controls to address difficulties related to lie detection. Finally, we propose a framework that encompasses the EU AI Act’s principles and serves as a foundation for future approaches and research projects. By analyzing current methodologies and considering future directions, the paper aims to provide a comprehensive understanding of the viability and consequences of deploying AI lie detection capabilities in border control.",included,0.8043087244033813,146
c1b237f25358a315527c64fc097e97c1d18edd39,preprocessed,citation,citation,semantic_scholar,Systems,2025.0,semantic_scholar,exploring key considerations for artificial intelligence robots in home healthcare using the unified theory of acceptance and use of technology and the fuzzy analytical hierarchy process method,https://www.semanticscholar.org/paper/c1b237f25358a315527c64fc097e97c1d18edd39,"Most countries face declining birth rates and an aging population, which makes the persistent healthcare labor shortage a pressing challenge. Introducing artificial intelligence (AI) robots into home healthcare could help address these issues. Exploring the primary considerations for integrating AI robots in home healthcare has become an urgent topic. However, previous studies have not systematically examined the factors influencing elderly individuals’ adoption of home healthcare AI robots, hindering an understanding of their acceptance and adoption. Furthermore, traditional methods overlook the relative importance of each consideration and cannot manage the ambiguity inherent in subjective human cognition, potentially leading to biased decision-making. To address these limitations, this study employs the unified theory of acceptance and use of technology (UTAUT) as a theoretical framework, integrating the modified Delphi method (MDM) and the fuzzy analytical hierarchy process (FAHP) to identify the key considerations. The research determined the order of importance of four evaluation criteria and fourteen evaluation sub-criteria, revealing that customization, accompany, and subjective norms are key factors that influence elderly individuals’ adoption of home healthcare AI robots.",not included,0.8062968552112579,147
d821b6252f111fe9263a49abdc399b63683df2d2,preprocessed,citation,citation,semantic_scholar,IEEE International Conference on Bioinformatics and Biomedicine,2024.0,semantic_scholar,"ethics of artificial intelligence: challenges, opportunities and future prospects",https://www.semanticscholar.org/paper/d821b6252f111fe9263a49abdc399b63683df2d2,"Artificial Intelligence (AI) has rapidly transformed numerous sectors, including healthcare, justice, and commerce, providing substantial benefits while also raising complex ethical questions: this article explores the main ethical challenges associated with AI, focusing on issues such as algorithmic bias, data privacy and security, transparency, and accountability. The importance of Explainable Artificial Intelligence (XAI) in enhancing the interpretability of algorithmic decisions is emphasized, particularly in healthcare, where model opacity can have a direct impact on patient outcomes. The paper further examines regulatory frameworks and ethical guidelines, including the European Union’s AI Act, advocating for a multidisciplinary approach that combines innovation and accountability to develop AI systems that respect human rights and foster user trust. In conclusion, the article underscores the need for interdisciplinary collaboration and adaptable regulations to ensure the ethical development of AI, promoting fairness and transparency.",not included,0.826026663184166,148
6242be8ef591473fd1e66283f9978688b72bb324,preprocessed,citation,citation,semantic_scholar,International Journal of Scientific Research and Modern Technology (IJSRMT),2024.0,semantic_scholar,"navigating the dual nature of deepfakes: ethical, legal, and technological perspectives on generative artificial intelligence ai) technology",https://www.semanticscholar.org/paper/6242be8ef591473fd1e66283f9978688b72bb324,"The rapid development of deepfake technology has opened up a range of groundbreaking opportunities while also introducing significant ethical challenges. This paper explores the complex impacts of deepfakes by drawing from fields such as computer science, ethics, media studies, and law. Through a multidisciplinary approach, we examine the technological foundations, uses, and societal effects of deepfakes. Our analysis includes case studies, expert interviews, and a thorough review of existing literature to highlight the dual nature of deepfakes—showcasing their potential benefits in entertainment and education, while also addressing the risks of misinformation and privacy violations. This study emphasizes the urgent need for improved detection methods, ethical guidelines, and strong legal frameworks to address the issues created by deepfakes. It calls for enhanced digital literacy and global cooperation to ensure that the advantages of generative AI are harnessed responsibly, while its inherent risks are minimized. The findings underscore the importance of effective detection strategies, ethical considerations, and legislative reforms to ensure deepfake technology is used in ways that benefit society.",not included,0.8288842886686325,149
a28eee3d30ea401fefca8a07332c1767b087bc7c,preprocessed,citation,citation,semantic_scholar,International Conference on Communication and Network Security,2024.0,semantic_scholar,the ethics of applying artificial intelligence (ai) for communication governance,https://www.semanticscholar.org/paper/a28eee3d30ea401fefca8a07332c1767b087bc7c,"This research paper explores the ethical considerations and implementations surrounding the use of artificial intelligence (AI) in communication governance. In an era marked by the increasing reliance on AI-driven technologies for communication, this study investigates the ethical Implementations, ethics, challenges, and potential benefits associated with AI's role in shaping and regulating information flow. Drawing upon a wide range of literature and discusses a comprehensive overview of applying AI in communication governance.",not included,0.8351606994867324,150
a27ce898e4103a8ea6bf20a7379ccdcbe2fe8111,preprocessed,citation,citation,semantic_scholar,Business and Politics,2024.0,semantic_scholar,investigating the politics and content of us state artificial intelligence legislation,https://www.semanticscholar.org/paper/a27ce898e4103a8ea6bf20a7379ccdcbe2fe8111,"
 The rapid emergence of artificial intelligence (AI) technology and its application by businesses has created a potential need for governmental regulation. While the federal government of the United States has largely sidestepped the issue of crafting law dictating limitations and expectations regarding the use of AI technology, US state legislatures have begun to take the lead in this area. Nonetheless, we know very little about how state legislatures have approached the design, pursuit, and adoption of AI policy and whether traditional political fault lines have manifested themselves in the AI issue area. Here, we gather data on the state-level adoption of AI policy, as well as roll call voting on AI bills (classified on the basis of consumer protection versus economic development), by state legislatures and analyze the political economy of AI legislation. We find that rising unemployment and inflation are negatively associated with a state’s AI policymaking. With respect to individual legislator support, we find that liberal lawmakers and Democrats are more likely to support bills establishing consumer protection requirements on AI usage. The results suggest that economic concerns loom large with AI and that traditional political fault lines may be establishing themselves in this area.",not included,0.8052007794380188,151
37eb0c2177acf85c74ace80a9365d610523bc64b,preprocessed,citation,citation,semantic_scholar,Digit. Gov. Res. Pract.,2024.0,semantic_scholar,introduction to the issue on artificial intelligence in the public sector: risks and benefits of ai for governments,https://www.semanticscholar.org/paper/37eb0c2177acf85c74ace80a9365d610523bc64b,"Artificial Intelligence (AI) is increasingly adopted by public sector organizations to provide better public services and to transform their internal processes. AI is now considered a key enabler for digital innovation and transformation in the public sector. However, AI is still relatively a new research area in the field of digital government. The term, AI, captures a wide range of technologies, techniques, and tools such as machine/deep learning, natural language processing, robotics, computer vision, and more recently Generative AI. While these AI technologies afford different applications and benefits in the government context, they also create social, ethical, and legal challenges. These challenges require solutions combining both technical (e.g., data and algorithmic solutions to minimize bias) and institutional (e.g., governance structures and processes) mechanisms. The special issue is a collection of articles that contribute to a better understanding of the issues associated with AI deployment in different areas of government operations. They cover AI applications in the areas of emergency response, policy analysis, public bids, and citizen participation. The contributions also address the challenge of realizing a legal transparency regime for AI in government and the effect of AI in bureaucratic decision-making.",not included,0.8033411532640458,152
e0c176370fcca6f8d11286f922c2154eaa69fc34,preprocessed,citation,citation,semantic_scholar,Business and Society Review,2024.0,semantic_scholar,what ethics can say on artificial intelligence: insights from a systematic literature review,https://www.semanticscholar.org/paper/e0c176370fcca6f8d11286f922c2154eaa69fc34,"The abundance of literature on ethical concerns regarding artificial intelligence (AI) highlights the need to systematize, integrate, and categorize existing efforts through a systematic literature review. The article aims to investigate prevalent concerns, proposed solutions, and prominent ethical approaches within the field. Considering 309 articles from the beginning of the publications in this field up until December 2021, this systematic literature review clarifies what the ethical concerns regarding AI are, and it charts them into two groups: (i) ethical concerns that arise from the design of AI and (ii) ethical concerns that arise from human–AI interactions. The analysis of the obtained sample highlights the most recurrent ethical concerns. Finally, it exposes the main proposals of the literature to handle the ethical concerns according to the main ethical approaches. It interprets the findings to lay the foundations for future research on the ethics of AI.",not included,0.8437141388654709,153
70a82befa02fea869b427024ec12146b719f03f6,preprocessed,citation,citation,semantic_scholar,Frontiers in Pharmacology,2023.0,semantic_scholar,mapping the regulatory landscape of ai in healthcare in africa,https://www.semanticscholar.org/paper/70a82befa02fea869b427024ec12146b719f03f6,"Introduction: Artificial intelligence (AI)-enhanced technology has seen unprecedented expansion in the recent past. This growth brings with it huge opportunities for the positive transformation of the economy, business, healthcare, and society. However, a critical question is whether, and to what extent, regulatory measures and mechanisms have been implemented to safeguard its design, development, and deployment. This paper offers a scoping exercise that maps the regulatory landscape of AI in healthcare (including health research) in certain African countries. Methods: This research is conducted across 12 African countries: Botswana, Cameroon, The Gambia, Ghana, Kenya, Malawi, Nigeria, Rwanda, South Africa, Tanzania, Uganda, and Zimbabwe. As limited specific AI legislation is found in these African countries, and because AI is informed by ancillary regulatory frameworks, we include data protection, digital health, consumer protection, and intellectual property in our research. A scoping review method was applied with a manual search of digital libraries with search terms customised for each repository consisting of core search terms for the various topics, including, among others, “law,” “regulation,” “artificial intelligence,” “data protection,” “intellectual property,” and “digital health”. Results and discussion: Analysis of the data demonstrated that while in the African countries under investigation there is no sui generis AI regulation, recent developments were found in areas that inform AI adoption, including in digital health, data protection, consumer protection, and intellectual property. Our findings highlight the fragmentation of the African AI regulatory landscape and illustrate the importance of continued AI regulatory development to ensure that Africa is well positioned for future AI adoption in health.",not included,0.8133152216672898,154
c524b16d98ccee9b433ed9364704426e235da1fe,preprocessed,citation,citation,semantic_scholar,Internet Research,2023.0,semantic_scholar,"ai governance: themes, knowledge gaps and future agendas",https://www.semanticscholar.org/paper/c524b16d98ccee9b433ed9364704426e235da1fe,"PurposeFollowing the surge of documents laying out organizations' ethical principles for their use of artificial intelligence (AI), there is a growing demand for translating ethical principles to practice through AI governance (AIG). AIG has emerged as a rapidly growing, yet fragmented, research area. This paper synthesizes the organizational AIG literature by outlining research themes and knowledge gaps as well as putting forward future agendas.Design/methodology/approachThe authors undertake a systematic literature review on AIG, addressing the current state of its conceptualization and suggesting future directions for AIG scholarship and practice. The review protocol was developed following recommended guidelines for systematic reviews and the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA).FindingsThe results of the authors’ review confirmed the assumption that AIG is an emerging research topic with few explicit definitions. Moreover, the authors’ review identified four themes in the AIG literature: technology, stakeholders and context, regulation and processes. The central knowledge gaps revealed were the limited understanding of AIG implementation, lack of attention to the AIG context, uncertain effectiveness of ethical principles and regulation, and insufficient operationalization of AIG processes. To address these gaps, the authors present four future AIG agendas: technical, stakeholder and contextual, regulatory, and process. Going forward, the authors propose focused empirical research on organizational AIG processes, the establishment of an AI oversight unit and collaborative governance as a research approach.Research limitations/implicationsTo address the identified knowledge gaps, the authors present the following working definition of AIG: AI governance is a system of rules, practices and processes employed to ensure an organization's use of AI technologies aligns with its strategies, objectives, and values, complete with legal requirements, ethical principles and the requirements set by stakeholders. Going forward, the authors propose focused empirical research on organizational AIG processes, the establishment of an AI oversight unit and collaborative governance as a research approach.Practical implicationsFor practitioners, the authors highlight training and awareness, stakeholder management and the crucial role of organizational culture, including senior management commitment.Social implicationsFor society, the authors review elucidates the multitude of stakeholders involved in AI governance activities and complexities related to balancing the needs of different stakeholders.Originality/valueBy delineating the AIG concept and the associated research themes, knowledge gaps and future agendas, the authors review builds a foundation for organizational AIG research, calling for broad contextual investigations and a deep understanding of AIG mechanisms. For practitioners, the authors highlight training and awareness, stakeholder management and the crucial role of organizational culture, including senior management commitment.",not included,0.8154128462076187,155
736f199a1df581800243a7b4908dff421b3ea6e4,preprocessed,citation,citation,semantic_scholar,Metaverse,2023.0,semantic_scholar,"life, death, and ai: exploring digital necromancy in popular culture—ethical considerations, technological limitations, and the pet cemetery conundrum",https://www.semanticscholar.org/paper/736f199a1df581800243a7b4908dff421b3ea6e4,"This article explores the rise of generative AI, particularly ChatGPT, and the combination of large language models (LLM) with robotics, exemplified by Ameca the Robot. It addresses the need to study the ethical considerations and potential implications of digital necromancy, which involves using AI to reanimate deceased individuals for various purposes. Reasons for desiring to engage with a disembodied or bodied replica of a person include the preservation of memories, emotional closure, cultural heritage and historical preservation, interacting with idols or influential figures, educational and research purposes, and creative expression and artistic endeavors. As such, this article examines historical examples of the practice in hologram concerts, CGI characters, and others in order to analyze the ethical concerns related to privacy, consent, and commercial gain. It delves into the challenges of accurately representing individual personalities, misrepresenting cultural context, and the limitations of available data. Furthermore, it explores the Pet Cemetery conundrum and its impact on the grieving process, mental health, and the moral implications of using AI to generate interactions with the deceased. By contemplating future use cases like interactive virtual assistants and realistic historical reenactments, the article highlights the importance of addressing ethical implications as these technologies continue to advance and contributes to the discourse on the responsible and ethical use of generative AI, LLM, and robotics in the context of digital resurrection, calling for ongoing discussions and considerations of AI rights, social dynamics, and the grieving process.",not included,0.8546511858701706,156
b0a332bd6af0519e59a7d4a319af3155ff603edd,preprocessed,citation,citation,semantic_scholar,MEDAAD,2023.0,semantic_scholar,assessing the ethical implications of artificial intelligence integration in media production and its impact on the creative industry,https://www.semanticscholar.org/paper/b0a332bd6af0519e59a7d4a319af3155ff603edd,"The addition of artificial intelligence (AI) to the media industry has transformed the creative industry, opening up new opportunities for creation, but it has also created significant ethical challenges. The main problem lies in the disruption that AI is causing to traditional notions of authorship, creativity and collaboration, as well as the bias and intellectual property issues associated with AI-generated products. The aim of this study is to examine these ethical issues when considering the benefits of AI in media quality, innovation and democracy Through case studies and comparative analysis this study identifies areas especially with AI enhancing and empowering creative workflows. The results show that although AI improves productivity by up to 75%, the challenges of bias, creativity and intellectual property remain unresolved Various recommendations are made for AI improve internal awareness, enhance human-AI collaboration, and streamline regulatory frameworks to ensure a balanced integration of AI in the creative industries.",not included,0.8554961800575256,157
2c6a9c4edec81788e9467e97f998fb4b358fece9,preprocessed,citation,citation,semantic_scholar,AKSELERASI: Jurnal Ilmiah Nasional,2022.0,semantic_scholar,politik hukum pidana tentang pengaturan tindak pidana di indonesia dalam kasus narkotika,https://www.semanticscholar.org/paper/2c6a9c4edec81788e9467e97f998fb4b358fece9,"This study was conducted with the aim of being able to analyze legal politics in regulating illegal drugs and narcotics in Indonesia. This research will be carried out using a normative legal approach and will focus on an inventory of positive law, legal findings in cases in concreto, legal comparisons, legal principles and doctrines, legal history, and legal systematics. The data in this study came from various primary and secondary legal data. The primary legal data in this article comes from various laws and regulations that are still relevant to the subject of discussion, such as Law Number 35 of 2009 concerning Narcotics. The secondary legal data is in the form of various understandings of law in primary data. The results of this study found that narcotics legal politics is the core of the formation of a legal system that contains laws and regulations that will regulate various criminal acts. In Articles 111 and 112, there is a fairly severe prison sentence, in which imprisonment for a minimum of 4 years and a maximum of 20 years, up to the death penalty.",not included,0.828378900885582,158
f123ef4e08b0eb5e4f1732a9ef35011d1bccf4df,preprocessed,citation,citation,semantic_scholar,arXiv.org,2022.0,semantic_scholar,"tackling problems, harvesting benefits - a systematic review of the regulatory debate around ai",https://www.semanticscholar.org/paper/f123ef4e08b0eb5e4f1732a9ef35011d1bccf4df,"How to integrate an emerging and all-pervasive technology such as AI into the structures and operations of our society is a question of contemporary politics, science and public debate. It has produced a considerable amount of international academic literature from different disciplines. This article analyzes the academic debate around the regulation of artificial intelligence (AI). The systematic review comprises a sample of 73 peer-reviewed journal articles published between January 1st, 2016, and December 31st, 2020. The analysis concentrates on societal risks and harms, questions of regulatory responsibility, and possible adequate policy frameworks, including risk-based and princi-ple-based approaches. The main interests are proposed regulatory approaches and instruments. Various forms of interventions such as bans, approvals, standard-setting, and disclosure are presented. The assessments of the included papers indicate the complexity of the field, which shows its prematurity and the remaining lack of clarity. By pre-senting a structured analysis of the academic debate, we contribute both empirically and conceptually to a better understanding of the nexus of AI and regulation and the underlying normative decisions. A comparison of the scientific proposals with the proposed European AI regulation illustrates the specific approach of the regulation, its strengths and weaknesses.",not included,0.8474408477544785,159
56c6d0df1d8742eeb9597d3b6bc7d8631cb98515,preprocessed,citation,citation,semantic_scholar,Frontiers of Computer Science,2022.0,semantic_scholar,governance of responsible ai: from ethical guidelines to cooperative policies,https://www.semanticscholar.org/paper/56c6d0df1d8742eeb9597d3b6bc7d8631cb98515,"The increasingly pervasive role of Artificial Intelligence (AI) in our societies is radically changing the way that social interaction takes place within all fields of knowledge. The obvious opportunities in terms of accuracy, speed and originality of research are accompanied by questions about the possible risks and the consequent responsibilities involved in such a disruptive technology. In recent years, this twofold aspect has led to an increase in analyses of the ethical and political implications of AI. As a result, there has been a proliferation of documents that seek to define the strategic objectives of AI together with the ethical precautions required for its acceptable development and deployment. Although the number of documents is certainly significant, doubts remain as to whether they can effectively play a role in safeguarding democratic decision-making processes. Indeed, a common feature of the national strategies and ethical guidelines published in recent years is that they only timidly address how to integrate civil society into the selection of AI objectives. Although scholars are increasingly advocating the necessity to include civil society, it remains unclear which modalities should be selected. If both national strategies and ethics guidelines appear to be neglecting the necessary role of a democratic scrutiny for identifying challenges, objectives, strategies and the appropriate regulatory measures that such a disruptive technology should undergo, the question is then, what measures can we advocate that are able to overcome such limitations? Considering the necessity to operate holistically with AI as a social object, what theoretical framework can we adopt in order to implement a model of governance? What conceptual methodology shall we develop that is able to offer fruitful insights to governance of AI? Drawing on the insights of classical pragmatist scholars, we propose a framework of democratic experimentation based on the method of social inquiry. In this article, we first summarize some of the main points of discussion around the potential societal, ethical and political issues of AI systems. We then identify the main answers and solutions by analyzing current national strategies and ethics guidelines. After showing the theoretical and practical limits of these approaches, we outline an alternative proposal that can help strengthening the active role of society in the discussion about the role and extent of AI systems.",not included,0.8405167400836945,160
fd8d42fa78292844749f999853ffb5fd1ff5b407,preprocessed,citation,citation,semantic_scholar,Frontiers in Human Dynamics,2022.0,semantic_scholar,how should public administrations foster the ethical development and use of artificial intelligence? a review of proposals for developing governance of ai,https://www.semanticscholar.org/paper/fd8d42fa78292844749f999853ffb5fd1ff5b407,"Recent advances in AI raise questions about its social impacts and implementation. In response, governments and public administrations seek to develop adequate governance frameworks to mitigate risks and maximize the potential of AI development and use. Such work largely deals with questions of how challenges and risks should be managed, which values and goals should be pursued, and through which institutional mechanisms and principles these goals could be achieved. In this paper, we conduct a systematic review of the existing literature on the development of AI governance for public administration. The article describes principles and means by which public administrations could guide and steer AI developers and users in adopting ethical and responsible practices. The reviewed literature indicates a need for public administrations to move away from top-down hierarchical governance principles and adopt forms of inclusive policy-making to ensure the actionability of ethical and responsibility principles in the successful governance of AI development and use. By combining the results, we propose a CIIA (Comprehensive, Inclusive, Institutionalized, and Actionable) framework that integrates the key aspects of the proposed development solutions into an ideal typical and comprehensive model for AI governance.",included,0.832381296157837,161
90af67551df118ee45c83b003c5739dc03b0ca45,preprocessed,citation,citation,semantic_scholar,Journal of Trends and Challenges in Artificial Intelligence,2024.0,semantic_scholar,artificial intelligence in the indian judiciary: a systematic analysis of potential applications and challenges in addressing case backlogs,https://www.semanticscholar.org/paper/90af67551df118ee45c83b003c5739dc03b0ca45,"This paper examines the potential role of artificial intelligence (AI) in addressing the critical issue of case backlogs plaguing the Indian judiciary. With over 40 million pending cases, Indian courts face a crisis of delayed justice that undermines the rule of law and public trust in the legal system. This research analyzes how AI technologies could be leveraged to streamline court processes, enhance judicial productivity, and ultimately reduce case pendency. Drawing on examples of AI implementation in judiciaries worldwide, the paper explores applications in case management, legal research, document review, and decision support. While acknowledging the transformative potential of AI, the analysis also grapples with significant challenges around data quality, algorithmic bias, privacy concerns, and the need to preserve human judgment in judicial decision-making. The paper argues that a carefully implemented AI strategy, combined with institutional reforms, could significantly improve efficiency in the Indian courts. However, any technological solutions must be tailored to the unique context of the Indian legal system and subject to appropriate governance frameworks. The research concludes with recommendations for a phased adoption of AI in the judiciary, emphasizing the need for further empirical study on the impacts of AI on judicial processes and outcomes.",not included,0.8011098295450211,162
ab0ec3c1c42a30eebf2b1104146aaf58539d57fa,preprocessed,citation,citation,semantic_scholar,Data &amp; Policy,2024.0,semantic_scholar,identifying stakeholder motivations in normative ai governance: a systematic literature review for research guidance,https://www.semanticscholar.org/paper/ab0ec3c1c42a30eebf2b1104146aaf58539d57fa,"
 Ethical guidelines and policy documents destined to guide AI innovations have been heralded as the solution to guard us against harmful effects or to increase public value. However, these guidelines and policy documents face persistent challenges. While these documents are often criticized for their abstraction and disconnection from real-world contexts, it also occurs that stakeholders may influence them for political or strategic reasons. While this last issue is frequently acknowledged, there is seldom a means or a method provided to explore it. To address this gap, the paper employs a combination of social constructivism and science & technology studies perspectives, along with desk research, to investigate whether prior research has examined the influence of stakeholder interests, strategies, or agendas on guidelines and policy documents. The study contributes to the discourse on AI governance by proposing a theoretical framework and methodologies to better analyze this underexplored area, aiming to enhance comprehension of the policymaking process within the rapidly evolving AI landscape. The findings underscore the need for a critical evaluation of the methodologies found and a further exploration of their utility. In addition, the results aim to stimulate ongoing critical debates on this subject.",not included,0.8079065144062042,163
dff39cc6d10d6532711b0205e245750d47bb1d90,preprocessed,citation,citation,semantic_scholar,"Lernen, Wissen, Daten, Analysen",2023.0,semantic_scholar,governance of artificial intelligence - a framework towards ethical ai applications,https://www.semanticscholar.org/paper/dff39cc6d10d6532711b0205e245750d47bb1d90,"Artificial intelligence (AI) has extensive potential in changing businesses. Various applications have been identified that are either already implemented, or under development. However, many – especially small and medium-sized – enterprises struggle with the potential problems that AI might cause. Leaders and managers are often willing to implement AI in their companies, but are looking for guidance, how they can ensure that the AI will have no negative impact on customers, employees or their business. To address this area of conflict, a governance framework is presented, which guides the development of AI solutions to address potential ethical challenges. The framework is rooted in the body of knowledge of the information systems discipline – especially in general IT governance frameworks and other proposed governance structures considering AI – and its content has been adapted specific to ethical issues in AI development and usage based on experts ’ insights.",included,0.8500882655382156,164
130deafe4c89877f22ff3475c3119492736cfc17,preprocessed,citation,citation,semantic_scholar,EGOV-CeDEM-ePart-*,2022.0,semantic_scholar,using artificial intelligence for legislation - thinking about and selecting realistic topics,https://www.semanticscholar.org/paper/130deafe4c89877f22ff3475c3119492736cfc17,"Parliaments are currently investigating the use of applications based on artificial intelligence (AI) technologies to perform certain tasks. Reflecting on conceivable tools, fields of application, usage scenarios and needs, it is reasonable to expect AI-induced changes in parliaments. This makes even more peculiar the fact that the introduction of AI in parliaments is a generally under-researched topic. This article contributes to the bridging of this gap by presenting empirical evidence for the future use of AI-based tools and services in the legislation workspace. The data were collected during a brainstorming exercise and a virtual workshop in 2021. The analysis sheds light in the prioritization of AI-based technologies within the parliamentary environment. In the course of the study, the relevance and the priority of more than 210 applications and topics of AI technologies in parliament have been investigated for several parliamentary sectors, including 36 proposals around law-making competencies and procedures, dubbed as “legislation”. The main findings regarding legislation are presented.",not included,0.8215756505727768,165
e38723cfd102520677aa9b872fdc8a7f2f32a64a,preprocessed,citation,citation,semantic_scholar,,,semantic_scholar,"artificial intelligence in e-governance in india: impact, challenges, and opportunities",https://www.semanticscholar.org/paper/e38723cfd102520677aa9b872fdc8a7f2f32a64a,"Artificial Intelligence (AI) has the potential to revolutionize India's administrative environment through the integration of AI into e-governance technologies, offering efficiency, accessibility, and inclusion. This study examines the various effects, difficulties, and prospects brought about by India's deployment of AI in e-governance. In order to better understand how AI may improve e-governance in India, the study looks at how advanced data analytics and predictive modelling can improve decision-making, improve service delivery, and expedite bureaucratic processes. It also covers how chatbots and virtual assistants driven by AI might improve public participation and enable responsive governance. The study explores the difficulties in applying AI to e-governance, including issues with data security, privacy, and algorithmic decision-making bias. It also discusses the digital divide and the necessity of enhancing government officials' capacity to use AI technologies efficiently. The study highlights important chances to use artificial intelligence (AI) in e-governance to solve social issues and promote sustainable development. These potentials include using AI to develop proactive policies, deliver personalized services, and launch focused initiatives in industries like agriculture, healthcare, and education. This study paper offers an in-depth examination of the implications, obstacles, and prospects of artificial intelligence in the process of revolutionizing e-governance in India.",not included,0.8053445428609848,166
9d6ce3cfdfe3859aff1fb1ac0559e4a765fec9d4,preprocessed,citation,citation,semantic_scholar,European journal of technology,2024.0,semantic_scholar,artificial intelligence and humanitarian supply chain resilience: mediating effect of localized logistics capacity,https://www.semanticscholar.org/paper/9d6ce3cfdfe3859aff1fb1ac0559e4a765fec9d4,"Purpose: The study examines the mediating effect of Localized logistics capacity on the association between Artificial intelligence and Humanitarian supply chain resilience among Humanitarian organizations. 
Materials and Methods: A cross-sectional survey and descriptive study involving 88 humanitarian firms in Uganda whose staff involved in relief operations were purposively selected. Data was analyzed using the Partial least squares structural equation modeling to test hypotheses and ascertain the mediating effect. 
Findings: The study indicates a significant indirect effect of Artificial Intelligence (AI) on humanitarian supply chain resilience (HSCR) and a direct impact of Artificial Intelligence on Localized logistics capacity (LLC). The results also confirmed a full mediation effect of LLC on the association between AI and HSCR. 
Implications to theory, Practice and Policy: The present study contributes deeper insights into how humanitarian organizations can develop adaptive capacities to navigate the complex landscape of humanitarian operations since it was established that logistics capacity is a conduit between artificial intelligence and humanitarian supply chain resilience. Managers should adopt artificial intelligence and build strong relationships will local logistics suppliers to achieve humanitarian supply chain resilience practices. Considering that this was a survey, a case study design with semi-structured research tools be used to have an in-depth understanding of the variables under study.",not included,0.8411970287561417,167
d335c28635fc44173df194d69ec4659df8229281,preprocessed,citation,citation,semantic_scholar,Revista Foco,2024.0,semantic_scholar,os direitos humanos e a utilização de inteligência artificial nos processos migratórios internacionais,https://www.semanticscholar.org/paper/d335c28635fc44173df194d69ec4659df8229281,"O presente artigo pretende abordar sobre efetivação dos direitos humanos diante da crescente utilização de ferramentas de inteligência artificial na gestão da migração pelos Estados Nacionais. Para isso, o artigo debate sobre perspectivas históricas, sociológicas e jurídicas da digitalização do mundo, da inteligência artificial e dos direitos humanos na migração internacional. Após, é apresentada uma pesquisa bibliográfica de artigos científicos em inglês feita na plataforma CAPES, utilizando os descritivos “artificial intelligence” AND migration AND “human rights”, que tratam sobre os riscos aos direitos humanos na utilização de programas de inteligência artificial pelos Estados Nacionais na gestão das migrações internacionais. Foram utilizados o método indutivo e a técnica de pesquisa bibliográfica. O artigo conclui pela importância e atualidade do tema, sendo imprescindível considerar os migrantes inseridos na formação da solução de sistemas aplicáveis à gestão de migração e não como simples objetos desses sistemas, garantido os princípios de direitos humanos em todas as fases de manejo da inteligência artificial (ethics by design).",not included,0.8043087244033813,168
c99afde97014dd6325d501845142d4195d58cfd9,preprocessed,citation,citation,semantic_scholar,Revue Internationale de la Croix-Rouge,2024.0,semantic_scholar,lost in digital translation? the humanitarian principles in the digital age,https://www.semanticscholar.org/paper/c99afde97014dd6325d501845142d4195d58cfd9,"
 The digital transformation creates significant opportunities and risks for humanitarian action. Current approaches to humanitarian innovation-related issues are too often driven by considerations of competition and relevance, relegating the fundamental humanitarian principles of humanity, impartiality, neutrality and independence to afterthoughts. By reasserting the place and role of these principles in humanitarian decision-making processes, this article argues that it is possible to better understand the political and ethical dimensions of the digital transformation, reverse counterproductive practices, and ultimately better mitigate the negative impact that technologies can have on the safety and dignity of people affected by humanitarian crises, and on principled humanitarian action.",not included,0.8062968552112579,169
d20e8ba5f85779a7e4a712633d7a5b60604ecd52,preprocessed,citation,citation,semantic_scholar,International Conference on Information &amp; Communication Technologies and Development 2022,2022.0,semantic_scholar,a qualitative difference: integrating qualitative data into humanitarian response operations,https://www.semanticscholar.org/paper/d20e8ba5f85779a7e4a712633d7a5b60604ecd52,"Recent developments in qualitative data analytics may generate helpful insights for humanitarian response. At the same time, humanitarian coordination efforts are embracing data sharing platforms to ease data flows. Combined, these two innovations could simultaneously offer operational insights across multiple humanitarian organizations. We pursue this potential through the QualMiner project, an18-month collaboration of the UN-led response to the Venezuelan forced migration crisis in Ecuador. In our efforts to integrate qualitative data, we developed applications with implications for local operations as well as platform features and analyzed data entry processes and information product designs. Our analysis finds the established quantitative system serves as an installed base enacting agency and generating three effects, namely framing, artifacts, and informing. We also find collaborative innovation with non-profit users results in direct and indirect factors shaping the data sharing platform's boundaries. Finally, our analysis provides a critical, yet depolarized [1], assessment of advanced analytics in the humanitarian context. These findings have implications for platform boundary theories and critical data studies in the humanitarian domain, as well as humanitarian information management practice.",not included,0.8288842886686325,170
0ef4f7572bb6a7321de2841245946f6151efb839,preprocessed,citation,citation,semantic_scholar,,,semantic_scholar,managing ethical risks of artiﬁcial intelligence in business applications,https://www.semanticscholar.org/paper/0ef4f7572bb6a7321de2841245946f6151efb839,"The introduction of artiﬁcial intelligence (AI) capabilities in business applications provides signiﬁcant beneﬁts but requires organizations to manage critical risks of AI ethical consequences. We survey a range of large organizations on their use of enterprise risk management (ERM) processes and toolsets to predict and control the ethical risks of AI. Four serious gaps in current ERM systems are identiﬁed from analyses of the survey results: (1) AI ethical principles do not translate eﬀectively to ethical practices; (2) Real-time monitoring of AI ethical risks is needed; (3) ERM systems emphasize economic not ethical risks; and (4) When ethical risks are identiﬁed, no solutions are readily at hand. To address these gaps, we propose a proactive approach to manage ethical risks by extending current ERM frameworks. An enhanced ERM (e-ERM) framework is designed and evaluated by subject matter expert focus groups. We conclude with observations and future research directions on the need for more aggressive pro-ethical management oversight as organizations move to ubiquitous use of AI-driven business applications.",included,0.8062968552112579,171
ace2ad5f557fc104927ddbdaa80d240604435bc9,preprocessed,citation,citation,semantic_scholar,Policy & Society,2025.0,semantic_scholar,responsible governance of generative ai: conceptualizing genai as complex adaptive systems,https://www.semanticscholar.org/paper/ace2ad5f557fc104927ddbdaa80d240604435bc9,"
 Organizations increasingly use Generative Artificial Intelligence (AI) to create strategic documents, legislation, and recommendations to support decision-making. Many current AI initiatives are technology-deterministic, whereas technology co-evolves with the social environment, resulting in new applications and situations. This paper presents a novel view of AI governance by organizations from the perspective of complex adaptive systems (CASs). AI is conceptualized as a socio-technological and adaptive system in which people, policies, systems, data, AI, processes, and other elements co-evolve. The CAS lens draws attention to focusing AI governance on the entire organization, taking an outward perspective and considering public values and societal concerns. Although there is no shortage of AI governance instruments, they differ in their effectiveness, and combinations of appropriate mechanisms should be selected to deal with AI’s evolving nature and complexity. A major challenge is that no responsibility, and therefore accountability, is taken due to the lack of understanding of the full socio-technological CAS. As such, joint accountability is needed in which involved parties work together.",included,0.8411970287561417,172
902df8034c087e415ee91cef7f536ffe710c61cf,preprocessed,citation,citation,semantic_scholar,Social Science Research Network,2024.0,semantic_scholar,"the rise of ai avatars: legal personhood, rights and liabilities in an evolving metaverse",https://www.semanticscholar.org/paper/902df8034c087e415ee91cef7f536ffe710c61cf,"Objective: to determine the legal implications of the development of autonomous and creative AI-based avatars and to shape the direction of the discourse on the responsible management of AI technologies in the meta-universe based on proactive interdisciplinary approaches.Methods: the research is based on a doctrinal legal approach, which allowed presenting a prospective analysis of the legal landscape in the field of AI avatars in the metaverse and to identify four key thematic areas of research: the evolution of AI avatars and the impact of the metaverse, the applicability of legal personhood, the liability for autonomous actions, and the problems of AI avatars in the field of creativity related to intellectual property and privacy.Results: the paper presents and analyzes predictive scenarios of AI avatars maximizing their influence in the metaverse space. The author notes that the emergence of AI-based avatars in the metaverse raises complex legal, ethical, philosophical and social issues that require urgent solutions. The potential impact of the increasing complexity of AI avatars on legal approaches is considered. As avatars become increasingly autonomous, questions arise about their legal status, rights, responsibilities, risks, and benefits to humans and society. The author analyzes the advantages and disadvantages of giving AI avatars the status of legal entities, as well as applying the concept of distributed responsibility to the consequences of their actions. Special attention is paid to the possible future dominance of super-intelligent AI-based avatars in the metaverse, taking into account the existing risks and needs in the field of governance.Scientific novelty: the article presents a new perspective on the problem of legal personality in the metaverse based on interdisciplinary analysis of the evolution of AI avatars. The research is aimed at achieving a balance between transformational potential and the protection of human rights and welfare through joint efforts. It is proposed to create legal and ethical norms that prioritize the safety and consistency of artificial intelligence technologies involved in the processes occurring in the metaverse.Practical significance: the conclusions and proposed solutions to the legal problems of personhood and liability can become the basis for revising the concept of legal personality, developing reliable mechanisms of responsibility and accountability, as well as ensuring the protection of human rights and values in the face of increasingly powerful entities based on artificial intelligence. This is associated with the formation and improvement of the legal landscape of process management and overcoming risks in the socially oriented and inclusive ecosystem of the metaverse.",not included,0.8043087244033813,173
b418f83d5e5a867dceaccb831e1ad74f8bad5372,preprocessed,citation,citation,semantic_scholar,,2024.0,semantic_scholar,the ethics of artificial intelligence in defence,https://www.semanticscholar.org/paper/b418f83d5e5a867dceaccb831e1ad74f8bad5372,"
 The volume establishes an ethical framework for the identification, analysis, and resolution of ethical challenges that arise from the uses of artificial intelligence (AI) in defence, ranging from intelligence analysis to cyberwarfare and autonomous weapon systems. It does so with the goal of advancing the relevant debate and to inform the ethical governance of AI in defence. Centring on the autonomy and learning capabilities of AI technologies, the work is rooted in AI ethics and Just War Theory. It provides a systemic conceptual analysis of the different uses of AI in defence and their ethical implications, proposes ethical principles and a methodology for their implementation in practice. It then translates this analysis into actionable recommendations for decision-maker and policymakers to foster ethical governance of AI in the defence sector.",not included,0.8062968552112579,174
188b35043c43bc6ca032ae4234f101cf7eab952c,preprocessed,citation,citation,semantic_scholar,arXiv.org,2024.0,semantic_scholar,bridging today and the future of humanity: ai safety in 2024 and beyond,https://www.semanticscholar.org/paper/188b35043c43bc6ca032ae4234f101cf7eab952c,"The advancements in generative AI inevitably raise concerns about their risks and safety implications, which, in return, catalyzes significant progress in AI safety. However, as this field continues to evolve, a critical question arises: are our current efforts on AI safety aligned with the advancements of AI as well as the long-term goal of human civilization? This paper presents a blueprint for an advanced human society and leverages this vision to guide current AI safety efforts. It outlines a future where the Internet of Everything becomes reality, and creates a roadmap of significant technological advancements towards this envisioned future. For each stage of the advancements, this paper forecasts potential AI safety issues that humanity may face. By projecting current efforts against this blueprint, this paper examines the alignment between the current efforts and the long-term needs, and highlights unique challenges and missions that demand increasing attention from AI safety practitioners in the 2020s. This vision paper aims to offer a broader perspective on AI safety, emphasizing that our current efforts should not only address immediate concerns but also anticipate potential risks in the expanding AI landscape, thereby promoting a safe and sustainable future of AI and human civilization.",not included,0.8065193235874176,175
305eeffe0bee53fa400a8ecad024f1c18c1280b8,preprocessed,citation,citation,semantic_scholar,"Transforming Government: People, Process and Policy",2024.0,semantic_scholar,"artificial intelligence and decision-making in government functions: opportunities, challenges and future research",https://www.semanticscholar.org/paper/305eeffe0bee53fa400a8ecad024f1c18c1280b8,"Purpose
Artificial intelligence (AI) has received much attention due to its promethean-like powers to transform the management and delivery of public sector services. Due to the proliferation of research articles in this context, research to date is fragmented into research streams based on different types of AI technologies or a specific government function of the public sector (e.g. health, education). The purpose of this study is to synthesize this literature, identify challenges and opportunities, and offer a research agenda that guides future inquiry.

Design/methodology/approach
This paper aggregates this fragmented body of knowledge by conducting a systematic literature review of AI research in public sector organisations in the Chartered Association of Business Schools (CABS)-ranked journals between 2012 and 2023.

Findings
The search strategy resulted in the retrieval of 2,870 papers, of which 61 were identified as primary papers relevant to this research. These primary papers are mapped to the ten classifications of the functions of government as classified by the Organisation for Economic Co-operation and Development (OECD), and the reported challenges and benefits aggregated.

Originality/value
This study advances knowledge by providing a state-of-the-art of AI research based the OECD classifications of government functions, reporting of claimed benefits and challenges and providing a research agenda for future research.
",not included,0.8288842886686325,176
d10504e04ddd8f45495c79fa687365533bcc5e38,preprocessed,citation,citation,semantic_scholar,Social Science Research Network,2024.0,semantic_scholar,the case for a broader approach to ai assurance: addressing 'hidden' harms in the development of artificial intelligence,https://www.semanticscholar.org/paper/d10504e04ddd8f45495c79fa687365533bcc5e38,"Artificial intelligence (AI) assurance is an umbrella term describing many approaches—such as impact assessment, audit, and certification procedures—used to provide evidence that an AI system is legal, ethical, and technically robust. AI assurance approaches largely focus on two overlapping categories of harms: deployment harms that emerge at, or after, the point of use, and individual harms that directly impact a person as an individual. Current approaches generally overlook upstream collective and societal harms associated with the development of systems, such as resource extraction and processing, exploitative labour practices and energy intensive model training. Thus, the scope of current AI assurance practice is insufficient for ensuring that AI is ethical in a holistic sense, i.e. in ways that are legally permissible, socially acceptable, economically viable and environmentally sustainable. This article addresses this shortcoming by arguing for a broader approach to AI assurance that is sensitive to the full scope of AI development and deployment harms. To do so, the article maps harms related to AI and highlights three examples of harmful practices that occur upstream in the AI supply chain and impact the environment, labour, and data exploitation. It then reviews assurance mechanisms used in adjacent industries to mitigate similar harms, evaluating their strengths, weaknesses, and how effectively they are being applied to AI. Finally, it provides recommendations as to how a broader approach to AI assurance can be implemented to mitigate harms more effectively across the whole AI supply chain.",not included,0.8351606994867324,177
5d64050bd6ad24b2853079af619c434898ce7740,preprocessed,citation,citation,semantic_scholar,arXiv.org,2024.0,semantic_scholar,ai governance and accountability: an analysis of anthropic's claude,https://www.semanticscholar.org/paper/5d64050bd6ad24b2853079af619c434898ce7740,"As AI systems become increasingly prevalent and impactful, the need for effective AI governance and accountability measures is paramount. This paper examines the AI governance landscape, focusing on Anthropic's Claude, a foundational AI model. We analyze Claude through the lens of the NIST AI Risk Management Framework and the EU AI Act, identifying potential threats and proposing mitigation strategies. The paper highlights the importance of transparency, rigorous benchmarking, and comprehensive data handling processes in ensuring the responsible development and deployment of AI systems. We conclude by discussing the social impact of AI governance and the ethical considerations surrounding AI accountability.",included,0.8206197261810303,178
4fd1fa39ef8cddf460f787f8b08e42f857f5dde8,preprocessed,citation,citation,semantic_scholar,Open international journal of informatics,2023.0,semantic_scholar,"a bibliometric review of academic collaboration in the governance, risk and compliance of artificial intelligence",https://www.semanticscholar.org/paper/4fd1fa39ef8cddf460f787f8b08e42f857f5dde8,"


As Artificial Intelligence becomes increasingly pervasive in its application, research efforts have shifted from the discourse on ethical principles to the mechanism of implementation that would eventually bring benefits to the world’s population while minimizing its risks. The adoption of best practices in governance, risk management and compliance would not only promote these objectives but also foster greater adoption of the technology by nations of the world. As Artificial Intelligence models are dependent upon the data ingested and used by algorithms, the input from researchers of different nationalities has the potential to reduce bias, enhance interoperability, and facilitate the generation of accurate decisions. Moreover, formulation of universal legislations and standards that are applicable to all nations would spur compliance and acceptance of AI solutions by its potential users. Thus, this study aims to shed light on the level of international collaboration among academicians by analyzing the publications in the past decade. The results show that the major contributors in this domain are the USA, UK, and China. Also, most of the contributions are from selected academic institutions in those countries only. Hence, greater collaborations can be forged with notable researchers from these institutions by researchers and practitioners from other countries to ensure that the development and use of AI can benefit all mankind in our increasingly connected societies.


",not included,0.8116682052612305,179
49a50fb2078540ca059b3d6d282acdf4a7abd0b9,preprocessed,citation,citation,semantic_scholar,Strategic Analysis,2023.0,semantic_scholar,trustworthy artificial intelligence: design of ai governance framework,https://www.semanticscholar.org/paper/49a50fb2078540ca059b3d6d282acdf4a7abd0b9,"Abstract This article presents the various challenges in the current system of AI governance and the correlation between data, algorithm, technology, governance, and geopolitics surrounding its successful implementation. The focal point of the article is the Adaptive-Hybrid AI Governance framework based on technical, ethical, and societal regulatory mechanisms that models trustworthy AI and the risks associated with it. The article highlights the need for trustworthy AI and how major countries are shaping their AI regulatory mechanisms. It presents a case study on AI governance in defence that elucidates ethical AI governance through various use cases.",included,0.8252739846706391,180
937aa75b6aa71d31f1f85e63401a989907f17b8a,preprocessed,citation,citation,semantic_scholar,Expert Syst. J. Knowl. Eng.,2023.0,semantic_scholar,artificial intelligence governance: ethical considerations and implications for social responsibility,https://www.semanticscholar.org/paper/937aa75b6aa71d31f1f85e63401a989907f17b8a,"A number of articles are increasingly raising awareness on the different uses of artificial intelligence (AI) technologies for customers and businesses. Many authors discuss about their benefits and possible challenges. However, for the time being, there is still limited research focused on AI principles and regulatory guidelines for the developers of expert systems like machine learning (ML) and/or deep learning (DL) technologies. This research addresses this knowledge gap in the academic literature. The objectives of this contribution are threefold: (i) It describes AI governance frameworks that were put forward by technology conglomerates, policy makers and by intergovernmental organizations, (ii) It sheds light on the extant literature on ‘AI governance’ as well as on the intersection of ‘AI’ and ‘corporate social responsibility’ (CSR), (iii) It identifies key dimensions of AI governance, and elaborates about the promotion of accountability and transparency; explainability, interpretability and reproducibility; fairness and inclusiveness; privacy and safety of end users, as well as on the prevention of risks and of cyber security issues from AI systems. This research implies that all those who are involved in the research, development and maintenance of AI systems, have social and ethical responsibilities to bear toward their consumers as well as to other stakeholders in society.",not included,0.819400179386139,181
b55cccad48a3f199ca89307d1e247393bb1e1a8f,preprocessed,citation,citation,semantic_scholar,IEEE Transactions on Technology and Society,2023.0,semantic_scholar,discerning between the “easy” and “hard” problems of ai governance,https://www.semanticscholar.org/paper/b55cccad48a3f199ca89307d1e247393bb1e1a8f,"While there is widespread consensus that artificial intelligence (AI) needs to be governed owing to its rapid diffusion and societal implications, the current scholarly discussion on AI governance is dispersed across numerous disciplines and problem domains. This paper clarifies the situation by discerning two problem areas, metaphorically titled the “easy” and “hard” problems of AI governance, using a dialectic theory synthesis approach. The “easy problem” of AI governance concerns how organizations’ design, development, and use of AI systems align with laws, values, and norms stemming from legislation, ethics guidelines, and the surrounding society. Organizations can provisionally solve the “easy problem” by implementing appropriate organizational mechanisms to govern data, algorithms, and algorithmic systems. The “hard problem” of AI governance concerns AI as a general-purpose technology that transforms organizations and societies. Rather than a matter to be resolved, the “hard problem” is a sensemaking process regarding socio-technical change. Partial solutions to the “hard problem” may open unforeseen issues. While societies should not lose track of the “hard problem” of AI governance, there is significant value in solving the “easy problem” for two reasons. First, the “easy problem” can be provisionally solved by tackling bias, harm, and transparency issues. Second, solving the “easy problem” helps solve the “hard problem,” as responsible organizational AI practices create virtuous rather than vicious cycles.",not included,0.8437141388654709,182
883e2aea0b82b8c16e0bafcc7160f4eedb276849,preprocessed,citation,citation,semantic_scholar,"2022 IEEE 28th International Conference on Engineering, Technology and Innovation (ICE/ITMC) & 31st International Association For Management of Technology (IAMOT) Joint Conference",2022.0,semantic_scholar,ai governance and ethics in public procurement: bridging the gap between theory and practice,https://www.semanticscholar.org/paper/883e2aea0b82b8c16e0bafcc7160f4eedb276849,"As Artificial Intelligence (AI) systems have become increasingly widespread, research in AI ethics has sparked. When developing the systems, many tools and methods are available for implementing AI ethics in practice. In addition, the research in AI governance is starting to activate, and some models for AI governance have already been introduced. Simultaneously, the role of the information systems (IS) procurement function has developed from its traditional operative role to a more strategic position, as the investments in IT have been on a constant rise. Success in procurement is found to be critical regarding the success of the development and implementation of information systems. But how are the existing tools and methods in AI ethics related to procurement practices? And how is procurement positioned in the proposed AI governance frameworks? This study answers these questions by setting up a research framework based on AI governance models and analyzing existing tools and methods in AI ethics.",included,0.8154128462076187,183
37730b6bc3fe8c5655780efba083c8401808acaf,preprocessed,citation,citation,semantic_scholar,arXiv.org,2022.0,semantic_scholar,putting ai ethics into practice: the hourglass model of organizational ai governance,https://www.semanticscholar.org/paper/37730b6bc3fe8c5655780efba083c8401808acaf,"The organizational use of artificial intelligence (AI) has rapidly spread across various sectors. Alongside the awareness of the benefits brought by AI, there is a growing consensus on the necessity of tackling the risks and potential harms, such as bias and discrimination, brought about by advanced AI technologies. A multitude of AI ethics principles have been proposed to tackle these risks, but the outlines of organizational processes and practices for ensuring socially responsible AI development are in a nascent state. To address the paucity of comprehensive governance models, we present an AI governance framework, the hourglass model of organizational AI governance, which targets organizations that develop and use AI systems. The framework is designed to help organizations deploying AI systems translate ethical AI principles into practice and align their AI systems and processes with the forthcoming European AI Act. The hourglass framework includes governance requirements at the environmental, organizational, and AI system levels. At the AI system level, we connect governance requirements to AI system life cycles to ensure governance throughout the system's life span. The governance model highlights the systemic nature of AI governance and opens new research avenues into its practical implementation, the mechanisms that connect different AI governance layers, and the dynamics between the AI governance actors. The model also offers a starting point for organizational decision-makers to consider the governance components needed to ensure social acceptability, mitigate risks, and realize the potential of AI.",included,0.8546511858701706,184
6035386cbdadfd80ccae0b103bda9d04f65b44fb,preprocessed,citation,citation,semantic_scholar,2022 IEEE International Conference on Big Data (Big Data),2022.0,semantic_scholar,towards implementing responsible ai,https://www.semanticscholar.org/paper/6035386cbdadfd80ccae0b103bda9d04f65b44fb,"As the deployment of artificial intelligence (AI) is changing many fields and industries, there are concerns about AI systems making decisions and recommendations without adequately considering various ethical aspects, such as accountability, reliability, transparency, explainability, contestability, privacy, and fairness. While many sets of AI ethics principles have been recently proposed that acknowledge these concerns, such principles are high-level and do not provide tangible advice on how to develop ethical and responsible AI systems. To gain insight on the possible implementation of the principles, we conducted an empirical investigation involving semi-structured interviews with a cohort of AI practitioners. The salient findings cover four aspects of AI system design and development, adapting processes used in software engineering: (i) high-level view, (ii) requirements engineering, (iii) design and implementation, (iv) deployment and operation.",included,0.8554961800575256,185
31564919d4d826e28388d6a63a6af5f6ca5aafb9,preprocessed,citation,citation,semantic_scholar,Internet Research,2022.0,semantic_scholar,how to explain ai systems to end users: a systematic literature review and research agenda,https://www.semanticscholar.org/paper/31564919d4d826e28388d6a63a6af5f6ca5aafb9,"PurposeInscrutable machine learning (ML) models are part of increasingly many information systems. Understanding how these models behave, and what their output is based on, is a challenge for developers let alone non-technical end users.Design/methodology/approachThe authors investigate how AI systems and their decisions ought to be explained for end users through a systematic literature review.FindingsThe authors’ synthesis of the literature suggests that AI system communication for end users has five high-level goals: (1) understandability, (2) trustworthiness, (3) transparency, (4) controllability and (5) fairness. The authors identified several design recommendations, such as offering personalized and on-demand explanations and focusing on the explainability of key functionalities instead of aiming to explain the whole system. There exists multiple trade-offs in AI system explanations, and there is no single best solution that fits all cases.Research limitations/implicationsBased on the synthesis, the authors provide a design framework for explaining AI systems to end users. The study contributes to the work on AI governance by suggesting guidelines on how to make AI systems more understandable, fair, trustworthy, controllable and transparent.Originality/valueThis literature review brings together the literature on AI system communication and explainable AI (XAI) for end users. Building on previous academic literature on the topic, it provides synthesized insights, design recommendations and future research agenda.",included,0.8145611852407455,186
230ca19a4385d0a82bdc1803f1ce56bf8a8eef12,preprocessed,citation,citation,semantic_scholar,IEEE Transactions on Technology and Society,2021.0,semantic_scholar,ai ethics principles in practice: perspectives of designers and developers,https://www.semanticscholar.org/paper/230ca19a4385d0a82bdc1803f1ce56bf8a8eef12,"As consensus across the various published AI ethics principles is approached, a gap remains between high-level principles and practical techniques that can be readily adopted to design and develop responsible AI systems. We examine the practices and experiences of researchers and engineers from Australia’s national scientific research agency (CSIRO), who are involved in designing and developing AI systems for many application areas. Semi-structured interviews were used to examine how the practices of the participants relate to and align with a set of high-level AI ethics principles proposed by the Australian Government. The principles comprise: (1) privacy protection and security, (2) reliability and safety, (3) transparency and explainability, (4) fairness, (5) contestability, (6) accountability, (7) human-centred values, (8) human, social and environmental well-being. Discussions on the gained insights from the interviews include various tensions and trade-offs between the principles, and provide suggestions for implementing each high-level principle. We also present suggestions aiming to enhance associated support mechanisms.",not included,0.828378900885582,187
b5ae05979fe80007a51f09206c9dd6c4e0646c78,preprocessed,citation,citation,semantic_scholar,EGOV-CeDEM-ePart-*,2024.0,semantic_scholar,designing organizational control mechanisms for consequential ai systems: towards a situated methodology (poster),https://www.semanticscholar.org/paper/b5ae05979fe80007a51f09206c9dd6c4e0646c78,"Artificial intelligence (AI) holds both potential benefits and significant risks for organizations, including biases, discrimination, opacity, and reduced human accountability. Technical systems, including AI, must be regulated to safeguard stakeholders’ interests and maintain proper functioning over time. However, the problem of designing practical controls for specific AI systems and organizations largely remains unresolved. To address this gap, we propose an initial methodology focusing on identifying and contextualizing stakeholders’ values within their local environments. We validate our approach through a case study in the Japanese life insurance industry, aiming to assess its repeatability and potential improvements. Our design method includes 10 steps which AI system developers can use to situate high-level institutions in the local context to control their AI systems. The validation efforts highlight the contextual nature of designing controls for AI systems, emphasizing the need for diverse control mechanisms to comply with stakeholders’ values.",not included,0.8474408477544785,188
48508056eed5988acf89022ad54da20e6746259b,preprocessed,citation,citation,semantic_scholar,HHAI Workshops,2024.0,semantic_scholar,beyond convenience: the ethical use of ai in everyday life,https://www.semanticscholar.org/paper/48508056eed5988acf89022ad54da20e6746259b,"While there is much scrutiny over the legal, policy, and design of AI, there is little written about how individual users should incorporate AI into their everyday lives. The important work being done to constrain and design AI in ways consistent with human values does little to constrain the use of AI by individuals. The possibilities open to us are seemingly limitless. If we are to use these technologies in a way consistent with our good lives we must know when some friction is necessary – for building skills, for enjoyment, or for keeping a sense of accomplishment and meaning. There is nothing convenient about delegating what is meaningful about being human to technology.",not included,0.8040409088134766,189
3213df64d12982d4b9880a559efb47d63134db06,preprocessed,citation,citation,semantic_scholar,Social Science Research Network,2023.0,semantic_scholar,governance of ai ethics: perspective from the global south (africa),https://www.semanticscholar.org/paper/3213df64d12982d4b9880a559efb47d63134db06,". The literature on artificial intelligence (AI) ethics is dominated by a Global North outlook, despite AI ethics not being uniform across societies. The aim of this exploratory study is to contribute to the literature on AI ethics by offering a governance perspective from the Global South. The study uses a qualitative methodology and semi-structured interviews to explore the views and proposals of South African-based AI practitioners and associated experts on mechanisms, methods, and measures to govern AI. The study identifies themes relevant to the governance of AI ethics for organisations in South Africa, focusing on external and internal measures that influence AI ethics. The study finds that organisations can take a range of measures to address AI ethics. Additionally, the study highlights differences between how AI ethics is approached in South Africa and the Global North.",not included,0.8418905466794968,190
3eb54a667b68f44a179b4f063a8828fdc3a570f4,preprocessed,citation,citation,semantic_scholar,,,semantic_scholar,governing through standards: artificial intelligence and values,https://www.semanticscholar.org/paper/3eb54a667b68f44a179b4f063a8828fdc3a570f4,": The upcoming European Union’s regulation on Artificial Intelligence (AI), known as the AI Act, has opened the door for the European Commission to request the development of supporting AI harmonised standards by European Standardisation Organisations (ESOs). The standardisation request will identify the areas in which ESOs are to develop standards based on the essential requirements in the AI Act. The current draft standardisation request establishes that deliverables are to take into account the policy objectives of the commission, such as ensuring that AI systems are in respect of Union values. For ESOs, this task is complicated by the diversified world-wide network of standards-developing organisations and working groups in AI. We examine the state of the art in AI standardisation, analyse how standards embed values and identify an approach that accommodates different sets of values. While currently, there is no harmonised approach to embed value consideration in AI standardisation, there is potential for an approach geared toward flexibility with space for different configurations. In the EU, the value of freedom as movement builds the basis and the need for flexible standards that enhance interoperability between regulatory contexts with different sets of values. In global terms, there is a need for a minimum threshold of agreed-upon values within AI standards that allow different configurations based on specific regulatory contexts.",not included,0.8262364834547042,191
